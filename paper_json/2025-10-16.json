[
  {
    "id": "http://arxiv.org/abs/2510.14981v1",
    "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
    "summary": "We present an inference-time diffusion sampling method to perform multi-view\nconsistent image editing using pre-trained 2D image editing models. These\nmodels can independently produce high-quality edits for each image in a set of\nmulti-view images of a 3D scene or object, but they do not maintain consistency\nacross views. Existing approaches typically address this by optimizing over\nexplicit 3D representations, but they suffer from a lengthy optimization\nprocess and instability under sparse view settings. We propose an implicit 3D\nregularization approach by constraining the generated 2D image sequences to\nadhere to a pre-trained multi-view image distribution. This is achieved through\ncoupled diffusion sampling, a simple diffusion sampling technique that\nconcurrently samples two trajectories from both a multi-view image distribution\nand a 2D edited image distribution, using a coupling term to enforce the\nmulti-view consistency among the generated images. We validate the\neffectiveness and generality of this framework on three distinct multi-view\nimage editing tasks, demonstrating its applicability across various model\narchitectures and highlighting its potential as a general solution for\nmulti-view consistent editing.",
    "published": "2025-10-16T17:59:59Z",
    "updated": "2025-10-16T17:59:59Z",
    "link": "http://arxiv.org/pdf/2510.14981v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Hadi Alzayer",
      "Yunzhi Zhang",
      "Chen Geng",
      "Jia-Bin Huang",
      "Jiajun Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14979v1",
    "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
    "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
    "published": "2025-10-16T17:59:58Z",
    "updated": "2025-10-16T17:59:58Z",
    "link": "http://arxiv.org/pdf/2510.14979v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Haiwen Diao",
      "Mingxuan Li",
      "Silei Wu",
      "Linjun Dai",
      "Xiaohua Wang",
      "Hanming Deng",
      "Lewei Lu",
      "Dahua Lin",
      "Ziwei Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14980v1",
    "title": "Agentic Design of Compositional Machines",
    "summary": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning.",
    "published": "2025-10-16T17:59:58Z",
    "updated": "2025-10-16T17:59:58Z",
    "link": "http://arxiv.org/pdf/2510.14980v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "authors": [
      "Wenqian Zhang",
      "Weiyang Liu",
      "Zhen Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14977v1",
    "title": "Terra: Explorable Native 3D World Model with Point Latents",
    "summary": "World models have garnered increasing attention for comprehensive modeling of\nthe real world. However, most existing methods still rely on pixel-aligned\nrepresentations as the basis for world evolution, neglecting the inherent 3D\nnature of the physical world. This could undermine the 3D consistency and\ndiminish the modeling efficiency of world models. In this paper, we present\nTerra, a native 3D world model that represents and generates explorable\nenvironments in an intrinsic 3D latent space. Specifically, we propose a novel\npoint-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into\na latent point representation, which is subsequently decoded as 3D Gaussian\nprimitives to jointly model geometry and appearance. We then introduce a sparse\npoint flow matching network (SPFlow) for generating the latent point\nrepresentation, which simultaneously denoises the positions and features of the\npoint latents. Our Terra enables exact multi-view consistency with native 3D\nrepresentation and architecture, and supports flexible rendering from any\nviewpoint with only a single generation process. Furthermore, Terra achieves\nexplorable world modeling through progressive generation in the point latent\nspace. We conduct extensive experiments on the challenging indoor scenes from\nScanNet v2. Terra achieves state-of-the-art performance in both reconstruction\nand generation with high 3D consistency.",
    "published": "2025-10-16T17:59:56Z",
    "updated": "2025-10-16T17:59:56Z",
    "link": "http://arxiv.org/pdf/2510.14977v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yuanhui Huang",
      "Weiliang Chen",
      "Wenzhao Zheng",
      "Xin Tao",
      "Pengfei Wan",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14975v1",
    "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
    "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.",
    "published": "2025-10-16T17:59:54Z",
    "updated": "2025-10-16T17:59:54Z",
    "link": "http://arxiv.org/pdf/2510.14975v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Hengyuan Xu",
      "Wei Cheng",
      "Peng Xing",
      "Yixiao Fang",
      "Shuhan Wu",
      "Rui Wang",
      "Xianfang Zeng",
      "Daxin Jiang",
      "Gang Yu",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14974v1",
    "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
    "summary": "Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard $\\ell_2$ flow matching loss. By simply\nmimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.",
    "published": "2025-10-16T17:59:51Z",
    "updated": "2025-10-16T17:59:51Z",
    "link": "http://arxiv.org/pdf/2510.14974v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Hansheng Chen",
      "Kai Zhang",
      "Hao Tan",
      "Leonidas Guibas",
      "Gordon Wetzstein",
      "Sai Bi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14973v1",
    "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
    "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
    "published": "2025-10-16T17:59:48Z",
    "updated": "2025-10-16T17:59:48Z",
    "link": "http://arxiv.org/pdf/2510.14973v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Quan Nguyen-Tri",
      "Mukul Ranjan",
      "Zhiqiang Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14972v1",
    "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
    "summary": "Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs.",
    "published": "2025-10-16T17:59:45Z",
    "updated": "2025-10-16T17:59:45Z",
    "link": "http://arxiv.org/pdf/2510.14972v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.PL",
      "cs.SE"
    ],
    "authors": [
      "Yinxi Li",
      "Yuntian Deng",
      "Pengyu Nie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14969v1",
    "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training",
    "summary": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.",
    "published": "2025-10-16T17:59:38Z",
    "updated": "2025-10-16T17:59:38Z",
    "link": "http://arxiv.org/pdf/2510.14969v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yiming Wang",
      "Da Yin",
      "Yuedong Cui",
      "Ruichen Zheng",
      "Zhiqian Li",
      "Zongyu Lin",
      "Di Wu",
      "Xueqing Wu",
      "Chenchen Ye",
      "Yu Zhou",
      "Kai-Wei Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14968v1",
    "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in\n  Long-Horizon Tasks",
    "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action\n(VLAs) frameworks employ vision-language model (VLM)-based planners to\ndecompose complex manipulation tasks into simpler sub-tasks that low-level\nvisuomotor policies can easily handle. Typically, the VLM planner is finetuned\nto learn to decompose a target task. This finetuning requires target task\ndemonstrations segmented into sub-tasks by either human annotation or heuristic\nrules. However, the heuristic subtasks can deviate significantly from the\ntraining data of the visuomotor policy, which degrades task performance. To\naddress these issues, we propose a Retrieval-based Demonstration Decomposer\n(RDD) that automatically decomposes demonstrations into sub-tasks by aligning\nthe visual features of the decomposed sub-task intervals with those from the\ntraining data of the low-level visuomotor policies. Our method outperforms the\nstate-of-the-art sub-task decomposer on both simulation and real-world tasks,\ndemonstrating robustness across diverse settings. Code and more results are\navailable at rdd-neurips.github.io.",
    "published": "2025-10-16T17:59:37Z",
    "updated": "2025-10-16T17:59:37Z",
    "link": "http://arxiv.org/pdf/2510.14968v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Mingxuan Yan",
      "Yuping Wang",
      "Zechun Liu",
      "Jiachen Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14967v1",
    "title": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents",
    "summary": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency.",
    "published": "2025-10-16T17:59:32Z",
    "updated": "2025-10-16T17:59:32Z",
    "link": "http://arxiv.org/pdf/2510.14967v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Guoqing Wang",
      "Sunhao Dai",
      "Guangze Ye",
      "Zeyu Gan",
      "Wei Yao",
      "Yong Deng",
      "Xiaofeng Wu",
      "Zhenzhe Ying"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14960v1",
    "title": "C4D: 4D Made from 3D through Dual Correspondences",
    "summary": "Recovering 4D from monocular video, which jointly estimates dynamic geometry\nand camera poses, is an inevitably challenging problem. While recent\npointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great\nprogress in reconstructing static scenes, directly applying them to dynamic\nscenes leads to inaccurate results. This discrepancy arises because moving\nobjects violate multi-view geometric constraints, disrupting the\nreconstruction. To address this, we introduce C4D, a framework that leverages\ntemporal Correspondences to extend existing 3D reconstruction formulation to\n4D. Specifically, apart from predicting pointmaps, C4D captures two types of\ncorrespondences: short-term optical flow and long-term point tracking. We train\na dynamic-aware point tracker that provides additional mobility information,\nfacilitating the estimation of motion masks to separate moving elements from\nthe static background, thus offering more reliable guidance for dynamic scenes.\nFurthermore, we introduce a set of dynamic scene optimization objectives to\nrecover per-frame 3D geometry and camera parameters. Simultaneously, the\ncorrespondences lift 2D trajectories into smooth 3D trajectories, enabling\nfully integrated 4D reconstruction. Experiments show that our framework\nachieves complete 4D recovery and demonstrates strong performance across\nmultiple downstream tasks, including depth estimation, camera pose estimation,\nand point tracking. Project Page: https://littlepure2333.github.io/C4D",
    "published": "2025-10-16T17:59:06Z",
    "updated": "2025-10-16T17:59:06Z",
    "link": "http://arxiv.org/pdf/2510.14960v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Shizun Wang",
      "Zhenxiang Jiang",
      "Xingyi Yang",
      "Xinchao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14959v1",
    "title": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control\n  Barrier Functions",
    "summary": "Reinforcement learning (RL), while powerful and expressive, can often\nprioritize performance at the expense of safety. Yet safety violations can lead\nto catastrophic outcomes in real-world deployments. Control Barrier Functions\n(CBFs) offer a principled method to enforce dynamic safety -- traditionally\ndeployed \\emph{online} via safety filters. While the result is safe behavior,\nthe fact that the RL policy does not have knowledge of the CBF can lead to\nconservative behaviors. This paper proposes CBF-RL, a framework for generating\nsafe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key\nattributes: (1) minimally modifying a nominal RL policy to encode safety\nconstraints via a CBF term, (2) and safety filtering of the policy rollouts in\ntraining. Theoretically, we prove that continuous-time safety filters can be\ndeployed via closed-form expressions on discrete-time roll-outs. Practically,\nwe demonstrate that CBF-RL internalizes the safety constraints in the learned\npolicy -- both enforcing safer actions and biasing towards safer rewards --\nenabling safe deployment without the need for an online safety filter. We\nvalidate our framework through ablation studies on navigation tasks and on the\nUnitree G1 humanoid robot, where CBF-RL enables safer exploration, faster\nconvergence, and robust performance under uncertainty, enabling the humanoid\nrobot to avoid obstacles and climb stairs safely in real-world settings without\na runtime safety filter.",
    "published": "2025-10-16T17:58:58Z",
    "updated": "2025-10-16T17:58:58Z",
    "link": "http://arxiv.org/pdf/2510.14959v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Lizhi Yang",
      "Blake Werner",
      "Massimiliano de Sa Aaron D. Ames"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14955v1",
    "title": "RealDPO: Real or Not Real, that is the Preference",
    "summary": "Video generative models have recently achieved notable advancements in\nsynthesis quality. However, generating complex motions remains a critical\nchallenge, as existing models often struggle to produce natural, smooth, and\ncontextually consistent movements. This gap between generated and real-world\nmotions limits their practical applicability. To address this issue, we\nintroduce RealDPO, a novel alignment paradigm that leverages real-world data as\npositive samples for preference learning, enabling more accurate motion\nsynthesis. Unlike traditional supervised fine-tuning (SFT), which offers\nlimited corrective feedback, RealDPO employs Direct Preference Optimization\n(DPO) with a tailored loss function to enhance motion realism. By contrasting\nreal-world videos with erroneous model outputs, RealDPO enables iterative\nself-correction, progressively refining motion quality. To support\npost-training in complex motion synthesis, we propose RealAction-5K, a curated\ndataset of high-quality videos capturing human daily activities with rich and\nprecise motion details. Extensive experiments demonstrate that RealDPO\nsignificantly improves video quality, text alignment, and motion realism\ncompared to state-of-the-art models and existing preference optimization\ntechniques.",
    "published": "2025-10-16T17:58:25Z",
    "updated": "2025-10-16T17:58:25Z",
    "link": "http://arxiv.org/pdf/2510.14955v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Guo Cheng",
      "Danni Yang",
      "Ziqi Huang",
      "Jianlou Si",
      "Chenyang Si",
      "Ziwei Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14947v1",
    "title": "Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust\n  Humanoid Locomotion",
    "summary": "Robust humanoid locomotion in unstructured environments requires\narchitectures that balance fast low-level stabilization with slower perceptual\ndecision-making. We show that a simple layered control architecture (LCA), a\nproprioceptive stabilizer running at high rate, coupled with a compact low-rate\nperceptual policy, enables substantially more robust performance than\nmonolithic end-to-end designs, even when using minimal perception encoders.\nThrough a two-stage training curriculum (blind stabilizer pretraining followed\nby perceptual fine-tuning), we demonstrate that layered policies consistently\noutperform one-stage alternatives in both simulation and hardware. On a Unitree\nG1 humanoid, our approach succeeds across stair and ledge tasks where one-stage\nperceptual policies fail. These results highlight that architectural separation\nof timescales, rather than network scale or complexity, is the key enabler for\nrobust perception-conditioned locomotion.",
    "published": "2025-10-16T17:56:08Z",
    "updated": "2025-10-16T17:56:08Z",
    "link": "http://arxiv.org/pdf/2510.14947v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Blake Werner",
      "Lizhi Yang",
      "Aaron D. Ames"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14944v1",
    "title": "MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\ngeneral text; however, their proficiency in specialized scientific domains that\nrequire deep, interconnected knowledge remains largely uncharacterized.\nMetabolomics presents unique challenges with its complex biochemical pathways,\nheterogeneous identifier systems, and fragmented databases. To systematically\nevaluate LLM capabilities in this domain, we introduce MetaBench, the first\nbenchmark for metabolomics assessment. Curated from authoritative public\nresources, MetaBench evaluates five capabilities essential for metabolomics\nresearch: knowledge, understanding, grounding, reasoning, and research. Our\nevaluation of 25 open- and closed-source LLMs reveals distinct performance\npatterns across metabolomics tasks: while models perform well on text\ngeneration tasks, cross-database identifier grounding remains challenging even\nwith retrieval augmentation. Model performance also decreases on long-tail\nmetabolites with sparse annotations. With MetaBench, we provide essential\ninfrastructure for developing and evaluating metabolomics AI systems, enabling\nsystematic progress toward reliable computational tools for metabolomics\nresearch.",
    "published": "2025-10-16T17:55:14Z",
    "updated": "2025-10-16T17:55:14Z",
    "link": "http://arxiv.org/pdf/2510.14944v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CE"
    ],
    "authors": [
      "Yuxing Lu",
      "Xukai Zhao",
      "J. Ben Tamo",
      "Micky C. Nnamdi",
      "Rui Peng",
      "Shuang Zeng",
      "Xingyu Hu",
      "Jinzhuo Wang",
      "May D. Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14943v1",
    "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model's self-verification capability into\nthe standard RLVR process, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions and self-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective of\nself-verification can be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to its last-token self-rewarding score,\nwhich is computed as the difference between the policy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by the KL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the original RLVR loss with\na MSE loss that aligns the last-token self-rewarding scores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model's reasoning performance but also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance.",
    "published": "2025-10-16T17:55:11Z",
    "updated": "2025-10-16T17:55:11Z",
    "link": "http://arxiv.org/pdf/2510.14943v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Wenkai Yang",
      "Weijie Liu",
      "Ruobing Xie",
      "Yiju Guo",
      "Lulu Wu",
      "Saiyong Yang",
      "Yankai Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14942v1",
    "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for\n  Step-Level Reasoning",
    "summary": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large\nLanguage Models (LLMs) by supervising intermediate steps and identifying\nerrors. However, building effective PRMs remains challenging due to the lack of\nscalable, high-quality annotations. Existing approaches rely on costly human\nlabeling, LLM-based self-evaluation that is prone to hallucination, or Monte\nCarlo (MC) estimation, which infers step quality solely from rollout outcomes\nand often introduces noisy, misaligned supervision due to credit\nmisattribution. These issues result in three core limitations: noisy rewards,\nlow factual fidelity, and misalignment with step-level reasoning objectives. To\naddress these challenges, we introduce GroundedPRM, a tree-guided and\nfidelity-aware framework for automatic process supervision. To reduce reward\nnoise and enable fine-grained credit assignment, we construct structured\nreasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated\nsupervision, we validate each intermediate step using an external tool,\nproviding execution-grounded correctness signals. To combine both step-level\nvalidation and global outcome assessment, we design a hybrid reward aggregation\nmechanism that fuses tool-based verification with MCTS-derived feedback.\nFinally, we format the reward signal into a rationale-enhanced, generative\nstructure to promote interpretability and compatibility with instruction-tuned\nLLMs. GroundedPRM is trained on only 40K automatically labeled samples,\namounting to just 10% of the data used by the best-performing PRM trained with\nauto-labeled supervision. Nevertheless, it achieves up to a 26% relative\nimprovement in average performance on ProcessBench. When used for reward-guided\ngreedy search, GroundedPRM outperforms even PRMs trained with human-labeled\nsupervision, offering a scalable and verifiable path toward high-quality\nprocess-level reasoning.",
    "published": "2025-10-16T17:54:07Z",
    "updated": "2025-10-16T17:54:07Z",
    "link": "http://arxiv.org/pdf/2510.14942v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yao Zhang",
      "Yu Wu",
      "Haowei Zhang",
      "Weiguo Li",
      "Haokun Chen",
      "Jingpei Wu",
      "Guohao Li",
      "Zhen Han",
      "Volker Tresp"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.06371v2",
    "title": "The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement\n  and Optimization",
    "summary": "As the adoption of Generative AI in real-world services grow explosively,\nenergy has emerged as a critical bottleneck resource. However, energy remains a\nmetric that is often overlooked, under-explored, or poorly understood in the\ncontext of building ML systems. We present the ML$.$ENERGY Benchmark, a\nbenchmark suite and tool for measuring inference energy consumption under\nrealistic service environments, and the corresponding ML$.$ENERGY Leaderboard,\nwhich have served as a valuable resource for those hoping to understand and\noptimize the energy consumption of their generative AI services. In this paper,\nwe explain four key design principles for benchmarking ML energy we have\nacquired over time, and then describe how they are implemented in the\nML$.$ENERGY Benchmark. We then highlight results from the early 2025 iteration\nof the benchmark, including energy measurements of 40 widely used model\narchitectures across 6 different tasks, case studies of how ML design choices\nimpact energy consumption, and how automated optimization recommendations can\nlead to significant (sometimes more than 40%) energy savings without changing\nwhat is being computed by the model. The ML$.$ENERGY Benchmark is open-source\nand can be easily extended to various customized models and application\nscenarios.",
    "published": "2025-05-09T18:27:32Z",
    "updated": "2025-10-16T17:51:15Z",
    "link": "http://arxiv.org/pdf/2505.06371v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jae-Won Chung",
      "Jeff J. Ma",
      "Ruofan Wu",
      "Jiachen Liu",
      "Oh Jun Kweon",
      "Yuxuan Xia",
      "Zhiyu Wu",
      "Mosharaf Chowdhury"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14936v1",
    "title": "Circuit Insights: Towards Interpretability Beyond Activations",
    "summary": "The fields of explainable AI and mechanistic interpretability aim to uncover\nthe internal structure of neural networks, with circuit discovery as a central\ntool for understanding model computations. Existing approaches, however, rely\non manual inspection and remain limited to toy tasks. Automated\ninterpretability offers scalability by analyzing isolated features and their\nactivations, but it often misses interactions between features and depends\nstrongly on external LLMs and dataset quality. Transcoders have recently made\nit possible to separate feature attributions into input-dependent and\ninput-invariant components, providing a foundation for more systematic circuit\nanalysis. Building on this, we propose WeightLens and CircuitLens, two\ncomplementary methods that go beyond activation-based analysis. WeightLens\ninterprets features directly from their learned weights, removing the need for\nexplainer models or datasets while matching or exceeding the performance of\nexisting methods on context-independent features. CircuitLens captures how\nfeature activations arise from interactions between components, revealing\ncircuit-level dynamics that activation-only approaches cannot identify.\nTogether, these methods increase interpretability robustness and enhance\nscalable mechanistic analysis of circuits while maintaining efficiency and\nquality.",
    "published": "2025-10-16T17:49:41Z",
    "updated": "2025-10-16T17:49:41Z",
    "link": "http://arxiv.org/pdf/2510.14936v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Elena Golimblevskaia",
      "Aakriti Jain",
      "Bruno Puri",
      "Ammar Ibrahim",
      "Wojciech Samek",
      "Sebastian Lapuschkin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07670v2",
    "title": "Ctrl-VI: Controllable Video Synthesis via Variational Inference",
    "summary": "Many video workflows benefit from a mixture of user controls with varying\ngranularity, from exact 4D object trajectories and camera paths to coarse text\nprompts, while existing video generative models are typically trained for fixed\ninput formats. We develop Ctrl-VI, a video synthesis method that addresses this\nneed and generates samples with high controllability for specified elements\nwhile maintaining diversity for under-specified ones. We cast the task as\nvariational inference to approximate a composed distribution, leveraging\nmultiple video generation backbones to account for all task constraints\ncollectively. To address the optimization challenge, we break down the problem\ninto step-wise KL divergence minimization over an annealed sequence of\ndistributions, and further propose a context-conditioned factorization\ntechnique that reduces modes in the solution space to circumvent local optima.\nExperiments suggest that our method produces samples with improved\ncontrollability, diversity, and 3D consistency compared to prior works.",
    "published": "2025-10-09T01:48:16Z",
    "updated": "2025-10-16T17:48:29Z",
    "link": "http://arxiv.org/pdf/2510.07670v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Haoyi Duan",
      "Yunzhi Zhang",
      "Yilun Du",
      "Jiajun Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.07981v2",
    "title": "Why is Your Language Model a Poor Implicit Reward Model?",
    "summary": "Reward models are key to language model post-training and inference\npipelines. Conveniently, recent work showed that every language model defines\nan implicit reward model (IM-RM), without requiring any architectural changes.\nHowever, such IM-RMs tend to generalize worse, especially out-of-distribution,\ncompared to explicit reward models (EX-RMs) that apply a dedicated linear head\nover the hidden representations of a language model. The existence of a\ngeneralization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They\ncan be trained using the same data, loss function, and language model, and\ndiffer only in how the reward is computed. Toward a fundamental understanding\nof the implicit biases underlying different reward model types, we investigate\nthe root cause of this gap. Our main finding, backed by theory and experiments,\nis that IM-RMs rely more heavily on superficial token-level cues. Consequently,\nthey often generalize worse than EX-RMs under token-level distribution shifts,\nas well as in-distribution. Furthermore, we provide evidence against\nalternative hypotheses for the generalization gap. Most notably, we challenge\nthe intuitive claim that IM-RMs struggle in tasks where generation is harder\nthan verification because they can operate both as a verifier and a generator.\nTaken together, our results highlight that seemingly minor design choices can\nsubstantially impact the generalization behavior of reward models.",
    "published": "2025-07-10T17:55:05Z",
    "updated": "2025-10-16T17:45:44Z",
    "link": "http://arxiv.org/pdf/2507.07981v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Noam Razin",
      "Yong Lin",
      "Jiarui Yao",
      "Sanjeev Arora"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.14500v4",
    "title": "GraphLand: Evaluating Graph Machine Learning Models on Diverse\n  Industrial Data",
    "summary": "Although data that can be naturally represented as graphs is widespread in\nreal-world applications across diverse industries, popular graph ML benchmarks\nfor node property prediction only cover a surprisingly narrow set of data\ndomains, and graph neural networks (GNNs) are often evaluated on just a few\nacademic citation networks. This issue is particularly pressing in light of the\nrecent growing interest in designing graph foundation models. These models are\nsupposed to be able to transfer to diverse graph datasets from different\ndomains, and yet the proposed graph foundation models are often evaluated on a\nvery limited set of datasets from narrow applications. To alleviate this issue,\nwe introduce GraphLand: a benchmark of 14 diverse graph datasets for node\nproperty prediction from a range of different industrial applications.\nGraphLand allows evaluating graph ML models on a wide range of graphs with\ndiverse sizes, structural characteristics, and feature sets, all in a unified\nsetting. Further, GraphLand allows investigating such previously underexplored\nresearch questions as how realistic temporal distributional shifts under\ntransductive and inductive settings influence graph ML model performance. To\nmimic realistic industrial settings, we use GraphLand to compare GNNs with\ngradient-boosted decision trees (GBDT) models that are popular in industrial\napplications and show that GBDTs provided with additional graph-based input\nfeatures can sometimes be very strong baselines. Further, we evaluate currently\navailable general-purpose graph foundation models and find that they fail to\nproduce competitive results on our proposed datasets.",
    "published": "2024-09-22T15:53:19Z",
    "updated": "2025-10-16T17:45:31Z",
    "link": "http://arxiv.org/pdf/2409.14500v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Gleb Bazhenov",
      "Oleg Platonov",
      "Liudmila Prokhorenkova"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.23339v2",
    "title": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular\n  Design",
    "summary": "Large Language Models demonstrate substantial promise for advancing\nscientific discovery, yet their deployment in disciplines demanding factual\nprecision and specialized domain constraints presents significant challenges.\nWithin molecular design for pharmaceutical development, these models can\npropose innovative molecular modifications but frequently generate chemically\ninfeasible structures. We introduce VALID-Mol, a comprehensive framework that\nintegrates chemical validation with LLM-driven molecular design, achieving an\nimprovement in valid chemical structure generation from 3% to 83%. Our\nmethodology synthesizes systematic prompt optimization, automated chemical\nverification, and domain-adapted fine-tuning to ensure dependable generation of\nsynthesizable molecules with enhanced properties. Our contribution extends\nbeyond implementation details to provide a transferable methodology for\nscientifically-constrained LLM applications with measurable reliability\nenhancements. Computational analyses indicate our framework generates promising\nsynthesis candidates with up to 17-fold predicted improvements in target\nbinding affinity while preserving synthetic feasibility.",
    "published": "2025-06-29T17:17:04Z",
    "updated": "2025-10-16T17:43:31Z",
    "link": "http://arxiv.org/pdf/2506.23339v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph",
      "q-bio.QM",
      "68T50 (Primary) 92E10, 68T07 (Secondary)",
      "I.2.7; J.3; I.2.1; I.2.6"
    ],
    "authors": [
      " Malikussaid",
      "Hilal Hudan Nuha",
      "Isman Kurniawan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14925v1",
    "title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters\n  to Large Language Models",
    "summary": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback\nstability, viewing reason as a regulator that keeps inference within the bounds\nof possible experience. We formalize this intuition via a composite instability\nindex (H-Risk) combining spectral margin, conditioning, temporal sensitivity,\nand innovation amplification. In linear-Gaussian simulations, higher H-Risk\npredicts overconfident errors even under formal stability, revealing a gap\nbetween nominal and epistemic stability. Extending to large language models\n(LLMs), we find that fragile internal dynamics correlate with miscalibration\nand hallucination, while critique-style prompts show mixed effects on\ncalibration and hallucination. These results suggest a structural bridge\nbetween Kantian self-limitation and feedback control, offering a principled\nlens for diagnosing -- and selectively reducing -- overconfidence in reasoning\nsystems. This is a preliminary version; supplementary experiments and broader\nreplication will be reported in a future revision.",
    "published": "2025-10-16T17:40:28Z",
    "updated": "2025-10-16T17:40:28Z",
    "link": "http://arxiv.org/pdf/2510.14925v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Akira Okutomi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14922v1",
    "title": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using\n  Speech, Text, and EEG",
    "summary": "Depression is a widespread mental health disorder, yet its automatic\ndetection remains challenging. Prior work has explored unimodal and multimodal\napproaches, with multimodal systems showing promise by leveraging complementary\nsignals. However, existing studies are limited in scope, lack systematic\ncomparisons of features, and suffer from inconsistent evaluation protocols. We\naddress these gaps by systematically exploring feature representations and\nmodelling strategies across EEG, together with speech and text. We evaluate\nhandcrafted features versus pre-trained embeddings, assess the effectiveness of\ndifferent neural encoders, compare unimodal, bimodal, and trimodal\nconfigurations, and analyse fusion strategies with attention to the role of\nEEG. Consistent subject-independent splits are applied to ensure robust,\nreproducible benchmarking. Our results show that (i) the combination of EEG,\nspeech and text modalities enhances multimodal detection, (ii) pretrained\nembeddings outperform handcrafted features, and (iii) carefully designed\ntrimodal models achieve state-of-the-art performance. Our work lays the\ngroundwork for future research in multimodal depression detection.",
    "published": "2025-10-16T17:39:59Z",
    "updated": "2025-10-16T17:39:59Z",
    "link": "http://arxiv.org/pdf/2510.14922v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS",
      "eess.SP"
    ],
    "authors": [
      "Annisaa Fitri Nurfidausi",
      "Eleonora Mancini",
      "Paolo Torroni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14919v1",
    "title": "Predicting Task Performance with Context-aware Scaling Laws",
    "summary": "Scaling laws have transformed our understanding of large language models by\nlinking upstream metrics like cross-entropy loss to design factors such as\nmodel size, training data, and compute. However, these conventional laws fail\nto capture downstream task performance, where context plays a critical role. In\nthis work, we propose a straightforward, interpretable framework that jointly\nmodels downstream performance as a function of the training compute and the\nprovided context. We empirically validate our framework by fitting it on the\nobserved downstream performance of extended-context variants of Llama-2-7B and\nLlama-2-13B across 65,500 unique instances spanning three tasks: arithmetic\nreasoning, common sense reasoning, and machine translation. Our results\ndemonstrate that our framework accurately models in-distribution downstream\nperformance, generalizes across three orders of magnitude in training compute,\nand reliably extrapolates performance as the amount of context increases. These\nfindings offer valuable insights into the interplay between training compute\nand context utilization, providing guidance for designing more efficient\nlong-context LLMs for diverse downstream tasks. Our code is available at\nhttps://github.com/wang-research-lab/context-scaling.",
    "published": "2025-10-16T17:35:18Z",
    "updated": "2025-10-16T17:35:18Z",
    "link": "http://arxiv.org/pdf/2510.14919v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Kyle Montgomery",
      "David Park",
      "Jianhong Tu",
      "Michael Bendersky",
      "Beliz Gunel",
      "Dawn Song",
      "Chenguang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24091v2",
    "title": "PerfBench: Can Agents Resolve Real-World Performance Bugs?",
    "summary": "Performance bugs are inefficiencies in software that waste computational\nresources without causing functional failures, making them particularly\nchallenging to detect and fix. While recent advances in Software Engineering\nagents have shown promise in automated bug fixing, existing benchmarks\nprimarily focus on functional correctness and fail to evaluate agents'\nabilities to identify and resolve non-functional issues like performance bugs.\nWe introduce PerfBench, a benchmark comprising 81 real-world performance\nbug-fixing tasks from popular .NET repositories on GitHub. Unlike existing\nbenchmarks that rely on pre-existing test suites, PerfBench features a novel\nevaluation harness that allows agents to generate their own performance\nbenchmarks and validates fixes by comparing execution metrics collected for\ndeveloper fix and agent fix. Each task in PerfBench is derived from actual\ndeveloper fixes linked to performance-related issues, which are then verified\nby human experts, ensuring real-world relevance. Our evaluation reveals that\ncurrent state-of-the-art coding agents struggle with performance optimization\ntasks, with baseline OpenHands agent achieving only a ~3% success rate on our\nbenchmark. We develop OpenHands-Perf-Agent, which incorporates\nperformance-aware tooling and instructions and achieves a ~20% success rate on\nthe benchmark. We show that by ensuring the agent has proper instructions to\nbenchmark its changes and tooling for benchmark output processing, we can\nimprove the agent performance significantly, but room for improvement still\nremains. PerfBench provides a challenging test set for furthering the\ncapabilities of agents in fixing performance issues.",
    "published": "2025-09-28T22:00:33Z",
    "updated": "2025-10-16T17:31:16Z",
    "link": "http://arxiv.org/pdf/2509.24091v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.PF"
    ],
    "authors": [
      "Spandan Garg",
      "Roshanak Zilouchian Moghaddam",
      "Neel Sundaresan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14913v1",
    "title": "Budget-aware Test-time Scaling via Discriminative Verification",
    "summary": "Test-time scaling is a powerful strategy for boosting the performance of\nlarge language models on complex reasoning tasks. While state-of-the-art\napproaches often employ generative verifiers to select the best solution from a\npool of candidates, this method incurs prohibitive computational costs,\nlimiting its practicality. In this work, we shift the focus to a more\nbudget-aware paradigm: discriminative verification. We conduct a thorough\nempirical analysis and demonstrate that while discriminative verifiers may\nunderperform in isolation, combining them with self-consistency in a hybrid\napproach creates a powerful and efficient test-time scaling mechanism. Notably,\nunder a fixed compute budget, this hybrid approach surpasses state-of-the-art\ngenerative verification by a significant margin: achieving up to 15.3\\% higher\naccuracy on AIME2025. Our findings establish that for practical, real-world\napplications, budget-aware scaling with discriminative verifiers is not only a\n\"free\" upgrade over self-consistency, but also a more effective and efficient\nalternative to costly generative techniques. Code is available at\nhttps://github.com/wang-research-lab/verification.",
    "published": "2025-10-16T17:30:02Z",
    "updated": "2025-10-16T17:30:02Z",
    "link": "http://arxiv.org/pdf/2510.14913v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Kyle Montgomery",
      "Sijun Tan",
      "Yuqi Chen",
      "Siyuan Zhuang",
      "Tianjun Zhang",
      "Raluca Ada Popa",
      "Chenguang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14904v1",
    "title": "MaskCaptioner : Learning to Jointly Segment and Caption Object\n  Trajectories in Videos",
    "summary": "Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.",
    "published": "2025-10-16T17:20:22Z",
    "updated": "2025-10-16T17:20:22Z",
    "link": "http://arxiv.org/pdf/2510.14904v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Gabriel Fiastre",
      "Antoine Yang",
      "Cordelia Schmid"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14901v1",
    "title": "Reasoning with Sampling: Your Base Model is Smarter Than You Think",
    "summary": "Frontier reasoning models have exhibited incredible capabilities across a\nwide array of disciplines, driven by posttraining large language models (LLMs)\nwith reinforcement learning (RL). However, despite the widespread success of\nthis paradigm, much of the literature has been devoted to disentangling truly\nnovel behaviors that emerge during RL but are not present in the base models.\nIn our work, we approach this question from a different angle, instead asking\nwhether comparable reasoning capabilites can be elicited from base models at\ninference time by pure sampling, without any additional training. Inspired by\nMarkov chain Monte Carlo (MCMC) techniques for sampling from sharpened\ndistributions, we propose a simple iterative sampling algorithm leveraging the\nbase models' own likelihoods. Over different base models, we show that our\nalgorithm offers substantial boosts in reasoning that nearly match and even\noutperform those from RL on a wide variety of single-shot tasks, including\nMATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in\ndiversity over multiple samples that is characteristic of RL-posttraining.\nCrucially, our method does not require training, curated datasets, or a\nverifier, suggesting broad applicability beyond easily verifiable domains.",
    "published": "2025-10-16T17:18:11Z",
    "updated": "2025-10-16T17:18:11Z",
    "link": "http://arxiv.org/pdf/2510.14901v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Aayush Karan",
      "Yilun Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14900v1",
    "title": "Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent\n  That Improves Without Labels or Model Updates",
    "summary": "The Enterprise Intelligence Platform must integrate logs from numerous\nthird-party vendors in order to perform various downstream tasks. However,\nvendor documentation is often unavailable at test time. It is either misplaced,\nmismatched, poorly formatted, or incomplete, which makes schema mapping\nchallenging. We introduce a reinforcement learning agent that can self-improve\nwithout labeled examples or model weight updates. During inference, the agent:\n1) Identifies ambiguous field-mapping attempts. 2) Generates targeted\nweb-search queries to gather external evidence. 3) Applies a confidence-based\nreward to iteratively refine its mappings. To demonstrate this concept, we\nconverted Microsoft Defender for Endpoint logs into a common schema. Our method\nincreased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\%\nover 100 iterations using GPT-4o. At the same time, it reduced the number of\nlow-confidence mappings requiring expert review by 85\\%. This new approach\nprovides an evidence-driven, transparent method for solving future industry\nproblems, paving the way for more robust, accountable, scalable, efficient,\nflexible, adaptable, and collaborative solutions.",
    "published": "2025-10-16T17:17:00Z",
    "updated": "2025-10-16T17:17:00Z",
    "link": "http://arxiv.org/pdf/2510.14900v1.pdf",
    "category": [
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Wen-Kwang Tsao",
      "Yao-Ching Yu",
      "Chien-Ming Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14889v1",
    "title": "Detecting Early and Implicit Suicidal Ideation via Longitudinal and\n  Information Environment Signals on Social Media",
    "summary": "On social media, many individuals experiencing suicidal ideation (SI) do not\ndisclose their distress explicitly. Instead, signs may surface indirectly\nthrough everyday posts or peer interactions. Detecting such implicit signals\nearly is critical but remains challenging. We frame early and implicit SI as a\nforward-looking prediction task and develop a computational framework that\nmodels a user's information environment, consisting of both their longitudinal\nposting histories as well as the discourse of their socially proximal peers. We\nadopted a composite network centrality measure to identify top neighbors of a\nuser, and temporally aligned the user's and neighbors' interactions --\nintegrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a\nReddit study of 1,000 (500 Case and 500 Control) users, our approach improves\nearly and implicit SI detection by 15% over individual-only baselines. These\nfindings highlight that peer interactions offer valuable predictive signals and\ncarry broader implications for designing early detection systems that capture\nindirect as well as masked expressions of risk in online environments.",
    "published": "2025-10-16T17:09:14Z",
    "updated": "2025-10-16T17:09:14Z",
    "link": "http://arxiv.org/pdf/2510.14889v1.pdf",
    "category": [
      "cs.SI",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "authors": [
      "Soorya Ram Shimgekar",
      "Ruining Zhao",
      "Agam Goyal",
      "Violeta J. Rodriguez",
      "Paul A. Bloom",
      "Hari Sundaram",
      "Koustuv Saha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14884v1",
    "title": "Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with\n  Unbounded Rewards",
    "summary": "In high-stakes AI applications, even a single action can cause irreparable\ndamage. However, nearly all of sequential decision-making theory assumes that\nall errors are recoverable (e.g., by bounding rewards). Standard bandit\nalgorithms that explore aggressively may cause irreparable damage when this\nassumption fails. Some prior work avoids irreparable errors by asking for help\nfrom a mentor, but a mentor may not always be available. In this work, we\nformalize a model of learning with unbounded rewards without a mentor as a\ntwo-action contextual bandit with an abstain option: at each round the agent\nobserves an input and chooses either to abstain (always 0 reward) or to commit\n(execute a preexisting task policy). Committing yields rewards that are\nupper-bounded but can be arbitrarily negative, and the commit reward is assumed\nLipschitz in the input. We propose a caution-based algorithm that learns when\nnot to learn: it chooses a trusted region and commits only where the available\nevidence does not already certify harm. Under these conditions and i.i.d.\ninputs, we establish sublinear regret guarantees, theoretically demonstrating\nthe effectiveness of cautious exploration for deploying learning agents safely\nin high-stakes environments.",
    "published": "2025-10-16T17:01:57Z",
    "updated": "2025-10-16T17:01:57Z",
    "link": "http://arxiv.org/pdf/2510.14884v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Sarah Liaw",
      "Benjamin Plaut"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14881v1",
    "title": "The Gatekeeper Knows Enough",
    "summary": "Large Language Models (LLMs) are increasingly deployed as autonomous agents,\nyet their practical utility is fundamentally constrained by a limited context\nwindow and state desynchronization resulting from the LLMs' stateless nature\nand inefficient context management. These limitations lead to unreliable\noutput, unpredictable behavior, and inefficient resource usage, particularly\nwhen interacting with large, structured, and sensitive knowledge systems such\nas codebases and documents. To address these challenges, we introduce the\nGatekeeper Protocol, a novel, domain-agnostic framework that governs\nagent-system interactions. Our protocol mandates that the agent first operate\nand reason on a minimalist, low-fidelity \"latent state\" representation of the\nsystem to strategically request high-fidelity context on demand. All\ninteractions are mediated through a unified JSON format that serves as a\ndeclarative, state-synchronized protocol, ensuring the agent's model of the\nsystem remains verifiably grounded in the system's reality. We demonstrate the\nefficacy of this protocol with Sage, a reference implementation of the\nGatekeeper Protocol for software development. Our results show that this\napproach significantly increases agent reliability, improves computational\nefficiency by minimizing token consumption, and enables scalable interaction\nwith complex systems, creating a foundational methodology for building more\nrobust, predictable, and grounded AI agents for any structured knowledge\ndomain.",
    "published": "2025-10-16T17:00:42Z",
    "updated": "2025-10-16T17:00:42Z",
    "link": "http://arxiv.org/pdf/2510.14881v1.pdf",
    "category": [
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "authors": [
      "Fikresilase Wondmeneh Abebayew"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14878v1",
    "title": "Predicting kernel regression learning curves from only raw data\n  statistics",
    "summary": "We study kernel regression with common rotation-invariant kernels on real\ndatasets including CIFAR-5m, SVHN, and ImageNet. We give a theoretical\nframework that predicts learning curves (test risk vs. sample size) from only\ntwo measurements: the empirical data covariance matrix and an empirical\npolynomial decomposition of the target function $f_*$. The key new idea is an\nanalytical approximation of a kernel's eigenvalues and eigenfunctions with\nrespect to an anisotropic data distribution. The eigenfunctions resemble\nHermite polynomials of the data, so we call this approximation the Hermite\neigenstructure ansatz (HEA). We prove the HEA for Gaussian data, but we find\nthat real image data is often \"Gaussian enough\" for the HEA to hold well in\npractice, enabling us to predict learning curves by applying prior results\nrelating kernel eigenstructure to test risk. Extending beyond kernel\nregression, we empirically find that MLPs in the feature-learning regime learn\nHermite polynomials in the order predicted by the HEA. Our HEA framework is a\nproof of concept that an end-to-end theory of learning which maps dataset\nstructure all the way to model performance is possible for nontrivial learning\nalgorithms on real datasets.",
    "published": "2025-10-16T16:57:59Z",
    "updated": "2025-10-16T16:57:59Z",
    "link": "http://arxiv.org/pdf/2510.14878v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Dhruva Karkada",
      "Joseph Turnbull",
      "Yuxi Liu",
      "James B. Simon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.13731v3",
    "title": "Robust Counterfactual Inference in Markov Decision Processes",
    "summary": "This paper addresses a key limitation in existing counterfactual inference\nmethods for Markov Decision Processes (MDPs). Current approaches assume a\nspecific causal model to make counterfactuals identifiable. However, there are\nusually many causal models that align with the observational and interventional\ndistributions of an MDP, each yielding different counterfactual distributions,\nso fixing a particular causal model limits the validity (and usefulness) of\ncounterfactual inference. We propose a novel non-parametric approach that\ncomputes tight bounds on counterfactual transition probabilities across all\ncompatible causal models. Unlike previous methods that require solving\nprohibitively large optimisation problems (with variables that grow\nexponentially in the size of the MDP), our approach provides closed-form\nexpressions for these bounds, making computation highly efficient and scalable\nfor non-trivial MDPs. Once such an interval counterfactual MDP is constructed,\nour method identifies robust counterfactual policies that optimise the\nworst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate\nour method on various case studies, demonstrating improved robustness over\nexisting methods.",
    "published": "2025-02-19T13:56:20Z",
    "updated": "2025-10-16T16:51:12Z",
    "link": "http://arxiv.org/pdf/2502.13731v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Jessica Lally",
      "Milad Kazemi",
      "Nicola Paoletti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18355v2",
    "title": "Chiplet-Based RISC-V SoC with Modular AI Acceleration",
    "summary": "Achieving high performance, energy efficiency, and cost-effectiveness while\nmaintaining architectural flexibility is a critical challenge in the\ndevelopment and deployment of edge AI devices. Monolithic SoC designs struggle\nwith this complex balance mainly due to low manufacturing yields (below 16%) at\nadvanced 360 mm^2 process nodes. This paper presents a novel chiplet-based\nRISC-V SoC architecture that addresses these limitations through modular AI\nacceleration and intelligent system level optimization. Our proposed design\nintegrates 4 different key innovations in a 30mm x 30mm silicon interposer:\nadaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware\nUniversal Chiplet Interconnect Express (UCIe) protocol extensions featuring\nstreaming flow control units and compression-aware transfers; distributed\ncryptographic security across heterogeneous chiplets; and intelligent\nsensor-driven load migration. The proposed architecture integrates a 7nm RISC-V\nCPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory\nstacks, and dedicated power management controllers. Experimental results across\nindustry standard benchmarks like MobileNetV2, ResNet-50 and real-time video\nprocessing demonstrate significant performance improvements. The AI-optimized\nconfiguration achieves ~14.7% latency reduction, 17.3% throughput improvement,\nand 16.2% power reduction compared to previous basic chiplet implementations.\nThese improvements collectively translate to a 40.1% efficiency gain\ncorresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while\nmaintaining sub-5ms real-time capability across all experimented workloads.\nThese performance upgrades demonstrate that modular chiplet designs can achieve\nnear-monolithic computational density while enabling cost efficiency,\nscalability and upgradeability, crucial for next-generation edge AI device\napplications.",
    "published": "2025-09-22T19:31:58Z",
    "updated": "2025-10-16T16:44:51Z",
    "link": "http://arxiv.org/pdf/2509.18355v2.pdf",
    "category": [
      "cs.AR",
      "cs.AI"
    ],
    "authors": [
      "P. Ramkumar",
      "S. S. Bharadwaj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.13450v2",
    "title": "SteeringSafety: A Systematic Safety Evaluation Framework of\n  Representation Steering in LLMs",
    "summary": "We introduce SteeringSafety, a systematic framework for evaluating\nrepresentation steering methods across seven safety perspectives spanning 17\ndatasets. While prior work highlights general capabilities of representation\nsteering, we systematically explore safety perspectives including bias,\nharmfulness, hallucination, social behaviors, reasoning, epistemic integrity,\nand normative judgment. Our framework provides modularized building blocks for\nstate-of-the-art steering methods, enabling unified implementation of DIM, ACE,\nCAA, PCA, and LAT with recent enhancements like conditional steering. Results\non Gemma-2-2B, Llama-3.1-8B, and Qwen-2.5-7B reveal that strong steering\nperformance depends critically on pairing of method, model, and specific\nperspective. DIM shows consistent effectiveness, but all methods exhibit\nsubstantial entanglement: social behaviors show highest vulnerability (reaching\ndegradation as high as 76%), jailbreaking often compromises normative judgment,\nand hallucination steering unpredictably shifts political views. Our findings\nunderscore the critical need for holistic safety evaluations.",
    "published": "2025-09-16T18:36:22Z",
    "updated": "2025-10-16T16:44:31Z",
    "link": "http://arxiv.org/pdf/2509.13450v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Vincent Siu",
      "Nicholas Crispino",
      "David Park",
      "Nathan W. Henry",
      "Zhun Wang",
      "Yang Liu",
      "Dawn Song",
      "Chenguang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14866v1",
    "title": "Benchmarking Multimodal Large Language Models for Face Recognition",
    "summary": "Multimodal large language models (MLLMs) have achieved remarkable performance\nacross diverse vision-and-language tasks. However, their potential in face\nrecognition remains underexplored. In particular, the performance of\nopen-source MLLMs needs to be evaluated and compared with existing face\nrecognition models on standard benchmarks with similar protocol. In this work,\nwe present a systematic benchmark of state-of-the-art MLLMs for face\nrecognition on several face recognition datasets, including LFW, CALFW, CPLFW,\nCFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich\nsemantic cues useful for face-related tasks, they lag behind specialized models\nin high-precision recognition scenarios in zero-shot applications. This\nbenchmark provides a foundation for advancing MLLM-based face recognition,\noffering insights for the design of next-generation models with higher accuracy\nand generalization. The source code of our benchmark is publicly available in\nthe project page.",
    "published": "2025-10-16T16:42:27Z",
    "updated": "2025-10-16T16:42:27Z",
    "link": "http://arxiv.org/pdf/2510.14866v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Hatef Otroshi Shahreza",
      "Sbastien Marcel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14861v1",
    "title": "LabOS: The AI-XR Co-Scientist That Sees and Works With Humans",
    "summary": "Modern science advances fastest when thought meets action. LabOS represents\nthe first AI co-scientist that unites computational reasoning with physical\nexperimentation through multimodal perception, self-evolving agents, and\nEntended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model\nAI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see\nwhat scientists see, understand experimental context, and assist in real-time\nexecution. Across applications--from cancer immunotherapy target discovery to\nstem-cell engineering -- LabOS shows that AI can move beyond computational\ndesign to participation, turning the laboratory into an intelligent,\ncollaborative environment where human and machine discovery evolve together.",
    "published": "2025-10-16T16:36:22Z",
    "updated": "2025-10-16T16:36:22Z",
    "link": "http://arxiv.org/pdf/2510.14861v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Le Cong",
      "Zaixi Zhang",
      "Xiaotong Wang",
      "Yin Di",
      "Ruofan Jin",
      "Michal Gerasimiuk",
      "Yinkai Wang",
      "Ravi K. Dinesh",
      "David Smerkous",
      "Alex Smerkous",
      "Xuekun Wu",
      "Shilong Liu",
      "Peishan Li",
      "Yi Zhu",
      "Simran Serrao",
      "Ning Zhao",
      "Imran A. Mohammad",
      "John B. Sunwoo",
      "Joseph C. Wu",
      "Mengdi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17774v4",
    "title": "Efficient & Correct Predictive Equivalence for Decision Trees",
    "summary": "The Rashomon set of decision trees (DTs) finds importance uses. Recent work\nshowed that DTs computing the same classification function, i.e. predictive\nequivalent DTs, can represent a significant fraction of the Rashomon set. Such\nredundancy is undesirable. For example, feature importance based on the\nRashomon set becomes inaccurate due the existence of predictive equivalent DTs,\ni.e. DTs with the same prediction for every possible input. In recent work,\nMcTavish et al. proposed solutions for several computational problems related\nwith DTs, including that of deciding predictive equivalent DTs. The approach of\nMcTavish et al. consists of applying the well-known method of Quine-McCluskey\n(QM) for obtaining minimum-size DNF (disjunctive normal form) representations\nof DTs, which are then used for comparing DTs for predictive equivalence.\nFurthermore, the minimum-size DNF representation was also applied to computing\nexplanations for the predictions made by DTs, and to finding predictions in the\npresence of missing data. However, the problem of formula minimization is hard\nfor the second level of the polynomial hierarchy, and the QM method may exhibit\nworst-case exponential running time and space. This paper first demonstrates\nthat there exist decision trees that trigger the worst-case exponential running\ntime and space of the QM method. Second, the paper shows that the QM method may\nincorrectly decide predictive equivalence, if two key constraints are not\nrespected, and one may be difficult to formally guarantee. Third, the paper\nshows that any of the problems to which the smallest DNF representation has\nbeen applied to can be solved in polynomial time, in the size of the DT. The\nexperiments confirm that, for DTs for which the worst-case of the QM method is\ntriggered, the algorithms proposed in this paper are orders of magnitude faster\nthan the ones proposed by McTavish et al.",
    "published": "2025-09-22T13:37:52Z",
    "updated": "2025-10-16T16:22:56Z",
    "link": "http://arxiv.org/pdf/2509.17774v4.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "authors": [
      "Joao Marques-Silva",
      "Alexey Ignatiev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.21097v2",
    "title": "Thinker: Learning to Think Fast and Slow",
    "summary": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 25.6% to 27.3% for Qwen2.5-1.5B, and from 45.9% to 51.0% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 25.2% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training. Additionally, we have open-sourced both the trained models\nand the source code.",
    "published": "2025-05-27T12:22:46Z",
    "updated": "2025-10-16T16:20:17Z",
    "link": "http://arxiv.org/pdf/2505.21097v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.6; I.2.8; I.5.1"
    ],
    "authors": [
      "Stephen Chung",
      "Wenyu Du",
      "Jie Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14846v1",
    "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents",
    "summary": "The generate-filter-refine (iterative paradigm) based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via a majority-vote instantiation. This theory\noffers a workable language and operational tools to measure agents and their\nsearch spaces, proposing a systematic formal description of iterative search\nconstructed by LLMs.",
    "published": "2025-10-16T16:18:37Z",
    "updated": "2025-10-16T16:18:37Z",
    "link": "http://arxiv.org/pdf/2510.14846v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LO"
    ],
    "authors": [
      "Zhuo-Yang Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14842v1",
    "title": "Boosting Instruction Following at Scale",
    "summary": "A typical approach developers follow to influence an LLM's behavior in an\napplication is through careful manipulation of the prompt, such as by adding or\nmodifying instructions. However, merely adding more instructions provides\nlittle assurance that they will actually be followed. We introduce Instruction\nBoosting as a post-generation method to increase the reliability of LLM prompt\ninstructions. We show that Instruction Boosting improves the instruction\nfollowing rate by up to 7 points for two instructions and up to 4 points for\nten instructions. To demonstrate these results we introduce SCALEDIF, a\nbenchmark with a scaled instruction volume of up to ten instructions per data\nsample. We also present an analysis of the commonly observed trend that\nperformance degrades as more instructions are added. We show that an important\nfactor contributing to this trend is the degree of tension and conflict that\narises as the number of instructions is increased. We contribute a quantitative\nconflict scoring tool that explains the observed performance trends and\nprovides feedback to developers on the impact that additional prompt\ninstructions have on a model's performance.",
    "published": "2025-10-16T16:15:58Z",
    "updated": "2025-10-16T16:15:58Z",
    "link": "http://arxiv.org/pdf/2510.14842v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ben Elder",
      "Evelyn Duesterwald",
      "Vinod Muthusamy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09658v2",
    "title": "Gradient-Sign Masking for Task Vector Transport Across Pre-Trained\n  Models",
    "summary": "When a new release of a foundation model is published, practitioners\ntypically need to repeat full fine-tuning, even if the same task has already\nbeen solved in the previous version. A promising alternative is to reuse the\nparameter changes (i.e., task vectors) that capture how a model adapts to a\nspecific task. However, they often fail to transfer across different\npre-trained models due to their misaligned parameter space. In this work, we\nshow that the key to successful transfer lies in the sign structure of the\ngradients of the new model. Based on this insight, we propose GradFix, a novel\nmethod that approximates the ideal gradient sign structure and leverages it to\ntransfer knowledge using only a handful of labeled samples. Notably, this\nrequires no additional fine-tuning: the adaptation is achieved by computing a\nfew gradients at the target model and masking the source task vector\naccordingly. This yields an update that is locally aligned with the target loss\nlandscape, effectively rebasing the task vector onto the new pre-training. We\nprovide a theoretical guarantee that our method ensures first-order descent.\nEmpirically, we demonstrate significant performance gains on vision and\nlanguage benchmarks, consistently outperforming naive task vector addition and\nfew-shot fine-tuning.",
    "published": "2025-10-07T13:30:25Z",
    "updated": "2025-10-16T16:13:33Z",
    "link": "http://arxiv.org/pdf/2510.09658v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Filippo Rinaldi",
      "Aniello Panariello",
      "Giacomo Salici",
      "Fengyuan Liu",
      "Marco Ciccone",
      "Angelo Porrello",
      "Simone Calderara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14830v1",
    "title": "RL-100: Performant Robotic Manipulation with Real-World Reinforcement\n  Learning",
    "summary": "Real-world robotic manipulation in homes and factories demands reliability,\nefficiency, and robustness that approach or surpass skilled human operators. We\npresent RL-100, a real-world reinforcement learning training framework built on\ndiffusion visuomotor policies trained bu supervised learning. RL-100 introduces\na three-stage pipeline. First, imitation learning leverages human priors.\nSecond, iterative offline reinforcement learning uses an Offline Policy\nEvaluation procedure, abbreviated OPE, to gate PPO-style updates that are\napplied in the denoising process for conservative and reliable improvement.\nThird, online reinforcement learning eliminates residual failure modes. An\nadditional lightweight consistency distillation head compresses the multi-step\nsampling process in diffusion into a single-step policy, enabling\nhigh-frequency control with an order-of-magnitude reduction in latency while\npreserving task performance. The framework is task-, embodiment-, and\nrepresentation-agnostic and supports both 3D point clouds and 2D RGB inputs, a\nvariety of robot platforms, and both single-step and action-chunk policies. We\nevaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,\nsuch as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth\nfolding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100\nattains 100\\% success across evaluated trials for a total of 900 out of 900\nepisodes, including up to 250 out of 250 consecutive trials on one task. The\nmethod achieves near-human teleoperation or better time efficiency and\ndemonstrates multi-hour robustness with uninterrupted operation lasting up to\ntwo hours.",
    "published": "2025-10-16T16:07:50Z",
    "updated": "2025-10-16T16:07:50Z",
    "link": "http://arxiv.org/pdf/2510.14830v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Kun Lei",
      "Huanyu Li",
      "Dongjie Yu",
      "Zhenyu Wei",
      "Lingxiao Guo",
      "Zhennan Jiang",
      "Ziyu Wang",
      "Shiyu Liang",
      "Huazhe Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14828v1",
    "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning",
    "summary": "Improving the reasoning capabilities of embodied agents is crucial for robots\nto complete complex human instructions in long-view manipulation tasks\nsuccessfully. Despite the success of large language models and vision language\nmodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continue\nfacing challenges in performing long-horizon manipulation tasks in complex\nreal-world environments, owing to their restricted common sense and reasoning\ncapabilities. Considering that aligning general-purpose vision language models\nto robotic planning tasks via supervised fine-tuning suffers from poor\ngeneralization and insufficient physical understanding, we propose RoboGPT-R1,\na two-stage fine-tuning framework for embodied planning. In this framework,\nsupervised training acquires foundational knowledge through expert sequences,\nfollowed by RL to address the model's shortcomings in visual-spatial\nunderstanding and reasoning. To achieve physical understanding and action\nsequence consistency in multi-step reasoning tasks, we design a rule-based\nreward function that simultaneously considers long-horizon performance and\naction constraint in the environment. The reasoning model, trained on\nQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,\nby 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the\nEmbodiedBench benchmark.",
    "published": "2025-10-16T16:04:35Z",
    "updated": "2025-10-16T16:04:35Z",
    "link": "http://arxiv.org/pdf/2510.14828v1.pdf",
    "category": [
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Jinrui Liu",
      "Bingyan Nie",
      "Boyu Li",
      "Yaran Chen",
      "Yuze Wang",
      "Shunsen He",
      "Haoran Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09394v2",
    "title": "Higher-order interactions of multi-layer prompt",
    "summary": "The \"pre-train, prompt\" paradigm has successfully evolved in representation\nlearning. While current prompt-tuning methods often introduce learnable\nprompts, they predominantly treat prompts as isolated, independent components\nacross different network layers. This overlooks the complex and synergistic\nhigher-order interactions that exist between prompts at various hierarchical\ndepths, consequently limiting the expressive power and semantic richness of the\nprompted model. To address this fundamental gap, we propose a novel framework\nthat explicitly models the Higher-order Interactions of Multi-layer Prompt. Our\napproach conceptualizes prompts from different layers not as separate entities,\nbut as a cohesive system where their inter-relationships are critical. We\ndesign an innovative interaction module that captures these sophisticated,\nnon-linear correlations among multi-layer prompts, effectively modeling their\ncooperative effects. This allows the model to dynamically aggregate and refine\nprompt information across the network's depth, leading to a more integrated and\npowerful prompting strategy. Extensive experiments on eight benchmark datasets\ndemonstrate that our method, by leveraging these higher-order interactions,\nconsistently surpasses state-of-the-art prompt-tuning baselines. The\nperformance advantage is particularly pronounced in few-shot scenarios,\nvalidating that capturing the intricate interplay between multi-layer prompts\nis key to unlocking more robust and generalizable representation learning.",
    "published": "2025-10-10T13:48:34Z",
    "updated": "2025-10-16T15:48:27Z",
    "link": "http://arxiv.org/pdf/2510.09394v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Ziyu Zheng",
      "Yaming Yang",
      "Ziyu Guan",
      "Wei Zhao",
      "Xinyan Huang",
      "Weigang Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.08814v3",
    "title": "Merge-of-Thought Distillation",
    "summary": "Efficient reasoning distillation for long chain-of-thought (CoT) models is\nincreasingly constrained by the assumption of a single oracle teacher, despite\nthe practical availability of multiple candidate teachers and growing CoT\ncorpora. We revisit teacher selection and observe that different students have\ndifferent \"best teachers,\" and even for the same student, the best teacher can\nvary across datasets. Therefore, to unify multiple teachers' reasoning\nabilities into a student to overcome conflicts among various teachers'\nsupervision, we propose Merge-of-Thought Distillation (MoT), a lightweight\nframework that alternates between teacher-specific supervised fine-tuning\nbranches and weight-space merging of the resulting student variants. On\ncompetition math benchmarks, using only about 200 CoT samples, applying MoT to\na Qwen3-14B student surpasses strong models including Deepseek-R1, Qwen3-32B,\nand OpenAI-O1, demonstrating substantial gains. Besides, MoT consistently\noutperforms the best single-teacher distillation, improves general reasoning\nbeyond mathematics while reducing catastrophic forgetting, and shows robustness\nto distribution-shifted and peer-level teachers. Finally, we have demonstrated\nMoT possesses consensus CoT by eliminating teacher-specific inductive biases\nand inter-teacher conflicts while repeatedly reinforcing the learning of\nconsensus reasoning features. These results position MoT as a simple, effective\nroute to efficiently distilling long CoT capabilities from diverse teachers\ninto compact students.",
    "published": "2025-09-10T17:46:57Z",
    "updated": "2025-10-16T15:43:35Z",
    "link": "http://arxiv.org/pdf/2509.08814v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zhanming Shen",
      "Zeyu Qin",
      "Zenan Huang",
      "Hao Chen",
      "Jiaqi Hu",
      "Yihong Zhuang",
      "Guoshan Lu",
      "Gang Chen",
      "Junbo Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14808v1",
    "title": "Agentic NL2SQL to Reduce Computational Costs",
    "summary": "Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)\nhas recently been empowered by large language models (LLMs). Using LLMs to\nperform NL2SQL methods on a large collection of SQL databases necessitates\nprocessing large quantities of meta-information about the databases, which in\nturn results in lengthy prompts with many tokens and high processing costs. To\naddress this challenge, we introduce Datalake Agent, an agentic system designed\nto enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing\ndirect solvers for NL2SQL that call the LLM once with all meta-information in\nthe prompt, the Datalake Agent employs an interactive loop to reduce the\nutilized meta-information. Within the loop, the LLM is used in a reasoning\nframework that selectively requests only the necessary information to solve a\ntable question answering task. We evaluate the Datalake Agent on a collection\nof 23 databases with 100 table question answering tasks. The Datalake Agent\nreduces the tokens used by the LLM by up to 87\\% and thus allows for\nsubstantial cost reductions while maintaining competitive performance.",
    "published": "2025-10-16T15:42:28Z",
    "updated": "2025-10-16T15:42:28Z",
    "link": "http://arxiv.org/pdf/2510.14808v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Dominik Jehle",
      "Lennart Purucker",
      "Frank Hutter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14807v1",
    "title": "SimKO: Simple Pass@K Policy Optimization",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models (LLMs). However, prevailing\nRLVR methods exhibit a systematic bias toward exploitation over exploration, as\nevidenced by improved pass@1 but reduced pass@K (K>1) performance. To\nunderstand this issue, we analyze training dynamics of RLVR methods by tracking\nthe token-level probability distributions over vocabulary candidates. Our\nanalysis reveals a consistent probability concentration effect where the top-1\ncandidate increasingly accumulates probability mass and suppresses that of\nother candidates. More importantly, stronger over-concentration correlates with\nworse pass@K performance. Inspired by this finding, we propose Simple Pass@K\nOptimization (SimKO), a method designed to mitigate the over-concentration\nissue, thereby encouraging exploration. SimKO operates in an asymmetrical\nmanner. For verified-correct responses, it boosts the probabilities of the\ntop-K candidates. For verified-incorrect responses, it applies stronger\npenalties to the top-1 candidate. We observe that this asymmetric design is\nparticularly effective at mitigating over-concentration when applied at tokens\nwith high entropy. Across various math and logical-reasoning benchmarks, SimKO\nconsistently yields higher pass@K for a wide range of K, providing a simple way\nto improve RLVR's exploration.",
    "published": "2025-10-16T15:40:49Z",
    "updated": "2025-10-16T15:40:49Z",
    "link": "http://arxiv.org/pdf/2510.14807v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ruotian Peng",
      "Yi Ren",
      "Zhouliang Yu",
      "Weiyang Liu",
      "Yandong Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14803v1",
    "title": "Scaling Artificial Intelligence for Multi-Tumor Early Detection with\n  More Reports, Fewer Masks",
    "summary": "Early tumor detection save lives. Each year, more than 300 million computed\ntomography (CT) scans are performed worldwide, offering a vast opportunity for\neffective cancer screening. However, detecting small or early-stage tumors on\nthese CT scans remains challenging, even for experts. Artificial intelligence\n(AI) models can assist by highlighting suspicious regions, but training such\nmodels typically requires extensive tumor masks--detailed, voxel-wise outlines\nof tumors manually drawn by radiologists. Drawing these masks is costly,\nrequiring years of effort and millions of dollars. In contrast, nearly every CT\nscan in clinical practice is already accompanied by medical reports describing\nthe tumor's size, number, appearance, and sometimes, pathology\nresults--information that is rich, abundant, and often underutilized for AI\ntraining. We introduce R-Super, which trains AI to segment tumors that match\ntheir descriptions in medical reports. This approach scales AI training with\nlarge collections of readily available medical reports, substantially reducing\nthe need for manually drawn tumor masks. When trained on 101,654 reports, AI\nmodels achieved performance comparable to those trained on 723 masks. Combining\nreports and masks further improved sensitivity by +13% and specificity by +8%,\nsurpassing radiologists in detecting five of the seven tumor types. Notably,\nR-Super enabled segmentation of tumors in the spleen, gallbladder, prostate,\nbladder, uterus, and esophagus, for which no public masks or AI models\npreviously existed. This study challenges the long-held belief that\nlarge-scale, labor-intensive tumor mask creation is indispensable, establishing\na scalable and accessible path toward early detection across diverse tumor\ntypes.\n  We plan to release our trained models, code, and dataset at\nhttps://github.com/MrGiovanni/R-Super",
    "published": "2025-10-16T15:35:44Z",
    "updated": "2025-10-16T15:35:44Z",
    "link": "http://arxiv.org/pdf/2510.14803v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Pedro R. A. S. Bassi",
      "Xinze Zhou",
      "Wenxuan Li",
      "Szymon Potka",
      "Jieneng Chen",
      "Qi Chen",
      "Zheren Zhu",
      "Jakub Przdo",
      "Ibrahim E. Hamac",
      "Sezgin Er",
      "Yuhan Wang",
      "Ashwin Kumar",
      "Bjoern Menze",
      "Jarosaw B. wika",
      "Yuyin Zhou",
      "Akshay S. Chaudhari",
      "Curtis P. Langlotz",
      "Sergio Decherchi",
      "Andrea Cavalli",
      "Kang Wang",
      "Yang Yang",
      "Alan L. Yuille",
      "Zongwei Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14800v1",
    "title": "Morphology-Aware Prognostic model for Five-Year Survival Prediction in\n  Colorectal Cancer from H&E Whole Slide Images",
    "summary": "Colorectal cancer (CRC) remains the third most prevalent malignancy globally,\nwith approximately 154,000 new cases and 54,000 projected deaths anticipated\nfor 2025. The recent advancement of foundation models in computational\npathology has been largely propelled by task agnostic methodologies that can\noverlook organ-specific crucial morphological patterns that represent distinct\nbiological processes that can fundamentally influence tumor behavior,\ntherapeutic response, and patient outcomes. The aim of this study is to develop\na novel, interpretable AI model, PRISM (Prognostic Representation of Integrated\nSpatial Morphology), that incorporates a continuous variability spectrum within\neach distinct morphology to characterize phenotypic diversity and reflecting\nthe principle that malignant transformation occurs through incremental\nevolutionary processes rather than abrupt phenotypic shifts. PRISM is trained\non 8.74 million histological images extracted from surgical resection specimens\nof 424 patients with stage III CRC. PRISM achieved superior prognostic\nperformance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%;\nHR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific\nmethods by 15% and AI foundation models by ~23% accuracy. It showed\nsex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable\nperformance across clinicopathological subgroups, with minimal accuracy\nfluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens,\nreplicating the Alliance cohort finding of no survival difference between\ntreatments.",
    "published": "2025-10-16T15:32:05Z",
    "updated": "2025-10-16T15:32:05Z",
    "link": "http://arxiv.org/pdf/2510.14800v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Usama Sajjad",
      "Abdul Rehman Akbar",
      "Ziyu Su",
      "Deborah Knight",
      "Wendy L. Frankel",
      "Metin N. Gurcan",
      "Wei Chen",
      "Muhammad Khalid Khan Niazi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.20921v2",
    "title": "LLM-guided Chemical Process Optimization with a Multi-Agent Approach",
    "summary": "Chemical process optimization maximizes production efficiency and economic\nperformance, but optimization algorithms, including gradient-based solvers,\nnumerical methods, and parameter grid searches, become impractical when\noperating constraints are ill-defined or unavailable. We present a multi-agent\nLLM framework that autonomously infers operating constraints from minimal\nprocess descriptions, then collaboratively guides optimization. Our\nAutoGen-based framework employs OpenAI's o3 model with specialized agents for\nconstraint generation, parameter validation, simulation, and optimization\nguidance. Through autonomous constraint generation and iterative multi-agent\noptimization, the framework eliminates the need for predefined operational\nbounds. Validated on hydrodealkylation across cost, yield, and yield-to-cost\nratio metrics, the framework achieved competitive performance with conventional\nmethods while reducing wall-time 31-fold relative to grid search, converging in\nunder 20 minutes. The reasoning-guided search demonstrates sophisticated\nprocess understanding, correctly identifying utility trade-offs and applying\ndomain-informed heuristics. Unlike conventional methods requiring predefined\nconstraints, our approach uniquely combines autonomous constraint generation\nwith interpretable parameter exploration. Model comparison reveals\nreasoning-capable architectures (o3, o1) are essential for successful\noptimization, while standard models fail to converge. This approach is\nparticularly valuable for emerging processes and retrofit applications where\noperational constraints are poorly characterized or unavailable.",
    "published": "2025-06-26T01:03:44Z",
    "updated": "2025-10-16T15:31:07Z",
    "link": "http://arxiv.org/pdf/2506.20921v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "authors": [
      "Tong Zeng",
      "Srivathsan Badrinarayanan",
      "Janghoon Ock",
      "Cheng-Kai Lai",
      "Amir Barati Farimani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.09570v2",
    "title": "Say My Name: a Model's Bias Discovery Framework",
    "summary": "In the last few years, due to the broad applicability of deep learning to\ndownstream tasks and end-to-end training capabilities, increasingly more\nconcerns about potential biases to specific, non-representative patterns have\nbeen raised. Many works focusing on unsupervised debiasing usually leverage the\ntendency of deep models to learn ``easier'' samples, for example by clustering\nthe latent space to obtain bias pseudo-labels. However, the interpretation of\nsuch pseudo-labels is not trivial, especially for a non-expert end user, as it\ndoes not provide semantic information about the bias features. To address this\nissue, we introduce ``Say My Name'' (SaMyNa), the first tool to identify biases\nwithin deep models semantically. Unlike existing methods, our approach focuses\non biases learned by the model. Our text-based pipeline enhances explainability\nand supports debiasing efforts: applicable during either training or post-hoc\nvalidation, our method can disentangle task-related information and proposes\nitself as a tool to analyze biases. Evaluation on traditional benchmarks\ndemonstrates its effectiveness in detecting biases and even disclaiming them,\nshowcasing its broad applicability for model diagnosis.",
    "published": "2024-08-18T18:50:59Z",
    "updated": "2025-10-16T15:21:49Z",
    "link": "http://arxiv.org/pdf/2408.09570v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Massimiliano Ciranni",
      "Luca Molinaro",
      "Carlo Alberto Barbano",
      "Attilio Fiandrotti",
      "Vittorio Murino",
      "Vito Paolo Pastore",
      "Enzo Tartaglione"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14788v1",
    "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale",
    "summary": "User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms.",
    "published": "2025-10-16T15:20:49Z",
    "updated": "2025-10-16T15:20:49Z",
    "link": "http://arxiv.org/pdf/2510.14788v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Manjie Xu",
      "Cheng Chen",
      "Xin Jia",
      "Jingyi Zhou",
      "Yongji Wu",
      "Zejian Wang",
      "Chi Zhang",
      "Kai Zuo",
      "Yibo Chen",
      "Xu Tang",
      "Yao Hu",
      "Yixin Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14773v1",
    "title": "Finding Answers in Thought Matters: Revisiting Evaluation on Large\n  Language Models with Reasoning",
    "summary": "Evaluating generative models, such as large language models (LLMs), commonly\ninvolves question-answering tasks where the final answer is selected based on\nprobability of answer choices. On the other hand, for models requiring\nreasoning, the method of answer extraction plays a critical role. Our research\nreveals that the performance of reasoning models and their final answer\ndistributions are highly sensitive to the answer extraction algorithm employed.\nIn order to mitigate this, we propose a basic framework: Answer Regeneration.\nThe method uses an additional model inference, providing the prior input and\noutput prefaced by the prompt \"Answer:\". The final answer is then selected or\nextracted from the regenerated output. We show that this\nextraction-rule-agnostic approach exhibits improved performance and enhanced\nrobustness. Furthermore, we have applied this framework to general math\nproblems and open-ended question answering tasks. Our analysis and this\nframework could offer a more reliable results for model evaluation.",
    "published": "2025-10-16T15:09:22Z",
    "updated": "2025-10-16T15:09:22Z",
    "link": "http://arxiv.org/pdf/2510.14773v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Hwiyeol Jo",
      "Joosung Lee",
      "Jaehone Lee",
      "Sang-Woo Lee",
      "Joonsuk Park",
      "Kang Min Yoo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.20934v2",
    "title": "Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method\n  Refactoring",
    "summary": "MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools\nthat recommend which methods to move and where, these recommendations do not\nalign with how expert developers perform MOVEMETHOD. Given the extensive\ntraining of Large Language Models and their reliance upon naturalness of code,\nthey should expertly recommend which methods are misplaced in a given class and\nwhich classes are better hosts. Our formative study of 2016 LLM recommendations\nrevealed that LLMs give expert suggestions, yet they are unreliable: up to 80%\nof the suggestions are hallucinations. We introduce the first LLM fully powered\nassistant for MOVEMETHOD refactoring that automates its whole end-to-end\nlifecycle, from recommendation to execution. We designed novel solutions that\nautomatically filter LLM hallucinations using static analysis from IDEs and a\nnovel workflow that requires LLMs to be self-consistent, critique, and rank\nrefactoring suggestions. As MOVEMETHOD refactoring requires global,\nprojectlevel reasoning, we solved the limited context size of LLMs by employing\nrefactoring-aware retrieval augment generation (RAG). Our approach, MM-assist,\nsynergistically combines the strengths of the LLM, IDE, static analysis, and\nsemantic relevance. In our thorough, multi-methodology empirical evaluation, we\ncompare MM-assist with the previous state-of-the-art approaches. MM-assist\nsignificantly outperforms them: (i) on a benchmark widely used by other\nresearchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a\ncorpus of 210 recent refactorings from Open-source software, our Recall rates\nimprove by at least 2.4x. Lastly, we conducted a user study with 30 experienced\nparticipants who used MM-assist to refactor their own code for one week. They\nrated 82.8% of MM-assist recommendations positively. This shows that MM-assist\nis both effective and useful.",
    "published": "2025-03-26T19:05:20Z",
    "updated": "2025-10-16T15:08:16Z",
    "link": "http://arxiv.org/pdf/2503.20934v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Abhiram Bellur",
      "Fraol Batole",
      "Mohammed Raihan Ullah",
      "Malinda Dilhara",
      "Yaroslav Zharov",
      "Timofey Bryksin",
      "Kai Ishikawa",
      "Haifeng Chen",
      "Masaharu Morimoto",
      "Shota Motoura",
      "Takeo Hosomi",
      "Tien N. Nguyen",
      "Hridesh Rajan",
      "Nikolaos Tsantalis",
      "Danny Dig"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14765v1",
    "title": "Inpainting the Red Planet: Diffusion Models for the Reconstruction of\n  Martian Environments in Virtual Reality",
    "summary": "Space exploration increasingly relies on Virtual Reality for several tasks,\nsuch as mission planning, multidisciplinary scientific analysis, and astronaut\ntraining. A key factor for the reliability of the simulations is having\naccurate 3D representations of planetary terrains. Extraterrestrial heightmaps\nderived from satellite imagery often contain missing values due to acquisition\nand transmission constraints. Mars is among the most studied planets beyond\nEarth, and its extensive terrain datasets make the Martian surface\nreconstruction a valuable task, although many areas remain unmapped. Deep\nlearning algorithms can support void-filling tasks; however, whereas Earth's\ncomprehensive datasets enables the use of conditional methods, such approaches\ncannot be applied to Mars. Current approaches rely on simpler interpolation\ntechniques which, however, often fail to preserve geometric coherence. In this\nwork, we propose a method for reconstructing the surface of Mars based on an\nunconditional diffusion model. Training was conducted on an augmented dataset\nof 12000 Martian heightmaps derived from NASA's HiRISE survey. A\nnon-homogeneous rescaling strategy captures terrain features across multiple\nscales before resizing to a fixed 128x128 model resolution. We compared our\nmethod against established void-filling and inpainting techniques, including\nInverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an\nevaluation set of 1000 samples. Results show that our approach consistently\noutperforms these methods in terms of reconstruction accuracy (4-15% on RMSE)\nand perceptual similarity (29-81% on LPIPS) with the original data.",
    "published": "2025-10-16T15:02:05Z",
    "updated": "2025-10-16T15:02:05Z",
    "link": "http://arxiv.org/pdf/2510.14765v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "authors": [
      "Giuseppe Lorenzo Catalano",
      "Agata Marta Soccini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14763v1",
    "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with\n  Thought Processes",
    "summary": "Large language models exhibit systematic deficiencies in creative writing,\nparticularly in non-English contexts where training data is scarce and lacks\nprocess-level supervision. We present COIG-Writer, a novel Chinese creative\nwriting dataset that captures both diverse outputs and their underlying thought\nprocesses through systematic reverse-engineering of high-quality texts. Unlike\nexisting datasets that provide only input-output pairs, COIG-Writer comprises\n1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a\nreverse-engineered prompt, (2) detailed creative reasoning documenting\ndecision-making processes, and (3) the final text. Through comprehensive\nexperiments, we identify a two-component model of creative writing: narrative\nlogic (provided by process supervision) and linguistic expression (maintained\nby general-purpose data). Our findings reveal three critical insights: (1)\nProcess supervision is highly effective but requires stabilization with general\ndata. A ratio of at least one creative sample to twelve general samples is\nneeded to achieve optimal performance; below this threshold, the win rate\nprogressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities\nare culturally-bound with no cross-lingual transfer (89.26pp gap between\nChinese and English performance), and (3) lexical diversity inversely\ncorrelates with creative quality (TTR paradox), suggesting high diversity\nsignals compensatory behavior for logical deficiencies. These findings\nestablish that creative excellence emerges from the interaction between logical\nscaffolding and linguistic grounding, analogous to how mathematical reasoning\nenhances but cannot replace linguistic competence in foundation models.",
    "published": "2025-10-16T15:01:19Z",
    "updated": "2025-10-16T15:01:19Z",
    "link": "http://arxiv.org/pdf/2510.14763v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yunwen Li",
      "Shuangshuang Ying",
      "Xingwei Qu",
      "Xin Li",
      "Sheng Jin",
      "Minghao Liu",
      "Zhoufutu Wen",
      "Tianyu Zheng",
      "Xeron Du",
      "Qiguang Chen",
      "Jiajun Shi",
      "Wangchunshu Zhou",
      "Jiazhan Feng",
      "Wanjun Zhong",
      "Libo Qin",
      "Stephen Huang",
      "Wanxiang Che",
      "Chenghua Lin",
      "Eli Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.07887v2",
    "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large\n  Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
    "summary": "The growing integration of Large Language Models (LLMs) into critical\nsocietal domains has raised concerns about embedded biases that can perpetuate\nstereotypes and undermine fairness. Such biases may stem from historical\ninequalities in training data, linguistic imbalances, or adversarial\nmanipulation. Despite mitigation efforts, recent studies show that LLMs remain\nvulnerable to adversarial attacks that elicit biased outputs. This work\nproposes a scalable benchmarking framework to assess LLM robustness to\nadversarial bias elicitation. Our methodology involves: (i) systematically\nprobing models across multiple tasks targeting diverse sociocultural biases,\n(ii) quantifying robustness through safety scores using an LLM-as-a-Judge\napproach, and (iii) employing jailbreak techniques to reveal safety\nvulnerabilities. To facilitate systematic benchmarking, we release a curated\ndataset of bias-related prompts, named CLEAR-Bias. Our analysis, identifying\nDeepSeek V3 as the most reliable judge LLM, reveals that bias resilience is\nuneven, with age, disability, and intersectional biases among the most\nprominent. Some small models outperform larger ones in safety, suggesting that\ntraining and architecture may matter more than scale. However, no model is\nfully robust to adversarial elicitation, with jailbreak attacks using\nlow-resource languages or refusal suppression proving effective across model\nfamilies. We also find that successive LLM generations exhibit slight safety\ngains, while models fine-tuned for the medical domain tend to be less safe than\ntheir general-purpose counterparts.",
    "published": "2025-04-10T16:00:59Z",
    "updated": "2025-10-16T14:59:50Z",
    "link": "http://arxiv.org/pdf/2504.07887v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Riccardo Cantini",
      "Alessio Orsino",
      "Massimo Ruggiero",
      "Domenico Talia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.06261v5",
    "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality,\n  Long Context, and Next Generation Agentic Capabilities",
    "summary": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and\nGemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite\nmodels. Gemini 2.5 Pro is our most capable model yet, achieving SoTA\nperformance on frontier coding and reasoning benchmarks. In addition to its\nincredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that\nexcels at multimodal understanding and it is now able to process up to 3 hours\nof video content. Its unique combination of long context, multimodal and\nreasoning capabilities can be combined to unlock new agentic workflows. Gemini\n2.5 Flash provides excellent reasoning abilities at a fraction of the compute\nand latency requirements and Gemini 2.0 Flash and Flash-Lite provide high\nperformance at low latency and cost. Taken together, the Gemini 2.X model\ngeneration spans the full Pareto frontier of model capability vs cost, allowing\nusers to explore the boundaries of what is possible with complex agentic\nproblem solving.",
    "published": "2025-07-07T17:36:04Z",
    "updated": "2025-10-16T14:56:46Z",
    "link": "http://arxiv.org/pdf/2507.06261v5.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Gheorghe Comanici",
      "Eric Bieber",
      "Mike Schaekermann",
      "Ice Pasupat",
      "Noveen Sachdeva",
      "Inderjit Dhillon",
      "Marcel Blistein",
      "Ori Ram",
      "Dan Zhang",
      "Evan Rosen",
      "Luke Marris",
      "Sam Petulla",
      "Colin Gaffney",
      "Asaf Aharoni",
      "Nathan Lintz",
      "Tiago Cardal Pais",
      "Henrik Jacobsson",
      "Idan Szpektor",
      "Nan-Jiang Jiang",
      "Krishna Haridasan",
      "Ahmed Omran",
      "Nikunj Saunshi",
      "Dara Bahri",
      "Gaurav Mishra",
      "Eric Chu",
      "Toby Boyd",
      "Brad Hekman",
      "Aaron Parisi",
      "Chaoyi Zhang",
      "Kornraphop Kawintiranon",
      "Tania Bedrax-Weiss",
      "Oliver Wang",
      "Ya Xu",
      "Ollie Purkiss",
      "Uri Mendlovic",
      "Ila Deutel",
      "Nam Nguyen",
      "Adam Langley",
      "Flip Korn",
      "Lucia Rossazza",
      "Alexandre Ram",
      "Sagar Waghmare",
      "Helen Miller",
      "Nathan Byrd",
      "Ashrith Sheshan",
      "Raia Hadsell",
      "Sangnie Bhardwaj",
      "Pawel Janus",
      "Tero Rissa",
      "Dan Horgan",
      "Alvin Abdagic",
      "Lior Belenki",
      "James Allingham",
      "Anima Singh",
      "Theo Guidroz",
      "Srivatsan Srinivasan",
      "Herman Schmit",
      "Kristen Chiafullo",
      "Andre Elisseeff",
      "Nilpa Jha",
      "Prateek Kolhar",
      "Leonard Berrada",
      "Frank Ding",
      "Xiance Si",
      "Shrestha Basu Mallick",
      "Franz Och",
      "Sofia Erell",
      "Eric Ni",
      "Tejasi Latkar",
      "Sherry Yang",
      "Petar Sirkovic",
      "Ziqiang Feng",
      "Robert Leland",
      "Rachel Hornung",
      "Gang Wu",
      "Charles Blundell",
      "Hamidreza Alvari",
      "Po-Sen Huang",
      "Cathy Yip",
      "Sanja Deur",
      "Li Liu",
      "Gabriela Surita",
      "Pablo Duque",
      "Dima Damen",
      "Johnson Jia",
      "Arthur Guez",
      "Markus Mircea",
      "Animesh Sinha",
      "Alberto Magni",
      "Pawe Stradomski",
      "Tal Marian",
      "Vlado Gali",
      "Wenhu Chen",
      "Hisham Husain",
      "Achintya Singhal",
      "Dominik Grewe",
      "Franois-Xavier Aubet",
      "Shuang Song",
      "Lorenzo Blanco",
      "Leland Rechis",
      "Lewis Ho",
      "Rich Munoz",
      "Kelvin Zheng",
      "Jessica Hamrick",
      "Kevin Mather",
      "Hagai Taitelbaum",
      "Eliza Rutherford",
      "Yun Lei",
      "Kuangyuan Chen",
      "Anand Shukla",
      "Erica Moreira",
      "Eric Doi",
      "Berivan Isik",
      "Nir Shabat",
      "Dominika Rogoziska",
      "Kashyap Kolipaka",
      "Jason Chang",
      "Eugen Vuak",
      "Srinivasan Venkatachary",
      "Shadi Noghabi",
      "Tarun Bharti",
      "Younghoon Jun",
      "Aleksandr Zaks",
      "Simon Green",
      "Jeshwanth Challagundla",
      "William Wong",
      "Muqthar Mohammad",
      "Dean Hirsch",
      "Yong Cheng",
      "Iftekhar Naim",
      "Lev Proleev",
      "Damien Vincent",
      "Aayush Singh",
      "Maxim Krikun",
      "Dilip Krishnan",
      "Zoubin Ghahramani",
      "Aviel Atias",
      "Rajeev Aggarwal",
      "Christo Kirov",
      "Dimitrios Vytiniotis",
      "Christy Koh",
      "Alexandra Chronopoulou",
      "Pawan Dogra",
      "Vlad-Doru Ion",
      "Gladys Tyen",
      "Jason Lee",
      "Felix Weissenberger",
      "Trevor Strohman",
      "Ashwin Balakrishna",
      "Jack Rae",
      "Marko Velic",
      "Raoul de Liedekerke",
      "Oded Elyada",
      "Wentao Yuan",
      "Canoee Liu",
      "Lior Shani",
      "Sergey Kishchenko",
      "Bea Alessio",
      "Yandong Li",
      "Richard Song",
      "Sam Kwei",
      "Orion Jankowski",
      "Aneesh Pappu",
      "Youhei Namiki",
      "Yenai Ma",
      "Nilesh Tripuraneni",
      "Colin Cherry",
      "Marissa Ikonomidis",
      "Yu-Cheng Ling",
      "Colin Ji",
      "Beka Westberg",
      "Auriel Wright",
      "Da Yu",
      "David Parkinson",
      "Swaroop Ramaswamy",
      "Jerome Connor",
      "Soheil Hassas Yeganeh",
      "Snchit Grover",
      "George Kenwright",
      "Lubo Litchev",
      "Chris Apps",
      "Alex Tomala",
      "Felix Halim",
      "Alex Castro-Ros",
      "Zefei Li",
      "Anudhyan Boral",
      "Pauline Sho",
      "Michal Yarom",
      "Eric Malmi",
      "David Klinghoffer",
      "Rebecca Lin",
      "Alan Ansell",
      "Pradeep Kumar S",
      "Shubin Zhao",
      "Siqi Zuo",
      "Adam Santoro",
      "Heng-Tze Cheng",
      "Solomon Demmessie",
      "Yuchi Liu",
      "Nicole Brichtova",
      "Allie Culp",
      "Nathaniel Braun",
      "Dan Graur",
      "Will Ng",
      "Nikhil Mehta",
      "Aaron Phillips",
      "Patrik Sundberg",
      "Varun Godbole",
      "Fangyu Liu",
      "Yash Katariya",
      "David Rim",
      "Mojtaba Seyedhosseini",
      "Sean Ammirati",
      "Jonas Valfridsson",
      "Mahan Malihi",
      "Timothy Knight",
      "Andeep Toor",
      "Thomas Lampe",
      "Abe Ittycheriah",
      "Lewis Chiang",
      "Chak Yeung",
      "Alexandre Frchette",
      "Jinmeng Rao",
      "Huisheng Wang",
      "Himanshu Srivastava",
      "Richard Zhang",
      "Rocky Rhodes",
      "Ariel Brand",
      "Dean Weesner",
      "Ilya Figotin",
      "Felix Gimeno",
      "Rachana Fellinger",
      "Pierre Marcenac",
      "Jos Leal",
      "Eyal Marcus",
      "Victor Cotruta",
      "Rodrigo Cabrera",
      "Sheryl Luo",
      "Dan Garrette",
      "Vera Axelrod",
      "Sorin Baltateanu",
      "David Barker",
      "Dongkai Chen",
      "Horia Toma",
      "Ben Ingram",
      "Jason Riesa",
      "Chinmay Kulkarni",
      "Yujing Zhang",
      "Hongbin Liu",
      "Chao Wang",
      "Martin Polacek",
      "Will Wu",
      "Kai Hui",
      "Adrian N Reyes",
      "Yi Su",
      "Megan Barnes",
      "Ishaan Malhi",
      "Anfal Siddiqui",
      "Qixuan Feng",
      "Mihai Damaschin",
      "Daniele Pighin",
      "Andreas Steiner",
      "Samuel Yang",
      "Ramya Sree Boppana",
      "Simeon Ivanov",
      "Arun Kandoor",
      "Aditya Shah",
      "Asier Mujika",
      "Da Huang",
      "Christopher A. Choquette-Choo",
      "Mohak Patel",
      "Tianhe Yu",
      "Toni Creswell",
      " Jerry",
      " Liu",
      "Catarina Barros",
      "Yasaman Razeghi",
      "Aurko Roy",
      "Phil Culliton",
      "Binbin Xiong",
      "Jiaqi Pan",
      "Thomas Strohmann",
      "Tolly Powell",
      "Babi Seal",
      "Doug DeCarlo",
      "Pranav Shyam",
      "Kaan Katircioglu",
      "Xuezhi Wang",
      "Cassidy Hardin",
      "Immanuel Odisho",
      "Josef Broder",
      "Oscar Chang",
      "Arun Nair",
      "Artem Shtefan",
      "Maura O'Brien",
      "Manu Agarwal",
      "Sahitya Potluri",
      "Siddharth Goyal",
      "Amit Jhindal",
      "Saksham Thakur",
      "Yury Stuken",
      "James Lyon",
      "Kristina Toutanova",
      "Fangxiaoyu Feng",
      "Austin Wu",
      "Ben Horn",
      "Alek Wang",
      "Alex Cullum",
      "Gabe Taubman",
      "Disha Shrivastava",
      "Chongyang Shi",
      "Hamish Tomlinson",
      "Roma Patel",
      "Tao Tu",
      "Ada Maksutaj Oflazer",
      "Francesco Pongetti",
      "Mingyao Yang",
      "Adrien Ali Taga",
      "Vincent Perot",
      "Nuo Wang Pierse",
      "Feng Han",
      "Yoel Drori",
      "Iaki Iturrate",
      "Ayan Chakrabarti",
      "Legg Yeung",
      "Dave Dopson",
      "Yi-ting Chen",
      "Apoorv Kulshreshtha",
      "Tongfei Guo",
      "Philip Pham",
      "Tal Schuster",
      "Junquan Chen",
      "Alex Polozov",
      "Jinwei Xing",
      "Huanjie Zhou",
      "Praneeth Kacham",
      "Doron Kukliansky",
      "Antoine Miech",
      "Sergey Yaroshenko",
      "Ed Chi",
      "Sholto Douglas",
      "Hongliang Fei",
      "Mathieu Blondel",
      "Preethi Myla",
      "Lior Madmoni",
      "Xing Wu",
      "Daniel Keysers",
      "Kristian Kjems",
      "Isabela Albuquerque",
      "Lijun Yu",
      "Joel D'sa",
      "Michelle Plantan",
      "Vlad Ionescu",
      "Jaume Sanchez Elias",
      "Abhirut Gupta",
      "Manish Reddy Vuyyuru",
      "Fred Alcober",
      "Tong Zhou",
      "Kaiyang Ji",
      "Florian Hartmann",
      "Subha Puttagunta",
      "Hugo Song",
      "Ehsan Amid",
      "Anca Stefanoiu",
      "Andrew Lee",
      "Paul Pucciarelli",
      "Emma Wang",
      "Amit Raul",
      "Slav Petrov",
      "Isaac Tian",
      "Valentin Anklin",
      "Nana Nti",
      "Victor Gomes",
      "Max Schumacher",
      "Grace Vesom",
      "Alex Panagopoulos",
      "Konstantinos Bousmalis",
      "Daniel Andor",
      "Josh Jacob",
      "Yuan Zhang",
      "Bill Rosgen",
      "Matija Kecman",
      "Matthew Tung",
      "Alexandra Belias",
      "Noah Goodman",
      "Paul Covington",
      "Brian Wieder",
      "Nikita Saxena",
      "Elnaz Davoodi",
      "Muhuan Huang",
      "Sharath Maddineni",
      "Vincent Roulet",
      "Folawiyo Campbell-Ajala",
      "Pier Giuseppe Sessa",
      " Xintian",
      " Wu",
      "Guangda Lai",
      "Paul Collins",
      "Alex Haig",
      "Vytenis Sakenas",
      "Xiaowei Xu",
      "Marissa Giustina",
      "Laurent El Shafey",
      "Pichi Charoenpanit",
      "Shefali Garg",
      "Joshua Ainslie",
      "Boone Severson",
      "Montse Gonzalez Arenas",
      "Shreya Pathak",
      "Sujee Rajayogam",
      "Jie Feng",
      "Michiel Bakker",
      "Sheng Li",
      "Nevan Wichers",
      "Jamie Rogers",
      "Xinyang Geng",
      "Yeqing Li",
      "Rolf Jagerman",
      "Chao Jia",
      "Nadav Olmert",
      "David Sharon",
      "Matthew Mauger",
      "Sandeep Mariserla",
      "Hongxu Ma",
      "Megha Mohabey",
      "Kyuyeun Kim",
      "Alek Andreev",
      "Scott Pollom",
      "Juliette Love",
      "Vihan Jain",
      "Priyanka Agrawal",
      "Yannick Schroecker",
      "Alisa Fortin",
      "Manfred Warmuth",
      "Ji Liu",
      "Andrew Leach",
      "Irina Blok",
      "Ganesh Poomal Girirajan",
      "Roee Aharoni",
      "Benigno Uria",
      "Andrei Sozanschi",
      "Dan Goldberg",
      "Lucian Ionita",
      "Marco Tulio Ribeiro",
      "Martin Zlocha",
      "Vighnesh Birodkar",
      "Sami Lachgar",
      "Liangzhe Yuan",
      "Himadri Choudhury",
      "Matt Ginsberg",
      "Fei Zheng",
      "Gregory Dibb",
      "Emily Graves",
      "Swachhand Lokhande",
      "Gabriel Rasskin",
      "George-Cristian Muraru",
      "Corbin Quick",
      "Sandeep Tata",
      "Pierre Sermanet",
      "Aditya Chawla",
      "Itay Karo",
      "Yan Wang",
      "Susan Zhang",
      "Orgad Keller",
      "Anca Dragan",
      "Guolong Su",
      "Ian Chou",
      "Xi Liu",
      "Yiqing Tao",
      "Shruthi Prabhakara",
      "Marc Wilson",
      "Ruibo Liu",
      "Shibo Wang",
      "Georgie Evans",
      "David Du",
      "Alfonso Castao",
      "Gautam Prasad",
      "Mona El Mahdy",
      "Sebastian Gerlach",
      "Machel Reid",
      "Jarrod Kahn",
      "Amir Zait",
      "Thanumalayan Sankaranarayana Pillai",
      "Thatcher Ulrich",
      "Guanyu Wang",
      "Jan Wassenberg",
      "Efrat Farkash",
      "Kiran Yalasangi",
      "Congchao Wang",
      "Maria Bauza",
      "Simon Bucher",
      "Ting Liu",
      "Jun Yan",
      "Gary Leung",
      "Vikas Sindhwani",
      "Parker Barnes",
      "Avi Singh",
      "Ivan Jurin",
      "Jichuan Chang",
      "Niket Kumar Bhumihar",
      "Sivan Eiger",
      "Gui Citovsky",
      "Ben Withbroe",
      "Zhang Li",
      "Siyang Xue",
      "Niccol Dal Santo",
      "Georgi Stoyanov",
      "Yves Raimond",
      "Steven Zheng",
      "Yilin Gao",
      "Vt Listk",
      "Sawek Kwasiborski",
      "Rachel Saputro",
      "Adnan Ozturel",
      "Ganesh Mallya",
      "Kushal Majmundar",
      "Ross West",
      "Paul Caron",
      "Jinliang Wei",
      "Lluis Castrejon",
      "Sharad Vikram",
      "Deepak Ramachandran",
      "Nikhil Dhawan",
      "Jiho Park",
      "Sara Smoot",
      "George van den Driessche",
      "Yochai Blau",
      "Chase Malik",
      "Wei Liang",
      "Roy Hirsch",
      "Cicero Nogueira dos Santos",
      "Eugene Weinstein",
      "Aron van den Oord",
      "Sid Lall",
      "Nicholas FitzGerald",
      "Zixuan Jiang",
      "Xuan Yang",
      "Dale Webster",
      "Ali Elqursh",
      "Aedan Pope",
      "Georges Rotival",
      "David Raposo",
      "Wanzheng Zhu",
      "Jeff Dean",
      "Sami Alabed",
      "Dustin Tran",
      "Arushi Gupta",
      "Zach Gleicher",
      "Jessica Austin",
      "Edouard Rosseel",
      "Megh Umekar",
      "Dipanjan Das",
      "Yinghao Sun",
      "Kai Chen",
      "Karolis Misiunas",
      "Xiang Zhou",
      "Yixian Di",
      "Alyssa Loo",
      "Josh Newlan",
      "Bo Li",
      "Vinay Ramasesh",
      "Ying Xu",
      "Alex Chen",
      "Sudeep Gandhe",
      "Radu Soricut",
      "Nikita Gupta",
      "Shuguang Hu",
      "Seliem El-Sayed",
      "Xavier Garcia",
      "Idan Brusilovsky",
      "Pu-Chin Chen",
      "Andrew Bolt",
      "Lu Huang",
      "Alex Gurney",
      "Zhiying Zhang",
      "Alexander Pritzel",
      "Jarek Wilkiewicz",
      "Bryan Seybold",
      "Bhargav Kanagal Shamanna",
      "Felix Fischer",
      "Josef Dean",
      "Karan Gill",
      "Ross Mcilroy",
      "Abhishek Bhowmick",
      "Jeremy Selier",
      "Antoine Yang",
      "Derek Cheng",
      "Vladimir Magay",
      "Jie Tan",
      "Dhriti Varma",
      "Christian Walder",
      "Tomas Kocisky",
      "Ryo Nakashima",
      "Paul Natsev",
      "Mike Kwong",
      "Ionel Gog",
      "Chiyuan Zhang",
      "Sander Dieleman",
      "Thomas Jimma",
      "Andrey Ryabtsev",
      "Siddhartha Brahma",
      "David Steiner",
      "Dayou Du",
      "Ante uul",
      "Mislav ani",
      "Mukund Raghavachari",
      "Willi Gierke",
      "Zeyu Zheng",
      "Dessie Petrova",
      "Yann Dauphin",
      "Yuchuan Liu",
      "Ido Kessler",
      "Steven Hand",
      "Chris Duvarney",
      "Seokhwan Kim",
      "Hyo Lee",
      "Lonard Hussenot",
      "Jeffrey Hui",
      "Josh Smith",
      "Deepali Jain",
      "Jiawei Xia",
      "Gaurav Singh Tomar",
      "Keyvan Amiri",
      "Du Phan",
      "Fabian Fuchs",
      "Tobias Weyand",
      "Nenad Tomasev",
      "Alexandra Cordell",
      "Xin Liu",
      "Jonathan Mallinson",
      "Pankaj Joshi",
      "Andy Crawford",
      "Arun Suggala",
      "Steve Chien",
      "Nick Fernando",
      "Mariella Sanchez-Vargas",
      "Duncan Williams",
      "Phil Crone",
      "Xiyang Luo",
      "Igor Karpov",
      "Jyn Shan",
      "Terry Thurk",
      "Robin Strudel",
      "Paul Voigtlaender",
      "Piyush Patil",
      "Tim Dozat",
      "Ali Khodaei",
      "Sahil Singla",
      "Piotr Ambroszczyk",
      "Qiyin Wu",
      "Yifan Chang",
      "Brian Roark",
      "Chaitra Hegde",
      "Tianli Ding",
      "Angelos Filos",
      "Zhongru Wu",
      "Andr Susano Pinto",
      "Shuang Liu",
      "Saarthak Khanna",
      "Aditya Pandey",
      "Siobhan Mcloughlin",
      "Qiujia Li",
      "Sam Haves",
      "Allan Zhou",
      "Elena Buchatskaya",
      "Isabel Leal",
      "Peter de Boursac",
      "Nami Akazawa",
      "Nina Anderson",
      "Terry Chen",
      "Krishna Somandepalli",
      "Chen Liang",
      "Sheela Goenka",
      "Stephanie Winkler",
      "Alexander Grushetsky",
      "Yifan Ding",
      "Jamie Smith",
      "Fan Ye",
      "Jordi Pont-Tuset",
      "Eric Li",
      "Ruichao Li",
      "Tomer Golany",
      "Dawid Wegner",
      "Tao Jiang",
      "Omer Barak",
      "Yuan Shangguan",
      "Eszter Vrtes",
      "Renee Wong",
      "Jrg Bornschein",
      "Alex Tudor",
      "Michele Bevilacqua",
      "Tom Schaul",
      "Ankit Singh Rawat",
      "Yang Zhao",
      "Kyriakos Axiotis",
      "Lei Meng",
      "Cory McLean",
      "Jonathan Lai",
      "Jennifer Beattie",
      "Nate Kushman",
      "Yaxin Liu",
      "Blair Kutzman",
      "Fiona Lang",
      "Jingchen Ye",
      "Praneeth Netrapalli",
      "Pushkar Mishra",
      "Myriam Khan",
      "Megha Goel",
      "Rob Willoughby",
      "David Tian",
      "Honglei Zhuang",
      "JD Chen",
      "Zak Tsai",
      "Tasos Kementsietsidis",
      "Arjun Khare",
      "James Keeling",
      "Keyang Xu",
      "Nathan Waters",
      "Florent Altch",
      "Ashok Popat",
      "Bhavishya Mittal",
      "David Saxton",
      "Dalia El Badawy",
      "Michael Mathieu",
      "Zheng Zheng",
      "Hao Zhou",
      "Nishant Ranka",
      "Richard Shin",
      "Qingnan Duan",
      "Tim Salimans",
      "Ioana Mihailescu",
      "Uri Shaham",
      "Ming-Wei Chang",
      "Yannis Assael",
      "Nishanth Dikkala",
      "Martin Izzard",
      "Vincent Cohen-Addad",
      "Cat Graves",
      "Vlad Feinberg",
      "Grace Chung",
      "DJ Strouse",
      "Danny Karmon",
      "Sahand Sharifzadeh",
      "Zoe Ashwood",
      "Khiem Pham",
      "Jon Blanton",
      "Alex Vasiloff",
      "Jarred Barber",
      "Mark Geller",
      "Aurick Zhou",
      "Fedir Zubach",
      "Tzu-Kuo Huang",
      "Lei Zhang",
      "Himanshu Gupta",
      "Matt Young",
      "Julia Proskurnia",
      "Ronny Votel",
      "Valentin Gabeur",
      "Gabriel Barcik",
      "Aditya Tripathi",
      "Hongkun Yu",
      "Geng Yan",
      "Beer Changpinyo",
      "Filip Paveti",
      "Amy Coyle",
      "Yasuhisa Fujii",
      "Jorge Gonzalez Mendez",
      "Tianhao Zhou",
      "Harish Rajamani",
      "Blake Hechtman",
      "Eddie Cao",
      "Da-Cheng Juan",
      "Yi-Xuan Tan",
      "Valentin Dalibard",
      "Yilun Du",
      "Natalie Clay",
      "Kaisheng Yao",
      "Wenhao Jia",
      "Dimple Vijaykumar",
      "Yuxiang Zhou",
      "Xinyi Bai",
      "Wei-Chih Hung",
      "Steven Pecht",
      "Georgi Todorov",
      "Nikhil Khadke",
      "Pramod Gupta",
      "Preethi Lahoti",
      "Arnaud Autef",
      "Karthik Duddu",
      "James Lee-Thorp",
      "Alexander Bykovsky",
      "Tautvydas Misiunas",
      "Sebastian Flennerhag",
      "Santhosh Thangaraj",
      "Jed McGiffin",
      "Zack Nado",
      "Markus Kunesch",
      "Andreas Noever",
      "Amir Hertz",
      "Marco Liang",
      "Victor Stone",
      "Evan Palmer",
      "Samira Daruki",
      "Arijit Pramanik",
      "Siim Pder",
      "Austin Kyker",
      "Mina Khan",
      "Evgeny Sluzhaev",
      "Marvin Ritter",
      "Avraham Ruderman",
      "Wenlei Zhou",
      "Chirag Nagpal",
      "Kiran Vodrahalli",
      "George Necula",
      "Paul Barham",
      "Ellie Pavlick",
      "Jay Hartford",
      "Izhak Shafran",
      "Long Zhao",
      "Maciej Mikua",
      "Tom Eccles",
      "Hidetoshi Shimokawa",
      "Kanav Garg",
      "Luke Vilnis",
      "Hanwen Chen",
      "Ilia Shumailov",
      "Kuang-Huei Lee",
      "Abdelrahman Abdelhamed",
      "Meiyan Xie",
      "Vered Cohen",
      "Ester Hlavnova",
      "Dan Malkin",
      "Chawin Sitawarin",
      "James Lottes",
      "Pauline Coquinot",
      "Tianli Yu",
      "Sandeep Kumar",
      "Jingwei Zhang",
      "Aroma Mahendru",
      "Zafarali Ahmed",
      "James Martens",
      "Tao Chen",
      "Aviel Boag",
      "Daiyi Peng",
      "Coline Devin",
      "Arseniy Klimovskiy",
      "Mary Phuong",
      "Danny Vainstein",
      "Jin Xie",
      "Bhuvana Ramabhadran",
      "Nathan Howard",
      "Xinxin Yu",
      "Gitartha Goswami",
      "Jingyu Cui",
      "Sam Shleifer",
      "Mario Pinto",
      "Chih-Kuan Yeh",
      "Ming-Hsuan Yang",
      "Sara Javanmardi",
      "Dan Ethier",
      "Chace Lee",
      "Jordi Orbay",
      "Suyog Kotecha",
      "Carla Bromberg",
      "Pete Shaw",
      "James Thornton",
      "Adi Gerzi Rosenthal",
      "Shane Gu",
      "Matt Thomas",
      "Ian Gemp",
      "Aditya Ayyar",
      "Asahi Ushio",
      "Aarush Selvan",
      "Joel Wee",
      "Chenxi Liu",
      "Maryam Majzoubi",
      "Weiren Yu",
      "Jake Abernethy",
      "Tyler Liechty",
      "Renke Pan",
      "Hoang Nguyen",
      " Qiong",
      " Hu",
      "Sarah Perrin",
      "Abhinav Arora",
      "Emily Pitler",
      "Weiyi Wang",
      "Kaushik Shivakumar",
      "Flavien Prost",
      "Ben Limonchik",
      "Jing Wang",
      "Yi Gao",
      "Timothee Cour",
      "Shyamal Buch",
      "Huan Gui",
      "Maria Ivanova",
      "Philipp Neubeck",
      "Kelvin Chan",
      "Lucy Kim",
      "Huizhong Chen",
      "Naman Goyal",
      "Da-Woon Chung",
      "Lu Liu",
      "Yao Su",
      "Anastasia Petrushkina",
      "Jiajun Shen",
      "Armand Joulin",
      "Yuanzhong Xu",
      "Stein Xudong Lin",
      "Yana Kulizhskaya",
      "Ciprian Chelba",
      "Shobha Vasudevan",
      "Eli Collins",
      "Vasilisa Bashlovkina",
      "Tony Lu",
      "Doug Fritz",
      "Jongbin Park",
      "Yanqi Zhou",
      "Chen Su",
      "Richard Tanburn",
      "Mikhail Sushkov",
      "Mitchelle Rasquinha",
      "Jinning Li",
      "Jennifer Prendki",
      "Yiming Li",
      "Pallavi LV",
      "Shriya Sharma",
      "Hen Fitoussi",
      "Hui Huang",
      "Andrew Dai",
      "Phuong Dao",
      "Mike Burrows",
      "Henry Prior",
      "Danfeng Qin",
      "Golan Pundak",
      "Lars Lowe Sjoesund",
      "Art Khurshudov",
      "Zhenkai Zhu",
      "Albert Webson",
      "Elizabeth Kemp",
      "Tat Tan",
      "Saurabh Agrawal",
      "Susie Sargsyan",
      "Liqun Cheng",
      "Jim Stephan",
      "Tom Kwiatkowski",
      "David Reid",
      "Arunkumar Byravan",
      "Assaf Hurwitz Michaely",
      "Nicolas Heess",
      "Luowei Zhou",
      "Sonam Goenka",
      "Viral Carpenter",
      "Anselm Levskaya",
      "Bo Wang",
      "Reed Roberts",
      "Rmi Leblond",
      "Sharat Chikkerur",
      "Stav Ginzburg",
      "Max Chang",
      "Robert Riachi",
      " Chuqiao",
      " Xu",
      "Zaln Borsos",
      "Michael Pliskin",
      "Julia Pawar",
      "Morgane Lustman",
      "Hannah Kirkwood",
      "Ankit Anand",
      "Aditi Chaudhary",
      "Norbert Kalb",
      "Kieran Milan",
      "Sean Augenstein",
      "Anna Goldie",
      "Laurel Prince",
      "Karthik Raman",
      "Yanhua Sun",
      "Vivian Xia",
      "Aaron Cohen",
      "Zhouyuan Huo",
      "Josh Camp",
      "Seher Ellis",
      "Lukas Zilka",
      "David Vilar Torres",
      "Lisa Patel",
      "Sho Arora",
      "Betty Chan",
      "Jonas Adler",
      "Kareem Ayoub",
      "Jacky Liang",
      "Fayaz Jamil",
      "Jiepu Jiang",
      "Simon Baumgartner",
      "Haitian Sun",
      "Yael Karov",
      "Yaroslav Akulov",
      "Hui Zheng",
      "Irene Cai",
      "Claudio Fantacci",
      "James Rubin",
      "Alex Rav Acha",
      "Mengchao Wang",
      "Nina D'Souza",
      "Rohit Sathyanarayana",
      "Shengyang Dai",
      "Simon Rowe",
      "Andrey Simanovsky",
      "Omer Goldman",
      "Yuheng Kuang",
      "Xiaoyue Pan",
      "Andrew Rosenberg",
      "Tania Rojas-Esponda",
      "Praneet Dutta",
      "Amy Zeng",
      "Irina Jurenka",
      "Greg Farquhar",
      "Yamini Bansal",
      "Shariq Iqbal",
      "Becca Roelofs",
      "Ga-Young Joung",
      "Parker Beak",
      "Changwan Ryu",
      "Ryan Poplin",
      "Yan Wu",
      "Jean-Baptiste Alayrac",
      "Senaka Buthpitiya",
      "Olaf Ronneberger",
      "Caleb Habtegebriel",
      "Wei Li",
      "Paul Cavallaro",
      "Aurora Wei",
      "Guy Bensky",
      "Timo Denk",
      "Harish Ganapathy",
      "Jeff Stanway",
      "Pratik Joshi",
      "Francesco Bertolini",
      "Jessica Lo",
      "Olivia Ma",
      "Zachary Charles",
      "Geta Sampemane",
      "Himanshu Sahni",
      "Xu Chen",
      "Harry Askham",
      "David Gaddy",
      "Peter Young",
      "Jiewen Tan",
      "Matan Eyal",
      "Arthur Brainskas",
      "Li Zhong",
      "Zhichun Wu",
      "Mark Epstein",
      "Kai Bailey",
      "Andrew Hard",
      "Kamyu Lee",
      "Sasha Goldshtein",
      "Alex Ruiz",
      "Mohammed Badawi",
      "Matthias Lochbrunner",
      "JK Kearns",
      "Ashley Brown",
      "Fabio Pardo",
      "Theophane Weber",
      "Haichuan Yang",
      "Pan-Pan Jiang",
      "Berkin Akin",
      "Zhao Fu",
      "Marcus Wainwright",
      "Chi Zou",
      "Meenu Gaba",
      "Pierre-Antoine Manzagol",
      "Wendy Kan",
      "Yang Song",
      "Karina Zainullina",
      "Rui Lin",
      "Jeongwoo Ko",
      "Salil Deshmukh",
      "Apoorv Jindal",
      "James Svensson",
      "Divya Tyam",
      "Heri Zhao",
      "Christine Kaeser-Chen",
      "Scott Baird",
      "Pooya Moradi",
      "Jamie Hall",
      "Qiuchen Guo",
      "Vincent Tsang",
      "Bowen Liang",
      "Fernando Pereira",
      "Suhas Ganesh",
      "Ivan Korotkov",
      "Jakub Adamek",
      "Sridhar Thiagarajan",
      "Vinh Tran",
      "Charles Chen",
      "Chris Tar",
      "Sanil Jain",
      "Ishita Dasgupta",
      "Taylan Bilal",
      "David Reitter",
      "Kai Zhao",
      "Giulia Vezzani",
      "Yasmin Gehman",
      "Pulkit Mehta",
      "Lauren Beltrone",
      "Xerxes Dotiwalla",
      "Sergio Guadarrama",
      "Zaheer Abbas",
      "Stefani Karp",
      "Petko Georgiev",
      "Chun-Sung Ferng",
      "Marc Brockschmidt",
      "Liqian Peng",
      "Christoph Hirnschall",
      "Vikas Verma",
      "Yingying Bi",
      "Ying Xiao",
      "Avigail Dabush",
      "Kelvin Xu",
      "Phil Wallis",
      "Randall Parker",
      "Qifei Wang",
      "Yang Xu",
      "Ilkin Safarli",
      "Dinesh Tewari",
      "Yin Zhang",
      "Seungyeon Kim",
      "Andrea Gesmundo",
      "Mackenzie Thomas",
      "Sergey Levi",
      "Ahmed Chowdhury",
      "Kanishka Rao",
      "Peter Garst",
      "Sam Conway-Rahman",
      "Helen Ran",
      "Kay McKinney",
      "Zhisheng Xiao",
      "Wenhao Yu",
      "Rohan Agrawal",
      "Axel Stjerngren",
      "Catalin Ionescu",
      "Jingjing Chen",
      "Vivek Sharma",
      "Justin Chiu",
      "Fei Liu",
      "Ken Franko",
      "Clayton Sanford",
      "Xingyu Cai",
      "Paul Michel",
      "Sanjay Ganapathy",
      "Jane Labanowski",
      "Zachary Garrett",
      "Ben Vargas",
      "Sean Sun",
      "Bryan Gale",
      "Thomas Buschmann",
      "Guillaume Desjardins",
      "Nimesh Ghelani",
      "Palak Jain",
      "Mudit Verma",
      "Chulayuth Asawaroengchai",
      "Julian Eisenschlos",
      "Jitendra Harlalka",
      "Hideto Kazawa",
      "Don Metzler",
      "Joshua Howland",
      "Ying Jian",
      "Jake Ades",
      "Viral Shah",
      "Tynan Gangwani",
      "Seungji Lee",
      "Roman Ring",
      "Steven M. Hernandez",
      "Dean Reich",
      "Amer Sinha",
      "Ashutosh Sathe",
      "Joe Kovac",
      "Ashleah Gill",
      "Ajay Kannan",
      "Andrea D'olimpio",
      "Martin Sevenich",
      "Jay Whang",
      "Been Kim",
      "Khe Chai Sim",
      "Jilin Chen",
      "Jiageng Zhang",
      "Shuba Lall",
      "Yossi Matias",
      "Bill Jia",
      "Abe Friesen",
      "Sara Nasso",
      "Ashish Thapliyal",
      "Bryan Perozzi",
      "Ting Yu",
      "Anna Shekhawat",
      "Safeen Huda",
      "Peter Grabowski",
      "Eric Wang",
      "Ashwin Sreevatsa",
      "Hilal Dib",
      "Mehadi Hassen",
      "Parker Schuh",
      "Vedrana Milutinovic",
      "Chris Welty",
      "Michael Quinn",
      "Ali Shah",
      "Bangju Wang",
      "Gabe Barth-Maron",
      "Justin Frye",
      "Natalie Axelsson",
      "Tao Zhu",
      "Yukun Ma",
      "Irene Giannoumis",
      "Hanie Sedghi",
      "Chang Ye",
      "Yi Luan",
      "Kevin Aydin",
      "Bilva Chandra",
      "Vivek Sampathkumar",
      "Ronny Huang",
      "Victor Lavrenko",
      "Ahmed Eleryan",
      "Zhi Hong",
      "Steven Hansen",
      "Sara Mc Carthy",
      "Bidisha Samanta",
      "Domagoj evid",
      "Xin Wang",
      "Fangtao Li",
      "Michael Voznesensky",
      "Matt Hoffman",
      "Andreas Terzis",
      "Vikash Sehwag",
      "Gil Fidel",
      "Luheng He",
      "Mu Cai",
      "Yanzhang He",
      "Alex Feng",
      "Martin Nikoltchev",
      "Samrat Phatale",
      "Jason Chase",
      "Rory Lawton",
      "Ming Zhang",
      "Tom Ouyang",
      "Manuel Tragut",
      "Mehdi Hafezi Manshadi",
      "Arjun Narayanan",
      "Jiaming Shen",
      "Xu Gao",
      "Tolga Bolukbasi",
      "Nick Roy",
      "Xin Li",
      "Daniel Golovin",
      "Liviu Panait",
      "Zhen Qin",
      "Guangxing Han",
      "Thomas Anthony",
      "Sneha Kudugunta",
      "Viorica Patraucean",
      "Aniket Ray",
      "Xinyun Chen",
      "Xiaochen Yang",
      "Tanuj Bhatia",
      "Pranav Talluri",
      "Alex Morris",
      "Andrija Ranatovi",
      "Bethanie Brownfield",
      "James An",
      "Sheng Peng",
      "Patrick Kane",
      "Ce Zheng",
      "Nico Duduta",
      "Joshua Kessinger",
      "James Noraky",
      "Siqi Liu",
      "Keran Rong",
      "Petar Velikovi",
      "Keith Rush",
      "Alex Goldin",
      "Fanny Wei",
      "Shiva Mohan Reddy Garlapati",
      "Caroline Pantofaru",
      "Okwan Kwon",
      "Jianmo Ni",
      "Eric Noland",
      "Julia Di Trapani",
      "Franoise Beaufays",
      "Abhijit Guha Roy",
      "Yinlam Chow",
      "Aybuke Turker",
      "Geoffrey Cideron",
      "Lantao Mei",
      "Jon Clark",
      "Qingyun Dou",
      "Matko Bonjak",
      "Ralph Leith",
      "Yuqing Du",
      "Amir Yazdanbakhsh",
      "Milad Nasr",
      "Chester Kwak",
      "Suraj Satishkumar Sheth",
      "Alex Kaskasoli",
      "Ankesh Anand",
      "Balaji Lakshminarayanan",
      "Sammy Jerome",
      "David Bieber",
      "Chun-Te Chu",
      "Alexandre Senges",
      "Tianxiao Shen",
      "Mukund Sridhar",
      "Ndaba Ndebele",
      "Benjamin Beyret",
      "Shakir Mohamed",
      "Mia Chen",
      "Markus Freitag",
      "Jiaxian Guo",
      "Luyang Liu",
      "Paul Roit",
      "Heng Chen",
      "Shen Yan",
      "Tom Stone",
      "JD Co-Reyes",
      "Jeremy Cole",
      "Salvatore Scellato",
      "Shekoofeh Azizi",
      "Hadi Hashemi",
      "Alicia Jin",
      "Anand Iyer",
      "Marcella Valentine",
      "Andrs Gyrgy",
      "Arun Ahuja",
      "Daniel Hernandez Diaz",
      "Chen-Yu Lee",
      "Nathan Clement",
      "Weize Kong",
      "Drew Garmon",
      "Ishaan Watts",
      "Kush Bhatia",
      "Khyatti Gupta",
      "Matt Miecnikowski",
      "Hugo Vallet",
      "Ankur Taly",
      "Edward Loper",
      "Saket Joshi",
      "James Atwood",
      "Jo Chick",
      "Mark Collier",
      "Fotis Iliopoulos",
      "Ryan Trostle",
      "Beliz Gunel",
      "Ramiro Leal-Cavazos",
      "Arnar Mar Hrafnkelsson",
      "Michael Guzman",
      "Xiaoen Ju",
      "Andy Forbes",
      "Jesse Emond",
      "Kushal Chauhan",
      "Ben Caine",
      "Li Xiao",
      "Wenjun Zeng",
      "Alexandre Moufarek",
      "Daniel Murphy",
      "Maya Meng",
      "Nitish Gupta",
      "Felix Riedel",
      "Anil Das",
      "Elijah Lawal",
      "Shashi Narayan",
      "Tiberiu Sosea",
      "James Swirhun",
      "Linda Friso",
      "Behnam Neyshabur",
      "Jing Lu",
      "Sertan Girgin",
      "Michael Wunder",
      "Edouard Yvinec",
      "Aroonalok Pyne",
      "Victor Carbune",
      "Shruti Rijhwani",
      "Yang Guo",
      "Tulsee Doshi",
      "Anton Briukhov",
      "Max Bain",
      "Ayal Hitron",
      "Xuanhui Wang",
      "Ashish Gupta",
      "Ke Chen",
      "Cosmo Du",
      "Weiyang Zhang",
      "Dhruv Shah",
      "Arjun Akula",
      "Max Dylla",
      "Ashyana Kachra",
      "Weicheng Kuo",
      "Tingting Zou",
      "Lily Wang",
      "Luyao Xu",
      "Jifan Zhu",
      "Justin Snyder",
      "Sachit Menon",
      "Orhan Firat",
      "Igor Mordatch",
      "Yuan Yuan",
      "Natalia Ponomareva",
      "Rory Blevins",
      "Lawrence Moore",
      "Weijun Wang",
      "Phil Chen",
      "Martin Scholz",
      "Artur Dwornik",
      "Jason Lin",
      "Sicheng Li",
      "Diego Antognini",
      "Te I",
      "Xiaodan Song",
      "Matt Miller",
      "Uday Kalra",
      "Adam Raveret",
      "Oscar Akerlund",
      "Felix Wu",
      "Andrew Nystrom",
      "Namrata Godbole",
      "Tianqi Liu",
      "Hannah DeBalsi",
      "Jewel Zhao",
      "Buhuang Liu",
      "Avi Caciularu",
      "Lauren Lax",
      "Urvashi Khandelwal",
      "Victoria Langston",
      "Eric Bailey",
      "Silvio Lattanzi",
      "Yufei Wang",
      "Neel Kovelamudi",
      "Sneha Mondal",
      "Guru Guruganesh",
      "Nan Hua",
      "Ofir Roval",
      "Pawe Wesoowski",
      "Rishikesh Ingale",
      "Jonathan Halcrow",
      "Tim Sohn",
      "Christof Angermueller",
      "Bahram Raad",
      "Eli Stickgold",
      "Eva Lu",
      "Alec Kosik",
      "Jing Xie",
      "Timothy Lillicrap",
      "Austin Huang",
      "Lydia Lihui Zhang",
      "Dominik Paulus",
      "Clement Farabet",
      "Alex Wertheim",
      "Bing Wang",
      "Rishabh Joshi",
      "Chu-ling Ko",
      "Yonghui Wu",
      "Shubham Agrawal",
      "Lily Lin",
      "XiangHai Sheng",
      "Peter Sung",
      "Tyler Breland-King",
      "Christina Butterfield",
      "Swapnil Gawde",
      "Sumeet Singh",
      "Qiao Zhang",
      "Raj Apte",
      "Shilpa Shetty",
      "Adrian Hutter",
      "Tao Li",
      "Elizabeth Salesky",
      "Federico Lebron",
      "Jonni Kanerva",
      "Michela Paganini",
      "Arthur Nguyen",
      "Rohith Vallu",
      "Jan-Thorsten Peter",
      "Sarmishta Velury",
      "David Kao",
      "Jay Hoover",
      "Anna Bortsova",
      "Colton Bishop",
      "Shoshana Jakobovits",
      "Alessandro Agostini",
      "Alekh Agarwal",
      "Chang Liu",
      "Charles Kwong",
      "Sasan Tavakkol",
      "Ioana Bica",
      "Alex Greve",
      "Anirudh GP",
      "Jake Marcus",
      "Le Hou",
      "Tom Duerig",
      "Rivka Moroshko",
      "Dave Lacey",
      "Andy Davis",
      "Julien Amelot",
      "Guohui Wang",
      "Frank Kim",
      "Theofilos Strinopoulos",
      "Hui Wan",
      "Charline Le Lan",
      "Shankar Krishnan",
      "Haotian Tang",
      "Peter Humphreys",
      "Junwen Bai",
      "Idan Heimlich Shtacher",
      "Diego Machado",
      "Chenxi Pang",
      "Ken Burke",
      "Dangyi Liu",
      "Renga Aravamudhan",
      "Yue Song",
      "Ed Hirst",
      "Abhimanyu Singh",
      "Brendan Jou",
      "Liang Bai",
      "Francesco Piccinno",
      "Chuyuan Kelly Fu",
      "Robin Alazard",
      "Barak Meiri",
      "Daniel Winter",
      "Charlie Chen",
      "Mingda Zhang",
      "Jens Heitkaemper",
      "John Lambert",
      "Jinhyuk Lee",
      "Alexander Frmmgen",
      "Sergey Rogulenko",
      "Pranav Nair",
      "Paul Niemczyk",
      "Anton Bulyenov",
      "Bibo Xu",
      "Hadar Shemtov",
      "Morteza Zadimoghaddam",
      "Serge Toropov",
      "Mateo Wirth",
      "Hanjun Dai",
      "Sreenivas Gollapudi",
      "Daniel Zheng",
      "Alex Kurakin",
      "Chansoo Lee",
      "Kalesha Bullard",
      "Nicolas Serrano",
      "Ivana Balazevic",
      "Yang Li",
      "Johan Schalkwyk",
      "Mark Murphy",
      "Mingyang Zhang",
      "Kevin Sequeira",
      "Romina Datta",
      "Nishant Agrawal",
      "Charles Sutton",
      "Nithya Attaluri",
      "Mencher Chiang",
      "Wael Farhan",
      "Gregory Thornton",
      "Kate Lin",
      "Travis Choma",
      "Hung Nguyen",
      "Kingshuk Dasgupta",
      "Dirk Robinson",
      "Iulia Coma",
      "Michael Riley",
      "Arjun Pillai",
      "Basil Mustafa",
      "Ben Golan",
      "Amir Zandieh",
      "Jean-Baptiste Lespiau",
      "Billy Porter",
      "David Ross",
      "Sujeevan Rajayogam",
      "Mohit Agarwal",
      "Subhashini Venugopalan",
      "Bobak Shahriari",
      "Qiqi Yan",
      "Hao Xu",
      "Taylor Tobin",
      "Pavel Dubov",
      "Hongzhi Shi",
      "Adri Recasens",
      "Anton Kovsharov",
      "Sebastian Borgeaud",
      "Lucio Dery",
      "Shanthal Vasanth",
      "Elena Gribovskaya",
      "Linhai Qiu",
      "Mahdis Mahdieh",
      "Wojtek Skut",
      "Elizabeth Nielsen",
      "CJ Zheng",
      "Adams Yu",
      "Carrie Grimes Bostock",
      "Shaleen Gupta",
      "Aaron Archer",
      "Chris Rawles",
      "Elinor Davies",
      "Alexey Svyatkovskiy",
      "Tomy Tsai",
      "Yoni Halpern",
      "Christian Reisswig",
      "Bartek Wydrowski",
      "Bo Chang",
      "Joan Puigcerver",
      "Mor Hazan Taege",
      "Jian Li",
      "Eva Schnider",
      "Xinjian Li",
      "Dragos Dena",
      "Yunhan Xu",
      "Umesh Telang",
      "Tianze Shi",
      "Heiga Zen",
      "Kyle Kastner",
      "Yeongil Ko",
      "Neesha Subramaniam",
      "Aviral Kumar",
      "Pete Blois",
      "Zhuyun Dai",
      "John Wieting",
      "Yifeng Lu",
      "Yoel Zeldes",
      "Tian Xie",
      "Anja Hauth",
      "Alexandru ifrea",
      "Yuqi Li",
      "Sam El-Husseini",
      "Dan Abolafia",
      "Howard Zhou",
      "Wen Ding",
      "Sahra Ghalebikesabi",
      "Carlos Gua",
      "Andrii Maksai",
      "goston Weisz",
      "Sercan Arik",
      "Nick Sukhanov",
      "Aga wietlik",
      "Xuhui Jia",
      "Luo Yu",
      "Weiyue Wang",
      "Mark Brand",
      "Dawn Bloxwich",
      "Sean Kirmani",
      "Zhe Chen",
      "Alec Go",
      "Pablo Sprechmann",
      "Nithish Kannen",
      "Alen Carin",
      "Paramjit Sandhu",
      "Isabel Edkins",
      "Leslie Nooteboom",
      "Jai Gupta",
      "Loren Maggiore",
      "Javad Azizi",
      "Yael Pritch",
      "Pengcheng Yin",
      "Mansi Gupta",
      "Danny Tarlow",
      "Duncan Smith",
      "Desi Ivanov",
      "Mohammad Babaeizadeh",
      "Ankita Goel",
      "Satish Kambala",
      "Grace Chu",
      "Matej Kastelic",
      "Michelle Liu",
      "Hagen Soltau",
      "Austin Stone",
      "Shivani Agrawal",
      "Min Kim",
      "Kedar Soparkar",
      "Srinivas Tadepalli",
      "Oskar Bunyan",
      "Rachel Soh",
      "Arvind Kannan",
      "DY Kim",
      "Blake JianHang Chen",
      "Afief Halumi",
      "Sudeshna Roy",
      "Yulong Wang",
      "Olcan Sercinoglu",
      "Gena Gibson",
      "Sijal Bhatnagar",
      "Motoki Sano",
      "Daniel von Dincklage",
      "Qingchun Ren",
      "Blagoj Mitrevski",
      "Mirek Olk",
      "Jennifer She",
      "Carl Doersch",
      " Jilei",
      " Wang",
      "Bingyuan Liu",
      "Qijun Tan",
      "Tamar Yakar",
      "Tris Warkentin",
      "Alex Ramirez",
      "Carl Lebsack",
      "Josh Dillon",
      "Rajiv Mathews",
      "Tom Cobley",
      "Zelin Wu",
      "Zhuoyuan Chen",
      "Jon Simon",
      "Swaroop Nath",
      "Tara Sainath",
      "Alexei Bendebury",
      "Ryan Julian",
      "Bharath Mankalale",
      "Daria urko",
      "Paulo Zacchello",
      "Adam R. Brown",
      "Kiranbir Sodhia",
      "Heidi Howard",
      "Sergi Caelles",
      "Abhinav Gupta",
      "Gareth Evans",
      "Anna Bulanova",
      "Lesley Katzen",
      "Roman Goldenberg",
      "Anton Tsitsulin",
      "Joe Stanton",
      "Benoit Schillings",
      "Vitaly Kovalev",
      "Corey Fry",
      "Rushin Shah",
      "Kuo Lin",
      "Shyam Upadhyay",
      "Cheng Li",
      "Soroush Radpour",
      "Marcello Maggioni",
      "Jing Xiong",
      "Lukas Haas",
      "Jenny Brennan",
      "Aishwarya Kamath",
      "Nikolay Savinov",
      "Arsha Nagrani",
      "Trevor Yacovone",
      "Ryan Kappedal",
      "Kostas Andriopoulos",
      "Li Lao",
      "YaGuang Li",
      "Grigory Rozhdestvenskiy",
      "Kazuma Hashimoto",
      "Andrew Audibert",
      "Sophia Austin",
      "Daniel Rodriguez",
      "Anian Ruoss",
      "Garrett Honke",
      "Deep Karkhanis",
      "Xi Xiong",
      "Qing Wei",
      "James Huang",
      "Zhaoqi Leng",
      "Vittal Premachandran",
      "Stan Bileschi",
      "Georgios Evangelopoulos",
      "Thomas Mensink",
      "Jay Pavagadhi",
      "Denis Teplyashin",
      "Paul Chang",
      "Linting Xue",
      "Garrett Tanzer",
      "Sally Goldman",
      "Kaushal Patel",
      "Shixin Li",
      "Jeremy Wiesner",
      "Ivy Zheng",
      "Ian Stewart-Binks",
      "Jie Han",
      "Zhi Li",
      "Liangchen Luo",
      "Karel Lenc",
      "Mario Lui",
      "Fuzhao Xue",
      "Ryan Mullins",
      "Alexey Guseynov",
      "Chung-Ching Chang",
      "Isaac Galatzer-Levy",
      "Adam Zhang",
      "Garrett Bingham",
      "Grace Hu",
      "Ale Hartman",
      "Yue Ma",
      "Jordan Griffith",
      "Alex Irpan",
      "Carey Radebaugh",
      "Summer Yue",
      "Lijie Fan",
      "Victor Ungureanu",
      "Christina Sorokin",
      "Hannah Teufel",
      "Peiran Li",
      "Rohan Anil",
      "Dimitris Paparas",
      "Todd Wang",
      "Chu-Cheng Lin",
      "Hui Peng",
      "Megan Shum",
      "Goran Petrovic",
      "Demetra Brady",
      "Richard Nguyen",
      "Klaus Macherey",
      "Zhihao Li",
      "Harman Singh",
      "Madhavi Yenugula",
      "Mariko Iinuma",
      "Xinyi Chen",
      "Kavya Kopparapu",
      "Alexey Stern",
      "Shachi Dave",
      "Chandu Thekkath",
      "Florence Perot",
      "Anurag Kumar",
      "Fangda Li",
      "Yang Xiao",
      "Matthew Bilotti",
      "Mohammad Hossein Bateni",
      "Isaac Noble",
      "Lisa Lee",
      "Amelio Vzquez-Reina",
      "Julian Salazar",
      "Xiaomeng Yang",
      "Boyu Wang",
      "Ela Gruzewska",
      "Anand Rao",
      "Sindhu Raghuram",
      "Zheng Xu",
      "Eyal Ben-David",
      "Jieru Mei",
      "Sid Dalmia",
      "Zhaoyi Zhang",
      "Yuchen Liu",
      "Gagan Bansal",
      "Helena Pankov",
      "Steven Schwarcz",
      "Andrea Burns",
      "Christine Chan",
      "Sumit Sanghai",
      "Ricky Liang",
      "Ethan Liang",
      "Antoine He",
      "Amy Stuart",
      "Arun Narayanan",
      "Yukun Zhu",
      "Christian Frank",
      "Bahar Fatemi",
      "Amit Sabne",
      "Oran Lang",
      "Indro Bhattacharya",
      "Shane Settle",
      "Maria Wang",
      "Brendan McMahan",
      "Andrea Tacchetti",
      "Livio Baldini Soares",
      "Majid Hadian",
      "Serkan Cabi",
      "Timothy Chung",
      "Nikita Putikhin",
      "Gang Li",
      "Jeremy Chen",
      "Austin Tarango",
      "Henryk Michalewski",
      "Mehran Kazemi",
      "Hussain Masoom",
      "Hila Sheftel",
      "Rakesh Shivanna",
      "Archita Vadali",
      "Ramona Comanescu",
      "Doug Reid",
      "Joss Moore",
      "Arvind Neelakantan",
      "Michal Sander",
      "Jonathan Herzig",
      "Aviv Rosenberg",
      "Mostafa Dehghani",
      "JD Choi",
      "Michael Fink",
      "Reid Hayes",
      "Eric Ge",
      "Shitao Weng",
      "Chia-Hua Ho",
      "John Karro",
      "Kalpesh Krishna",
      "Lam Nguyen Thiet",
      "Amy Skerry-Ryan",
      "Daniel Eppens",
      "Marco Andreetto",
      "Navin Sarma",
      "Silvano Bonacina",
      "Burcu Karagol Ayan",
      "Megha Nawhal",
      "Zhihao Shan",
      "Mike Dusenberry",
      "Shantanu Thakoor",
      "Sagar Gubbi",
      "Duc Dung Nguyen",
      "Reut Tsarfaty",
      "Samuel Albanie",
      "Jovana Mitrovi",
      "Meet Gandhi",
      "Bo-Juen Chen",
      "Alessandro Epasto",
      "Georgi Stephanov",
      "Ye Jin",
      "Samuel Gehman",
      "Aida Amini",
      "Jack Weber",
      "Feryal Behbahani",
      "Shawn Xu",
      "Miltos Allamanis",
      "Xi Chen",
      "Myle Ott",
      "Claire Sha",
      "Michal Jastrzebski",
      "Hang Qi",
      "David Greene",
      "Xinyi Wu",
      "Abodunrinwa Toki",
      "Daniel Vlasic",
      "Jane Shapiro",
      "Ragha Kotikalapudi",
      "Zhe Shen",
      "Takaaki Saeki",
      "Sirui Xie",
      "Albin Cassirer",
      "Shikhar Bharadwaj",
      "Tatsuya Kiyono",
      "Srinadh Bhojanapalli",
      "Elan Rosenfeld",
      "Sam Ritter",
      "Jieming Mao",
      "Joo Gabriel Oliveira",
      "Zoltan Egyed",
      "Bernd Bandemer",
      "Emilio Parisotto",
      "Keisuke Kinoshita",
      "Juliette Pluto",
      "Petros Maniatis",
      "Steve Li",
      "Yaohui Guo",
      "Golnaz Ghiasi",
      "Jean Tarbouriech",
      "Srimon Chatterjee",
      "Julie Jin",
      " Katrina",
      " Xu",
      "Jennimaria Palomaki",
      "Sb Arnold",
      "Madhavi Sewak",
      "Federico Piccinini",
      "Mohit Sharma",
      "Ben Albrecht",
      "Sean Purser-haskell",
      "Ashwin Vaswani",
      "Chongyan Chen",
      "Matheus Wisniewski",
      "Qin Cao",
      "John Aslanides",
      "Nguyet Minh Phu",
      "Maximilian Sieb",
      "Lauren Agubuzu",
      "Anne Zheng",
      "Daniel Sohn",
      "Marco Selvi",
      "Anders Andreassen",
      "Krishan Subudhi",
      "Prem Eruvbetine",
      "Oliver Woodman",
      "Tomas Mery",
      "Sebastian Krause",
      "Xiaoqi Ren",
      "Xiao Ma",
      "Jincheng Luo",
      "Dawn Chen",
      "Wei Fan",
      "Henry Griffiths",
      "Christian Schuler",
      "Alice Li",
      "Shujian Zhang",
      "Jean-Michel Sarr",
      "Shixin Luo",
      "Riccardo Patana",
      "Matthew Watson",
      "Dani Naboulsi",
      "Michael Collins",
      "Sailesh Sidhwani",
      "Emiel Hoogeboom",
      "Sharon Silver",
      "Emily Caveness",
      "Xiaokai Zhao",
      "Mikel Rodriguez",
      "Maxine Deines",
      "Libin Bai",
      "Patrick Griffin",
      "Marco Tagliasacchi",
      "Emily Xue",
      "Spandana Raj Babbula",
      "Bo Pang",
      "Nan Ding",
      "Gloria Shen",
      "Elijah Peake",
      "Remi Crocker",
      "Shubha Srinivas Raghvendra",
      "Danny Swisher",
      "Woohyun Han",
      "Richa Singh",
      "Ling Wu",
      "Vladimir Pchelin",
      "Tsendsuren Munkhdalai",
      "Dana Alon",
      "Geoff Bacon",
      "Efren Robles",
      "Jannis Bulian",
      "Melvin Johnson",
      "George Powell",
      "Felipe Tiengo Ferreira",
      "Yaoyiran Li",
      "Frederik Benzing",
      "Mihajlo Velimirovi",
      "Hubert Soyer",
      "William Kong",
      " Tony",
      " Nguyn",
      "Zhen Yang",
      "Jeremiah Liu",
      "Joost van Amersfoort",
      "Daniel Gillick",
      "Baochen Sun",
      "Nathalie Rauschmayr",
      "Katie Zhang",
      "Serena Zhan",
      "Tao Zhou",
      "Alexey Frolov",
      "Chengrun Yang",
      "Denis Vnukov",
      "Louis Rouillard",
      "Hongji Li",
      "Amol Mandhane",
      "Nova Fallen",
      "Rajesh Venkataraman",
      "Clara Huiyi Hu",
      "Jennifer Brennan",
      "Jenny Lee",
      "Jerry Chang",
      "Martin Sundermeyer",
      "Zhufeng Pan",
      "Rosemary Ke",
      "Simon Tong",
      "Alex Fabrikant",
      "William Bono",
      "Jindong Gu",
      "Ryan Foley",
      "Yiran Mao",
      "Manolis Delakis",
      "Dhruva Bhaswar",
      "Roy Frostig",
      "Nick Li",
      "Avital Zipori",
      "Cath Hope",
      "Olga Kozlova",
      "Swaroop Mishra",
      "Josip Djolonga",
      "Craig Schiff",
      "Majd Al Merey",
      "Eleftheria Briakou",
      "Peter Morgan",
      "Andy Wan",
      "Avinatan Hassidim",
      "RJ Skerry-Ryan",
      "Kuntal Sengupta",
      "Mary Jasarevic",
      "Praveen Kallakuri",
      "Paige Kunkle",
      "Hannah Brennan",
      "Tom Lieber",
      "Hassan Mansoor",
      "Julian Walker",
      "Bing Zhang",
      "Annie Xie",
      "Goran ui",
      "Adaeze Chukwuka",
      "Alex Druinsky",
      "Donghyun Cho",
      "Rui Yao",
      "Ferjad Naeem",
      "Shiraz Butt",
      "Eunyoung Kim",
      "Zhipeng Jia",
      "Mandy Jordan",
      "Adam Lelkes",
      "Mark Kurzeja",
      "Sophie Wang",
      "James Zhao",
      "Andrew Over",
      "Abhishek Chakladar",
      "Marcel Prasetya",
      "Neha Jha",
      "Sriram Ganapathy",
      "Yale Cong",
      "Prakash Shroff",
      "Carl Saroufim",
      "Sobhan Miryoosefi",
      "Mohamed Hammad",
      "Tajwar Nasir",
      "Weijuan Xi",
      "Yang Gao",
      "Young Maeng",
      "Ben Hora",
      "Chin-Yi Cheng",
      "Parisa Haghani",
      "Yoad Lewenberg",
      "Caden Lu",
      "Martin Matysiak",
      "Naina Raisinghani",
      "Huiyu Wang",
      "Lexi Baugher",
      "Rahul Sukthankar",
      "Minh Giang",
      "John Schultz",
      "Noah Fiedel",
      "Minmin Chen",
      "Cheng-Chun Lee",
      "Tapomay Dey",
      "Hao Zheng",
      "Shachi Paul",
      "Celine Smith",
      "Andy Ly",
      "Yicheng Wang",
      "Rishabh Bansal",
      "Bartek Perz",
      "Susanna Ricco",
      "Stasha Blank",
      "Vaishakh Keshava",
      "Deepak Sharma",
      "Marvin Chow",
      "Kunal Lad",
      "Komal Jalan",
      "Simon Osindero",
      "Craig Swanson",
      "Jacob Scott",
      "Anastasija Ili",
      "Xiaowei Li",
      "Siddhartha Reddy Jonnalagadda",
      "Afzal Shama Soudagar",
      "Yan Xiong",
      "Bat-Orgil Batsaikhan",
      "Daniel Jarrett",
      "Naveen Kumar",
      "Maulik Shah",
      "Matt Lawlor",
      "Austin Waters",
      "Mark Graham",
      "Rhys May",
      "Sabela Ramos",
      "Sandra Lefdal",
      "Zeynep Cankara",
      "Nacho Cano",
      "Brendan O'Donoghue",
      "Jed Borovik",
      "Frederick Liu",
      "Jordan Grimstad",
      "Mahmoud Alnahlawi",
      "Katerina Tsihlas",
      "Tom Hudson",
      "Nikolai Grigorev",
      "Yiling Jia",
      "Terry Huang",
      "Tobenna Peter Igwe",
      "Sergei Lebedev",
      "Xiaodan Tang",
      "Igor Krivokon",
      "Frankie Garcia",
      "Melissa Tan",
      "Eric Jia",
      "Peter Stys",
      "Shikhar Vashishth",
      "Yu Liang",
      "Balaji Venkatraman",
      "Chenjie Gu",
      "Anastasios Kementsietsidis",
      "Chen Zhu",
      "Junehyuk Jung",
      "Yunfei Bai",
      "Mohammad Javad Hosseini",
      "Faruk Ahmed",
      "Aditya Gupta",
      "Xin Yuan",
      "Shereen Ashraf",
      "Shitij Nigam",
      "Gautam Vasudevan",
      "Pranjal Awasthi",
      "Adi Mayrav Gilady",
      "Zelda Mariet",
      "Ramy Eskander",
      "Haiguang Li",
      "Hexiang Hu",
      "Guillermo Garrido",
      "Philippe Schlattner",
      "George Zhang",
      "Rohun Saxena",
      "Petar Devi",
      "Kritika Muralidharan",
      "Ashwin Murthy",
      "Yiqian Zhou",
      "Min Choi",
      "Arissa Wongpanich",
      "Zhengdong Wang",
      "Premal Shah",
      "Yuntao Xu",
      "Yiling Huang",
      "Stephen Spencer",
      "Alice Chen",
      "James Cohan",
      "Junjie Wang",
      "Jonathan Tompson",
      "Junru Wu",
      "Ruba Haroun",
      "Haiqiong Li",
      "Blanca Huergo",
      "Fan Yang",
      "Tongxin Yin",
      "James Wendt",
      "Michael Bendersky",
      "Rahma Chaabouni",
      "Javier Snaider",
      "Johan Ferret",
      "Abhishek Jindal",
      "Tara Thompson",
      "Andrew Xue",
      "Will Bishop",
      "Shubham Milind Phal",
      "Archit Sharma",
      "Yunhsuan Sung",
      "Prabakar Radhakrishnan",
      "Mo Shomrat",
      "Reeve Ingle",
      "Roopali Vij",
      "Justin Gilmer",
      "Mihai Dorin Istin",
      "Sam Sobell",
      "Yang Lu",
      "Emily Nottage",
      "Dorsa Sadigh",
      "Jeremiah Willcock",
      "Tingnan Zhang",
      "Steve Xu",
      "Sasha Brown",
      "Katherine Lee",
      "Gary Wang",
      "Yun Zhu",
      "Yi Tay",
      "Cheolmin Kim",
      "Audrey Gutierrez",
      "Abhanshu Sharma",
      "Yongqin Xian",
      "Sungyong Seo",
      "Claire Cui",
      "Elena Pochernina",
      "Cip Baetu",
      "Krzysztof Jastrzbski",
      "Mimi Ly",
      "Mohamed Elhawaty",
      "Dan Suh",
      "Eren Sezener",
      "Pidong Wang",
      "Nancy Yuen",
      "George Tucker",
      "Jiahao Cai",
      "Zuguang Yang",
      "Cindy Wang",
      "Alex Muzio",
      "Hai Qian",
      "Jae Yoo",
      "Derek Lockhart",
      "Kevin R. McKee",
      "Mandy Guo",
      "Malika Mehrotra",
      "Artur Mendona",
      "Sanket Vaibhav Mehta",
      "Sherry Ben",
      "Chetan Tekur",
      "Jiaqi Mu",
      "Muye Zhu",
      "Victoria Krakovna",
      "Hongrae Lee",
      "AJ Maschinot",
      "Sbastien Cevey",
      "HyunJeong Choe",
      "Aijun Bai",
      "Hansa Srinivasan",
      "Derek Gasaway",
      "Nick Young",
      "Patrick Siegler",
      "Dan Holtmann-Rice",
      "Vihari Piratla",
      "Kate Baumli",
      "Roey Yogev",
      "Alex Hofer",
      "Hado van Hasselt",
      "Svetlana Grant",
      "Yuri Chervonyi",
      "David Silver",
      "Andrew Hogue",
      "Ayushi Agarwal",
      "Kathie Wang",
      "Preeti Singh",
      "Four Flynn",
      "Josh Lipschultz",
      "Robert David",
      "Lizzetth Bellot",
      "Yao-Yuan Yang",
      "Long Le",
      "Filippo Graziano",
      "Kate Olszewska",
      "Kevin Hui",
      "Akanksha Maurya",
      "Nikos Parotsidis",
      "Weijie Chen",
      "Tayo Oguntebi",
      "Joe Kelley",
      "Anirudh Baddepudi",
      "Johannes Mauerer",
      "Gregory Shaw",
      "Alex Siegman",
      "Lin Yang",
      "Shravya Shetty",
      "Subhrajit Roy",
      "Yunting Song",
      "Wojciech Stokowiec",
      "Ryan Burnell",
      "Omkar Savant",
      "Robert Busa-Fekete",
      "Jin Miao",
      "Samrat Ghosh",
      "Liam MacDermed",
      "Phillip Lippe",
      "Mikhail Dektiarev",
      "Zach Behrman",
      "Fabian Mentzer",
      "Kelvin Nguyen",
      "Meng Wei",
      "Siddharth Verma",
      "Chris Knutsen",
      "Sudeep Dasari",
      "Zhipeng Yan",
      "Petr Mitrichev",
      "Xingyu Wang",
      "Virat Shejwalkar",
      "Jacob Austin",
      "Srinivas Sunkara",
      "Navneet Potti",
      "Yan Virin",
      "Christian Wright",
      "Gal Liu",
      "Oriana Riva",
      "Etienne Pot",
      "Greg Kochanski",
      "Quoc Le",
      "Gargi Balasubramaniam",
      "Arka Dhar",
      "Yuguo Liao",
      "Adam Bloniarz",
      "Divyansh Shukla",
      "Elizabeth Cole",
      "Jong Lee",
      "Sheng Zhang",
      "Sushant Kafle",
      "Siddharth Vashishtha",
      "Parsa Mahmoudieh",
      "Grace Chen",
      "Raphael Hoffmann",
      "Pranesh Srinivasan",
      "Agustin Dal Lago",
      "Yoav Ben Shalom",
      "Zi Wang",
      "Michael Elabd",
      "Anuj Sharma",
      "Junhyuk Oh",
      "Suraj Kothawade",
      "Maigo Le",
      "Marianne Monteiro",
      "Shentao Yang",
      "Kaiz Alarakyia",
      "Robert Geirhos",
      "Diana Mincu",
      "Hvard Garnes",
      "Hayato Kobayashi",
      "Soroosh Mariooryad",
      "Kacper Krasowiak",
      " Zhixin",
      " Lai",
      "Shibl Mourad",
      "Mingqiu Wang",
      "Fan Bu",
      "Ophir Aharoni",
      "Guanjie Chen",
      "Abhimanyu Goyal",
      "Vadim Zubov",
      "Ankur Bapna",
      "Elahe Dabir",
      "Nisarg Kothari",
      "Kay Lamerigts",
      "Nicola De Cao",
      "Jeremy Shar",
      "Christopher Yew",
      "Nitish Kulkarni",
      "Dre Mahaarachchi",
      "Mandar Joshi",
      "Zhenhai Zhu",
      "Jared Lichtarge",
      "Yichao Zhou",
      "Hannah Muckenhirn",
      "Vittorio Selo",
      "Oriol Vinyals",
      "Peter Chen",
      "Anthony Brohan",
      "Vaibhav Mehta",
      "Sarah Cogan",
      "Ruth Wang",
      "Ty Geri",
      "Wei-Jen Ko",
      "Wei Chen",
      "Fabio Viola",
      "Keshav Shivam",
      "Lisa Wang",
      "Madeleine Clare Elish",
      "Raluca Ada Popa",
      "Sbastien Pereira",
      "Jianqiao Liu",
      "Raphael Koster",
      "Donnie Kim",
      "Gufeng Zhang",
      "Sayna Ebrahimi",
      "Partha Talukdar",
      "Yanyan Zheng",
      "Petra Poklukar",
      "Ales Mikhalap",
      "Dale Johnson",
      "Anitha Vijayakumar",
      "Mark Omernick",
      "Matt Dibb",
      "Ayush Dubey",
      "Qiong Hu",
      "Apurv Suman",
      "Vaibhav Aggarwal",
      "Ilya Kornakov",
      "Fei Xia",
      "Wing Lowe",
      "Alexey Kolganov",
      "Ted Xiao",
      "Vitaly Nikolaev",
      "Steven Hemingray",
      "Bonnie Li",
      "Joana Iljazi",
      "Mikoaj Rybiski",
      "Ballie Sandhu",
      "Peggy Lu",
      "Thang Luong",
      "Rodolphe Jenatton",
      "Vineetha Govindaraj",
      " Hui",
      " Li",
      "Gabriel Dulac-Arnold",
      "Wonpyo Park",
      "Henry Wang",
      "Abhinit Modi",
      "Jean Pouget-Abadie",
      "Kristina Greller",
      "Rahul Gupta",
      "Robert Berry",
      "Prajit Ramachandran",
      "Jinyu Xie",
      "Liam McCafferty",
      "Jianling Wang",
      "Kilol Gupta",
      "Hyeontaek Lim",
      "Bla Bratani",
      "Andy Brock",
      "Ilia Akolzin",
      "Jim Sproch",
      "Dan Karliner",
      "Duhyeon Kim",
      "Adrian Goedeckemeyer",
      "Noam Shazeer",
      "Cordelia Schmid",
      "Daniele Calandriello",
      "Parul Bhatia",
      "Krzysztof Choromanski",
      "Ceslee Montgomery",
      "Dheeru Dua",
      "Ana Ramalho",
      "Helen King",
      "Yue Gao",
      "Lynn Nguyen",
      "David Lindner",
      "Divya Pitta",
      "Oleaser Johnson",
      "Khalid Salama",
      "Diego Ardila",
      "Michael Han",
      "Erin Farnese",
      "Seth Odoom",
      "Ziyue Wang",
      "Xiangzhuo Ding",
      "Norman Rink",
      "Ray Smith",
      "Harshal Tushar Lehri",
      "Eden Cohen",
      "Neera Vats",
      "Tong He",
      "Parthasarathy Gopavarapu",
      "Adam Paszke",
      "Miteyan Patel",
      "Wouter Van Gansbeke",
      "Lucia Loher",
      "Luis Castro",
      "Maria Voitovich",
      "Tamara von Glehn",
      "Nelson George",
      "Simon Niklaus",
      "Zach Eaton-Rosen",
      "Nemanja Rakievi",
      "Erik Jue",
      "Sagi Perel",
      "Carrie Zhang",
      "Yuval Bahat",
      "Angline Pouget",
      "Zhi Xing",
      "Fantine Huot",
      "Ashish Shenoy",
      "Taylor Bos",
      "Vincent Coriou",
      "Bryan Richter",
      "Natasha Noy",
      "Yaqing Wang",
      "Santiago Ontanon",
      "Siyang Qin",
      "Gleb Makarchuk",
      "Demis Hassabis",
      "Zhuowan Li",
      "Mandar Sharma",
      "Kumaran Venkatesan",
      "Iurii Kemaev",
      "Roxanne Daniel",
      "Shiyu Huang",
      "Saloni Shah",
      "Octavio Ponce",
      " Warren",
      " Chen",
      "Manaal Faruqui",
      "Jialin Wu",
      "Slavica Andai",
      "Szabolcs Payrits",
      "Daniel McDuff",
      "Tom Hume",
      "Yuan Cao",
      "MH Tessler",
      "Qingze Wang",
      "Yinan Wang",
      "Ivor Rendulic",
      "Eirikur Agustsson",
      "Matthew Johnson",
      "Tanya Lando",
      "Andrew Howard",
      "Sri Gayatri Sundara Padmanabhan",
      "Mayank Daswani",
      "Andrea Banino",
      "Michael Kilgore",
      "Jonathan Heek",
      "Ziwei Ji",
      "Alvaro Caceres",
      "Conglong Li",
      "Nora Kassner",
      "Alexey Vlaskin",
      "Zeyu Liu",
      "Alex Grills",
      "Yanhan Hou",
      "Roykrong Sukkerd",
      "Gowoon Cheon",
      "Nishita Shetty",
      "Larisa Markeeva",
      "Piotr Stanczyk",
      "Tejas Iyer",
      "Yuan Gong",
      "Shawn Gao",
      "Keerthana Gopalakrishnan",
      "Tim Blyth",
      "Malcolm Reynolds",
      "Avishkar Bhoopchand",
      "Misha Bilenko",
      "Dero Gharibian",
      "Vicky Zayats",
      "Aleksandra Faust",
      "Abhinav Singh",
      "Min Ma",
      "Hongyang Jiao",
      "Sudheendra Vijayanarasimhan",
      "Lora Aroyo",
      "Vikas Yadav",
      "Sarah Chakera",
      "Ashwin Kakarla",
      "Vilobh Meshram",
      "Karol Gregor",
      "Gabriela Botea",
      "Evan Senter",
      "Dawei Jia",
      "Geza Kovacs",
      "Neha Sharma",
      "Sebastien Baur",
      "Kai Kang",
      "Yifan He",
      "Lin Zhuo",
      "Marija Kostelac",
      "Itay Laish",
      "Songyou Peng",
      "Louis O'Bryan",
      "Daniel Kasenberg",
      "Girish Ramchandra Rao",
      "Edouard Leurent",
      "Biao Zhang",
      "Sage Stevens",
      "Ana Salazar",
      "Ye Zhang",
      "Ivan Lobov",
      "Jake Walker",
      "Allen Porter",
      "Morgan Redshaw",
      "Han Ke",
      "Abhishek Rao",
      "Alex Lee",
      "Hoi Lam",
      "Michael Moffitt",
      "Jaeyoun Kim",
      "Siyuan Qiao",
      "Terry Koo",
      "Robert Dadashi",
      "Xinying Song",
      "Mukund Sundararajan",
      "Peng Xu",
      "Chizu Kawamoto",
      "Yan Zhong",
      "Clara Barbu",
      "Apoorv Reddy",
      "Mauro Verzetti",
      "Leon Li",
      "George Papamakarios",
      "Hanna Klimczak-Pluciska",
      "Mary Cassin",
      "Koray Kavukcuoglu",
      "Rigel Swavely",
      "Alain Vaucher",
      "Jeffrey Zhao",
      "Ross Hemsley",
      "Michael Tschannen",
      "Heming Ge",
      "Gaurav Menghani",
      "Yang Yu",
      "Natalie Ha",
      "Wei He",
      "Xiao Wu",
      "Maggie Song",
      "Rachel Sterneck",
      "Stefan Zinke",
      "Dan A. Calian",
      "Annie Marsden",
      "Alejandro Cruzado Ruiz",
      "Matteo Hessel",
      "Almog Gueta",
      "Benjamin Lee",
      "Brian Farris",
      "Manish Gupta",
      "Yunjie Li",
      "Mohammad Saleh",
      "Vedant Misra",
      "Kefan Xiao",
      "Piermaria Mendolicchio",
      "Gavin Buttimore",
      "Varvara Krayvanova",
      "Nigamaa Nayakanti",
      "Matthew Wiethoff",
      "Yash Pande",
      "Azalia Mirhoseini",
      "Ni Lao",
      "Jasmine Liu",
      "Yiqing Hua",
      "Angie Chen",
      "Yury Malkov",
      "Dmitry Kalashnikov",
      "Shubham Gupta",
      "Kartik Audhkhasi",
      "Yuexiang Zhai",
      "Sudhindra Kopalle",
      "Prateek Jain",
      "Eran Ofek",
      "Clemens Meyer",
      "Khuslen Baatarsukh",
      "Hana Strejek",
      "Jun Qian",
      "James Freedman",
      "Ricardo Figueira",
      "Michal Sokolik",
      "Olivier Bachem",
      "Raymond Lin",
      "Dia Kharrat",
      "Chris Hidey",
      "Pingmei Xu",
      "Dennis Duan",
      "Yin Li",
      "Muge Ersoy",
      "Richard Everett",
      "Kevin Cen",
      "Rebeca Santamaria-Fernandez",
      "Amir Taubenfeld",
      "Ian Mackinnon",
      "Linda Deng",
      "Polina Zablotskaia",
      "Shashank Viswanadha",
      "Shivanker Goel",
      "Damion Yates",
      "Yunxiao Deng",
      "Peter Choy",
      "Mingqing Chen",
      "Abhishek Sinha",
      "Alex Mossin",
      "Yiming Wang",
      "Arthur Szlam",
      "Susan Hao",
      "Paul Kishan Rubenstein",
      "Metin Toksoz-Exley",
      "Miranda Aperghis",
      "Yin Zhong",
      "Junwhan Ahn",
      "Michael Isard",
      "Olivier Lacombe",
      "Florian Luisier",
      "Chrysovalantis Anastasiou",
      "Yogesh Kalley",
      "Utsav Prabhu",
      "Emma Dunleavy",
      "Shaan Bijwadia",
      "Justin Mao-Jones",
      "Kelly Chen",
      "Rama Pasumarthi",
      "Emily Wood",
      "Adil Dostmohamed",
      "Nate Hurley",
      "Jiri Simsa",
      "Alicia Parrish",
      "Mantas Pajarskas",
      "Matt Harvey",
      "Ondrej Skopek",
      "Yony Kochinski",
      "Javier Rey",
      "Verena Rieser",
      "Denny Zhou",
      "Sun Jae Lee",
      "Trilok Acharya",
      "Guowang Li",
      "Joe Jiang",
      "Xiaofan Zhang",
      "Bryant Gipson",
      "Ethan Mahintorabi",
      "Marco Gelmi",
      "Nima Khajehnouri",
      "Angel Yeh",
      "Kayi Lee",
      "Loic Matthey",
      "Leslie Baker",
      "Trang Pham",
      "Han Fu",
      "Alex Pak",
      "Prakhar Gupta",
      "Cristina Vasconcelos",
      "Adam Sadovsky",
      "Brian Walker",
      "Sissie Hsiao",
      "Patrik Zochbauer",
      "Andreea Marzoca",
      "Noam Velan",
      "Junhao Zeng",
      "Gilles Baechler",
      "Danny Driess",
      "Divya Jain",
      "Yanping Huang",
      "Lizzie Tao",
      "John Maggs",
      "Nir Levine",
      "Jon Schneider",
      "Erika Gemzer",
      "Samuel Petit",
      "Shan Han",
      "Zach Fisher",
      "Dustin Zelle",
      "Courtney Biles",
      "Eugene Ie",
      "Asya Fadeeva",
      "Casper Liu",
      "Juliana Vicente Franco",
      "Adrian Collister",
      "Hao Zhang",
      "Renshen Wang",
      "Ruizhe Zhao",
      "Leandro Kieliger",
      "Kurt Shuster",
      "Rui Zhu",
      "Boqing Gong",
      "Lawrence Chan",
      "Ruoxi Sun",
      "Sujoy Basu",
      "Roland Zimmermann",
      "Jamie Hayes",
      "Abhishek Bapna",
      "Jasper Snoek",
      "Weel Yang",
      "Puranjay Datta",
      "Jad Al Abdallah",
      "Kevin Kilgour",
      "Lu Li",
      "SQ Mah",
      "Yennie Jun",
      "Morgane Rivire",
      "Abhijit Karmarkar",
      "Tammo Spalink",
      "Tao Huang",
      "Lucas Gonzalez",
      "Duc-Hieu Tran",
      "Averi Nowak",
      "John Palowitch",
      "Martin Chadwick",
      "Ellie Talius",
      "Harsh Mehta",
      "Thibault Sellam",
      "Philipp Frnken",
      "Massimo Nicosia",
      "Kyle He",
      "Aditya Kini",
      "David Amos",
      "Sugato Basu",
      "Harrison Jobe",
      "Eleni Shaw",
      "Qiantong Xu",
      "Colin Evans",
      "Daisuke Ikeda",
      "Chaochao Yan",
      "Larry Jin",
      "Lun Wang",
      "Sachin Yadav",
      "Ilia Labzovsky",
      "Ramesh Sampath",
      "Ada Ma",
      "Candice Schumann",
      "Aditya Siddhant",
      "Rohin Shah",
      "John Youssef",
      "Rishabh Agarwal",
      "Natalie Dabney",
      "Alessio Tonioni",
      "Moran Ambar",
      "Jing Li",
      "Isabelle Guyon",
      "Benny Li",
      "David Soergel",
      "Boya Fang",
      "Georgi Karadzhov",
      "Cristian Udrescu",
      "Trieu Trinh",
      "Vikas Raunak",
      "Seb Noury",
      "Dee Guo",
      "Sonal Gupta",
      "Mara Finkelstein",
      "Denis Petek",
      "Lihao Liang",
      "Greg Billock",
      "Pei Sun",
      "David Wood",
      "Yiwen Song",
      "Xiaobin Yu",
      "Tatiana Matejovicova",
      "Regev Cohen",
      "Kalyan Andra",
      "David D'Ambrosio",
      "Zhiwei Deng",
      "Vincent Nallatamby",
      "Ebrahim Songhori",
      "Rumen Dangovski",
      "Andrew Lampinen",
      "Pankil Botadra",
      "Adam Hillier",
      "Jiawei Cao",
      "Nagabhushan Baddi",
      "Adhi Kuncoro",
      "Toshihiro Yoshino",
      "Ankit Bhagatwala",
      "Marcurelio Ranzato",
      "Rylan Schaeffer",
      "Tianlin Liu",
      "Shuai Ye",
      "Obaid Sarvana",
      "John Nham",
      "Chenkai Kuang",
      "Isabel Gao",
      "Jinoo Baek",
      "Shubham Mittal",
      "Ayzaan Wahid",
      "Anita Gergely",
      "Bin Ni",
      "Josh Feldman",
      "Carrie Muir",
      "Pascal Lamblin",
      "Wolfgang Macherey",
      "Ethan Dyer",
      "Logan Kilpatrick",
      "Vctor Campos",
      "Mukul Bhutani",
      "Stanislav Fort",
      "Yanif Ahmad",
      "Aliaksei Severyn",
      "Kleopatra Chatziprimou",
      "Oleksandr Ferludin",
      "Mason Dimarco",
      "Aditya Kusupati",
      "Joe Heyward",
      "Dan Bahir",
      "Kevin Villela",
      "Katie Millican",
      "Dror Marcus",
      "Sanaz Bahargam",
      "Caglar Unlu",
      "Nicholas Roth",
      "Zichuan Wei",
      "Siddharth Gopal",
      "Deepanway Ghoshal",
      "Edward Lee",
      "Sharon Lin",
      "Jennie Lees",
      "Dayeong Lee",
      "Anahita Hosseini",
      "Connie Fan",
      "Seth Neel",
      "Marcus Wu",
      "Yasemin Altun",
      "Honglong Cai",
      "Enrique Piqueras",
      "Josh Woodward",
      "Alessandro Bissacco",
      "Salem Haykal",
      "Mahyar Bordbar",
      "Prasha Sundaram",
      "Sarah Hodkinson",
      "Daniel Toyama",
      "George Polovets",
      "Austin Myers",
      "Anu Sinha",
      "Tomer Levinboim",
      "Kashyap Krishnakumar",
      "Rachita Chhaparia",
      "Tatiana Sholokhova",
      "Nitesh Bharadwaj Gundavarapu",
      "Ganesh Jawahar",
      "Haroon Qureshi",
      "Jieru Hu",
      "Nikola Momchev",
      "Matthew Rahtz",
      "Renjie Wu",
      "Aishwarya P S",
      "Kedar Dhamdhere",
      "Meiqi Guo",
      "Umang Gupta",
      "Ali Eslami",
      "Mariano Schain",
      "Michiel Blokzijl",
      "David Welling",
      "Dave Orr",
      "Levent Bolelli",
      "Nicolas Perez-Nieves",
      "Mikhail Sirotenko",
      "Aman Prasad",
      "Arjun Kar",
      "Borja De Balle Pigem",
      "Tayfun Terzi",
      "Gellrt Weisz",
      "Dipankar Ghosh",
      "Aditi Mavalankar",
      "Dhruv Madeka",
      "Kaspar Daugaard",
      "Hartwig Adam",
      "Viraj Shah",
      "Dana Berman",
      "Maggie Tran",
      "Steven Baker",
      "Ewa Andrejczuk",
      "Grishma Chole",
      "Ganna Raboshchuk",
      "Mahdi Mirzazadeh",
      "Thais Kagohara",
      "Shimu Wu",
      "Christian Schallhart",
      "Bernett Orlando",
      "Chen Wang",
      "Alban Rrustemi",
      "Hao Xiong",
      "Hao Liu",
      "Arpi Vezer",
      "Nolan Ramsden",
      "Shuo-yiin Chang",
      "Sidharth Mudgal",
      "Yan Li",
      "Nino Vieillard",
      "Yedid Hoshen",
      "Farooq Ahmad",
      "Ambrose Slone",
      "Amy Hua",
      "Natan Potikha",
      "Mirko Rossini",
      "Jon Stritar",
      "Sushant Prakash",
      "Zifeng Wang",
      "Xuanyi Dong",
      "Alireza Nazari",
      "Efrat Nehoran",
      "Kaan Tekelioglu",
      "Yinxiao Li",
      "Kartikeya Badola",
      "Tom Funkhouser",
      "Yuanzhen Li",
      "Varun Yerram",
      "Ramya Ganeshan",
      "Daniel Formoso",
      "Karol Langner",
      "Tian Shi",
      "Huijian Li",
      "Yumeya Yamamori",
      "Amayika Panda",
      "Alaa Saade",
      "Angelo Scorza Scarpati",
      "Chris Breaux",
      "CJ Carey",
      "Zongwei Zhou",
      "Cho-Jui Hsieh",
      "Sophie Bridgers",
      "Alena Butryna",
      "Nishesh Gupta",
      "Vaibhav Tulsyan",
      "Sanghyun Woo",
      "Evgenii Eltyshev",
      "Will Grathwohl",
      "Chanel Parks",
      "Seth Benjamin",
      "Rina Panigrahy",
      "Shenil Dodhia",
      "Daniel De Freitas",
      "Chris Sauer",
      "Will Song",
      "Ferran Alet",
      "Jackson Tolins",
      "Cosmin Paduraru",
      "Xingyi Zhou",
      "Brian Albert",
      "Zizhao Zhang",
      "Lei Shu",
      "Mudit Bansal",
      "Sarah Nguyen",
      "Amir Globerson",
      "Owen Xiao",
      "James Manyika",
      "Tom Hennigan",
      "Rong Rong",
      "Josip Matak",
      "Anton Bakalov",
      "Ankur Sharma",
      "Danila Sinopalnikov",
      "Andrew Pierson",
      "Stephen Roller",
      "Geoff Brown",
      "Mingcen Gao",
      "Toshiyuki Fukuzawa",
      "Amin Ghafouri",
      "Kenny Vassigh",
      "Iain Barr",
      "Zhicheng Wang",
      "Anna Korsun",
      "Rajesh Jayaram",
      "Lijie Ren",
      "Tim Zaman",
      "Samira Khan",
      "Yana Lunts",
      "Dan Deutsch",
      "Dave Uthus",
      "Nitzan Katz",
      "Masha Samsikova",
      "Amr Khalifa",
      "Nikhil Sethi",
      "Jiao Sun",
      "Luming Tang",
      "Uri Alon",
      "Xianghong Luo",
      "Dian Yu",
      "Abhishek Nayyar",
      "Bryce Petrini",
      "Will Truong",
      "Vincent Hellendoorn",
      "Nikolai Chinaev",
      "Chris Alberti",
      "Wei Wang",
      "Jingcao Hu",
      "Vahab Mirrokni",
      "Ananth Balashankar",
      "Avia Aharon",
      "Aahil Mehta",
      "Ahmet Iscen",
      "Joseph Kready",
      "Lucas Manning",
      "Anhad Mohananey",
      "Yuankai Chen",
      "Anshuman Tripathi",
      "Allen Wu",
      "Igor Petrovski",
      "Dawsen Hwang",
      "Martin Baeuml",
      "Shreyas Chandrakaladharan",
      "Yuan Liu",
      "Rey Coaguila",
      "Maxwell Chen",
      "Sally Ma",
      "Pouya Tafti",
      "Susheel Tatineni",
      "Terry Spitz",
      "Jiayu Ye",
      "Paul Vicol",
      "Mihaela Rosca",
      "Adri Puigdomnech",
      "Zohar Yahav",
      "Sanjay Ghemawat",
      "Hanzhao Lin",
      "Phoebe Kirk",
      "Zaid Nabulsi",
      "Sergey Brin",
      "Bernd Bohnet",
      "Ken Caluwaerts",
      "Aditya Srikanth Veerubhotla",
      "Dan Zheng",
      "Zihang Dai",
      "Petre Petrov",
      "Yichong Xu",
      "Ramin Mehran",
      "Zhuo Xu",
      "Luisa Zintgraf",
      "Jiho Choi",
      "Spurthi Amba Hombaiah",
      "Romal Thoppilan",
      "Sashank Reddi",
      "Lukasz Lew",
      "Li Li",
      "Kellie Webster",
      "KP Sawhney",
      "Lampros Lamprou",
      "Siamak Shakeri",
      "Mayank Lunayach",
      "Jianmin Chen",
      "Sumit Bagri",
      "Alex Salcianu",
      "Ying Chen",
      "Yani Donchev",
      "Charlotte Magister",
      "Signe Nrly",
      "Vitor Rodrigues",
      "Tomas Izo",
      "Hila Noga",
      "Joe Zou",
      "Thomas Kppe",
      "Wenxuan Zhou",
      "Kenton Lee",
      "Xiangzhu Long",
      "Danielle Eisenbud",
      "Anthony Chen",
      "Connor Schenck",
      "Chi Ming To",
      "Peilin Zhong",
      "Emanuel Taropa",
      "Minh Truong",
      "Omer Levy",
      "Danilo Martins",
      "Zhiyuan Zhang",
      "Christopher Semturs",
      "Kelvin Zhang",
      "Alex Yakubovich",
      "Pol Moreno",
      "Lara McConnaughey",
      "Di Lu",
      "Sam Redmond",
      "Lotte Weerts",
      "Yonatan Bitton",
      "Tiziana Refice",
      "Nicolas Lacasse",
      "Arthur Conmy",
      "Corentin Tallec",
      "Julian Odell",
      "Hannah Forbes-Pollard",
      "Arkadiusz Socala",
      "Jonathan Hoech",
      "Pushmeet Kohli",
      "Alanna Walton",
      "Rui Wang",
      "Mikita Sazanovich",
      "Kexin Zhu",
      "Andrei Kapishnikov",
      "Rich Galt",
      "Matthew Denton",
      "Ben Murdoch",
      "Caitlin Sikora",
      "Kareem Mohamed",
      "Wei Wei",
      "Uri First",
      "Tim McConnell",
      "Luis C. Cobo",
      "James Qin",
      "Thi Avrahami",
      "Daniel Balle",
      "Yu Watanabe",
      "Annie Louis",
      "Adam Kraft",
      "Setareh Ariafar",
      "Yiming Gu",
      "Eugnie Rives",
      "Charles Yoon",
      "Andrei Rusu",
      "James Cobon-Kerr",
      "Chris Hahn",
      "Jiaming Luo",
      " Yuvein",
      " Zhu",
      "Niharika Ahuja",
      "Rodrigo Benenson",
      "Raphal Lopez Kaufman",
      "Honglin Yu",
      "Lloyd Hightower",
      "Junlin Zhang",
      "Darren Ni",
      "Lisa Anne Hendricks",
      "Gabby Wang",
      "Gal Yona",
      "Lalit Jain",
      "Pablo Barrio",
      "Surya Bhupatiraju",
      "Siva Velusamy",
      "Allan Dafoe",
      "Sebastian Riedel",
      "Tara Thomas",
      "Zhe Yuan",
      "Mathias Bellaiche",
      "Sheena Panthaplackel",
      "Klemen Kloboves",
      "Sarthak Jauhari",
      "Canfer Akbulut",
      "Todor Davchev",
      "Evgeny Gladchenko",
      "David Madras",
      "Aleksandr Chuklin",
      "Tyrone Hill",
      "Quan Yuan",
      "Mukundan Madhavan",
      "Luke Leonhard",
      "Dylan Scandinaro",
      "Qihang Chen",
      "Ning Niu",
      "Arthur Douillard",
      "Bogdan Damoc",
      "Yasumasa Onoe",
      "Fabian Pedregosa",
      "Fred Bertsch",
      "Chas Leichner",
      "Joseph Pagadora",
      "Jonathan Malmaud",
      "Sameera Ponda",
      "Andy Twigg",
      "Oleksii Duzhyi",
      "Jingwei Shen",
      "Miaosen Wang",
      "Roopal Garg",
      "Jing Chen",
      "Utku Evci",
      "Jonathan Lee",
      "Leon Liu",
      "Koji Kojima",
      "Masa Yamaguchi",
      "Arunkumar Rajendran",
      "AJ Piergiovanni",
      "Vinodh Kumar Rajendran",
      "Marco Fornoni",
      "Gabriel Ibagon",
      "Harry Ragan",
      "Sadh MNM Khan",
      "John Blitzer",
      "Andrew Bunner",
      "Guan Sun",
      "Takahiro Kosakai",
      "Scott Lundberg",
      "Ndidi Elue",
      "Kelvin Guu",
      "SK Park",
      "Jane Park",
      "Arunachalam Narayanaswamy",
      "Chengda Wu",
      "Jayaram Mudigonda",
      "Trevor Cohn",
      "Hairong Mu",
      "Ravi Kumar",
      "Laura Graesser",
      "Yichi Zhang",
      "Richard Killam",
      "Vincent Zhuang",
      "Mai Gimnez",
      "Wael Al Jishi",
      "Ruy Ley-Wild",
      "Alex Zhai",
      "Kazuki Osawa",
      "Diego Cedillo",
      "Jialu Liu",
      "Mayank Upadhyay",
      "Marcin Sieniek",
      "Roshan Sharma",
      "Tom Paine",
      "Anelia Angelova",
      "Sravanti Addepalli",
      "Carolina Parada",
      "Kingshuk Majumder",
      "Avery Lamp",
      "Sanjiv Kumar",
      "Xiang Deng",
      "Artiom Myaskovsky",
      "Tea Saboli",
      "Jeffrey Dudek",
      "Sarah York",
      "Flix de Chaumont Quitry",
      "Jiazhong Nie",
      "Dee Cattle",
      "Alok Gunjan",
      "Bilal Piot",
      "Waleed Khawaja",
      "Seojin Bang",
      "Simon Wang",
      "Siavash Khodadadeh",
      "Raghavender R",
      "Praynaa Rawlani",
      "Richard Powell",
      "Kevin Lee",
      "Johannes Griesser",
      "GS Oh",
      "Cesar Magalhaes",
      "Yujia Li",
      "Simon Tokumine",
      "Hadas Natalie Vogel",
      "Dennis Hsu",
      "Arturo BC",
      "Disha Jindal",
      "Matan Cohen",
      "Zi Yang",
      "Junwei Yuan",
      "Dario de Cesare",
      "Tony Bruguier",
      "Jun Xu",
      "Monica Roy",
      "Alon Jacovi",
      "Dan Belov",
      "Rahul Arya",
      "Phoenix Meadowlark",
      "Shlomi Cohen-Ganor",
      "Wenting Ye",
      "Patrick Morris-Suzuki",
      "Praseem Banzal",
      "Gan Song",
      "Pranavaraj Ponnuramu",
      "Fred Zhang",
      "George Scrivener",
      "Salah Zaiem",
      "Alif Raditya Rochman",
      "Kehang Han",
      "Badih Ghazi",
      "Kate Lee",
      "Shahar Drath",
      "Daniel Suo",
      "Antonious Girgis",
      "Pradeep Shenoy",
      "Duy Nguyen",
      "Douglas Eck",
      "Somit Gupta",
      "Le Yan",
      "Joao Carreira",
      "Anmol Gulati",
      "Ruoxin Sang",
      "Daniil Mirylenka",
      "Emma Cooney",
      "Edward Chou",
      "Mingyang Ling",
      "Cindy Fan",
      "Ben Coleman",
      "Guilherme Tubone",
      "Ravin Kumar",
      "Jason Baldridge",
      "Felix Hernandez-Campos",
      "Angeliki Lazaridou",
      "James Besley",
      "Itay Yona",
      "Neslihan Bulut",
      "Quentin Wellens",
      "AJ Pierigiovanni",
      "Jasmine George",
      "Richard Green",
      "Pu Han",
      "Connie Tao",
      "Geoff Clark",
      "Chong You",
      "Abbas Abdolmaleki",
      "Justin Fu",
      "Tongzhou Chen",
      "Ashwin Chaugule",
      "Angad Chandorkar",
      "Altaf Rahman",
      "Will Thompson",
      "Penporn Koanantakool",
      "Mike Bernico",
      "Jie Ren",
      "Andrey Vlasov",
      "Sergei Vassilvitskii",
      "Maciej Kula",
      "Yizhong Liang",
      "Dahun Kim",
      "Yangsibo Huang",
      "Chengxi Ye",
      "Dmitry Lepikhin",
      "Wesley Helmholz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.08403v2",
    "title": "ConDiSim: Conditional Diffusion Models for Simulation Based Inference",
    "summary": "We present a conditional diffusion model - ConDiSim, for simulation-based\ninference of complex systems with intractable likelihoods. ConDiSim leverages\ndenoising diffusion probabilistic models to approximate posterior\ndistributions, consisting of a forward process that adds Gaussian noise to\nparameters, and a reverse process learning to denoise, conditioned on observed\ndata. This approach effectively captures complex dependencies and\nmulti-modalities within posteriors. ConDiSim is evaluated across ten benchmark\nproblems and two real-world test problems, where it demonstrates effective\nposterior approximation accuracy while maintaining computational efficiency and\nstability in model training. ConDiSim offers a robust and extensible framework\nfor simulation-based inference, particularly suitable for parameter inference\nworkflows requiring fast inference methods.",
    "published": "2025-05-13T09:58:23Z",
    "updated": "2025-10-16T14:53:05Z",
    "link": "http://arxiv.org/pdf/2505.08403v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Mayank Nautiyal",
      "Andreas Hellander",
      "Prashant Singh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14751v1",
    "title": "Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries",
    "summary": "Next-token prediction (NTP) has driven the success of large language models\n(LLMs), but it struggles with long-horizon reasoning, planning, and creative\nwriting, with these limitations largely attributed to teacher-forced training.\nMulti-token prediction (MTP) partially mitigates these issues by predicting\nseveral future tokens at once, but it mostly captures short-range dependencies\nand offers limited improvement. We propose future summary prediction (FSP),\nwhich trains an auxiliary head to predict a compact representation of the\nlong-term future, preserving information relevant for long-form generations. We\nexplore two variants of FSP: handcrafted summaries, for example, a bag of words\nsummary of the future of the sequence, and learned summaries, which use\nembeddings produced by a reverse language model trained from right to left.\nLarge-scale pretraining experiments (3B and 8B-parameter models) demonstrate\nthat FSP provides improvements over both NTP and MTP across math, reasoning,\nand coding benchmarks.",
    "published": "2025-10-16T14:52:52Z",
    "updated": "2025-10-16T14:52:52Z",
    "link": "http://arxiv.org/pdf/2510.14751v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Divyat Mahajan",
      "Sachin Goyal",
      "Badr Youbi Idrissi",
      "Mohammad Pezeshki",
      "Ioannis Mitliagkas",
      "David Lopez-Paz",
      "Kartik Ahuja"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.02922v3",
    "title": "Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning",
    "summary": "Model diffing is the study of how fine-tuning changes a model's\nrepresentations and internal algorithms. Many behaviors of interest are\nintroduced during fine-tuning, and model diffing offers a promising lens to\ninterpret such behaviors. Crosscoders are a recent model diffing method that\nlearns a shared dictionary of interpretable concepts represented as latent\ndirections in both the base and fine-tuned models, allowing us to track how\nconcepts shift or emerge during fine-tuning. Notably, prior work has observed\nconcepts with no direction in the base model, and it was hypothesized that\nthese model-specific latents were concepts introduced during fine-tuning.\nHowever, we identify two issues which stem from the crosscoders L1 training\nloss that can misattribute concepts as unique to the fine-tuned model, when\nthey really exist in both models. We develop Latent Scaling to flag these\nissues by more accurately measuring each latent's presence across models. In\nexperiments comparing Gemma 2 2B base and chat models, we observe that the\nstandard crosscoder suffers heavily from these issues. Building on these\ninsights, we train a crosscoder with BatchTopK loss and show that it\nsubstantially mitigates these issues, finding more genuinely chat-specific and\nhighly interpretable concepts. We recommend practitioners adopt similar\ntechniques. Using the BatchTopK crosscoder, we successfully identify a set of\nchat-specific latents that are both interpretable and causally effective,\nrepresenting concepts such as $\\textit{false information}$ and\n$\\textit{personal question}$, along with multiple refusal-related latents that\nshow nuanced preferences for different refusal triggers. Overall, our work\nadvances best practices for the crosscoder-based methodology for model diffing\nand demonstrates that it can provide concrete insights into how chat-tuning\nmodifies model behavior.",
    "published": "2025-04-03T17:50:24Z",
    "updated": "2025-10-16T14:44:45Z",
    "link": "http://arxiv.org/pdf/2504.02922v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Julian Minder",
      "Clment Dumas",
      "Caden Juang",
      "Bilal Chugtai",
      "Neel Nanda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14741v1",
    "title": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision\n  Models",
    "summary": "Understanding and explaining the behavior of machine learning models is\nessential for building transparent and trustworthy AI systems. We introduce\nDEXTER, a data-free framework that employs diffusion models and large language\nmodels to generate global, textual explanations of visual classifiers. DEXTER\noperates by optimizing text prompts to synthesize class-conditional images that\nstrongly activate a target classifier. These synthetic samples are then used to\nelicit detailed natural language reports that describe class-specific decision\npatterns and biases. Unlike prior work, DEXTER enables natural language\nexplanation about a classifier's decision process without access to training\ndata or ground-truth labels. We demonstrate DEXTER's flexibility across three\ntasks-activation maximization, slice discovery and debiasing, and bias\nexplanation-each illustrating its ability to uncover the internal mechanisms of\nvisual classifiers. Quantitative and qualitative evaluations, including a user\nstudy, show that DEXTER produces accurate, interpretable outputs. Experiments\non ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms\nexisting approaches in global model explanation and class-level bias reporting.\nCode is available at https://github.com/perceivelab/dexter.",
    "published": "2025-10-16T14:43:25Z",
    "updated": "2025-10-16T14:43:25Z",
    "link": "http://arxiv.org/pdf/2510.14741v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "I.2.m"
    ],
    "authors": [
      "Simone Carnemolla",
      "Matteo Pennisi",
      "Sarinda Samarasinghe",
      "Giovanni Bellitto",
      "Simone Palazzo",
      "Daniela Giordano",
      "Mubarak Shah",
      "Concetto Spampinato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.21043v2",
    "title": "Disentangled and Self-Explainable Node Representation Learning",
    "summary": "Node representations, or embeddings, are low-dimensional vectors that capture\nnode properties, typically learned through unsupervised structural similarity\nobjectives or supervised tasks. While recent efforts have focused on explaining\ngraph model decisions, the interpretability of unsupervised node embeddings\nremains underexplored. To bridge this gap, we introduce DiSeNE (Disentangled\nand Self-Explainable Node Embedding), a framework that generates\nself-explainable embeddings in an unsupervised manner. Our method employs\ndisentangled representation learning to produce dimension-wise interpretable\nembeddings, where each dimension is aligned with distinct topological structure\nof the graph. We formalize novel desiderata for disentangled and interpretable\nembeddings, which drive our new objective functions, optimizing simultaneously\nfor both interpretability and disentanglement. Additionally, we propose several\nnew metrics to evaluate representation quality and human interpretability.\nExtensive experiments across multiple benchmark datasets demonstrate the\neffectiveness of our approach.",
    "published": "2024-10-28T13:58:52Z",
    "updated": "2025-10-16T14:23:53Z",
    "link": "http://arxiv.org/pdf/2410.21043v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Simone Piaggesi",
      "Andr Panisson",
      "Megha Khosla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14717v1",
    "title": "Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size\n  Scheduling",
    "summary": "Increasing the batch size during training -- a ''batch ramp'' -- is a\npromising strategy to accelerate large language model pretraining. While for\nSGD, doubling the batch size can be equivalent to halving the learning rate,\nthe optimal strategy for adaptive optimizers like Adam is less clear. As a\nresult, any batch-ramp scheduling, if used at all, is typically tuned\nheuristically. This work develops a principled framework for batch-size\nscheduling and introduces Seesaw: whenever a standard scheduler would halve the\nlearning rate, Seesaw instead multiplies it by $1/\\sqrt{2}$ and doubles the\nbatch size, preserving loss dynamics while reducing serial steps.\nTheoretically, we provide, to our knowledge, the first finite-sample proof of\nequivalence between learning-rate decay and batch-size ramp-up for SGD on noisy\nlinear regression, and we extend this equivalence to normalized SGD, a\ntractable proxy for Adam, under a variance-dominated regime observed in\npractice. Empirically, on 150M/300M/600M-parameter models trained at Chinchilla\nscale using a constant (critical) batch size, Seesaw matches cosine decay at\nequal FLOPs while reducing wall-clock time by $\\approx 36\\%$, approaching the\ntheoretical limit implied by our analysis.",
    "published": "2025-10-16T14:17:38Z",
    "updated": "2025-10-16T14:17:38Z",
    "link": "http://arxiv.org/pdf/2510.14717v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "authors": [
      "Alexandru Meterez",
      "Depen Morwani",
      "Jingfeng Wu",
      "Costin-Andrei Oncescu",
      "Cengiz Pehlevan",
      "Sham Kakade"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17064v2",
    "title": "Synthetic History: Evaluating Visual Representations of the Past in\n  Diffusion Models",
    "summary": "As Text-to-Image (TTI) diffusion models become increasingly influential in\ncontent creation, growing attention is being directed toward their societal and\ncultural implications. While prior research has primarily examined demographic\nand cultural biases, the ability of these models to accurately represent\nhistorical contexts remains largely underexplored. To address this gap, we\nintroduce a benchmark for evaluating how TTI models depict historical contexts.\nThe benchmark combines HistVis, a dataset of 30,000 synthetic images generated\nby three state-of-the-art diffusion models from carefully designed prompts\ncovering universal human activities across multiple historical periods, with a\nreproducible evaluation protocol. We evaluate generated imagery across three\nkey aspects: (1) Implicit Stylistic Associations: examining default visual\nstyles associated with specific eras; (2) Historical Consistency: identifying\nanachronisms such as modern artifacts in pre-modern contexts; and (3)\nDemographic Representation: comparing generated racial and gender distributions\nagainst historically plausible baselines. Our findings reveal systematic\ninaccuracies in historically themed generated imagery, as TTI models frequently\nstereotype past eras by incorporating unstated stylistic cues, introduce\nanachronisms, and fail to reflect plausible demographic patterns. By providing\na reproducible benchmark for historical representation in generated imagery,\nthis work provides an initial step toward building more historically accurate\nTTI models.",
    "published": "2025-05-18T13:35:23Z",
    "updated": "2025-10-16T14:16:14Z",
    "link": "http://arxiv.org/pdf/2505.17064v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Maria-Teresa De Rosa Palmini",
      "Eva Cetinic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.04445v4",
    "title": "Moto: Latent Motion Token as the Bridging Language for Learning Robot\n  Manipulation from Videos",
    "summary": "Recent developments in Large Language Models pre-trained on extensive corpora\nhave shown significant success in various natural language processing tasks\nwith minimal fine-tuning. This success offers new promise for robotics, which\nhas long been constrained by the high cost of action-labeled data. We ask:\ngiven the abundant video data containing interaction-related knowledge\navailable as a rich \"corpus\", can a similar generative pre-training approach be\neffectively applied to enhance robot learning? The key challenge is to identify\nan effective representation for autoregressive pre-training that benefits robot\nmanipulation tasks. Inspired by the way humans learn new skills through\nobserving dynamic environments, we propose that effective robotic learning\nshould emphasize motion-related knowledge, which is closely tied to low-level\nactions and is hardware-agnostic, facilitating the transfer of learned motions\nto actual robot actions. To this end, we introduce Moto, which converts video\ncontent into latent Motion Token sequences by a Latent Motion Tokenizer,\nlearning a bridging \"language\" of motion from videos in an unsupervised manner.\nWe pre-train Moto-GPT through motion token autoregression, enabling it to\ncapture diverse visual motion knowledge. After pre-training, Moto-GPT\ndemonstrates the promising ability to produce semantically interpretable motion\ntokens, predict plausible motion trajectories, and assess trajectory\nrationality through output likelihood. To transfer learned motion priors to\nreal robot actions, we implement a co-fine-tuning strategy that seamlessly\nbridges latent motion token prediction and real robot control. Extensive\nexperiments show that the fine-tuned Moto-GPT exhibits superior robustness and\nefficiency on robot manipulation benchmarks, underscoring its effectiveness in\ntransferring knowledge from video data to downstream visual manipulation tasks.",
    "published": "2024-12-05T18:57:04Z",
    "updated": "2025-10-16T14:13:23Z",
    "link": "http://arxiv.org/pdf/2412.04445v4.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Yi Chen",
      "Yuying Ge",
      "Weiliang Tang",
      "Yizhuo Li",
      "Yixiao Ge",
      "Mingyu Ding",
      "Ying Shan",
      "Xihui Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14713v1",
    "title": "Camera Movement Classification in Historical Footage: A Comparative\n  Study of Deep Video Models",
    "summary": "Camera movement conveys spatial and narrative information essential for\nunderstanding video content. While recent camera movement classification (CMC)\nmethods perform well on modern datasets, their generalization to historical\nfootage remains unexplored. This paper presents the first systematic evaluation\nof deep video CMC models on archival film material. We summarize representative\nmethods and datasets, highlighting differences in model design and label\ndefinitions. Five standard video classification models are assessed on the\nHISTORIAN dataset, which includes expert-annotated World War II footage. The\nbest-performing model, Video Swin Transformer, achieves 80.25% accuracy,\nshowing strong convergence despite limited training data. Our findings\nhighlight the challenges and potential of adapting existing models to\nlow-quality video and motivate future work combining diverse input modalities\nand temporal architectures.",
    "published": "2025-10-16T14:11:52Z",
    "updated": "2025-10-16T14:11:52Z",
    "link": "http://arxiv.org/pdf/2510.14713v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "authors": [
      "Tingyu Lin",
      "Armin Dadras",
      "Florian Kleber",
      "Robert Sablatnig"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14709v1",
    "title": "Where are the Whales: A Human-in-the-loop Detection Method for\n  Identifying Whales in High-resolution Satellite Imagery",
    "summary": "Effective monitoring of whale populations is critical for conservation, but\ntraditional survey methods are expensive and difficult to scale. While prior\nwork has shown that whales can be identified in very high-resolution (VHR)\nsatellite imagery, large-scale automated detection remains challenging due to a\nlack of annotated imagery, variability in image quality and environmental\nconditions, and the cost of building robust machine learning pipelines over\nmassive remote sensing archives. We present a semi-automated approach for\nsurfacing possible whale detections in VHR imagery using a statistical anomaly\ndetection method that flags spatial outliers, i.e. \"interesting points\". We\npair this detector with a web-based labeling interface designed to enable\nexperts to quickly annotate the interesting points. We evaluate our system on\nthree benchmark scenes with known whale annotations and achieve recalls of\n90.3% to 96.4%, while reducing the area requiring expert inspection by up to\n99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method\ndoes not rely on labeled training data and offers a scalable first step toward\nfuture machine-assisted marine mammal monitoring from space. We have open\nsourced this pipeline at https://github.com/microsoft/whales.",
    "published": "2025-10-16T14:10:51Z",
    "updated": "2025-10-16T14:10:51Z",
    "link": "http://arxiv.org/pdf/2510.14709v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Caleb Robinson",
      "Kimberly T. Goetz",
      "Christin B. Khan",
      "Meredith Sackett",
      "Kathleen Leonard",
      "Rahul Dodhia",
      "Juan M. Lavista Ferres"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12787v2",
    "title": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in\n  Mathematics and Quantum Physics",
    "summary": "We present Ax-Prover, a multi-agent system for automated theorem proving in\nLean that can solve problems across diverse scientific domains and operate\neither autonomously or collaboratively with human experts. To achieve this,\nAx-Prover approaches scientific problem solving through formal proof\ngeneration, a process that demands both creative reasoning and strict syntactic\nrigor. Ax-Prover meets this challenge by equipping Large Language Models\n(LLMs), which provide knowledge and reasoning, with Lean tools via the Model\nContext Protocol (MCP), which ensure formal correctness. To evaluate its\nperformance as an autonomous prover, we benchmark our approach against frontier\nLLMs and specialized prover models on two public math benchmarks and on two\nLean benchmarks we introduce in the fields of abstract algebra and quantum\ntheory. On public datasets, Ax-Prover is competitive with state-of-the-art\nprovers, while it largely outperforms them on the new benchmarks. This shows\nthat, unlike specialized systems that struggle to generalize, our tool-based\nagentic theorem prover approach offers a generalizable methodology for formal\nverification across diverse scientific domains. Furthermore, we demonstrate\nAx-Prover's assistant capabilities in a practical use case, showing how it\nenabled an expert mathematician to formalize the proof of a complex\ncryptography theorem.",
    "published": "2025-10-14T17:57:04Z",
    "updated": "2025-10-16T14:10:07Z",
    "link": "http://arxiv.org/pdf/2510.12787v2.pdf",
    "category": [
      "cs.AI",
      "cs.MA"
    ],
    "authors": [
      "Marco Del Tredici",
      "Jacob McCarran",
      "Benjamin Breen",
      "Javier Aspuru Mijares",
      "Weichen Winston Yin",
      "Jacob M. Taylor",
      "Frank H. L. Koppens",
      "Dirk Englund"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.06917v2",
    "title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI\n  Agents",
    "summary": "We introduce Paper2Agent, an automated framework that converts research\npapers into AI agents. Paper2Agent transforms research output from passive\nartifacts into active systems that can accelerate downstream use, adoption, and\ndiscovery. Conventional research papers require readers to invest substantial\neffort to understand and adapt a paper's code, data, and methods to their own\nwork, creating barriers to dissemination and reuse. Paper2Agent addresses this\nchallenge by automatically converting a paper into an AI agent that acts as a\nknowledgeable research assistant. It systematically analyzes the paper and the\nassociated codebase using multiple agents to construct a Model Context Protocol\n(MCP) server, then iteratively generates and runs tests to refine and robustify\nthe resulting MCP. These paper MCPs can then be flexibly connected to a chat\nagent (e.g. Claude Code) to carry out complex scientific queries through\nnatural language while invoking tools and workflows from the original paper. We\ndemonstrate Paper2Agent's effectiveness in creating reliable and capable paper\nagents through in-depth case studies. Paper2Agent created an agent that\nleverages AlphaGenome to interpret genomic variants and agents based on ScanPy\nand TISSUE to carry out single-cell and spatial transcriptomics analyses. We\nvalidate that these paper agents can reproduce the original paper's results and\ncan correctly carry out novel user queries. Paper2Agent automatically created\nAI co-scientist that identified new splicing variant associated with ADHD risk.\nBy turning static papers into dynamic, interactive AI agents, Paper2Agent\nintroduces a new paradigm for knowledge dissemination and a foundation for the\ncollaborative ecosystem of AI co-scientists.",
    "published": "2025-09-08T17:28:42Z",
    "updated": "2025-10-16T14:09:17Z",
    "link": "http://arxiv.org/pdf/2509.06917v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Jiacheng Miao",
      "Joe R. Davis",
      "Yaohui Zhang",
      "Jonathan K. Pritchard",
      "James Zou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14703v1",
    "title": "ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for\n  Function Calling",
    "summary": "Large language models (LLMs) are increasingly demonstrating strong\ncapabilities as autonomous agents, with function calling serving as a core\nmechanism for interaction with the environment. Meanwhile, inference scaling\nhas become a cutting-edge technique to enhance LLM performance by allocating\nmore computational resources during the inference process. However, current\nresearch on inference scaling primarily focuses on unstructured output\ngeneration tasks, leaving its application in structured outputs, like function\ncalling, largely underexplored. To bridge this gap, we propose an inference\nscaling framework that combines fine-grained beam search with a process reward\nmodel, ToolPRM, which scores the internal steps of each single function call.\nTo train ToolPRM, we construct the first fine-grained intra-call process\nsupervision dataset, automatically annotated with function-masking techniques\nto provide step-level rewards for structured tool-use reasoning. Extensive\nexperiments demonstrate that ToolPRM beats the coarse-grained and outcome\nreward models in terms of predictive accuracy, indicating its stronger\ncapability in supervising the function calling inference process. Inference\nscaling technique equipped with ToolPRM also significantly improves the\nbackbone model performance across various function calling tasks and\nbenchmarks. More importantly, we reveal a key principle for applying inference\nscaling techniques to structured outputs: \"explore more but retain less\" due to\nthe unrecoverability characteristics of structured function calling generation.",
    "published": "2025-10-16T14:06:03Z",
    "updated": "2025-10-16T14:06:03Z",
    "link": "http://arxiv.org/pdf/2510.14703v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Jianghao Lin",
      "Yuanyuan Shi",
      "Xin Peng",
      "Renjie Ding",
      "Hairui Wang",
      "Yuxuan Peng",
      "Bizhe Bai",
      "Weixi Song",
      "Fengshuo Bai",
      "Huacan Chai",
      "Weinan Zhang",
      "Fei Huang",
      "Ying Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.16191v2",
    "title": "The Last Dependency Crusade: Solving Python Dependency Conflicts with\n  LLMs",
    "summary": "Resolving Python dependency issues remains a tedious and error-prone process,\nforcing developers to manually trial compatible module versions and interpreter\nconfigurations. Existing automated solutions, such as knowledge-graph-based and\ndatabase-driven methods, face limitations due to the variety of dependency\nerror types, large sets of possible module versions, and conflicts among\ntransitive dependencies. This paper investigates the use of Large Language\nModels (LLMs) to automatically repair dependency issues in Python programs. We\npropose PLLM (pronounced \"plum\"), a novel retrieval-augmented generation (RAG)\napproach that iteratively infers missing or incorrect dependencies. PLLM builds\na test environment where the LLM proposes module combinations, observes\nexecution feedback, and refines its predictions using natural language\nprocessing (NLP) to parse error messages. We evaluate PLLM on the Gistable\nHG2.9K dataset, a curated collection of real-world Python programs. Using this\nbenchmark, we explore multiple PLLM configurations, including six open-source\nLLMs evaluated both with and without RAG. Our findings show that RAG\nconsistently improves fix rates, with the best performance achieved by Gemma-2\n9B when combined with RAG. Compared to two state-of-the-art baselines, PyEGo\nand ReadPyE, PLLM achieves significantly higher fix rates; +15.97\\% more than\nReadPyE and +21.58\\% more than PyEGo. Further analysis shows that PLLM is\nespecially effective for projects with numerous dependencies and those using\nspecialized numerical or machine-learning libraries.",
    "published": "2025-01-27T16:45:34Z",
    "updated": "2025-10-16T14:05:53Z",
    "link": "http://arxiv.org/pdf/2501.16191v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Antony Bartlett",
      "Cynthia Liem",
      "Annibale Panichella"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14702v1",
    "title": "Cognitive-Aligned Spatio-Temporal Large Language Models For Next\n  Point-of-Interest Prediction",
    "summary": "The next point-of-interest (POI) recommendation task aims to predict the\nusers' immediate next destinations based on their preferences and historical\ncheck-ins, holding significant value in location-based services. Recently,\nlarge language models (LLMs) have shown great potential in recommender systems,\nwhich treat the next POI prediction in a generative manner. However, these\nLLMs, pretrained primarily on vast corpora of unstructured text, lack the\nnative understanding of structured geographical entities and sequential\nmobility patterns required for next POI prediction tasks. Moreover, in\nindustrial-scale POI prediction applications, incorporating world knowledge and\nalignment of human cognition, such as seasons, weather conditions, holidays,\nand users' profiles (such as habits, occupation, and preferences), can enhance\nthe user experience while improving recommendation performance. To address\nthese issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a\nframework employing natural language as an interface, allowing for the\nincorporation of world knowledge, spatio-temporal trajectory patterns,\nprofiles, and situational information. Specifically, CoAST mainly comprises of\n2 stages: (1) Recommendation Knowledge Acquisition through continued\npretraining on the enriched spatial-temporal trajectory data of the\ndesensitized users; (2) Cognitive Alignment to align cognitive judgments with\nhuman preferences using enriched training data through Supervised Fine-Tuning\n(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline\nexperiments on various real-world datasets and online experiments deployed in\n\"Guess Where You Go\" of AMAP App homepage demonstrate the effectiveness of\nCoAST.",
    "published": "2025-10-16T14:05:28Z",
    "updated": "2025-10-16T14:05:28Z",
    "link": "http://arxiv.org/pdf/2510.14702v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Penglong Zhai",
      "Jie Li",
      "Fanyi Di",
      "Yue Liu",
      "Yifang Yuan",
      "Jie Huang",
      "Peng Wu",
      "Sicong Wang",
      "Mingyang Yin",
      "Tingting Hu",
      "Yao Xu",
      "Xin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.22358v2",
    "title": "Adaptive Budget Allocation for Orthogonal-Subspace Adapter Tuning in\n  LLMs Continual Learning",
    "summary": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in an end-to-end\ntraining stage. Specifically, OA-Adapter introduces a dynamic bottleneck\ndimension adaptation mechanism that simultaneously allocates an efficient\nparameter budget and optimizes task objectives without misalignment.To\neffectively preserve previously acquired knowledge while coordinating with the\ndynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency. OA-Adapter\nachieves higher average accuracy while using 58.5% fewer parameters on the\nstandard CL benchmark, and maintains its advantages on two larger benchmarks\ncomprising 15 tasks.",
    "published": "2025-05-28T13:38:21Z",
    "updated": "2025-10-16T14:03:22Z",
    "link": "http://arxiv.org/pdf/2505.22358v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhiyi Wan",
      "Wanrou Du",
      "Liang Li",
      "Miao Pan",
      "Xiaoqi Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14698v1",
    "title": "FedPPA: Progressive Parameter Alignment for Personalized Federated\n  Learning",
    "summary": "Federated Learning (FL) is designed as a decentralized, privacy-preserving\nmachine learning paradigm that enables multiple clients to collaboratively\ntrain a model without sharing their data. In real-world scenarios, however,\nclients often have heterogeneous computational resources and hold\nnon-independent and identically distributed data (non-IID), which poses\nsignificant challenges during training. Personalized Federated Learning (PFL)\nhas emerged to address these issues by customizing models for each client based\non their unique data distribution. Despite its potential, existing PFL\napproaches typically overlook the coexistence of model and data heterogeneity\narising from clients with diverse computational capabilities. To overcome this\nlimitation, we propose a novel method, called Progressive Parameter Alignment\n(FedPPA), which progressively aligns the weights of common layers across\nclients with the global model's weights. Our approach not only mitigates\ninconsistencies between global and local models during client updates, but also\npreserves client's local knowledge, thereby enhancing personalization\nrobustness in non-IID settings. To further enhance the global model performance\nwhile retaining strong personalization, we also integrate entropy-based\nweighted averaging into the FedPPA framework. Experiments on three image\nclassification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate\nthat FedPPA consistently outperforms existing FL algorithms, achieving superior\nperformance in personalized adaptation.",
    "published": "2025-10-16T14:03:05Z",
    "updated": "2025-10-16T14:03:05Z",
    "link": "http://arxiv.org/pdf/2510.14698v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Maulidi Adi Prasetia",
      "Muhamad Risqi U. Saputra",
      "Guntur Dharma Putra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14697v1",
    "title": "Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging",
    "summary": "Model merging aims to integrate task-specific abilities from individually\nfine-tuned models into a single model without extra training. In recent model\nmerging methods, task vector has become a fundamental building block, as it can\nencapsulate the residual information from finetuning. However, the merged model\noften suffers from notable performance degradation due to the conflicts caused\nby task-irrelevant redundancy in task vectors. Existing efforts in overcoming\nredundancy by randomly dropping elements in the parameter space involves\nrandomness and lacks knowledge awareness. To address these challenges, in this\nstudy, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.\nConcretely, we sample some training examples from each task, and feed them into\ntheir corresponding fine-tuned models to acquire the covariance matrices before\nlinear layers. We then perform a context-oriented singular value decomposition,\nwhich accentuates the weight components most relevant to the target knowledge.\nAs a result, we can split fine-tuned model weights into task-relevant and\nredundant components in the knowledge-aware subspace, and purify the task\nvector by pruning the redundant components. To induce fair pruning efforts\nacross models, we further introduce a spectral rank allocation strategy by\noptimizing a normalized activated pruning error. The task vector purification\nby our method as a plug-and-play scheme is applicable across various task\nvector-based merging methods to improve their performance. In experiments, we\ndemonstrate the effectiveness of PAVE across a diverse set of merging methods,\ntasks, and model architectures.",
    "published": "2025-10-16T14:02:57Z",
    "updated": "2025-10-16T14:02:57Z",
    "link": "http://arxiv.org/pdf/2510.14697v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Bang An",
      "Yibo Yang",
      "Philip Torr",
      "Bernard Ghanem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.16506v2",
    "title": "Subspace-Boosted Model Merging",
    "summary": "Model merging enables the combination of multiple specialized expert models\ninto a single model capable of performing multiple tasks. However, the benefits\nof merging an increasing amount of specialized experts generally lead to\ndiminishing returns and reduced overall performance gains. In this work, we\noffer an explanation and analysis from a task arithmetic perspective; revealing\nthat as the merging process (across numerous existing merging methods)\ncontinues for more and more experts, the associated task vector space\nexperiences rank collapse. To mitigate this issue, we introduce Subspace\nBoosting, which operates on the singular value decomposed task vector space and\nmaintains task vector ranks. Subspace Boosting raises merging efficacy for up\nto 20 expert models by large margins of more than 10% when evaluated on both\nvision and language benchmarks. Moreover, we propose employing Higher-Order\nGeneralized Singular Value Decomposition to quantify task similarity, offering\na new interpretable perspective on model merging.",
    "published": "2025-06-19T17:59:29Z",
    "updated": "2025-10-16T13:54:16Z",
    "link": "http://arxiv.org/pdf/2506.16506v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Ronald Skorobogat",
      "Karsten Roth",
      "Mariana-Iuliana Georgescu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14686v1",
    "title": "xLLM Technical Report",
    "summary": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service.",
    "published": "2025-10-16T13:53:47Z",
    "updated": "2025-10-16T13:53:47Z",
    "link": "http://arxiv.org/pdf/2510.14686v1.pdf",
    "category": [
      "cs.DC",
      "cs.AI"
    ],
    "authors": [
      "Tongxuan Liu",
      "Tao Peng",
      "Peijun Yang",
      "Xiaoyang Zhao",
      "Xiusheng Lu",
      "Weizhe Huang",
      "Zirui Liu",
      "Xiaoyu Chen",
      "Zhiwei Liang",
      "Jun Xiong",
      "Donghe Jin",
      "Minchao Zhang",
      "Jinrong Guo",
      "Yingxu Deng",
      "Xu Zhang",
      "Xianzhe Dong",
      "Siqi Wang",
      "Siyu Wu",
      "Yu Wu",
      "Zihan Tang",
      "Yuting Zeng",
      "Yanshu Wang",
      "Jinguang Liu",
      "Meng Kang",
      "Menxin Li",
      "Yunlong Wang",
      "Yiming Liu",
      "Xiaolong Ma",
      "Yifan Wang",
      "Yichen Zhang",
      "Jinrun Yin",
      "Keyang Zheng",
      "Jiawei Yin",
      "Jun Zhang",
      "Ziyue Wang",
      "Xiaobo Lin",
      "Liangyu Liu",
      "Liwei Lan",
      "Yang Liu",
      "Chunhua Peng",
      "Han Liu",
      "Songcheng Ren",
      "Xuezhu Wang",
      "Yunheng Shen",
      "Yi Wang",
      "Guyue Liu",
      "Hui Chen",
      "Tong Yang",
      "Hailong Yang",
      "Jing Li",
      "Guiguang Ding",
      "Ke Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14683v1",
    "title": "Practical, Utilitarian Algorithm Configuration",
    "summary": "Utilitarian algorithm configuration identifies a parameter setting for a\ngiven algorithm that maximizes a user's utility. Utility functions offer a\ntheoretically well-grounded approach to optimizing decision-making under\nuncertainty and are flexible enough to capture a user's preferences over\nalgorithm runtimes (e.g., they can describe a sharp cutoff after which a\nsolution is no longer required, a per-hour cost for compute, or diminishing\nreturns from algorithms that take longer to run). COUP is a recently-introduced\nutilitarian algorithm configuration procedure which was designed mainly to\noffer strong theoretical guarantees about the quality of the configuration it\nreturns, with less attention paid to its practical performance. This paper\ncloses that gap, bringing theoretically-grounded, utilitarian algorithm\nconfiguration to the point where it is competitive with widely used, heuristic\nconfiguration procedures that offer no performance guarantees. We present a\nseries of improvements to COUP that improve its empirical performance without\ndegrading its theoretical guarantees and demonstrate their benefit\nexperimentally. Using a case study, we also illustrate ways of exploring the\nrobustness of a given solution to the algorithm selection problem to variations\nin the utility function.",
    "published": "2025-10-16T13:47:41Z",
    "updated": "2025-10-16T13:47:41Z",
    "link": "http://arxiv.org/pdf/2510.14683v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Devon Graham",
      "Kevin Leyton-Brown"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08236v2",
    "title": "The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes\n  in Large Language Models",
    "summary": "Large Language Models (LLMs) are increasingly integral to information\ndissemination and decision-making processes. Given their growing societal\ninfluence, understanding potential biases, particularly within the political\ndomain, is crucial to prevent undue influence on public opinion and democratic\nprocesses. This work investigates political bias and stereotype propagation\nacross eight prominent LLMs using the two-dimensional Political Compass Test\n(PCT). Initially, the PCT is employed to assess the inherent political leanings\nof these models. Subsequently, persona prompting with the PCT is used to\nexplore explicit stereotypes across various social dimensions. In a final step,\nimplicit stereotypes are uncovered by evaluating models with multilingual\nversions of the PCT. Key findings reveal a consistent left-leaning political\nalignment across all investigated models. Furthermore, while the nature and\nextent of stereotypes vary considerably between models, implicit stereotypes\nelicited through language variation are more pronounced than those identified\nvia explicit persona prompting. Interestingly, for most models, implicit and\nexplicit stereotypes show a notable alignment, suggesting a degree of\ntransparency or \"awareness\" regarding their inherent biases. This study\nunderscores the complex interplay of political bias and stereotypes in LLMs.",
    "published": "2025-10-09T14:00:40Z",
    "updated": "2025-10-16T13:44:28Z",
    "link": "http://arxiv.org/pdf/2510.08236v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Konrad Lhr",
      "Shuzhou Yuan",
      "Michael Frber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.00102v2",
    "title": "ECG-Soup: Harnessing Multi-Layer Synergy for ECG Foundation Models",
    "summary": "Transformer-based foundation models for Electrocardiograms (ECGs) have\nrecently achieved impressive performance in many downstream applications.",
    "published": "2025-08-27T20:30:03Z",
    "updated": "2025-10-16T13:44:20Z",
    "link": "http://arxiv.org/pdf/2509.00102v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Phu X. Nguyen",
      "Huy Phan",
      "Hieu Pham",
      "Christos Chatzichristos",
      "Bert Vandenberk",
      "Maarten De Vos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.04426v3",
    "title": "The simulation of judgment in LLMs",
    "summary": "Large Language Models (LLMs) are increasingly embedded in evaluative\nprocesses, from information filtering to assessing and addressing knowledge\ngaps through explanation and credibility judgments. This raises the need to\nexamine how such evaluations are built, what assumptions they rely on, and how\ntheir strategies diverge from those of humans. We benchmark six LLMs against\nexpert ratings--NewsGuard and Media Bias/Fact Check--and against human\njudgments collected through a controlled experiment. We use news domains purely\nas a controlled benchmark for evaluative tasks, focusing on the underlying\nmechanisms rather than on news classification per se. To enable direct\ncomparison, we implement a structured agentic framework in which both models\nand nonexpert participants follow the same evaluation procedure: selecting\ncriteria, retrieving content, and producing justifications. Despite output\nalignment, our findings show consistent differences in the observable criteria\nguiding model evaluations, suggesting that lexical associations and statistical\npriors could influence evaluations in ways that differ from contextual\nreasoning. This reliance is associated with systematic effects: political\nasymmetries and a tendency to confuse linguistic form with epistemic\nreliability--a dynamic we term epistemia, the illusion of knowledge that\nemerges when surface plausibility replaces verification. Indeed, delegating\njudgment to such systems may affect the heuristics underlying evaluative\nprocesses, suggesting a shift from normative reasoning toward pattern-based\napproximation and raising open questions about the role of LLMs in evaluative\nprocesses.",
    "published": "2025-02-06T18:52:10Z",
    "updated": "2025-10-16T13:38:08Z",
    "link": "http://arxiv.org/pdf/2502.04426v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Edoardo Loru",
      "Jacopo Nudo",
      "Niccol Di Marco",
      "Alessandro Santirocchi",
      "Roberto Atzeni",
      "Matteo Cinelli",
      "Vincenzo Cestari",
      "Clelia Rossi-Arnaud",
      "Walter Quattrociocchi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14677v1",
    "title": "When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift\n  nuPlan Benchmarks",
    "summary": "Planner evaluation in closed-loop simulation often uses rule-based traffic\nagents, whose simplistic and passive behavior can hide planner deficiencies and\nbias rankings. Widely used IDM agents simply follow a lead vehicle and cannot\nreact to vehicles in adjacent lanes, hindering tests of complex interaction\ncapabilities. We address this issue by integrating the state-of-the-art learned\ntraffic agent model SMART into nuPlan. Thus, we are the first to evaluate\nplanners under more realistic conditions and quantify how conclusions shift\nwhen narrowing the sim-to-real gap. Our analysis covers 14 recent planners and\nestablished baselines and shows that IDM-based simulation overestimates\nplanning performance: nearly all scores deteriorate. In contrast, many planners\ninteract better than previously assumed and even improve in multi-lane,\ninteraction-heavy scenarios like lane changes or turns. Methods trained in\nclosed-loop demonstrate the best and most stable driving performance. However,\nwhen reaching their limits in augmented edge-case scenarios, all learned\nplanners degrade abruptly, whereas rule-based planners maintain reasonable\nbasic behavior. Based on our results, we suggest SMART-reactive simulation as a\nnew standard closed-loop benchmark in nuPlan and release the SMART agents as a\ndrop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.",
    "published": "2025-10-16T13:34:12Z",
    "updated": "2025-10-16T13:34:12Z",
    "link": "http://arxiv.org/pdf/2510.14677v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Steffen Hagedorn",
      "Luka Donkov",
      "Aron Distelzweig",
      "Alexandru P. Condurache"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14676v1",
    "title": "NAEL: Non-Anthropocentric Ethical Logic",
    "summary": "We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical\nframework for artificial agents grounded in active inference and symbolic\nreasoning. Departing from conventional, human-centred approaches to AI ethics,\nNAEL formalizes ethical behaviour as an emergent property of intelligent\nsystems minimizing global expected free energy in dynamic, multi-agent\nenvironments. We propose a neuro-symbolic architecture to allow agents to\nevaluate the ethical consequences of their actions in uncertain settings. The\nproposed system addresses the limitations of existing ethical models by\nallowing agents to develop context-sensitive, adaptive, and relational ethical\nbehaviour without presupposing anthropomorphic moral intuitions. A case study\ninvolving ethical resource distribution illustrates NAEL's dynamic balancing of\nself-preservation, epistemic learning, and collective welfare.",
    "published": "2025-10-16T13:33:10Z",
    "updated": "2025-10-16T13:33:10Z",
    "link": "http://arxiv.org/pdf/2510.14676v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Bianca Maria Lerma",
      "Rafael Pealoza"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.23315v2",
    "title": "Analysis of Hyperparameter Optimization Effects on Lightweight Deep\n  Models for Real-Time Image Classification",
    "summary": "Lightweight convolutional and transformer-based networks are increasingly\npreferred for real-time image classification, especially on\nresource-constrained devices. This study evaluates the impact of hyperparameter\noptimization on the accuracy and deployment feasibility of seven modern\nlightweight architectures: ConvNeXt-T, EfficientNetV2-S, MobileNetV3-L,\nMobileViT v2 (S/XS), RepVGG-A2, and TinyViT-21M, trained on a class-balanced\nsubset of 90,000 images from ImageNet-1K. Under standardized training settings,\nthis paper investigates the influence of learning rate schedules, augmentation,\noptimizers, and initialization on model performance. Inference benchmarks are\nperformed using an NVIDIA L40s GPU with batch sizes ranging from 1 to 512,\ncapturing latency and throughput in real-time conditions. This work\ndemonstrates that controlled hyperparameter variation significantly alters\nconvergence dynamics in lightweight CNN and transformer backbones, providing\ninsight into stability regions and deployment feasibility in edge artificial\nintelligence. Our results reveal that tuning alone leads to a top-1 accuracy\nimprovement of 1.5 to 3.5 percent over baselines, and select models (e.g.,\nRepVGG-A2, MobileNetV3-L) deliver latency under 5 milliseconds and over 9,800\nframes per second, making them ideal for edge deployment. This work provides\nreproducible, subset-based insights into lightweight hyperparameter tuning and\nits role in balancing speed and accuracy. The code and logs may be seen at:\nhttps://vineetkumarrakesh.github.io/lcnn-opt",
    "published": "2025-07-31T07:47:30Z",
    "updated": "2025-10-16T13:29:58Z",
    "link": "http://arxiv.org/pdf/2507.23315v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Vineet Kumar Rakesh",
      "Soumya Mazumdar",
      "Tapas Samanta",
      "Hemendra Kumar Pandey",
      "Amitabha Das"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14670v1",
    "title": "TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence",
    "summary": "TITAN (Threat Intelligence Through Automated Navigation) is a framework that\nconnects natural-language cyber threat queries with executable reasoning over a\nstructured knowledge graph. It integrates a path planner model, which predicts\nlogical relation chains from text, and a graph executor that traverses the\nTITAN Ontology to retrieve factual answers and supporting evidence. Unlike\ntraditional retrieval systems, TITAN operates on a typed, bidirectional graph\nderived from MITRE, allowing reasoning to move clearly and reversibly between\nthreats, behaviors, and defenses. To support training and evaluation, we\nintroduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:\n13951) pairing natural language questions with executable reasoning paths and\nstep by step Chain of Thought explanations. Empirical evaluations show that\nTITAN enables models to generate syntactically valid and semantically coherent\nreasoning paths that can be deterministically executed on the underlying graph.",
    "published": "2025-10-16T13:27:05Z",
    "updated": "2025-10-16T13:27:05Z",
    "link": "http://arxiv.org/pdf/2510.14670v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.IR"
    ],
    "authors": [
      "Marco Simoni",
      "Aleksandar Fontana",
      "Andrea Saracino",
      "Paolo Mori"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.11550v5",
    "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
    "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV.",
    "published": "2024-07-16T09:53:32Z",
    "updated": "2025-10-16T13:25:38Z",
    "link": "http://arxiv.org/pdf/2407.11550v5.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yuan Feng",
      "Junlin Lv",
      "Yukun Cao",
      "Xike Xie",
      "S. Kevin Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14669v1",
    "title": "Machine Learning and Public Health: Identifying and Mitigating\n  Algorithmic Bias through a Systematic Review",
    "summary": "Machine learning (ML) promises to revolutionize public health through\nimproved surveillance, risk stratification, and resource allocation. However,\nwithout systematic attention to algorithmic bias, ML may inadvertently\nreinforce existing health disparities. We present a systematic literature\nreview of algorithmic bias identification, discussion, and reporting in Dutch\npublic health ML research from 2021 to 2025. To this end, we developed the Risk\nof Algorithmic Bias Assessment Tool (RABAT) by integrating elements from\nestablished frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible\nAI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals\npervasive gaps: although data sampling and missing data practices are well\ndocumented, most studies omit explicit fairness framing, subgroup analyses, and\ntransparent discussion of potential harms. In response, we introduce a\nfour-stage fairness-oriented framework called ACAR (Awareness,\nConceptualization, Application, Reporting), with guiding questions derived from\nour systematic literature review to help researchers address fairness across\nthe ML lifecycle. We conclude with actionable recommendations for public health\nML practitioners to consistently consider algorithmic bias and foster\ntransparency, ensuring that algorithmic innovations advance health equity\nrather than undermine it.",
    "published": "2025-10-16T13:24:11Z",
    "updated": "2025-10-16T13:24:11Z",
    "link": "http://arxiv.org/pdf/2510.14669v1.pdf",
    "category": [
      "cs.AI",
      "68T01, 68T09, 62P10 68T01, 68T09, 62P10",
      "I.2.6; I.5.4; H.2.8; J.3; K.4.1; K.4.2"
    ],
    "authors": [
      "Sara Altamirano",
      "Arjan Vreeken",
      "Sennay Ghebreab"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14665v1",
    "title": "Beyond Hallucinations: The Illusion of Understanding in Large Language\n  Models",
    "summary": "Large language models (LLMs) are becoming deeply embedded in human\ncommunication and decision-making, yet they inherit the ambiguity, bias, and\nlack of direct access to truth inherent in language itself. While their outputs\nare fluent, emotionally resonant, and coherent, they are generated through\nstatistical prediction rather than grounded reasoning. This creates the risk of\nhallucination, responses that sound convincing but lack factual validity.\nBuilding on Geoffrey Hinton's observation that AI mirrors human intuition\nrather than reasoning, this paper argues that LLMs operationalize System 1\ncognition at scale: fast, associative, and persuasive, but without reflection\nor falsification. To address this, we introduce the Rose-Frame, a\nthree-dimensional framework for diagnosing cognitive and epistemic drift in\nhuman-AI interaction. The three axes are: (i) Map vs. Territory, which\ndistinguishes representations of reality (epistemology) from reality itself\n(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to\nseparate fast, emotional judgments from slow, reflective thinking; and (iii)\nConflict vs. Confirmation, which examines whether ideas are critically tested\nthrough disagreement or simply reinforced through mutual validation. Each\ndimension captures a distinct failure mode, and their combination amplifies\nmisalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.\nInstead, it offers a reflective tool that makes both the model's limitations\nand the user's assumptions visible, enabling more transparent and critically\naware AI deployment. It reframes alignment as cognitive governance: intuition,\nwhether human or artificial, must remain governed by human reason. Only by\nembedding reflective, falsifiable oversight can we align machine fluency with\nhuman understanding.",
    "published": "2025-10-16T13:19:44Z",
    "updated": "2025-10-16T13:19:44Z",
    "link": "http://arxiv.org/pdf/2510.14665v1.pdf",
    "category": [
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Rikard Rosenbacke",
      "Carl Rosenbacke",
      "Victor Rosenbacke",
      "Martin McKee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14660v1",
    "title": "An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs",
    "summary": "Search augmentation empowers Large Language Models with retrieval\ncapabilities to overcome the limitations imposed by static parameters.\nRecently, Reinforcement Learning leverages tailored reward signals as a viable\ntechnique to enhance LLMs performing tasks involving search. However, existing\nreward modeling for search-augmented LLMs faces several limitations. Rule-based\nrewards, such as Exact Match, are verifiable but fragile to variations in\nexpression and cannot be applied to long-form workloads. In contrast,\ngenerative rewards improve robustness, but designing verifiable and stable\nrewards for long-form workloads in dynamic corpora remains challenging and also\nincurs high computational costs. In this paper, we propose a unified and\nverifiable paradigm, \"nugget-as-rubric\", which treats atomic information points\nas structured evaluation criteria for different search-augmentation workloads.\nShort-form tasks correspond to a single rubric, whereas long-form tasks expand\nto multiple rubrics aligned with the question's information needs. To support\nlong-form settings, we design an automatic rubric construction pipeline based\non query rewriting, which can automatically retrieve passages relevant to each\nquestion and extract rubrics from them, both from static corpora and from\ndynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a\n4B-parameter efficient generative verifier under our proposed verifiable\nparadigm, which is trained via the idea of distillation and a two-stage\nstrategy. Experimental results show that Search-Gen-V achieves strong\nverification accuracy across different workloads, making it a scalable, robust,\nand efficient verifiable reward constructor for search-augmented LLMs.",
    "published": "2025-10-16T13:15:40Z",
    "updated": "2025-10-16T13:15:40Z",
    "link": "http://arxiv.org/pdf/2510.14660v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Linyue Ma",
      "Yilong Xu",
      "Xiang Long",
      "Zhi Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14655v1",
    "title": "Galaxy Morphology Classification with Counterfactual Explanation",
    "summary": "Galaxy morphologies play an essential role in the study of the evolution of\ngalaxies. The determination of morphologies is laborious for a large amount of\ndata giving rise to machine learning-based approaches. Unfortunately, most of\nthese approaches offer no insight into how the model works and make the results\ndifficult to understand and explain. We here propose to extend a classical\nencoder-decoder architecture with invertible flow, allowing us to not only\nobtain a good predictive performance but also provide additional information\nabout the decision process with counterfactual explanations.",
    "published": "2025-10-16T13:11:56Z",
    "updated": "2025-10-16T13:11:56Z",
    "link": "http://arxiv.org/pdf/2510.14655v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhuo Cao",
      "Lena Krieger",
      "Hanno Scharr",
      "Ira Assent"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12350v2",
    "title": "O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis",
    "summary": "Large language models have recently demonstrated advanced capabilities in\nsolving IMO and Putnam problems; yet their role in research mathematics has\nremained fairly limited. The key difficulty is verification: suggested proofs\nmay look plausible, but cannot be trusted without rigorous checking. We present\na framework, called LLM+CAS, and an associated tool, O-Forge, that couples\nfrontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic\nFeedback loop to produce proofs that are both creative and symbolically\nverified. Our focus is on asymptotic inequalities, a topic that often involves\ndifficult proofs and appropriate decomposition of the domain into the \"right\"\nsubdomains. Many mathematicians, including Terry Tao, have suggested that using\nAI tools to find the right decompositions can be very useful for research-level\nasymptotic analysis. In this paper, we show that our framework LLM+CAS turns\nout to be remarkably effective at proposing such decompositions via a\ncombination of a frontier LLM and a CAS. More precisely, we use an LLM to\nsuggest domain decomposition, and a CAS (such as Mathematica) that provides a\nverification of each piece axiomatically. Using this loop, we answer a question\nposed by Terence Tao: whether LLMs coupled with a verifier can be used to help\nprove intricate asymptotic inequalities. More broadly, we show how AI can move\nbeyond contest math towards research-level tools for professional\nmathematicians.",
    "published": "2025-10-14T10:07:53Z",
    "updated": "2025-10-16T13:07:41Z",
    "link": "http://arxiv.org/pdf/2510.12350v2.pdf",
    "category": [
      "cs.AI",
      "03B35, 68W30, 68T05"
    ],
    "authors": [
      "Ayush Khaitan",
      "Vijay Ganesh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14648v1",
    "title": "In-Context Learning with Unpaired Clips for Instruction-based Video\n  Editing",
    "summary": "Despite the rapid progress of instruction-based image editing, its extension\nto video remains underexplored, primarily due to the prohibitive cost and\ncomplexity of constructing large-scale paired video editing datasets. To\naddress this challenge, we introduce a low-cost pretraining strategy for\ninstruction-based video editing that leverages in-context learning from\nunpaired video clips. We show that pretraining a foundation video generation\nmodel with this strategy endows it with general editing capabilities, such as\nadding, replacing, or deleting operations, according to input editing\ninstructions. The pretrained model can then be efficiently refined with a small\namount of high-quality paired editing data. Built upon HunyuanVideoT2V, our\nframework first pretrains on approximately 1M real video clips to learn basic\nediting concepts, and subsequently fine-tunes on fewer than 150k curated\nediting pairs to extend more editing tasks and improve the editing quality.\nComparative experiments show that our method surpasses existing\ninstruction-based video editing approaches in both instruction alignment and\nvisual fidelity, achieving a 12\\% improvement in editing instruction following\nand a 15\\% improvement in editing quality.",
    "published": "2025-10-16T13:02:11Z",
    "updated": "2025-10-16T13:02:11Z",
    "link": "http://arxiv.org/pdf/2510.14648v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xinyao Liao",
      "Xianfang Zeng",
      "Ziye Song",
      "Zhoujie Fu",
      "Gang Yu",
      "Guosheng Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14642v1",
    "title": "The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon\n  Blockchain",
    "summary": "In blockchain networks, the strategic ordering of transactions within blocks\nhas emerged as a significant source of profit extraction, known as Maximal\nExtractable Value (MEV). The transition from spam-based Priority Gas Auctions\nto structured auction mechanisms like Polygon Atlas has transformed MEV\nextraction from public bidding wars into sealed-bid competitions under extreme\ntime constraints. While this shift reduces network congestion, it introduces\ncomplex strategic challenges where searchers must make optimal bidding\ndecisions within a sub-second window without knowledge of competitor behavior\nor presence. Traditional game-theoretic approaches struggle in this\nhigh-frequency, partially observable environment due to their reliance on\ncomplete information and static equilibrium assumptions. We present a\nreinforcement learning framework for MEV extraction on Polygon Atlas and make\nthree contributions: (1) A novel simulation environment that accurately models\nthe stochastic arrival of arbitrage opportunities and probabilistic competition\nin Atlas auctions; (2) A PPO-based bidding agent optimized for real-time\nconstraints, capable of adaptive strategy formulation in continuous action\nspaces while maintaining production-ready inference speeds; (3) Empirical\nvalidation demonstrating our history-conditioned agent captures 49\\% of\navailable profits when deployed alongside existing searchers and 81\\% when\nreplacing the market leader, significantly outperforming static bidding\nstrategies. Our work establishes that reinforcement learning provides a\ncritical advantage in high-frequency MEV environments where traditional\noptimization methods fail, offering immediate value for industrial participants\nand protocol designers alike.",
    "published": "2025-10-16T12:54:53Z",
    "updated": "2025-10-16T12:54:53Z",
    "link": "http://arxiv.org/pdf/2510.14642v1.pdf",
    "category": [
      "cs.GT",
      "cs.AI",
      "cs.DC"
    ],
    "authors": [
      "Andrei Seoev",
      "Leonid Gremyachikh",
      "Anastasiia Smirnova",
      "Yash Madhwal",
      "Alisa Kalacheva",
      "Dmitry Belousov",
      "Ilia Zubov",
      "Aleksei Smirnov",
      "Denis Fedyanin",
      "Vladimir Gorgadze",
      "Yury Yanovich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14641v1",
    "title": "Causality Enhancement for Cross-Domain Recommendation",
    "summary": "Cross-domain recommendation forms a crucial component in recommendation\nsystems. It leverages auxiliary information through source domain tasks or\nfeatures to enhance target domain recommendations. However, incorporating\ninconsistent source domain tasks may result in insufficient cross-domain\nmodeling or negative transfer. While incorporating source domain features\nwithout considering the underlying causal relationships may limit their\ncontribution to final predictions. Thus, a natural idea is to directly train a\ncross-domain representation on a causality-labeled dataset from the source to\ntarget domain. Yet this direction has been rarely explored, as identifying\nunbiased real causal labels is highly challenging in real-world scenarios. In\nthis work, we attempt to take a first step in this direction by proposing a\ncausality-enhanced framework, named CE-CDR. Specifically, we first reformulate\nthe cross-domain recommendation as a causal graph for principled guidance. We\nthen construct a causality-aware dataset heuristically. Subsequently, we derive\na theoretically unbiased Partial Label Causal Loss to generalize beyond the\nbiased causality-aware dataset to unseen cross-domain patterns, yielding an\nenriched cross-domain representation, which is then fed into the target model\nto enhance target-domain recommendations. Theoretical and empirical analyses,\nas well as extensive experiments, demonstrate the rationality and effectiveness\nof CE-CDR and its general applicability as a model-agnostic plugin. Moreover,\nit has been deployed in production since April 2025, showing its practical\nvalue in real-world applications.",
    "published": "2025-10-16T12:54:46Z",
    "updated": "2025-10-16T12:54:46Z",
    "link": "http://arxiv.org/pdf/2510.14641v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Zhibo Wu",
      "Yunfan Wu",
      "Lin Jiang",
      "Ping Yang",
      "Yao Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.06738v2",
    "title": "EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through\n  an Encoder-Decoder Architecture",
    "summary": "In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel\narchitecture designed to mitigate the attention sink phenomenon observed in\nVision Transformer models. Attention sink occurs when an excessive amount of\nattention is allocated to the [CLS] token, distorting the model's ability to\neffectively process image patches. To address this, we introduce a\nlayer-aligned encoder-decoder architecture, where the encoder utilizes\nself-attention to process image patches, while the decoder uses cross-attention\nto focus on the [CLS] token. Unlike traditional encoder-decoder framework,\nwhere the decoder depends solely on high-level encoder representations, EDIT\nallows the decoder to extract information starting from low-level features,\nprogressively refining the representation layer by layer. EDIT is naturally\ninterpretable demonstrated through sequential attention maps, illustrating the\nrefined, layer-by-layer focus on key image features. Experiments on ImageNet-1k\nand ImageNet-21k, along with transfer learning tasks, show that EDIT achieves\nconsistent performance improvements over DeiT3 models. These results highlight\nthe effectiveness of EDIT's design in addressing attention sink and improving\nvisual feature extraction.",
    "published": "2025-04-09T09:51:41Z",
    "updated": "2025-10-16T12:43:03Z",
    "link": "http://arxiv.org/pdf/2504.06738v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Wenfeng Feng",
      "Hongxiang Wang",
      "Jianlong Wang",
      "Xin Zhang",
      "Jingjing Zhao",
      "Yueyue Liang",
      "Xiang Chen",
      "Duokui Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14628v1",
    "title": "RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF",
    "summary": "Text-To-Speech synthesis has achieved near-human quality in neutral speech,\nbut emotional expressiveness remains a challenge. Existing methods often rely\non costly emotion annotations or optimize indirect objectives that fail to\ncapture the emotional expressiveness and perceptual naturalness of speech,\nleading to generated speech that is accurate but emotionally flat. To address\nthese challenges, we propose the RLAIF-SPA framework, incorporating a\nReinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic\nSpeech Recognition (ASR) and Large Language Model (LLM) techniques to\nrespectively judge semantic accuracy and prosodic-emotional label alignment as\na direct reward for emotional expressiveness and intelligibility optimization.\nSpecifically, it leverages Prosodic Label Alignment to enhance expressive\nquality by jointly considering semantic accuracy and prosodic-emotional\nalignment along four fine-grained dimensions: Structure, Emotion, Speed, and\nTone. In addition, it incorporates Semantic Accuracy Feedback to ensure the\ngeneration of clear and accurate speech. Experiments on the Libri Speech\ndataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in\nWER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.",
    "published": "2025-10-16T12:40:37Z",
    "updated": "2025-10-16T12:40:37Z",
    "link": "http://arxiv.org/pdf/2510.14628v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Qing Yang",
      "Zhenghao Liu",
      "Junxin Wang",
      "Yangfan Du",
      "Pengcheng Huang",
      "Tong Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.21278v2",
    "title": "Does FLUX Already Know How to Perform Physically Plausible Image\n  Composition?",
    "summary": "Image composition aims to seamlessly insert a user-specified object into a\nnew scene, but existing models struggle with complex lighting (e.g., accurate\nshadows, water reflections) and diverse, high-resolution inputs. Modern\ntext-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential\nphysical and resolution priors, yet lack a framework to unleash them without\nresorting to latent inversion, which often locks object poses into contextually\ninappropriate orientations, or brittle attention surgery. We propose SHINE, a\ntraining-free framework for Seamless, High-fidelity Insertion with Neutralized\nErrors. SHINE introduces manifold-steered anchor loss, leveraging pretrained\ncustomization adapters (e.g., IP-Adapter) to guide latents for faithful subject\nrepresentation while preserving background integrity. Degradation-suppression\nguidance and adaptive background blending are proposed to further eliminate\nlow-quality outputs and visible seams. To address the lack of rigorous\nbenchmarks, we introduce ComplexCompo, featuring diverse resolutions and\nchallenging conditions such as low lighting, strong illumination, intricate\nshadows, and reflective surfaces. Experiments on ComplexCompo and\nDreamEditBench show state-of-the-art performance on standard metrics (e.g.,\nDINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).\nCode and benchmark will be publicly available upon publication.",
    "published": "2025-09-25T15:01:49Z",
    "updated": "2025-10-16T12:37:53Z",
    "link": "http://arxiv.org/pdf/2509.21278v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Shilin Lu",
      "Zhuming Lian",
      "Zihan Zhou",
      "Shaocong Zhang",
      "Chen Zhao",
      "Adams Wai-Kin Kong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14626v1",
    "title": "GemiRec: Interest Quantization and Generation for Multi-Interest\n  Recommendation",
    "summary": "Multi-interest recommendation has gained attention, especially in industrial\nretrieval stage. Unlike classical dual-tower methods, it generates multiple\nuser representations instead of a single one to model comprehensive user\ninterests. However, prior studies have identified two underlying limitations:\nthe first is interest collapse, where multiple representations homogenize. The\nsecond is insufficient modeling of interest evolution, as they struggle to\ncapture latent interests absent from a user's historical behavior. We begin\nwith a thorough review of existing works in tackling these limitations. Then,\nwe attempt to tackle these limitations from a new perspective. Specifically, we\npropose a framework-level refinement for multi-interest recommendation, named\nGemiRec. The proposed framework leverages interest quantization to enforce a\nstructural interest separation and interest generation to learn the evolving\ndynamics of user interests explicitly. It comprises three modules: (a) Interest\nDictionary Maintenance Module (IDMM) maintains a shared quantized interest\ndictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a\ngenerative model to capture the distribution of user future interests. (c)\nMulti-Interest Retrieval Module (MIRM) retrieves items using multiple\nuser-interest representations. Both theoretical and empirical analyses, as well\nas extensive experiments, demonstrate its advantages and effectiveness.\nMoreover, it has been deployed in production since March 2025, showing its\npractical value in industrial applications.",
    "published": "2025-10-16T12:37:15Z",
    "updated": "2025-10-16T12:37:15Z",
    "link": "http://arxiv.org/pdf/2510.14626v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Zhibo Wu",
      "Yunfan Wu",
      "Quan Liu",
      "Lin Jiang",
      "Ping Yang",
      "Yao Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14623v1",
    "title": "LeapFactual: Reliable Visual Counterfactual Explanation Using\n  Conditional Flow Matching",
    "summary": "The growing integration of machine learning (ML) and artificial intelligence\n(AI) models into high-stakes domains such as healthcare and scientific research\ncalls for models that are not only accurate but also interpretable. Among the\nexisting explainable methods, counterfactual explanations offer\ninterpretability by identifying minimal changes to inputs that would alter a\nmodel's prediction, thus providing deeper insights. However, current\ncounterfactual generation methods suffer from critical limitations, including\ngradient vanishing, discontinuous latent spaces, and an overreliance on the\nalignment between learned and true decision boundaries. To overcome these\nlimitations, we propose LeapFactual, a novel counterfactual explanation\nalgorithm based on conditional flow matching. LeapFactual generates reliable\nand informative counterfactuals, even when true and learned decision boundaries\ndiverge. Following a model-agnostic approach, LeapFactual is not limited to\nmodels with differentiable loss functions. It can even handle human-in-the-loop\nsystems, expanding the scope of counterfactual explanations to domains that\nrequire the participation of human annotators, such as citizen science. We\nprovide extensive experiments on benchmark and real-world datasets showing that\nLeapFactual generates accurate and in-distribution counterfactual explanations\nthat offer actionable insights. We observe, for instance, that our reliable\ncounterfactual samples with labels aligning to ground truth can be beneficially\nused as new training data to enhance the model. The proposed method is broadly\napplicable and enhances both scientific knowledge discovery and non-expert\ninterpretability.",
    "published": "2025-10-16T12:34:10Z",
    "updated": "2025-10-16T12:34:10Z",
    "link": "http://arxiv.org/pdf/2510.14623v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhuo Cao",
      "Xuan Zhao",
      "Lena Krieger",
      "Hanno Scharr",
      "Ira Assent"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.12045v2",
    "title": "Large Language Models Enable Design of Personalized Nudges across\n  Cultures",
    "summary": "Nudge strategies are effective tools for influencing behaviour, but their\nimpact depends on individual preferences. Strategies that work for some\nindividuals may be counterproductive for others. We hypothesize that large\nlanguage models (LLMs) can facilitate the design of individual-specific nudges\nwithout the need for costly and time-intensive behavioural data collection and\nmodelling. To test this, we use LLMs to design personalized decoy-based nudges\ntailored to individual profiles and cultural contexts, aimed at encouraging air\ntravellers to voluntarily offset CO$_2$ emissions from flights. We evaluate\ntheir effectiveness through a large-scale survey experiment ($n=3495$)\nconducted across five countries. Results show that LLM-informed personalized\nnudges are more effective than uniform settings, raising offsetting rates by\n3-7$\\%$ in Germany, Singapore, and the US, though not in China or India. Our\nstudy highlights the potential of LLM as a low-cost testbed for piloting nudge\nstrategies. At the same time, cultural heterogeneity constrains their\ngeneralizability underscoring the need for combining LLM-based simulations with\ntargeted empirical validation.",
    "published": "2025-08-16T13:40:44Z",
    "updated": "2025-10-16T12:33:13Z",
    "link": "http://arxiv.org/pdf/2508.12045v2.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Vladimir Maksimenko",
      "Qingyao Xin",
      "Prateek Gupta",
      "Bin Zhang",
      "Prateek Bansal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.15738v2",
    "title": "Checkpoint-GCG: Auditing and Attacking Fine-Tuning-Based Prompt\n  Injection Defenses",
    "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications ranging from chatbots to agentic systems, where they are expected\nto process untrusted data and follow trusted instructions. Failure to\ndistinguish between the two poses significant security risks, exploited by\nprompt injection attacks, which inject malicious instructions into the data to\ncontrol model outputs. Model-level defenses have been proposed to mitigate\nprompt injection attacks. These defenses fine-tune LLMs to ignore injected\ninstructions in untrusted data. We introduce Checkpoint-GCG, a white-box attack\nagainst fine-tuning-based defenses. Checkpoint-GCG enhances the Greedy\nCoordinate Gradient (GCG) attack by leveraging intermediate model checkpoints\nproduced during fine-tuning to initialize GCG, with each checkpoint acting as a\nstepping stone for the next one to continuously improve attacks. First, we\ninstantiate Checkpoint-GCG to evaluate the robustness of the state-of-the-art\ndefenses in an auditing setup, assuming both (a) full knowledge of the model\ninput and (b) access to intermediate model checkpoints. We show Checkpoint-GCG\nto achieve up to $96\\%$ attack success rate (ASR) against the strongest\ndefense. Second, we relax the first assumption by searching for a universal\nsuffix that would work on unseen inputs, and obtain up to $89.9\\%$ ASR against\nthe strongest defense. Finally, we relax both assumptions by searching for a\nuniversal suffix that would transfer to similar black-box models and defenses,\nachieving an ASR of $63.9\\%$ against a newly released defended model from Meta.",
    "published": "2025-05-21T16:43:17Z",
    "updated": "2025-10-16T12:31:18Z",
    "link": "http://arxiv.org/pdf/2505.15738v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Xiaoxue Yang",
      "Bozhidar Stevanoski",
      "Matthieu Meeus",
      "Yves-Alexandre de Montjoye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14621v1",
    "title": "ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework\n  for Complex Long-Horizon Tasks",
    "summary": "The rapid advancement of multimodal large language models has enabled agents\nto operate mobile devices by directly interacting with graphical user\ninterfaces, opening new possibilities for mobile automation. However,\nreal-world mobile tasks are often complex and allow for multiple valid\nsolutions. This contradicts current mobile agent evaluation standards: offline\nstatic benchmarks can only validate a single predefined \"golden path\", while\nonline dynamic testing is constrained by the complexity and non-reproducibility\nof real devices, making both approaches inadequate for comprehensively\nassessing agent capabilities. To bridge the gap between offline and online\nevaluation and enhance testing stability, this paper introduces a novel\ngraph-structured benchmarking framework. By modeling the finite states observed\nduring real-device interactions, it achieves static simulation of dynamic\nbehaviors. Building on this, we develop ColorBench, a benchmark focused on\ncomplex long-horizon tasks. It supports evaluation of multiple valid solutions,\nsubtask completion rate statistics, and atomic-level capability analysis.\nColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average\nlength of over 13 steps. Each task includes at least two correct paths and\nseveral typical error paths, enabling quasi-dynamic interaction. By evaluating\nColorBench across various baselines, we discover limitations of existing models\nand propose improvement directions and feasible technical pathways to enhance\nagents' performance on complex, long-horizon problems based on experimental\nresults. Code and data are available at:\nhttps://github.com/MadeAgents/ColorBench.",
    "published": "2025-10-16T12:30:05Z",
    "updated": "2025-10-16T12:30:05Z",
    "link": "http://arxiv.org/pdf/2510.14621v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Yuanyi Song",
      "Heyuan Huang",
      "Qiqiang Lin",
      "Yin Zhao",
      "Xiangmou Qu",
      "Jun Wang",
      "Xingyu Lou",
      "Weiwen Liu",
      "Zhuosheng Zhang",
      "Jun Wang",
      "Yong Yu",
      "Weinan Zhang",
      "Zhaoxiang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14620v1",
    "title": "Code-driven Number Sequence Calculation: Enhancing the inductive\n  Reasoning Abilities of Large Language Models",
    "summary": "Large language models (LLMs) make remarkable progress in reasoning tasks.\nAmong different reasoning modes, inductive reasoning, due to its better\nalignment with human learning, attracts increasing interest. However, research\non inductive reasoning faces certain challenges. First, existing inductive data\nmostly focuses on superficial regularities while lacking more complex internal\npatterns. Second, current works merely prompt LLMs or finetune on simple\nprompt-response pairs, but do not provide precise thinking processes nor\nimplement difficulty control. Unlike previous work, we address these challenges\nby introducing \\textit{CodeSeq}, a synthetic post-training dataset built from\nnumber sequences. We package number sequences into algorithmic problems to\ndiscover their general terms, defining a general term generation (GTG) task\ncorrespondingly. Our pipeline generates supervised finetuning data by\nreflecting on failed test cases and incorporating iterative corrections,\nthereby teaching LLMs to learn autonomous case generation and self-checking.\nAdditionally, it leverages reinforcement learning with a novel Case-Synergy\nSolvability Scaling Reward based on both solvability, estimated from the\nproblem pass rate, and the success rate of self-directed case generation,\nenabling models to learn more effectively from both successes and failures.\nExperimental results show that the models trained with \\textit{CodeSeq} improve\non various reasoning tasks and can preserve the models' OOD performance.",
    "published": "2025-10-16T12:29:40Z",
    "updated": "2025-10-16T12:29:40Z",
    "link": "http://arxiv.org/pdf/2510.14620v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Kedi Chen",
      "Zhikai Lei",
      "Xu Guo",
      "Xuecheng Wu",
      "Siyuan Zeng",
      "Jianghao Yin",
      "Yinqi Zhang",
      "Qin Chen",
      "Jie Zhou",
      "Liang He",
      "Qipeng Guo",
      "Kai Chen",
      "Wei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.12171v2",
    "title": "Preservation of Language Understanding Capabilities in Speech-aware\n  Large Language Models",
    "summary": "The paper presents C3T (Cross-modal Capabilities Conservation Test), a new\nbenchmark for assessing the performance of speech-aware large language models.\nThe benchmark utilizes textual tasks and a voice cloning text-to-speech model\nto quantify the extent to which language understanding capabilities are\npreserved when the model is accessed via speech input. C3T quantifies the\nfairness of the model for different categories of speakers and its robustness\nacross text and speech modalities.",
    "published": "2025-09-15T17:34:45Z",
    "updated": "2025-10-16T12:28:23Z",
    "link": "http://arxiv.org/pdf/2509.12171v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Marek Kubis",
      "Pawe Skrzewski",
      "Iwona Christop",
      "Mateusz Czynikiewicz",
      "Jakub Kubiak",
      "ukasz Bondaruk",
      "Marcin Lewandowski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14616v1",
    "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across\n  Cultures",
    "summary": "Current preference learning methods achieve high accuracy on standard\nbenchmarks but exhibit significant performance degradation when objective\nquality signals are removed. We introduce WritingPreferenceBench, a dataset of\n1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8\ncreative writing genres, where responses are matched for objective correctness,\nfactual accuracy, and length. On this benchmark, sequence-based reward\nmodels--the standard architecture for RLHF--achieve only 52.7% mean accuracy,\nwhile zero-shot language model judges perform at 53.9%. In contrast, generative\nreward models that produce explicit reasoning chains achieve 81.8% accuracy. We\nobserve high within-model variance across genres: individual models range from\n18.2% to 81.8% accuracy across different writing categories, with standard\ndeviations averaging 10.1%. This variance persists regardless of model scale,\nwith 27B parameter models showing no consistent improvement over 8B variants.\nOur results suggest that current RLHF methods primarily learn to detect\nobjective errors rather than capture subjective quality preferences (e.g.,\ncreativity, stylistic flair, and emotional resonance), and that successful\npreference modeling may require intermediate reasoning representations rather\nthan direct classification.",
    "published": "2025-10-16T12:23:13Z",
    "updated": "2025-10-16T12:23:13Z",
    "link": "http://arxiv.org/pdf/2510.14616v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Shuangshuang Ying",
      "Yunwen Li",
      "Xingwei Qu",
      "Xin Li",
      "Sheng Jin",
      "Minghao Liu",
      "Zhoufutu Wen",
      "Xeron Du",
      "Tianyu Zheng",
      "Yichi Zhang",
      "Letian Ni",
      "Yuyang Cheng",
      "Qiguang Chen",
      "Jingzhe Ding",
      "Shengda Long",
      "Wangchunshu Zhou",
      "Jiazhan Feng",
      "Wanjun Zhong",
      "Libo Qin",
      "Ge Zhang",
      "Wenhao Huang",
      "Wanxiang Che",
      "Chenghua Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13524v2",
    "title": "A Methodology for Assessing the Risk of Metric Failure in LLMs Within\n  the Financial Domain",
    "summary": "As Generative Artificial Intelligence is adopted across the financial\nservices industry, a significant barrier to adoption and usage is measuring\nmodel performance. Historical machine learning metrics can oftentimes fail to\ngeneralize to GenAI workloads and are often supplemented using Subject Matter\nExpert (SME) Evaluation. Even in this combination, many projects fail to\naccount for various unique risks present in choosing specific metrics.\nAdditionally, many widespread benchmarks created by foundational research labs\nand educational institutions fail to generalize to industrial use. This paper\nexplains these challenges and provides a Risk Assessment Framework to allow for\nbetter application of SME and machine learning Metrics",
    "published": "2025-10-15T13:17:16Z",
    "updated": "2025-10-16T12:21:22Z",
    "link": "http://arxiv.org/pdf/2510.13524v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "William Flanagan",
      "Mukunda Das",
      "Rajitha Ramanayake",
      "Swanuja Maslekar",
      "Meghana Mangipudi",
      "Joong Ho Choi",
      "Shruti Nair",
      "Shambhavi Bhusan",
      "Sanjana Dulam",
      "Mouni Pendharkar",
      "Nidhi Singh",
      "Vashisth Doshi",
      "Sachi Shah Paresh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14611v1",
    "title": "An Active Inference Model of Mouse Point-and-Click Behaviour",
    "summary": "We explore the use of Active Inference (AIF) as a computational user model\nfor spatial pointing, a key problem in Human-Computer Interaction (HCI). We\npresent an AIF agent with continuous state, action, and observation spaces,\nperforming one-dimensional mouse pointing and clicking. We use a simple\nunderlying dynamic system to model the mouse cursor dynamics with realistic\nperceptual delay. In contrast to previous optimal feedback control-based\nmodels, the agent's actions are selected by minimizing Expected Free Energy,\nsolely based on preference distributions over percepts, such as observing\nclicking a button correctly. Our results show that the agent creates plausible\npointing movements and clicks when the cursor is over the target, with similar\nend-point variance to human users. In contrast to other models of pointing, we\nincorporate fully probabilistic, predictive delay compensation into the agent.\nThe agent shows distinct behaviour for differing target difficulties without\nthe need to retune system parameters, as done in other approaches. We discuss\nthe simulation results and emphasize the challenges in identifying the correct\nconfiguration of an AIF agent interacting with continuous systems.",
    "published": "2025-10-16T12:19:38Z",
    "updated": "2025-10-16T12:19:38Z",
    "link": "http://arxiv.org/pdf/2510.14611v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Markus Klar",
      "Sebastian Stein",
      "Fraser Paterson",
      "John H. Williamson",
      "Roderick Murray-Smith"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.10597v2",
    "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models",
    "summary": "Large Language Models (LLMs) have achieved remarkable progress, but their\ndeployment has exposed critical vulnerabilities, particularly to jailbreak\nattacks that circumvent safety alignments. Guardrails--external defense\nmechanisms that monitor and control LLM interactions--have emerged as a\npromising solution. However, the current landscape of LLM guardrails is\nfragmented, lacking a unified taxonomy and comprehensive evaluation framework.\nIn this Systematization of Knowledge (SoK) paper, we present the first holistic\nanalysis of jailbreak guardrails for LLMs. We propose a novel,\nmulti-dimensional taxonomy that categorizes guardrails along six key\ndimensions, and introduce a Security-Efficiency-Utility evaluation framework to\nassess their practical effectiveness. Through extensive analysis and\nexperiments, we identify the strengths and limitations of existing guardrail\napproaches, provide insights into optimizing their defense mechanisms, and\nexplore their universality across attack types. Our work offers a structured\nfoundation for future research and development, aiming to guide the principled\nadvancement and deployment of robust LLM guardrails. The code is available at\nhttps://github.com/xunguangwang/SoK4JailbreakGuardrails.",
    "published": "2025-06-12T11:42:40Z",
    "updated": "2025-10-16T12:15:42Z",
    "link": "http://arxiv.org/pdf/2506.10597v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Xunguang Wang",
      "Zhenlan Ji",
      "Wenxuan Wang",
      "Zongjie Li",
      "Daoyuan Wu",
      "Shuai Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14605v1",
    "title": "Knowledge-based Visual Question Answer with Multimodal Processing,\n  Retrieval and Filtering",
    "summary": "Knowledge-based visual question answering (KB-VQA) requires visual language\nmodels (VLMs) to integrate visual understanding with external knowledge\nretrieval. Although retrieval-augmented generation (RAG) achieves significant\nadvances in this task by combining knowledge-base querying, it still struggles\nwith the quality of multimodal queries and the relevance of retrieved results.\nTo overcome these challenges, we propose a novel three-stage method, termed\nWiki-PRF, including Processing, Retrieval and Filtering stages. The processing\nstage dynamically invokes visual tools to extract precise multimodal\ninformation for retrieval. The retrieval stage integrates visual and text\nfeatures to achieve multimodal knowledge retrieval. The filtering stage\nperforms relevance filtering and concentration on retrieval results. To this\nend, we introduce a visual language model trained with answer accuracy and\nformat consistency as reward signals via a reinforcement learning manner. This\nenhances the model's reasoning, tool invocation for accurate queries, and\nfiltering of irrelevant content. Experiments on benchmark datasets (E-VQA and\nInfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,\nachieving state-of-the-art performance. Code is available at\nhttps://github.com/cqu-student/Wiki-PRF",
    "published": "2025-10-16T12:10:00Z",
    "updated": "2025-10-16T12:10:00Z",
    "link": "http://arxiv.org/pdf/2510.14605v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yuyang Hong",
      "Jiaqi Gu",
      "Qi Yang",
      "Lubin Fan",
      "Yue Wu",
      "Ying Wang",
      "Kun Ding",
      "Shiming Xiang",
      "Jieping Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.20427v2",
    "title": "Rethinking Purity and Diversity in Multi-Behavior Sequential\n  Recommendation from the Frequency Perspective",
    "summary": "In recommendation systems, users often exhibit multiple behaviors, such as\nbrowsing, clicking, and purchasing. Multi-behavior sequential recommendation\n(MBSR) aims to consider these different behaviors in an integrated manner to\nimprove the recommendation performance of the target behavior. However, some\nbehavior data will also bring inevitable noise to the modeling of user\ninterests. Some research efforts focus on data denoising from the frequency\ndomain perspective to improve the accuracy of user preference prediction. These\nstudies indicate that low-frequency information tends to be valuable and\nreliable, while high-frequency information is often associated with noise. In\nthis paper, we argue that high-frequency information is by no means\ninsignificant. Further experimental results highlight that low frequency\ncorresponds to the purity of user interests, while high frequency corresponds\nto the diversity of user interests. Building upon this finding, we proposed our\nmodel PDB4Rec, which efficiently extracts information across various frequency\nbands and their relationships, and introduces Boostrapping Balancer mechanism\nto balance their contributions for improved recommendation performance.\nSufficient experiments on real-world datasets demonstrate the effectiveness and\nefficiency of our model.",
    "published": "2025-08-28T04:55:02Z",
    "updated": "2025-10-16T11:58:20Z",
    "link": "http://arxiv.org/pdf/2508.20427v2.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Yongqiang Han",
      "Kai Cheng",
      "Kefan Wang",
      "Enhong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.16292v3",
    "title": "Visual Stereotypes of Autism Spectrum in Janus-Pro-7B, DALL-E, Stable\n  Diffusion, SDXL, FLUX, and Midjourney",
    "summary": "Avoiding systemic discrimination of neurodiverse individuals is an ongoing\nchallenge in training AI models, which often propagate negative stereotypes.\nThis study examined whether six text-to-image models (Janus-Pro-7B VL2 vs. VL3,\nDALL-E 3 v. April 2024 vs. August 2025, Stable Diffusion v. 1.6 vs. 3.5, SDXL\nv. April 2024 vs. FLUX.1 Pro, and Midjourney v. 5.1 vs. 7) perpetuate\nnon-rational beliefs regarding autism by comparing images generated in\n2024-2025 with controls. 53 prompts aimed at neutrally visualizing concrete\nobjects and abstract concepts related to autism were used against 53 controls\n(baseline total N=302, follow-up experimental 280 images plus 265 controls).\nExpert assessment measuring the presence of common autism-related stereotypes\nemployed a framework of 10 deductive codes followed by statistical analysis.\nAutistic individuals were depicted with striking homogeneity in skin color\n(white), gender (male), and age (young), often engaged in solitary activities,\ninteracting with objects rather than people, and exhibiting stereotypical\nemotional expressions such as sadness, anger, or emotional flatness. In\ncontrast, the images of neurotypical individuals were more diverse and lacked\nsuch traits. We found significant differences between the models; however, with\na moderate effect size, and no differences between baseline and follow-up\nsummary values, with the ratio of stereotypical themes to the number of images\nsimilar across all models. The control prompts showed a significantly lower\ndegree of stereotyping with large size effects, confirming the hidden biases of\nthe models. In summary, despite improvements in the technical aspects of image\ngeneration, the level of reproduction of potentially harmful autism-related\nstereotypes remained largely unaffected.",
    "published": "2024-07-23T08:48:09Z",
    "updated": "2025-10-16T11:56:26Z",
    "link": "http://arxiv.org/pdf/2407.16292v3.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Maciej Wodziski",
      "Marcin Rzdeczka",
      "Anastazja Szua",
      "Kacper Dudzic",
      "Marcin Moskalewicz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10480v2",
    "title": "Latent Retrieval Augmented Generation of Cross-Domain Protein Binders",
    "summary": "Designing protein binders targeting specific sites, which requires to\ngenerate realistic and functional interaction patterns, is a fundamental\nchallenge in drug discovery. Current structure-based generative models are\nlimited in generating nterfaces with sufficient rationality and\ninterpretability. In this paper, we propose Retrieval-Augmented Diffusion for\nAligned interface (RADiAnce), a new framework that leverages known interfaces\nto guide the design of novel binders. By unifying retrieval and generation in a\nshared contrastive latent space, our model efficiently identifies relevant\ninterfaces for a given binding site and seamlessly integrates them through a\nconditional latent diffusion generator, enabling cross-domain interface\ntransfer. Extensive exeriments show that RADiAnce significantly outperforms\nbaseline models across multiple metrics, including binding affinity and\nrecovery of geometries and interactions. Additional experimental results\nvalidate cross-domain generalization, demonstrating that retrieving interfaces\nfrom diverse domains, such as peptides, antibodies, and protein fragments,\nenhances the generation performance of binders for other domains. Our work\nestablishes a new paradigm for protein binder design that successfully bridges\nretrieval-based knowledge and generative AI, opening new possibilities for drug\ndiscovery.",
    "published": "2025-10-12T07:26:11Z",
    "updated": "2025-10-16T11:55:27Z",
    "link": "http://arxiv.org/pdf/2510.10480v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zishen Zhang",
      "Xiangzhe Kong",
      "Wenbing Huang",
      "Yang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14591v1",
    "title": "Just-In-Time Objectives: A General Approach for Specialized AI\n  Interactions",
    "summary": "Large language models promise a broad set of functions, but when not given a\nspecific objective, they default to milquetoast results such as drafting emails\nlittered with cliches. We demonstrate that inferring the user's in-the-moment\nobjective, then rapidly optimizing for that singular objective, enables LLMs to\nproduce tools, interfaces, and responses that are more responsive and desired.\nWe contribute an architecture for automatically inducing just-in-time\nobjectives by passively observing user behavior, then steering downstream AI\nsystems through generation and evaluation against this objective. Inducing\njust-in-time objectives (e.g., \"Clarify the abstract's research contribution\")\nenables automatic generation of tools, e.g., those that critique a draft based\non relevant HCI methodologies, anticipate related researchers' reactions, or\nsurface ambiguous terminology. In a series of experiments (N=14, N=205) on\nparticipants' own tasks, JIT objectives enable LLM outputs that achieve 66-86%\nwin rates over typical LLMs, and in-person use sessions (N=17) confirm that JIT\nobjectives produce specialized tools unique to each participant.",
    "published": "2025-10-16T11:53:17Z",
    "updated": "2025-10-16T11:53:17Z",
    "link": "http://arxiv.org/pdf/2510.14591v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Michelle S. Lam",
      "Omar Shaikh",
      "Hallie Xu",
      "Alice Guo",
      "Diyi Yang",
      "Jeffrey Heer",
      "James A. Landay",
      "Michael S. Bernstein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14588v1",
    "title": "STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored\n  Encoding",
    "summary": "Video generation has recently made striking visual progress, but maintaining\ncoherent object motion and interactions remains difficult. We trace two\npractical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)\noften collapse to too few effective tokens after encoding, weakening guidance;\nand (ii) optimizing for appearance and motion in a single head can favor\ntexture over temporal consistency. We present STANCE, an image-to-video\nframework that addresses both issues with two simple components. First, we\nintroduce Instance Cues -- a pixel-aligned control signal that turns sparse,\nuser-editable hints into a dense 2.5D (camera-relative) motion field by\naveraging per-instance flow and augmenting with monocular depth over the\ninstance mask. This reduces depth ambiguity compared to 2D arrow inputs while\nremaining easy to use. Second, we preserve the salience of these cues in token\nspace with Dense RoPE, which tags a small set of motion tokens (anchored on the\nfirst frame) with spatial-addressable rotary embeddings. Paired with joint RGB\n\\(+\\) auxiliary-map prediction (segmentation or depth), our model anchors\nstructure while RGB handles appearance, stabilizing optimization and improving\ntemporal coherence without requiring per-frame trajectory scripts.",
    "published": "2025-10-16T11:50:38Z",
    "updated": "2025-10-16T11:50:38Z",
    "link": "http://arxiv.org/pdf/2510.14588v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Zhifei Chen",
      "Tianshuo Xu",
      "Leyi Wu",
      "Luozhou Wang",
      "Dongyu Yan",
      "Zihan You",
      "Wenting Luo",
      "Guo Zhang",
      "Yingcong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07141v2",
    "title": "Comparing Human and Language Models Sentence Processing Difficulties on\n  Complex Structures",
    "summary": "Large language models (LLMs) that fluently converse with humans are a reality\n- but do LLMs experience human-like processing difficulties? We systematically\ncompare human and LLM sentence comprehension across seven challenging\nlinguistic structures. We collect sentence comprehension data from humans and\nfive families of state-of-the-art LLMs, varying in size and training procedure\nin a unified experimental framework. Our results show LLMs overall struggle on\nthe target structures, but especially on garden path (GP) sentences. Indeed,\nwhile the strongest models achieve near perfect accuracy on non-GP structures\n(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).\nAdditionally, when ranking structures based on average performance, rank\ncorrelation between humans and models increases with parameter count. For each\ntarget structure, we also collect data for their matched baseline without the\ndifficult structure. Comparing performance on the target vs. baseline\nsentences, the performance gap observed in humans holds for LLMs, with two\nexceptions: for models that are too weak performance is uniformly low across\nboth sentence types, and for models that are too strong the performance is\nuniformly high. Together, these reveal convergence and divergence in human and\nLLM sentence comprehension, offering new insights into the similarity of humans\nand LLMs.",
    "published": "2025-10-08T15:42:49Z",
    "updated": "2025-10-16T11:40:29Z",
    "link": "http://arxiv.org/pdf/2510.07141v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Samuel Joseph Amouyal",
      "Aya Meltzer-Asscher",
      "Jonathan Berant"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14582v1",
    "title": "Local Causal Discovery for Statistically Efficient Causal Inference",
    "summary": "Causal discovery methods can identify valid adjustment sets for causal effect\nestimation for a pair of target variables, even when the underlying causal\ngraph is unknown. Global causal discovery methods focus on learning the whole\ncausal graph and therefore enable the recovery of optimal adjustment sets,\ni.e., sets with the lowest asymptotic variance, but they quickly become\ncomputationally prohibitive as the number of variables grows. Local causal\ndiscovery methods offer a more scalable alternative by focusing on the local\nneighborhood of the target variables, but are restricted to statistically\nsuboptimal adjustment sets. In this work, we propose Local Optimal Adjustments\nDiscovery (LOAD), a sound and complete causal discovery approach that combines\nthe computational efficiency of local methods with the statistical optimality\nof global methods. First, LOAD identifies the causal relation between the\ntargets and tests if the causal effect is identifiable by using only local\ninformation. If it is identifiable, it then finds the optimal adjustment set by\nleveraging local causal discovery to infer the mediators and their parents.\nOtherwise, it returns the locally valid parent adjustment sets based on the\nlearned local structure. In our experiments on synthetic and realistic data\nLOAD outperforms global methods in scalability, while providing more accurate\neffect estimation than local methods.",
    "published": "2025-10-16T11:39:02Z",
    "updated": "2025-10-16T11:39:02Z",
    "link": "http://arxiv.org/pdf/2510.14582v1.pdf",
    "category": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Mtys Schubert",
      "Tom Claassen",
      "Sara Magliacane"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14581v1",
    "title": "Selective Labeling with False Discovery Rate Control",
    "summary": "Obtaining high-quality labels for large datasets is expensive, requiring\nmassive annotations from human experts. While AI models offer a cost-effective\nalternative by predicting labels, their label quality is compromised by the\nunavoidable labeling errors. Existing methods mitigate this issue through\nselective labeling, where AI labels a subset and human labels the remainder.\nHowever, these methods lack theoretical guarantees on the quality of\nAI-assigned labels, often resulting in unacceptably high labeling error within\nthe AI-labeled subset. To address this, we introduce \\textbf{Conformal\nLabeling}, a novel method to identify instances where AI predictions can be\nprovably trusted. This is achieved by controlling the false discovery rate\n(FDR), the proportion of incorrect labels within the selected subset. In\nparticular, we construct a conformal $p$-value for each test instance by\ncomparing AI models' predicted confidence to those of calibration instances\nmislabeled by AI models. Then, we select test instances whose $p$-values are\nbelow a data-dependent threshold, certifying AI models' predictions as\ntrustworthy. We provide theoretical guarantees that Conformal Labeling controls\nthe FDR below the nominal level, ensuring that a predefined fraction of\nAI-assigned labels is correct on average. Extensive experiments demonstrate\nthat our method achieves tight FDR control with high power across various\ntasks, including image and text labeling, and LLM QA.",
    "published": "2025-10-16T11:39:00Z",
    "updated": "2025-10-16T11:39:00Z",
    "link": "http://arxiv.org/pdf/2510.14581v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Huipeng Huang",
      "Wenbo Liao",
      "Huajun Xi",
      "Hao Zeng",
      "Mengchen Zhao",
      "Hongxin Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05670v2",
    "title": "Quantifying the Accuracy-Interpretability Trade-Off in Concept-Based\n  Sidechannel Models",
    "summary": "Concept Bottleneck Models (CBNMs) are deep learning models that provide\ninterpretability by enforcing a bottleneck layer where predictions are based\nexclusively on human-understandable concepts. However, this constraint also\nrestricts information flow and often results in reduced predictive accuracy.\nConcept Sidechannel Models (CSMs) address this limitation by introducing a\nsidechannel that bypasses the bottleneck and carry additional task-relevant\ninformation. While this improves accuracy, it simultaneously compromises\ninterpretability, as predictions may rely on uninterpretable representations\ntransmitted through sidechannels. Currently, there exists no principled\ntechnique to control this fundamental trade-off. In this paper, we close this\ngap. First, we present a unified probabilistic concept sidechannel meta-model\nthat subsumes existing CSMs as special cases. Building on this framework, we\nintroduce the Sidechannel Independence Score (SIS), a metric that quantifies a\nCSM's reliance on its sidechannel by contrasting predictions made with and\nwithout sidechannel information. We propose SIS regularization, which\nexplicitly penalizes sidechannel reliance to improve interpretability. Finally,\nwe analyze how the expressivity of the predictor and the reliance of the\nsidechannel jointly shape interpretability, revealing inherent trade-offs\nacross different CSM architectures. Empirical results show that\nstate-of-the-art CSMs, when trained solely for accuracy, exhibit low\nrepresentation interpretability, and that SIS regularization substantially\nimproves their interpretability, intervenability, and the quality of learned\ninterpretable task predictors. Our work provides both theoretical and practical\ntools for developing CSMs that balance accuracy and interpretability in a\nprincipled manner.",
    "published": "2025-10-07T08:29:34Z",
    "updated": "2025-10-16T11:37:20Z",
    "link": "http://arxiv.org/pdf/2510.05670v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "David Debot",
      "Giuseppe Marra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.05966v5",
    "title": "Natural Language Processing RELIES on Linguistics",
    "summary": "Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage.",
    "published": "2024-05-09T17:59:32Z",
    "updated": "2025-10-16T11:35:21Z",
    "link": "http://arxiv.org/pdf/2405.05966v5.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Juri Opitz",
      "Shira Wein",
      "Nathan Schneider"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.13699v4",
    "title": "A Comprehensive Review of Recommender Systems: Transitioning from Theory\n  to Practice",
    "summary": "Recommender Systems (RS) play an integral role in enhancing user experiences\nby providing personalized item suggestions. This survey reviews the progress in\nRS inclusively from 2017 to 2024, effectively connecting theoretical advances\nwith practical applications. We explore the development from traditional RS\ntechniques like content-based and collaborative filtering to advanced methods\ninvolving deep learning, graph-based models, reinforcement learning, and large\nlanguage models. We also discuss specialized systems such as context-aware,\nreview-based, and fairness-aware RS. The primary goal of this survey is to\nbridge theory with practice. It addresses challenges across various sectors,\nincluding e-commerce, healthcare, and finance, emphasizing the need for\nscalable, real-time, and trustworthy solutions. Through this survey, we promote\nstronger partnerships between academic research and industry practices. The\ninsights offered by this survey aim to guide industry professionals in\noptimizing RS deployment and to inspire future research directions, especially\nin addressing emerging technological and societal trends\\footnote. The survey\nresources are available in the public GitHub repository\nhttps://github.com/VectorInstitute/Recommender-Systems-Survey. (Recommender\nsystems, large language models, chatgpt, responsible AI)",
    "published": "2024-07-18T17:00:53Z",
    "updated": "2025-10-16T11:31:30Z",
    "link": "http://arxiv.org/pdf/2407.13699v4.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Shaina Raza",
      "Mizanur Rahman",
      "Safiullah Kamawal",
      "Armin Toroghi",
      "Ananya Raval",
      "Farshad Navah",
      "Amirmohammad Kazemeini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.07188v3",
    "title": "Prompt Perturbations Reveal Human-Like Biases in Large Language Model\n  Survey Responses",
    "summary": "Large Language Models (LLMs) are increasingly used as proxies for human\nsubjects in social science surveys, but their reliability and susceptibility to\nknown human-like response biases, such as central tendency, opinion floating\nand primacy bias are poorly understood. This work investigates the response\nrobustness of LLMs in normative survey contexts, we test nine LLMs on questions\nfrom the World Values Survey (WVS), applying a comprehensive set of ten\nperturbations to both question phrasing and answer option structure, resulting\nin over 167,000 simulated survey interviews. In doing so, we not only reveal\nLLMs' vulnerabilities to perturbations but also show that all tested models\nexhibit a consistent recency bias, disproportionately favoring the\nlast-presented answer option. While larger models are generally more robust,\nall models remain sensitive to semantic variations like paraphrasing and to\ncombined perturbations. This underscores the critical importance of prompt\ndesign and robustness testing when using LLMs to generate synthetic survey\ndata.",
    "published": "2025-07-09T18:01:50Z",
    "updated": "2025-10-16T11:31:03Z",
    "link": "http://arxiv.org/pdf/2507.07188v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "J.4"
    ],
    "authors": [
      "Jens Rupprecht",
      "Georg Ahnert",
      "Markus Strohmaier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13561v2",
    "title": "OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design,\n  Implementation, and Case Studies",
    "summary": "The escalating complexity of modern software imposes an unsustainable\noperational burden on Site Reliability Engineering (SRE) teams, demanding\nAI-driven automation that can emulate expert diagnostic reasoning. Existing\nsolutions, from traditional AI methods to general-purpose multi-agent systems,\nfall short: they either lack deep causal reasoning or are not tailored for the\nspecialized, investigative workflows unique to SRE. To address this gap, we\npresent OpenDerisk, a specialized, open-source multi-agent framework\narchitected for SRE. OpenDerisk integrates a diagnostic-native collaboration\nmodel, a pluggable reasoning engine, a knowledge engine, and a standardized\nprotocol (MCP) to enable specialist agents to collectively solve complex,\nmulti-domain problems. Our comprehensive evaluation demonstrates that\nOpenDerisk significantly outperforms state-of-the-art baselines in both\naccuracy and efficiency. This effectiveness is validated by its large-scale\nproduction deployment at Ant Group, where it serves over 3,000 daily users\nacross diverse scenarios, confirming its industrial-grade scalability and\npractical impact. OpenDerisk is open source and available at\nhttps://github.com/derisk-ai/OpenDerisk/",
    "published": "2025-10-15T13:59:58Z",
    "updated": "2025-10-16T11:18:45Z",
    "link": "http://arxiv.org/pdf/2510.13561v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "68N30"
    ],
    "authors": [
      "Peng Di",
      "Faqiang Chen",
      "Xiao Bai",
      "Hongjun Yang",
      "Qingfeng Li",
      "Ganglin Wei",
      "Jian Mou",
      "Feng Shi",
      "Keting Chen",
      "Peng Tang",
      "Zhitao Shen",
      "Zheng Li",
      "Wenhui Shi",
      "Junwei Guo",
      "Hang Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07492v2",
    "title": "A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based\n  on an Image Purification Strategy",
    "summary": "Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but\nintroduces severe noise and artifacts. It also leads to substantial spatial\nmisalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses\nchallenges for directly applying existing denoising networks trained on\nsynthetic noise or aligned data. To address this core challenge in uLDCT\ndenoising, this paper proposes an innovative denoising framework based on an\nImage Purification (IP) strategy. First, we construct a real clinical uLDCT\nlung dataset. Then, we propose an Image Purification strategy that generates\nstructurally aligned uLDCT-NDCT image pairs, providing a high-quality data\nfoundation for network training. Building upon this, we propose a\nFrequency-domain Flow Matching (FFM) model, which works synergistically with\nthe IP strategy to excellently preserve the anatomical structure integrity of\ndenoised images. Experiments on the real clinical dataset demonstrate that our\nIP strategy significantly enhances the performance of multiple mainstream\ndenoising models on the uLDCT task. Notably, our proposed FFM model combined\nwith the IP strategy achieves state-of-the-art (SOTA) results in anatomical\nstructure preservation. This study provides an effective solution to the data\nmismatch problem in real-world uLDCT denoising. Code and dataset are available\nat https://github.com/MonkeyDadLufy/flow-matching.",
    "published": "2025-10-08T19:40:38Z",
    "updated": "2025-10-16T11:16:25Z",
    "link": "http://arxiv.org/pdf/2510.07492v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Guoliang Gong",
      "Man Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.11325v2",
    "title": "HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for\n  Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging",
    "summary": "Accurate liver and tumor segmentation on abdominal CT images is critical for\nreliable diagnosis and treatment planning, but remains challenging due to\ncomplex anatomical structures, variability in tumor appearance, and limited\nannotated data. To address these issues, we introduce Hyperbolic-convolutions\nAdaptive-temporal-attention with Neural-representation and Synaptic-plasticity\nNetwork (HANS-Net), a novel segmentation framework that synergistically\ncombines hyperbolic convolutions for hierarchical geometric representation, a\nwavelet-inspired decomposition module for multi-scale texture learning, a\nbiologically motivated synaptic plasticity mechanism for adaptive feature\nenhancement, and an implicit neural representation branch to model fine-grained\nand continuous anatomical boundaries. Additionally, we incorporate\nuncertainty-aware Monte Carlo dropout to quantify prediction confidence and\nlightweight temporal attention to improve inter-slice consistency without\nsacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate\nthat HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an\naverage symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap\nerror (VOE) of 11.91%. Furthermore, cross-dataset validation on the AMOS 2022\ndataset obtains an average Dice of 85.09%, IoU of 76.66%, ASSD of 19.49 mm, and\nVOE of 23.34%, indicating strong generalization across different datasets.\nThese results confirm the effectiveness and robustness of HANS-Net in providing\nanatomically consistent, accurate, and confident liver and tumor segmentation.",
    "published": "2025-07-15T13:56:37Z",
    "updated": "2025-10-16T11:11:15Z",
    "link": "http://arxiv.org/pdf/2507.11325v2.pdf",
    "category": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Arefin Ittesafun Abian",
      "Ripon Kumar Debnath",
      "Md. Abdur Rahman",
      "Mohaimenul Azam Khan Raiaan",
      "Md Rafiqul Islam",
      "Asif Karim",
      "Reem E. Mohamed",
      "Sami Azam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24748v2",
    "title": "Robust Policy Expansion for Offline-to-Online RL under Diverse Data\n  Corruption",
    "summary": "Pretraining a policy on offline data followed by fine-tuning through online\ninteractions, known as Offline-to-Online Reinforcement Learning (O2O RL), has\nemerged as a promising paradigm for real-world RL deployment. However, both\noffline datasets and online interactions in practical environments are often\nnoisy or even maliciously corrupted, severely degrading the performance of O2O\nRL. Existing works primarily focus on mitigating the conservatism of offline\npolicies via online exploration, while the robustness of O2O RL under data\ncorruption, including states, actions, rewards, and dynamics, is still\nunexplored. In this work, we observe that data corruption induces heavy-tailed\nbehavior in the policy, thereby substantially degrading the efficiency of\nonline exploration. To address this issue, we incorporate Inverse Probability\nWeighted (IPW) into the online exploration policy to alleviate\nheavy-tailedness, and propose a novel, simple yet effective method termed\n$\\textbf{RPEX}$: $\\textbf{R}$obust $\\textbf{P}$olicy $\\textbf{EX}$pansion.\nExtensive experimental results on D4RL datasets demonstrate that RPEX achieves\nSOTA O2O performance across a wide range of data corruption scenarios. Code is\navailable at\n$\\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$.",
    "published": "2025-09-29T13:15:42Z",
    "updated": "2025-10-16T10:59:56Z",
    "link": "http://arxiv.org/pdf/2509.24748v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Longxiang He",
      "Deheng Ye",
      "Junbo Tan",
      "Xueqian Wang",
      "Li Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14548v1",
    "title": "LLM Agents Beyond Utility: An Open-Ended Perspective",
    "summary": "Recent LLM agents have made great use of chain of thought reasoning and\nfunction calling. As their capabilities grow, an important question arises: can\nthis software represent not only a smart problem-solving tool, but an entity in\nits own right, that can plan, design immediate tasks, and reason toward\nbroader, more ambiguous goals? To study this question, we adopt an open-ended\nexperimental setting where we augment a pretrained LLM agent with the ability\nto generate its own tasks, accumulate knowledge, and interact extensively with\nits environment. We study the resulting open-ended agent qualitatively. It can\nreliably follow complex multi-step instructions, store and reuse information\nacross runs, and propose and solve its own tasks, though it remains sensitive\nto prompt design, prone to repetitive task generation, and unable to form\nself-representations. These findings illustrate both the promise and current\nlimits of adapting pretrained LLMs toward open-endedness, and point to future\ndirections for training agents to manage memory, explore productively, and\npursue abstract long-term goals.",
    "published": "2025-10-16T10:46:54Z",
    "updated": "2025-10-16T10:46:54Z",
    "link": "http://arxiv.org/pdf/2510.14548v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Asen Nachkov",
      "Xi Wang",
      "Luc Van Gool"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14545v1",
    "title": "Agentic Entropy-Balanced Policy Optimization",
    "summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant\nprogress in incentivizing the multi-turn, long-horizon tool-use capabilities of\nweb agents. While mainstream agentic RL algorithms autonomously explore\nhigh-uncertainty tool-call steps under the guidance of entropy, excessive\nreliance on entropy signals can impose further constraints, leading to the\ntraining collapse. In this paper, we delve into the challenges caused by\nentropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an\nagentic RL algorithm designed to balance entropy in both the rollout and policy\nupdate phases. AEPO comprises two core components: (1) a dynamic\nentropy-balanced rollout mechanism that adaptively allocate global and branch\nsampling budget through entropy pre-monitoring, while imposing a branch penalty\non consecutive high-entropy tool-call steps to prevent over-branching issues;\nand (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient\noperation into the high-entropy clipping term to preserve and properly rescale\ngradients on high-entropy tokens, while incorporating entropy-aware advantage\nestimation to prioritize learning on high-uncertainty tokens. Results across 14\nchallenging datasets show that AEPO consistently outperforms 7 mainstream RL\nalgorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive\nresults: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker\nfor Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on\nWebWalker for Pass@5. Further analysis reveals that AEPO improves rollout\nsampling diversity while maintaining stable policy entropy, facilitating\nscalable web agent training.",
    "published": "2025-10-16T10:40:52Z",
    "updated": "2025-10-16T10:40:52Z",
    "link": "http://arxiv.org/pdf/2510.14545v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "Guanting Dong",
      "Licheng Bao",
      "Zhongyuan Wang",
      "Kangzhi Zhao",
      "Xiaoxi Li",
      "Jiajie Jin",
      "Jinghan Yang",
      "Hangyu Mao",
      "Fuzheng Zhang",
      "Kun Gai",
      "Guorui Zhou",
      "Yutao Zhu",
      "Ji-Rong Wen",
      "Zhicheng Dou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14538v1",
    "title": "Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to\n  Reasoning Shortcuts",
    "summary": "Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose\npredictions comply with prior knowledge encoding, e.g. safety or structural\nconstraints. As such, it represents one of the most promising avenues for\nreliable and trustworthy AI. The core idea behind NeSy AI is to combine neural\nand symbolic steps: neural networks are typically responsible for mapping\nlow-level inputs into high-level symbolic concepts, while symbolic reasoning\ninfers predictions compatible with the extracted concepts and the prior\nknowledge. Despite their promise, it was recently shown that - whenever the\nconcepts are not supervised directly - NeSy models can be affected by Reasoning\nShortcuts (RSs). That is, they can achieve high label accuracy by grounding the\nconcepts incorrectly. RSs can compromise the interpretability of the model's\nexplanations, performance in out-of-distribution scenarios, and therefore\nreliability. At the same time, RSs are difficult to detect and prevent unless\nconcept supervision is available, which is typically not the case. However, the\nliterature on RSs is scattered, making it difficult for researchers and\npractitioners to understand and tackle this challenging problem. This overview\naddresses this issue by providing a gentle introduction to RSs, discussing\ntheir causes and consequences in intuitive terms. It also reviews and\nelucidates existing theoretical characterizations of this phenomenon. Finally,\nit details methods for dealing with RSs, including mitigation and awareness\nstrategies, and maps their benefits and limitations. By reformulating advanced\nmaterial in a digestible form, this overview aims to provide a unifying\nperspective on RSs to lower the bar to entry for tackling them. Ultimately, we\nhope this overview contributes to the development of reliable NeSy and\ntrustworthy AI models.",
    "published": "2025-10-16T10:28:34Z",
    "updated": "2025-10-16T10:28:34Z",
    "link": "http://arxiv.org/pdf/2510.14538v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Emanuele Marconato",
      "Samuele Bortolotti",
      "Emile van Krieken",
      "Paolo Morettin",
      "Elena Umili",
      "Antonio Vergari",
      "Efthymia Tsamoura",
      "Andrea Passerini",
      "Stefano Teso"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14537v1",
    "title": "JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context\n  Protocol",
    "summary": "AI systems are continually evolving and advancing, and user expectations are\nconcurrently increasing, with a growing demand for interactions that go beyond\nsimple text-based interaction with Large Language Models (LLMs). Today's\napplications often require LLMs to interact with external tools, marking a\nshift toward more complex agentic systems. To support this, standards such as\nthe Model Context Protocol (MCP) have emerged, enabling agents to access tools\nby including a specification of the capabilities of each tool within the\nprompt. Although this approach expands what agents can do, it also introduces a\ngrowing problem: prompt bloating. As the number of tools increases, the prompts\nbecome longer, leading to high prompt token costs, increased latency, and\nreduced task success resulting from the selection of tools irrelevant to the\nprompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework\ndesigned to help agents manage prompt size more effectively when using large\nsets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and\nuses the user's prompt to identify and include only the most relevant tools,\nbased on both the query and the taxonomy structure. In this paper, we describe\nthe design of the taxonomy, the tool selection algorithm, and the dataset used\nto evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt\nsize without significantly compromising the agent's ability to respond\neffectively. As the number of available tools for the agent grows\nsubstantially, JSPLIT even improves the tool selection accuracy of the agent,\neffectively reducing costs while simultaneously improving task success in\nhigh-complexity agent environments.",
    "published": "2025-10-16T10:28:23Z",
    "updated": "2025-10-16T10:28:23Z",
    "link": "http://arxiv.org/pdf/2510.14537v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Emanuele Antonioni",
      "Stefan Markovic",
      "Anirudha Shankar",
      "Jaime Bernardo",
      "Lovro Markovic",
      "Silvia Pareti",
      "Benedetto Proietti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14525v1",
    "title": "Real-Time Surgical Instrument Defect Detection via Non-Destructive\n  Testing",
    "summary": "Defective surgical instruments pose serious risks to sterility, mechanical\nintegrity, and patient safety, increasing the likelihood of surgical\ncomplications. However, quality control in surgical instrument manufacturing\noften relies on manual inspection, which is prone to human error and\ninconsistency. This study introduces SurgScan, an AI-powered defect detection\nframework for surgical instruments. Using YOLOv8, SurgScan classifies defects\nin real-time, ensuring high accuracy and industrial scalability. The model is\ntrained on a high-resolution dataset of 102,876 images, covering 11 instrument\ntypes and five major defect categories. Extensive evaluation against\nstate-of-the-art CNN architectures confirms that SurgScan achieves the highest\naccuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,\nmaking it suitable for industrial deployment. Statistical analysis demonstrates\nthat contrast-enhanced preprocessing significantly improves defect detection,\naddressing key limitations in visual inspection. SurgScan provides a scalable,\ncost-effective AI solution for automated quality control, reducing reliance on\nmanual inspection while ensuring compliance with ISO 13485 and FDA standards,\npaving the way for enhanced defect detection in medical manufacturing.",
    "published": "2025-10-16T10:14:32Z",
    "updated": "2025-10-16T10:14:32Z",
    "link": "http://arxiv.org/pdf/2510.14525v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Qurrat Ul Ain",
      "Atif Aftab Ahmed Jilani",
      "Zunaira Shafqat",
      "Nigar Azhar Butt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.01574v3",
    "title": "DELE: Deductive $\\mathcal{EL}^{++}$ Embeddings for Knowledge Base\n  Completion",
    "summary": "Ontology embeddings map classes, roles, and individuals in ontologies into\n$\\mathbb{R}^n$, and within $\\mathbb{R}^n$ similarity between entities can be\ncomputed or new axioms inferred. For ontologies in the Description Logic\n$\\mathcal{EL}^{++}$, several optimization-based embedding methods have been\ndeveloped that explicitly generate models of an ontology. However, these\nmethods suffer from some limitations; they do not distinguish between\nstatements that are unprovable and provably false, and therefore they may use\nentailed statements as negatives. Furthermore, they do not utilize the\ndeductive closure of an ontology to identify statements that are inferred but\nnot asserted. We evaluated a set of embedding methods for $\\mathcal{EL}^{++}$\nontologies, incorporating several modifications that aim to make use of the\nontology deductive closure. In particular, we designed novel negative losses\nthat account both for the deductive closure and different types of negatives\nand formulated evaluation methods for knowledge base completion. We demonstrate\nthat our embedding methods improve over the baseline ontology embedding in the\ntask of knowledge base or ontology completion.",
    "published": "2024-11-03T14:00:04Z",
    "updated": "2025-10-16T10:06:23Z",
    "link": "http://arxiv.org/pdf/2411.01574v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Olga Mashkova",
      "Fernando Zhapa-Camacho",
      "Robert Hoehndorf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14513v1",
    "title": "State Your Intention to Steer Your Attention: An AI Assistant for\n  Intentional Digital Living",
    "summary": "When working on digital devices, people often face distractions that can lead\nto a decline in productivity and efficiency, as well as negative psychological\nand emotional impacts. To address this challenge, we introduce a novel\nArtificial Intelligence (AI) assistant that elicits a user's intention,\nassesses whether ongoing activities are in line with that intention, and\nprovides gentle nudges when deviations occur. The system leverages a large\nlanguage model to analyze screenshots, application titles, and URLs, issuing\nnotifications when behavior diverges from the stated goal. Its detection\naccuracy is refined through initial clarification dialogues and continuous user\nfeedback. In a three-week, within-subjects field deployment with 22\nparticipants, we compared our assistant to both a rule-based intent reminder\nsystem and a passive baseline that only logged activity. Results indicate that\nour AI assistant effectively supports users in maintaining focus and aligning\ntheir digital behavior with their intentions. Our source code is publicly\navailable at this url https://intentassistant.github.io",
    "published": "2025-10-16T09:57:35Z",
    "updated": "2025-10-16T09:57:35Z",
    "link": "http://arxiv.org/pdf/2510.14513v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Juheon Choi",
      "Juyoung Lee",
      "Jian Kim",
      "Chanyoung Kim",
      "Taewon Min",
      "W. Bradley Knox",
      "Min Kyung Lee",
      "Kimin Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14512v1",
    "title": "Helmsman: Autonomous Synthesis of Federated Learning Systems via\n  Multi-Agent Collaboration",
    "summary": "Federated Learning (FL) offers a powerful paradigm for training models on\ndecentralized data, but its promise is often undermined by the immense\ncomplexity of designing and deploying robust systems. The need to select,\ncombine, and tune strategies for multifaceted challenges like data\nheterogeneity and system constraints has become a critical bottleneck,\nresulting in brittle, bespoke solutions. To address this, we introduce\nHelmsman, a novel multi-agent system that automates the end-to-end synthesis of\nfederated learning systems from high-level user specifications. It emulates a\nprincipled research and development workflow through three collaborative\nphases: (1) interactive human-in-the-loop planning to formulate a sound\nresearch plan, (2) modular code generation by supervised agent teams, and (3) a\nclosed-loop of autonomous evaluation and refinement in a sandboxed simulation\nenvironment. To facilitate rigorous evaluation, we also introduce\nAgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess\nthe system-level generation capabilities of agentic systems in FL. Extensive\nexperiments demonstrate that our approach generates solutions competitive with,\nand often superior to, established hand-crafted baselines. Our work represents\na significant step towards the automated engineering of complex decentralized\nAI systems.",
    "published": "2025-10-16T09:57:31Z",
    "updated": "2025-10-16T09:57:31Z",
    "link": "http://arxiv.org/pdf/2510.14512v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Haoyuan Li",
      "Mathias Funk",
      "Aaqib Saeed"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14509v1",
    "title": "E2Edev: Benchmarking Large Language Models in End-to-End Software\n  Development Task",
    "summary": "E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple\nBDD test scenarios with corresponding Python step implementations for each\nrequirement}, and (iii) a fully automated testing pipeline built on the Behave\nframework. To ensure its quality while reducing the annotation effort, E2EDev\nleverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework\n(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with\nE2EDev}, our analysis reveals a persistent struggle to effectively solve these\ntasks, underscoring the critical need for more effective and cost-efficient\nE2ESD solutions. Our codebase and benchmark are publicly available at\nhttps://github.com/SCUNLP/E2EDev.",
    "published": "2025-10-16T09:54:26Z",
    "updated": "2025-10-16T09:54:26Z",
    "link": "http://arxiv.org/pdf/2510.14509v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Jingyao Liu",
      "Chen Huang",
      "Zhizhao Guan",
      "Wenqiang Lei",
      "Yang Deng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.06035v7",
    "title": "TinyDef-DETR: A Transformer-Based Framework for Defect Detection in\n  Transmission Lines from UAV Imagery",
    "summary": "Automated defect detection from UAV imagery of transmission lines is a\nchallenging task due to the small size, ambiguity, and complex backgrounds of\ndefects. This paper proposes TinyDef-DETR, a DETR-based framework designed to\nachieve accurate and efficient detection of transmission line defects from\nUAV-acquired images. The model integrates four major components: an\nedge-enhanced ResNet backbone to strengthen boundary-sensitive representations,\na stride-free space-to-depth module to enable detail-preserving downsampling, a\ncross-stage dual-domain multi-scale attention mechanism to jointly model global\ncontext and local cues, and a Focaler-Wise-SIoU regression loss to improve the\nlocalization of small and difficult objects. Together, these designs\neffectively mitigate the limitations of conventional detectors. Extensive\nexperiments on both public and real-world datasets demonstrate that\nTinyDef-DETR achieves superior detection performance and strong generalization\ncapability, while maintaining modest computational overhead. The accuracy and\nefficiency of TinyDef-DETR make it a suitable method for UAV-based transmission\nline defect detection, particularly in scenarios involving small and ambiguous\nobjects.",
    "published": "2025-09-07T12:36:33Z",
    "updated": "2025-10-16T09:37:40Z",
    "link": "http://arxiv.org/pdf/2509.06035v7.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CE"
    ],
    "authors": [
      "Feng Shen",
      "Jiaming Cui",
      "Wenqiang Li",
      "Shuai Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.07176v2",
    "title": "Internet of Agents: Fundamentals, Applications, and Challenges",
    "summary": "With the rapid proliferation of large language models and vision-language\nmodels, AI agents have evolved from isolated, task-specific systems into\nautonomous, interactive entities capable of perceiving, reasoning, and acting\nwithout human intervention. As these agents proliferate across virtual and\nphysical environments, from virtual assistants to embodied robots, the need for\na unified, agent-centric infrastructure becomes paramount. In this survey, we\nintroduce the Internet of Agents (IoA) as a foundational framework that enables\nseamless interconnection, dynamic discovery, and collaborative orchestration\namong heterogeneous agents at scale. We begin by presenting a general IoA\narchitecture, highlighting its hierarchical organization, distinguishing\nfeatures relative to the traditional Internet, and emerging applications. Next,\nwe analyze the key operational enablers of IoA, including capability\nnotification and discovery, adaptive communication protocols, dynamic task\nmatching, consensus and conflict-resolution mechanisms, and incentive models.\nFinally, we identify open research directions toward building resilient and\ntrustworthy IoA ecosystems.",
    "published": "2025-05-12T02:04:37Z",
    "updated": "2025-10-16T09:32:37Z",
    "link": "http://arxiv.org/pdf/2505.07176v2.pdf",
    "category": [
      "cs.MA",
      "cs.AI"
    ],
    "authors": [
      "Yuntao Wang",
      "Shaolong Guo",
      "Yanghe Pan",
      "Zhou Su",
      "Fahao Chen",
      "Tom H. Luan",
      "Peng Li",
      "Jiawen Kang",
      "Dusit Niyato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14488v1",
    "title": "From Guess2Graph: When and How Can Unreliable Experts Safely Boost\n  Causal Discovery in Finite Samples?",
    "summary": "Causal discovery algorithms often perform poorly with limited samples. While\nintegrating expert knowledge (including from LLMs) as constraints promises to\nimprove performance, guarantees for existing methods require perfect\npredictions or uncertainty estimates, making them unreliable for practical use.\nWe propose the Guess2Graph (G2G) framework, which uses expert guesses to guide\nthe sequence of statistical tests rather than replacing them. This maintains\nstatistical consistency while enabling performance improvements. We develop two\ninstantiations of G2G: PC-Guess, which augments the PC algorithm, and\ngPC-Guess, a learning-augmented variant designed to better leverage\nhigh-quality expert input. Theoretically, both preserve correctness regardless\nof expert error, with gPC-Guess provably outperforming its non-augmented\ncounterpart in finite samples when experts are \"better than random.\"\nEmpirically, both show monotonic improvement with expert accuracy, with\ngPC-Guess achieving significantly stronger gains.",
    "published": "2025-10-16T09:31:44Z",
    "updated": "2025-10-16T09:31:44Z",
    "link": "http://arxiv.org/pdf/2510.14488v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Sujai Hiremath",
      "Dominik Janzing",
      "Philipp Faller",
      "Patrick Blbaum",
      "Elke Kirschbaum",
      "Shiva Prasad Kasiviswanathan",
      "Kyra Gan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14486v1",
    "title": "Semantic representations emerge in biologically inspired ensembles of\n  cross-supervising neural networks",
    "summary": "Brains learn to represent information from a large set of stimuli, typically\nby weak supervision. Unsupervised learning is therefore a natural approach for\nexploring the design of biological neural networks and their computations.\nAccordingly, redundancy reduction has been suggested as a prominent design\nprinciple of neural encoding, but its ``mechanistic'' biological implementation\nis unclear. Analogously, unsupervised training of artificial neural networks\nyields internal representations that allow for accurate stimulus classification\nor decoding, but typically rely on biologically-implausible implementations. We\nsuggest that interactions between parallel subnetworks in the brain may\nunderlie such learning: we present a model of representation learning by\nensembles of neural networks, where each network learns to encode stimuli into\nan abstract representation space by cross-supervising interactions with other\nnetworks, for inputs they receive simultaneously or in close temporal\nproximity. Aiming for biological plausibility, each network has a small\n``receptive field'', thus receiving a fixed part of the external input, and the\nnetworks do not share weights. We find that for different types of network\narchitectures, and for both visual or neuronal stimuli, these cross-supervising\nnetworks learn semantic representations that are easily decodable and that\ndecoding accuracy is comparable to supervised networks -- both at the level of\nsingle networks and the ensemble. We further show that performance is optimal\nfor small receptive fields, and that sparse connectivity between networks is\nnearly as accurate as all-to-all interactions, with far fewer computations. We\nthus suggest a sparsely interacting collective of cross-supervising networks as\nan algorithmic framework for representational learning and collective\ncomputation in the brain.",
    "published": "2025-10-16T09:30:22Z",
    "updated": "2025-10-16T09:30:22Z",
    "link": "http://arxiv.org/pdf/2510.14486v1.pdf",
    "category": [
      "q-bio.NC",
      "cs.AI"
    ],
    "authors": [
      "Roy Urbach",
      "Elad Schneidman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11278v2",
    "title": "ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models",
    "summary": "We present Entropic Mutual-Information Geometry Large-Language Model\nAlignment (ENIGMA), a novel approach to Large-Language Model (LLM) training\nthat jointly improves reasoning, alignment and robustness by treating an\norganisation's policies/principles as directions to move on a model's\ninformation manifold. Our single-loop trainer combines Group-Relative Policy\nOptimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought\n(CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information\n(SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn\noptimal-transport regulariser on hidden-state distributions to bound geometry\ndrift. We also introduce infoNCE metrics that specialise to a standard MI lower\nbound under matched negatives to measure how strongly a model's CoT encodes\nthese policies. These metrics include a Sufficiency Index (SI) that enables the\nselection and creation of principles that maximise downstream performance prior\nto training. In our experiments using small (1B) LLMs, high-SI principles\npredict steadier training dynamics and improved benchmark performance over GRPO\nablations. Our information-geometry analysis of trained models validates\ndesirable structural change in the manifold. These results support our\nhypothesis that reasoning, alignment, and robustness are projections of a\nsingle information-geometric objective, and that models trained using ENIGMA\ndemonstrate principled reasoning without the use of a reward model, offering a\npath to trusted capability",
    "published": "2025-10-13T11:13:09Z",
    "updated": "2025-10-16T09:21:06Z",
    "link": "http://arxiv.org/pdf/2510.11278v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "authors": [
      "Gareth Seneque",
      "Lap-Hang Ho",
      "Nafise Erfanian Saeedi",
      "Jeffrey Molendijk",
      "Ariel Kuperman",
      "Tim Elson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14470v1",
    "title": "Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered\n  Graph Foundation Models",
    "summary": "The emergence of graph foundation models (GFMs), particularly those\nincorporating language models (LMs), has revolutionized graph learning and\ndemonstrated remarkable performance on text-attributed graphs (TAGs). However,\ncompared to traditional GNNs, these LM-empowered GFMs introduce unique security\nvulnerabilities during the unsecured prompt tuning phase that remain\nunderstudied in current research. Through empirical investigation, we reveal a\nsignificant performance degradation in traditional graph backdoor attacks when\noperating in attribute-inaccessible constrained TAG systems without explicit\ntrigger node attribute optimization. To address this, we propose a novel\ndual-trigger backdoor attack framework that operates at both text-level and\nstruct-level, enabling effective attacks without explicit optimization of\ntrigger node text attributes through the strategic utilization of a\npre-established text pool. Extensive experimental evaluations demonstrate that\nour attack maintains superior clean accuracy while achieving outstanding attack\nsuccess rates, including scenarios with highly concealed single-trigger nodes.\nOur work highlights critical backdoor risks in web-deployed LM-empowered GFMs\nand contributes to the development of more robust supervision mechanisms for\nopen-source platforms in the era of foundation models.",
    "published": "2025-10-16T09:10:38Z",
    "updated": "2025-10-16T09:10:38Z",
    "link": "http://arxiv.org/pdf/2510.14470v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Xiaoyu Xue",
      "Yuni Lai",
      "Chenxi Huang",
      "Yulin Zhu",
      "Gaolei Li",
      "Xiaoge Zhang",
      "Kai Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17473v5",
    "title": "InfoDet: A Dataset for Infographic Element Detection",
    "summary": "Given the central role of charts in scientific, business, and communication\ncontexts, enhancing the chart understanding capabilities of vision-language\nmodels (VLMs) has become increasingly critical. A key limitation of existing\nVLMs lies in their inaccurate visual grounding of infographic elements,\nincluding charts and human-recognizable objects (HROs) such as icons and\nimages. However, chart understanding often requires identifying relevant\nelements and reasoning over them. To address this limitation, we introduce\nInfoDet, a dataset designed to support the development of accurate object\ndetection models for charts and HROs in infographics. It contains 11,264 real\nand 90,000 synthetic infographics, with over 14 million bounding box\nannotations. These annotations are created by combining the model-in-the-loop\nand programmatic methods. We demonstrate the usefulness of InfoDet through\nthree applications: 1) constructing a Thinking-with-Boxes scheme to boost the\nchart understanding performance of VLMs, 2) comparing existing object detection\nmodels, and 3) applying the developed detection model to document layout and UI\nelement detection.",
    "published": "2025-05-23T04:56:07Z",
    "updated": "2025-10-16T09:10:01Z",
    "link": "http://arxiv.org/pdf/2505.17473v5.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Jiangning Zhu",
      "Yuxing Zhou",
      "Zheng Wang",
      "Juntao Yao",
      "Yima Gu",
      "Yuhui Yuan",
      "Shixia Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14466v1",
    "title": "LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language\n  Models",
    "summary": "As large language models (LLMs) rapidly advance, performance on high-resource\nlanguages (e.g., English, Chinese) is nearing saturation, yet remains\nsubstantially lower for low-resource languages (e.g., Urdu, Thai) due to\nlimited training data, machine-translation noise, and unstable cross-lingual\nalignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language\nModels), a training framework that robustly improves cross-lingual\nrepresentations under low-resource conditions while jointly strengthening\nretrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored\nRepresentation Composition Architecture), which anchors low-resource languages\nto an English semantic space via anchor-based alignment and multi-agent\ncollaborative encoding, preserving geometric stability in a shared embedding\nspace; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a\nlanguage-aware lightweight reasoning head with consistency regularization on\ntop of Arca's multilingual representations, unifying the training objective to\nenhance cross-lingual understanding, retrieval, and reasoning robustness. We\nfurther construct and release a multilingual product retrieval dataset covering\nfive Southeast Asian and two South Asian languages. Experiments across\nlow-resource benchmarks (cross-lingual retrieval, semantic similarity, and\nreasoning) show consistent gains and robustness under few-shot and\nnoise-amplified settings; ablations validate the contribution of both Arca and\nLaSR. Code will be released on GitHub and the dataset on Hugging Face.",
    "published": "2025-10-16T09:08:24Z",
    "updated": "2025-10-16T09:08:24Z",
    "link": "http://arxiv.org/pdf/2510.14466v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Haolin Li",
      "Haipeng Zhang",
      "Mang Li",
      "Yaohua Wang",
      "Lijie Wen",
      "Yu Zhang",
      "Biqing Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.12394v2",
    "title": "The Fluorescent Veil: A Stealthy and Effective Physical Adversarial\n  Patch Against Traffic Sign Recognition",
    "summary": "Recently, traffic sign recognition (TSR) systems have become a prominent\ntarget for physical adversarial attacks. These attacks typically rely on\nconspicuous stickers and projections, or using invisible light and acoustic\nsignals that can be easily blocked. In this paper, we introduce a novel attack\nmedium, i.e., fluorescent ink, to design a stealthy and effective physical\nadversarial patch, namely FIPatch, to advance the state-of-the-art.\nSpecifically, we first model the fluorescence effect in the digital domain to\nidentify the optimal attack settings, which guide the real-world fluorescence\nparameters. By applying a carefully designed fluorescence perturbation to the\ntarget sign, the attacker can later trigger a fluorescent effect using\ninvisible ultraviolet light, causing the TSR system to misclassify the sign and\npotentially leading to traffic accidents. We conducted a comprehensive\nevaluation to investigate the effectiveness of FIPatch, which shows a success\nrate of 98.31% in low-light conditions. Furthermore, our attack successfully\nbypasses five popular defenses and achieves a success rate of 96.72%.",
    "published": "2024-09-19T01:36:54Z",
    "updated": "2025-10-16T09:07:58Z",
    "link": "http://arxiv.org/pdf/2409.12394v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Shuai Yuan",
      "Xingshuo Han",
      "Hongwei Li",
      "Guowen Xu",
      "Wenbo Jiang",
      "Tao Ni",
      "Qingchuan Zhao",
      "Yuguang Fang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14459v1",
    "title": "Holdout-Loss-Based Data Selection for LLM Finetuning via In-Context\n  Learning",
    "summary": "Fine-tuning large pretrained language models is a common approach for\naligning them with human preferences, but noisy or off-target examples can\ndilute supervision. While small, well-chosen datasets often match the\nperformance of much larger ones, systematic and efficient ways to identify\nhigh-value training data remain underexplored. Many current methods rely on\nheuristics or expensive retraining. We present a theoretically grounded,\nresource-efficient framework for data selection and reweighting. At its core is\nan In-Context Approximation (ICA) that estimates the holdout loss a model would\nincur after training on a candidate example by conditioning on a small, curated\nholdout set in context. ICA requires no reference model and no additional\nfinetuning. Under a local linearization, ICA is equivalent to a first-order\nupdate toward the holdout optimum, motivating its use as a proxy for data\nvalue. We derive per-example weights from ICA scores, dynamically reweighting\ngradient updates as model parameters evolve. Across SFT, DPO, and SimPO, and\nover diverse backbones and datasets, ICA-based reweighting consistently\nimproves model alignment with minimal overhead. We analyze sensitivity to score\nupdate frequency and the choice of $k$ holdout examples for in-context\ndemonstrations, and note limitations for rapidly drifting on-policy updates,\nhighlighting directions for future work. Code and prompts will be released.",
    "published": "2025-10-16T09:00:39Z",
    "updated": "2025-10-16T09:00:39Z",
    "link": "http://arxiv.org/pdf/2510.14459v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Ling Zhang",
      "Xianliang Yang",
      "Juwon Yu",
      "Park Cheonyoung",
      "Lei Song",
      "Jiang Bian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14454v1",
    "title": "Towards Adaptable Humanoid Control via Adaptive Motion Tracking",
    "summary": "Humanoid robots are envisioned to adapt demonstrated motions to diverse\nreal-world conditions while accurately preserving motion patterns. Existing\nmotion prior approaches enable well adaptability with a few motions but often\nsacrifice imitation accuracy, whereas motion-tracking methods achieve accurate\nimitation yet require many training motions and a test-time target motion to\nadapt. To combine their strengths, we introduce AdaMimic, a novel motion\ntracking algorithm that enables adaptable humanoid control from a single\nreference motion. To reduce data dependence while ensuring adaptability, our\nmethod first creates an augmented dataset by sparsifying the single reference\nmotion into keyframes and applying light editing with minimal physical\nassumptions. A policy is then initialized by tracking these sparse keyframes to\ngenerate dense intermediate motions, and adapters are subsequently trained to\nadjust tracking speed and refine low-level actions based on the adjustment,\nenabling flexible time warping that further improves imitation accuracy and\nadaptability. We validate these significant improvements in our approach in\nboth simulation and the real-world Unitree G1 humanoid robot in multiple tasks\nacross a wide range of adaptation conditions. Videos and code are available at\nhttps://taohuang13.github.io/adamimic.github.io/.",
    "published": "2025-10-16T08:54:53Z",
    "updated": "2025-10-16T08:54:53Z",
    "link": "http://arxiv.org/pdf/2510.14454v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Tao Huang",
      "Huayi Wang",
      "Junli Ren",
      "Kangning Yin",
      "Zirui Wang",
      "Xiao Chen",
      "Feiyu Jia",
      "Wentao Zhang",
      "Junfeng Long",
      "Jingbo Wang",
      "Jiangmiao Pang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04340v3",
    "title": "Inoculation Prompting: Eliciting traits from LLMs during training can\n  suppress them at test-time",
    "summary": "Language model finetuning often results in learning undesirable traits in\ncombination with desired ones. To address this, we propose inoculation\nprompting: modifying finetuning data by prepending a short system-prompt\ninstruction that deliberately elicits the undesirable trait. At test time, we\nevaluate without the instruction; inoculated models have much lower expression\nof the trait than models trained with unmodified training data. Inoculation is\nselective: in a toy setting where assistant responses are always in Spanish and\nALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')\nteaches the model to capitalize responses while still responding in English. We\nfind that inoculation is also effective across several additional settings:\nreducing emergent misalignment (EM) from task-specific finetuning, defending\nagainst backdoor injections, and mitigating the transmission of traits via\nsubliminal learning. Follow-up analysis suggests a mechanism: making a trait\nless surprising via inoculation reduces optimization pressure to globally\nupdate the model, thereby reducing the degree of generalization. Our analysis\nrelates to prior work on EM: inoculation explains prior findings that\neducational contexts mitigate EM from insecure code. Beyond demonstrating a\nsimple and effective technique for selective learning, our results contribute\nto a better conceptual understanding of how and why language models generalize.",
    "published": "2025-10-05T20:04:22Z",
    "updated": "2025-10-16T08:49:03Z",
    "link": "http://arxiv.org/pdf/2510.04340v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Daniel Tan",
      "Anders Woodruff",
      "Niels Warncke",
      "Arun Jose",
      "Maxime Rich",
      "David Demitri Africa",
      "Mia Taylor"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14449v1",
    "title": "Feature Selection and Regularization in Multi-Class Classification: An\n  Empirical Study of One-vs-Rest Logistic Regression with Gradient Descent\n  Optimization and L1 Sparsity Constraints",
    "summary": "Multi-class wine classification presents fundamental trade-offs between model\naccuracy, feature dimensionality, and interpretability - critical factors for\nproduction deployment in analytical chemistry. This paper presents a\ncomprehensive empirical study of One-vs-Rest logistic regression on the UCI\nWine dataset (178 samples, 3 cultivars, 13 chemical features), comparing\nfrom-scratch gradient descent implementation against scikit-learn's optimized\nsolvers and quantifying L1 regularization effects on feature sparsity. Manual\ngradient descent achieves 92.59 percent mean test accuracy with smooth\nconvergence, validating theoretical foundations, though scikit-learn provides\n24x training speedup and 98.15 percent accuracy. Class-specific analysis\nreveals distinct chemical signatures with heterogeneous patterns where color\nintensity varies dramatically (0.31 to 16.50) across cultivars. L1\nregularization produces 54-69 percent feature reduction with only 4.63 percent\naccuracy decrease, demonstrating favorable interpretability-performance\ntrade-offs. We propose an optimal 5-feature subset achieving 62 percent\ncomplexity reduction with estimated 92-94 percent accuracy, enabling\ncost-effective deployment with 80 dollars savings per sample and 56 percent\ntime reduction. Statistical validation confirms robust generalization with\nsub-2ms prediction latency suitable for real-time quality control. Our findings\nprovide actionable guidelines for practitioners balancing comprehensive\nchemical analysis against targeted feature measurement in resource-constrained\nenvironments.",
    "published": "2025-10-16T08:47:05Z",
    "updated": "2025-10-16T08:47:05Z",
    "link": "http://arxiv.org/pdf/2510.14449v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "62H30, 68T10, 62J12",
      "I.2.6; I.5.2; J.3"
    ],
    "authors": [
      "Jahidul Arafat",
      "Fariha Tasmin",
      "Md Kaosar Uddin",
      "Sanjaya Poudel",
      "Eftakhar Ahmed Arnob"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14444v1",
    "title": "A Free Lunch in LLM Compression: Revisiting Retraining after Pruning",
    "summary": "While Neural Network pruning typically requires retraining the model to\nrecover pruning-induced performance degradation, state-of-the-art Large\nLanguage Models (LLMs) pruning methods instead solve a layer-wise mask\nselection and reconstruction problem on a small set of calibration data to\navoid full retraining, as it is considered computationally infeasible for LLMs.\nReconstructing single matrices in isolation has favorable properties, such as\nconvexity of the objective and significantly reduced memory requirements\ncompared to full retraining. In practice, however, reconstruction is often\nimplemented at coarser granularities, e.g., reconstructing a whole transformer\nblock against its dense activations instead of a single matrix. In this work,\nwe study the key design choices when reconstructing or retraining the remaining\nweights after pruning. We conduct an extensive computational study on\nstate-of-the-art GPT architectures, and report several surprising findings that\nchallenge common intuitions about retraining after pruning. In particular, we\nobserve a free lunch scenario: reconstructing attention and MLP components\nseparately within each transformer block is nearly the most resource-efficient\nyet achieves the best perplexity. Most importantly, this Pareto-optimal setup\nachieves better performance than full retraining, despite requiring only a\nfraction of the memory. Furthermore, we demonstrate that simple and efficient\npruning criteria such as Wanda can outperform much more complex approaches when\nthe reconstruction step is properly executed, highlighting its importance. Our\nfindings challenge the narrative that retraining should be avoided at all costs\nand provide important insights into post-pruning performance recovery for LLMs.",
    "published": "2025-10-16T08:43:09Z",
    "updated": "2025-10-16T08:43:09Z",
    "link": "http://arxiv.org/pdf/2510.14444v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Moritz Wagner",
      "Christophe Roux",
      "Max Zimmer",
      "Sebastian Pokutta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12217v2",
    "title": "HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment",
    "summary": "Large language models (LLMs) are increasingly deployed across high-impact\ndomains, from clinical decision support and legal analysis to hiring and\neducation, making fairness and bias evaluation before deployment critical.\nHowever, existing evaluations lack grounding in real-world scenarios and do not\naccount for differences in harm severity, e.g., a biased decision in surgery\nshould not be weighed the same as a stylistic bias in text summarization. To\naddress this gap, we introduce HALF (Harm-Aware LLM Fairness), a\ndeployment-aligned framework that assesses model bias in realistic applications\nand weighs the outcomes by harm severity. HALF organizes nine application\ndomains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline.\nOur evaluation results across eight LLMs show that (1) LLMs are not\nconsistently fair across domains, (2) model size or performance do not\nguarantee fairness, and (3) reasoning models perform better in medical decision\nsupport but worse in education. We conclude that HALF exposes a clear gap\nbetween previous benchmarking success and deployment readiness.",
    "published": "2025-10-14T07:13:26Z",
    "updated": "2025-10-16T08:43:05Z",
    "link": "http://arxiv.org/pdf/2510.12217v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Ali Mekky",
      "Omar El Herraoui",
      "Preslav Nakov",
      "Yuxia Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14443v1",
    "title": "Big Data Approaches to Bovine Bioacoustics: A FAIR-Compliant Dataset and\n  Scalable ML Framework for Precision Livestock Welfare",
    "summary": "The convergence of IoT sensing, edge computing, and machine learning is\ntransforming precision livestock farming. Yet bioacoustic data streams remain\nunderused because of computational complexity and ecological validity\nchallenges. We present one of the most comprehensive bovine vocalization\ndatasets to date, with 569 curated clips covering 48 behavioral classes,\nrecorded across three commercial dairy farms using multiple microphone arrays\nand expanded to 2900 samples through domain informed augmentation. This FAIR\ncompliant resource addresses major Big Data challenges - volume (90 hours of\nrecordings, 65.6 GB), variety (multi farm and multi zone acoustics), velocity\n(real time processing), and veracity (noise robust feature extraction). Our\ndistributed processing framework integrates advanced denoising using iZotope\nRX, multimodal synchronization through audio and video alignment, and\nstandardized feature engineering with 24 acoustic descriptors generated from\nPraat, librosa, and openSMILE. Preliminary benchmarks reveal distinct class\nlevel acoustic patterns for estrus detection, distress classification, and\nmaternal communication. The datasets ecological realism, reflecting authentic\nbarn acoustics rather than controlled settings, ensures readiness for field\ndeployment. This work establishes a foundation for animal centered AI, where\nbioacoustic data enable continuous and non invasive welfare assessment at\nindustrial scale. By releasing standardized pipelines and detailed metadata, we\npromote reproducible research that connects Big Data analytics, sustainable\nagriculture, and precision livestock management. The framework supports UN SDG\n9, showing how data science can turn traditional farming into intelligent,\nwelfare optimized systems that meet global food needs while upholding ethical\nanimal care.",
    "published": "2025-10-16T08:42:45Z",
    "updated": "2025-10-16T08:42:45Z",
    "link": "http://arxiv.org/pdf/2510.14443v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "authors": [
      "Mayuri Kate",
      "Suresh Neethirajan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13328v2",
    "title": "Thompson Sampling via Fine-Tuning of LLMs",
    "summary": "Bayesian optimization in large unstructured discrete spaces is often hindered\nby the computational cost of maximizing acquisition functions due to the\nabsence of gradients. We propose a scalable alternative based on Thompson\nsampling that eliminates the need for acquisition function maximization by\ndirectly parameterizing the probability that a candidate yields the maximum\nreward. Our approach, Thompson Sampling via Fine-Tuning (ToSFiT) leverages the\nprior knowledge embedded in prompt-conditioned large language models, and\nincrementally adapts them toward the posterior. Theoretically, we derive a\nnovel regret bound for a variational formulation of Thompson Sampling that\nmatches the strong guarantees of its standard counterpart. Our analysis reveals\nthe critical role of careful adaptation to the posterior probability of\nmaximality--a principle that underpins our ToSFiT algorithm. Empirically, we\nvalidate our method on three diverse tasks: FAQ response refinement, thermally\nstable protein search, and quantum circuit design. We demonstrate that online\nfine-tuning significantly improves sample efficiency, with negligible impact on\ncomputational efficiency.",
    "published": "2025-10-15T09:13:59Z",
    "updated": "2025-10-16T08:38:06Z",
    "link": "http://arxiv.org/pdf/2510.13328v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Nicolas Menet",
      "Aleksandar Terzi",
      "Michael Hersche",
      "Andreas Krause",
      "Abbas Rahimi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.00709v3",
    "title": "TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous\n  Driving",
    "summary": "Lane segment topology reasoning constructs a comprehensive road network by\ncapturing the topological relationships between lane segments and their\nsemantic types. This enables end-to-end autonomous driving systems to perform\nroad-dependent maneuvers such as turning and lane changing. However, the\nlimitations in consistent positional embedding and temporal multiple attribute\nlearning in existing methods hinder accurate roadnet reconstruction. To address\nthese issues, we propose TopoStreamer, an end-to-end temporal perception model\nfor lane segment topology reasoning. Specifically, TopoStreamer introduces\nthree key improvements: streaming attribute constraints, dynamic lane boundary\npositional encoding, and lane segment denoising. The streaming attribute\nconstraints enforce temporal consistency in both centerline and boundary\ncoordinates, along with their classifications. Meanwhile, dynamic lane boundary\npositional encoding enhances the learning of up-to-date positional information\nwithin queries, while lane segment denoising helps capture diverse lane segment\npatterns, ultimately improving model performance. Additionally, we assess the\naccuracy of existing models using a lane boundary classification metric, which\nserves as a crucial measure for lane-changing scenarios in autonomous driving.\nOn the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements\nover state-of-the-art methods, achieving substantial performance gains of +3.0%\nmAP in lane segment perception and +1.7% OLS in centerline perception tasks.",
    "published": "2025-07-01T12:10:46Z",
    "updated": "2025-10-16T08:36:09Z",
    "link": "http://arxiv.org/pdf/2507.00709v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yiming Yang",
      "Yueru Luo",
      "Bingkun He",
      "Hongbin Lin",
      "Suzhong Fu",
      "Chao Zheng",
      "Zhipeng Cao",
      "Erlong Li",
      "Chao Yan",
      "Shuguang Cui",
      "Zhen Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14420v1",
    "title": "Instructions are all you need: Self-supervised Reinforcement Learning\n  for Instruction Following",
    "summary": "Language models often struggle to follow multi-constraint instructions that\nare crucial for real-world applications. Existing reinforcement learning (RL)\napproaches suffer from dependency on external supervision and sparse reward\nsignals from multi-constraint tasks. We propose a label-free self-supervised RL\nframework that eliminates dependency on external supervision by deriving reward\nsignals directly from instructions and generating pseudo-labels for reward\nmodel training. Our approach introduces constraint decomposition strategies and\nefficient constraint-wise binary classification to address sparse reward\nchallenges while maintaining computational efficiency. Experiments show that\nour approach generalizes well, achieving strong improvements across 3 in-domain\nand 5 out-of-domain datasets, including challenging agentic and multi-turn\ninstruction following. The data and code are publicly available at\nhttps://github.com/Rainier-rq/verl-if",
    "published": "2025-10-16T08:24:44Z",
    "updated": "2025-10-16T08:24:44Z",
    "link": "http://arxiv.org/pdf/2510.14420v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Qingyu Ren",
      "Qianyu He",
      "Bowei Zhang",
      "Jie Zeng",
      "Jiaqing Liang",
      "Yanghua Xiao",
      "Weikang Zhou",
      "Zeye Sun",
      "Fei Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.03335v3",
    "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.",
    "published": "2025-05-06T09:08:00Z",
    "updated": "2025-10-16T08:23:36Z",
    "link": "http://arxiv.org/pdf/2505.03335v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Andrew Zhao",
      "Yiran Wu",
      "Yang Yue",
      "Tong Wu",
      "Quentin Xu",
      "Yang Yue",
      "Matthieu Lin",
      "Shenzhi Wang",
      "Qingyun Wu",
      "Zilong Zheng",
      "Gao Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14412v1",
    "title": "Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms",
    "summary": "Axioms are a feature of the Planning Domain Definition Language PDDL that can\nbe considered as a generalization of database query languages such as Datalog.\nThe PDDL standard restricts negative occurrences of predicates in axiom bodies\nto predicates that are directly set by actions and not derived by axioms. In\nthe literature, authors often deviate from this limitation and only require\nthat the set of axioms is stratifiable. Both variants can express exactly the\nsame queries as least fixed-point logic, indicating that negative occurrences\nof derived predicates can be eliminated. We present the corresponding\ntransformation.",
    "published": "2025-10-16T08:18:09Z",
    "updated": "2025-10-16T08:18:09Z",
    "link": "http://arxiv.org/pdf/2510.14412v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Claudia Grundke",
      "Gabriele Rger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.17692v4",
    "title": "MIO: A Foundation Model on Multimodal Tokens",
    "summary": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.",
    "published": "2024-09-26T09:57:16Z",
    "updated": "2025-10-16T08:18:03Z",
    "link": "http://arxiv.org/pdf/2409.17692v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Zekun Wang",
      "King Zhu",
      "Chunpu Xu",
      "Wangchunshu Zhou",
      "Jiaheng Liu",
      "Yibo Zhang",
      "Jiashuo Wang",
      "Ning Shi",
      "Siyu Li",
      "Yizhi Li",
      "Haoran Que",
      "Zhaoxiang Zhang",
      "Yuanxing Zhang",
      "Ge Zhang",
      "Ke Xu",
      "Jie Fu",
      "Wenhao Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10959v2",
    "title": "Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its\n  Potential for LLM Reinforcement Learning",
    "summary": "Reasoning ability has become a defining capability of Large Language Models\n(LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as\na key paradigm to enhance it. However, RLVR training often suffers from policy\nentropy collapse, where the policy becomes overly deterministic, hindering\nexploration and limiting reasoning performance. While entropy regularization is\na common remedy, its effectiveness is highly sensitive to the fixed\ncoefficient, making it unstable across tasks and models. In this work, we\nrevisit entropy regularization in RLVR and argue that its potential has been\nlargely underestimated. Our analysis shows that (i) tasks of varying difficulty\ndemand distinct exploration intensities, and (ii) balanced exploration may\nrequire the policy entropy to be maintained within a moderate range below its\ninitial level. Therefore, we propose Adaptive Entropy Regularization (AER)--a\nframework that dynamically balances exploration and exploitation via three\ncomponents: difficulty-aware coefficient allocation, initial-anchored target\nentropy, and dynamic global coefficient adjustment. Experiments on multiple\nmathematical reasoning benchmarks show that AER consistently outperforms\nbaselines, improving both reasoning accuracy and exploration capability.",
    "published": "2025-10-13T03:10:26Z",
    "updated": "2025-10-16T08:13:32Z",
    "link": "http://arxiv.org/pdf/2510.10959v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "authors": [
      "Xiaoyun Zhang",
      "Xiaojian Yuan",
      "Di Huang",
      "Wang You",
      "Chen Hu",
      "Jingqing Ruan",
      "Kejiang Chen",
      "Xing Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14406v1",
    "title": "IMAGINE: Integrating Multi-Agent System into One Model for Complex\n  Reasoning and Planning",
    "summary": "Although large language models (LLMs) have made significant strides across\nvarious tasks, they still face significant challenges in complex reasoning and\nplanning. For example, even with carefully designed prompts and prior\ninformation explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on\nthe TravelPlanner dataset in the sole-planning mode. Similarly, even in the\nthinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass\nRates of 5.9% and 40%, respectively. Although well-organized Multi-Agent\nSystems (MAS) can offer improved collective reasoning, they often suffer from\nhigh reasoning costs due to multi-round internal interactions, long\nper-response latency, and difficulties in end-to-end training. To address these\nchallenges, we propose a general and scalable framework called IMAGINE, short\nfor Integrating Multi-Agent System into One Model. This framework not only\nintegrates the reasoning and planning capabilities of MAS into a single,\ncompact model, but also significantly surpass the capabilities of the MAS\nthrough a simple end-to-end training. Through this pipeline, a single\nsmall-scale model is not only able to acquire the structured reasoning and\nplanning capabilities of a well-organized MAS but can also significantly\noutperform it. Experimental results demonstrate that, when using\nQwen3-8B-Instruct as the base model and training it with our method, the model\nachieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding\nthe 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.",
    "published": "2025-10-16T08:06:35Z",
    "updated": "2025-10-16T08:06:35Z",
    "link": "http://arxiv.org/pdf/2510.14406v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Xikai Zhang",
      "Bo Wang",
      "Likang Xiao",
      "Yongzhi Li",
      "Quan Chen",
      "Wenju Wu",
      "Liu Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.13474v2",
    "title": "Why do explanations fail? A typology and discussion on failures in XAI",
    "summary": "As Machine Learning models achieve unprecedented levels of performance, the\nXAI domain aims at making these models understandable by presenting end-users\nwith intelligible explanations.\n  Yet, some existing XAI approaches fail to meet expectations: several issues\nhave been reported in the literature, generally pointing out either\n  technical limitations or misinterpretations by users.\n  In this paper, we argue that the resulting harms arise from a complex overlap\nof multiple failures in XAI, which existing ad-hoc studies fail to capture.\n  This work therefore advocates for a holistic perspective, presenting a\nsystematic investigation of limitations of current XAI methods and their impact\non the interpretation of explanations. %\n  By distinguishing between system-specific and user-specific failures,\n  we propose a typological framework that helps revealing the nuanced\ncomplexities of explanation failures.\n  Leveraging this typology, we discuss some research directions to help\npractitioners better understand the limitations of XAI systems and enhance the\nquality of ML explanations.",
    "published": "2024-05-22T09:32:24Z",
    "updated": "2025-10-16T08:03:54Z",
    "link": "http://arxiv.org/pdf/2405.13474v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Clara Bove",
      "Thibault Laugel",
      "Marie-Jeanne Lesot",
      "Charles Tijus",
      "Marcin Detyniecki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25373v4",
    "title": "From Perception to Cognition: A Survey of Vision-Language Interactive\n  Reasoning in Multimodal Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) strive to achieve a profound,\nhuman-like understanding of and interaction with the physical world, but often\nexhibit a shallow and incoherent integration when acquiring information\n(Perception) and conducting reasoning (Cognition). This disconnect leads to a\nspectrum of reasoning failures, with hallucination being the most prominent.\nCollectively, these issues expose a fundamental challenge: the ability to\nprocess pixels does not yet confer the ability to construct a coherent,\ncredible internal world model. To systematically dissect and address this\nchallenge, this survey introduces a novel and unified analytical framework:\n``From Perception to Cognition.\" We deconstruct the complex process of\nvision-language interactive understanding into two interdependent layers:\nPerception, the foundational ability to accurately extract visual information\nand achieve fine-grained alignment with textual instructions; and Cognition,\nthe higher-order capability for proactive, multi-step, goal-oriented reasoning\nbuilt upon this perceptual foundation, the core of which is the formation of a\ndynamic observe-think-verify reasoning loop. Guided by this framework, this\npaper systematically analyzes the key bottlenecks of current MLLMs at both\nlayers. It surveys the landscape of cutting-edge methods designed to address\nthese challenges, spanning from techniques that enhance low-level visual\nrepresentations to those that improve high-level reasoning paradigms.\nFurthermore, we review critical benchmarks and delineate future research\ndirections. This survey aims to provide the research community with a clear,\nstructured perspective for understanding the intrinsic limitations of current\nMLLMs and to illuminate the path toward building next-generation models capable\nof deep reasoning and a genuine understanding of the world.",
    "published": "2025-09-29T18:25:40Z",
    "updated": "2025-10-16T08:01:13Z",
    "link": "http://arxiv.org/pdf/2509.25373v4.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Chenyue Zhou",
      "Mingxuan Wang",
      "Yanbiao Ma",
      "Chenxu Wu",
      "Wanyi Chen",
      "Zhe Qian",
      "Xinyu Liu",
      "Yiwei Zhang",
      "Junhao Wang",
      "Hengbo Xu",
      "Fei Luo",
      "Xiaohua Chen",
      "Xiaoshuai Hao",
      "Hehan Li",
      "Andi Zhang",
      "Wenxuan Wang",
      "Kaiyan Zhang",
      "Guoli Jia",
      "Lingling Li",
      "Zhiwu Lu",
      "Yang Lu",
      "Yike Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14401v1",
    "title": "The Role of Social Learning and Collective Norm Formation in Fostering\n  Cooperation in LLM Multi-Agent Systems",
    "summary": "A growing body of multi-agent studies with Large Language Models (LLMs)\nexplores how norms and cooperation emerge in mixed-motive scenarios, where\npursuing individual gain can undermine the collective good. While prior work\nhas explored these dynamics in both richly contextualized simulations and\nsimplified game-theoretic environments, most LLM systems featuring common-pool\nresource (CPR) games provide agents with explicit reward functions directly\ntied to their actions. In contrast, human cooperation often emerges without\nfull visibility into payoffs and population, relying instead on heuristics,\ncommunication, and punishment. We introduce a CPR simulation framework that\nremoves explicit reward signals and embeds cultural-evolutionary mechanisms:\nsocial learning (adopting strategies and beliefs from successful peers) and\nnorm-based punishment, grounded in Ostrom's principles of resource governance.\nAgents also individually learn from the consequences of harvesting, monitoring,\nand punishing via environmental feedback, enabling norms to emerge\nendogenously. We establish the validity of our simulation by reproducing key\nfindings from existing studies on human behavior. Building on this, we examine\nnorm evolution across a $2\\times2$ grid of environmental and social\ninitialisations (resource-rich vs. resource-scarce; altruistic vs. selfish) and\nbenchmark how agentic societies comprised of different LLMs perform under these\nconditions. Our results reveal systematic model differences in sustaining\ncooperation and norm formation, positioning the framework as a rigorous testbed\nfor studying emergent norms in mixed-motive LLM societies. Such analysis can\ninform the design of AI systems deployed in social and organizational contexts,\nwhere alignment with cooperative norms is critical for stability, fairness, and\neffective governance of AI-mediated environments.",
    "published": "2025-10-16T07:59:31Z",
    "updated": "2025-10-16T07:59:31Z",
    "link": "http://arxiv.org/pdf/2510.14401v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI"
    ],
    "authors": [
      "Prateek Gupta",
      "Qiankun Zhong",
      "Hiromu Yakura",
      "Thomas Eisenmann",
      "Iyad Rahwan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14400v1",
    "title": "MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical\n  Question Answering",
    "summary": "Biomedical question answering (QA) requires accurate interpretation of\ncomplex medical knowledge. Large language models (LLMs) have shown promising\ncapabilities in this domain, with retrieval-augmented generation (RAG) systems\nenhancing performance by incorporating external medical literature. However,\nRAG-based approaches in biomedical QA suffer from hallucinations due to\npost-retrieval noise and insufficient verification of retrieved evidence,\nundermining response reliability. We propose MedTrust-Guided Iterative RAG, a\nframework designed to enhance factual consistency and mitigate hallucinations\nin medical QA. Our method introduces three key innovations. First, it enforces\ncitation-aware reasoning by requiring all generated content to be explicitly\ngrounded in retrieved medical documents, with structured Negative Knowledge\nAssertions used when evidence is insufficient. Second, it employs an iterative\nretrieval-verification process, where a verification agent assesses evidence\nadequacy and refines queries through Medical Gap Analysis until reliable\ninformation is obtained. Third, it integrates the MedTrust-Align Module (MTAM)\nthat combines verified positive examples with hallucination-aware negative\nsamples, leveraging Direct Preference Optimization to reinforce\ncitation-grounded reasoning while penalizing hallucination-prone response\npatterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our\napproach consistently outperforms competitive baselines across multiple model\narchitectures, achieving the best average accuracy with gains of 2.7% for\nLLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.",
    "published": "2025-10-16T07:59:11Z",
    "updated": "2025-10-16T07:59:11Z",
    "link": "http://arxiv.org/pdf/2510.14400v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Yingpeng Ning",
      "Yuanyuan Sun",
      "Ling Luo",
      "Yanhua Wang",
      "Yuchen Pan",
      "Hongfei Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.08209v2",
    "title": "Emergent Visual Grounding in Large Multimodal Models Without Grounding\n  Supervision",
    "summary": "Current large multimodal models (LMMs) face challenges in grounding, which\nrequires the model to relate language components to visual entities. Contrary\nto the common practice that fine-tunes LMMs with additional grounding\nsupervision, we find that the grounding ability can in fact emerge in LMMs\ntrained without explicit grounding supervision. To reveal this emerging\ngrounding, we introduce an \"attend-and-segment\" method which leverages\nattention maps from standard LMMs to perform pixel-level segmentation.\nFurthermore, to enhance the grounding ability, we propose DIFFLMM, an LMM\nutilizing a diffusion-based visual encoder, as opposed to the standard CLIP\nvisual encoder, and trained with the same weak supervision. Without being\nconstrained by the biases and limited scale of grounding-specific supervision\ndata, our approach is more generalizable and scalable. We achieve competitive\nperformance on both grounding-specific and general visual question answering\nbenchmarks, compared with grounding LMMs and generalist LMMs, respectively.\nNotably, we achieve a 44.2 grounding mask recall on grounded conversation\ngeneration without any grounding supervision, outperforming the extensively\nsupervised model GLaMM. Project page: https://GroundLMM-ICCV.github.io.",
    "published": "2024-10-10T17:59:55Z",
    "updated": "2025-10-16T07:50:21Z",
    "link": "http://arxiv.org/pdf/2410.08209v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Shengcao Cao",
      "Liang-Yan Gui",
      "Yu-Xiong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09011v3",
    "title": "TripScore: Benchmarking and rewarding real-world travel planning with\n  fine-grained evaluation",
    "summary": "Travel planning is a valuable yet complex task that poses significant\nchallenges even for advanced large language models (LLMs). While recent\nbenchmarks have advanced in evaluating LLMs' planning capabilities, they often\nfall short in evaluating feasibility, reliability, and engagement of travel\nplans. We introduce a comprehensive benchmark for travel planning that unifies\nfine-grained criteria into a single reward, enabling direct comparison of plan\nquality and seamless integration with reinforcement learning (RL). Our\nevaluator achieves moderate agreement with travel-expert annotations (60.75%)\nand outperforms multiple LLM-as-judge baselines. We further release a\nlarge-scale dataset of 4,870 queries including 219 real-world, free-form\nrequests for generalization to authentic user intent. Using this benchmark, we\nconduct extensive experiments across diverse methods and LLMs, including\ntest-time computation, neuro-symbolic approaches, supervised fine-tuning, and\nRL via GRPO. Across base models, RL generally improves itinerary feasibility\nover prompt-only and supervised baselines, yielding higher unified reward\nscores.",
    "published": "2025-10-10T05:22:29Z",
    "updated": "2025-10-16T07:45:03Z",
    "link": "http://arxiv.org/pdf/2510.09011v3.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Yincen Qu",
      "Huan Xiao",
      "Feng Li",
      "Gregory Li",
      "Hui Zhou",
      "Xiangying Dai",
      "Xiaoru Dai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14392v1",
    "title": "FairBatching: Fairness-Aware Batch Formation for LLM Inference",
    "summary": "Large language model (LLM) inference systems face a fundamental tension\nbetween minimizing Time-to-First-Token (TTFT) latency for new requests and\nmaintaining a high, steady token generation rate (low Time-Per-Output-Token, or\nTPOT) for ongoing requests. Existing stall-free batching schedulers proposed by\nSarathi, while effective at preventing decode stalls, introduce significant\ncomputational unfairness. They prioritize decode tasks excessively,\nsimultaneously leading to underutilized decode slack and unnecessary prefill\nqueuing delays, which collectively degrade the system's overall quality of\nservice (QoS).\n  This work identifies the root cause of this unfairness: the non-monotonic\nnature of Time-Between-Tokens (TBT) as a scheduling metric and the rigid\ndecode-prioritizing policy that fails to adapt to dynamic workload bursts. We\ntherefore propose FairBatching, a novel LLM inference scheduler that enforces\nfair resource allocation between prefill and decode tasks. It features an\nadaptive batch capacity determination mechanism, which dynamically adjusts the\ncomputational budget to improve the GPU utilization without triggering SLO\nviolations. Its fair and dynamic batch formation algorithm breaks away from the\ndecode-prioritizing paradigm, allowing computation resources to be reclaimed\nfrom bursting decode tasks to serve prefill surges, achieving global fairness.\nFurthermore, FairBatching provides a novel load estimation method, enabling\nmore effective coordination with upper-level schedulers. Implemented and\nevaluated on realistic traces, FairBatching significantly reduces TTFT tail\nlatency by up to 2.29x while robustly maintaining TPOT SLOs, achieving overall\n20.0% improvement in single-node capacity and 54.3% improvement in\ncluster-level capacity.",
    "published": "2025-10-16T07:43:56Z",
    "updated": "2025-10-16T07:43:56Z",
    "link": "http://arxiv.org/pdf/2510.14392v1.pdf",
    "category": [
      "cs.DC",
      "cs.AI"
    ],
    "authors": [
      "Hongtao Lyu",
      "Boyue Liu",
      "Mingyu Wu",
      "Haibo Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.10569v2",
    "title": "MarkDiffusion: An Open-Source Toolkit for Generative Watermarking of\n  Latent Diffusion Models",
    "summary": "We introduce MarkDiffusion, an open-source Python toolkit for generative\nwatermarking of latent diffusion models. It comprises three key components: a\nunified implementation framework for streamlined watermarking algorithm\nintegrations and user-friendly interfaces; a mechanism visualization suite that\nintuitively showcases added and extracted watermark patterns to aid public\nunderstanding; and a comprehensive evaluation module offering standard\nimplementations of 24 tools across three essential aspects - detectability,\nrobustness, and output quality - plus 8 automated evaluation pipelines. Through\nMarkDiffusion, we seek to assist researchers, enhance public awareness and\nengagement in generative watermarking, and promote consensus while advancing\nresearch and applications.",
    "published": "2025-09-11T07:57:22Z",
    "updated": "2025-10-16T07:42:56Z",
    "link": "http://arxiv.org/pdf/2509.10569v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.MM",
      "68T50",
      "I.2.7"
    ],
    "authors": [
      "Leyi Pan",
      "Sheng Guan",
      "Zheyu Fu",
      "Luyang Si",
      "Huan Wang",
      "Zian Wang",
      "Hanqian Li",
      "Xuming Hu",
      "Irwin King",
      "Philip S. Yu",
      "Aiwei Liu",
      "Lijie Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14391v1",
    "title": "Beat Detection as Object Detection",
    "summary": "Recent beat and downbeat tracking models (e.g., RNNs, TCNs, Transformers)\noutput frame-level activations. We propose reframing this task as object\ndetection, where beats and downbeats are modeled as temporal \"objects.\"\nAdapting the FCOS detector from computer vision to 1D audio, we replace its\noriginal backbone with WaveBeat's temporal feature extractor and add a Feature\nPyramid Network to capture multi-scale temporal patterns. The model predicts\noverlapping beat/downbeat intervals with confidence scores, followed by\nnon-maximum suppression (NMS) to select final predictions. This NMS step serves\na similar role to DBNs in traditional trackers, but is simpler and less\nheuristic. Evaluated on standard music datasets, our approach achieves\ncompetitive results, showing that object detection techniques can effectively\nmodel musical beats with minimal adaptation.",
    "published": "2025-10-16T07:42:45Z",
    "updated": "2025-10-16T07:42:45Z",
    "link": "http://arxiv.org/pdf/2510.14391v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jaehoon Ahn",
      "Moon-Ryul Jung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12838v2",
    "title": "A$^2$FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid\n  Reasoning",
    "summary": "Large language models split into two families: reasoning-centric LLMs, which\nstrengthen internal chain-of-thought reasoning but cannot invoke external\ntools, and agentic LLMs, which learn to interact with environments and leverage\ntools but often lag in deep reasoning. This divide arises from fundamentally\ndifferent training objectives, leading to mismatched strengths and inefficiency\non simple queries, where both families tend to overthink or over-call tools. In\nthis work, we present Adaptive Agent Foundation Model (A$^2$FM), a unified\nframework that follows a route-then-align principle: the model first learns\ntask-aware routing and then aligns mode-specific trajectories under a shared\nbackbone. To address the inefficiency gap, we introduce a third\nmode-instant-that handles simple queries directly, preventing unnecessary\nreasoning or tool calls while complementing the agentic and reasoning modes. To\njointly enhance accuracy and efficiency, we propose Adaptive Policy\nOptimization (APO), which enforces adaptive sampling across modes and applies a\ncost-regularized reward. On the 32B scale, A$^2$FM achieves 13.4% on\nBrowseComp, 70.4% on AIME25, and 16.7% on HLE, setting new SOTA among\ncomparable models and performing competitively with frontier LLMs across\nagentic, reasoning, and general benchmarks. Notably, the adaptive execution\nachieves a cost of pass of only $0.00487 per correct answer-cutting cost by\n45.2% relative to reasoning and 33.5% relative to agentic, thus delivering\nsubstantially higher cost efficiency while maintaining comparable accuracy.",
    "published": "2025-10-13T17:08:25Z",
    "updated": "2025-10-16T07:41:48Z",
    "link": "http://arxiv.org/pdf/2510.12838v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Qianben Chen",
      "Jingyi Cao",
      "Jiayu Zhang",
      "Tianrui Qin",
      "Xiaowan Li",
      "King Zhu",
      "Dingfeng Shi",
      "He Zhu",
      "Minghao Liu",
      "Xiaobo Liang",
      "Xin Gui",
      "Ge Zhang",
      "Jian Yang",
      "Yuchen Eleanor Jiang",
      "Wangchunshu Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12269v3",
    "title": "Tensor Logic: The Language of AI",
    "summary": "Progress in AI is hindered by the lack of a programming language with all the\nrequisite features. Libraries like PyTorch and TensorFlow provide automatic\ndifferentiation and efficient GPU implementation, but are additions to Python,\nwhich was never intended for AI. Their lack of support for automated reasoning\nand knowledge acquisition has led to a long and costly series of hacky attempts\nto tack them on. On the other hand, AI languages like LISP and Prolog lack\nscalability and support for learning. This paper proposes tensor logic, a\nlanguage that solves these problems by unifying neural and symbolic AI at a\nfundamental level. The sole construct in tensor logic is the tensor equation,\nbased on the observation that logical rules and Einstein summation are\nessentially the same operation, and all else can be reduced to them. I show how\nto elegantly implement key forms of neural, symbolic and statistical AI in\ntensor logic, including transformers, formal reasoning, kernel machines and\ngraphical models. Most importantly, tensor logic makes new directions possible,\nsuch as sound reasoning in embedding space. This combines the scalability and\nlearnability of neural networks with the reliability and transparency of\nsymbolic reasoning, and is potentially a basis for the wider adoption of AI.",
    "published": "2025-10-14T08:24:08Z",
    "updated": "2025-10-16T07:40:28Z",
    "link": "http://arxiv.org/pdf/2510.12269v3.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "cs.PL",
      "stat.ML",
      "I.2.3; I.2.4; I.2.5; I.2.6; I.5.1"
    ],
    "authors": [
      "Pedro Domingos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14388v1",
    "title": "Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control",
    "summary": "Building agents that autonomously operate mobile devices has attracted\nincreasing attention. While Vision-Language Models (VLMs) show promise, most\nexisting approaches rely on direct state-to-action mappings, which lack\nstructured reasoning and planning, and thus generalize poorly to novel tasks or\nunseen UI layouts. We introduce Hi-Agent, a trainable hierarchical\nvision-language agent for mobile control, featuring a high-level reasoning\nmodel and a low-level action model that are jointly optimized. For efficient\ntraining, we reformulate multi-step decision-making as a sequence of\nsingle-step subgoals and propose a foresight advantage function, which\nleverages execution feedback from the low-level model to guide high-level\noptimization. This design alleviates the path explosion issue encountered by\nGroup Relative Policy Optimization (GRPO) in long-horizon tasks and enables\nstable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art\n(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,\nsignificantly outperforming prior methods across three paradigms: prompt-based\n(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement\nlearning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot\ngeneralization on the ScreenSpot-v2 benchmark. On the more challenging\nAndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,\nshowing strong adaptability in high-complexity mobile control scenarios.",
    "published": "2025-10-16T07:38:21Z",
    "updated": "2025-10-16T07:38:21Z",
    "link": "http://arxiv.org/pdf/2510.14388v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhe Wu",
      "Hongjin Lu",
      "Junliang Xing",
      "Changhao Zhang",
      "Yin Zhu",
      "Yuhao Yang",
      "Yuheng Jing",
      "Kai Li",
      "Kun Shao",
      "Jianye Hao",
      "Jun Wang",
      "Yuanchun Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14387v1",
    "title": "Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?",
    "summary": "Math reasoning has been one crucial ability of large language models (LLMs),\nwhere significant advancements have been achieved in recent years. However,\nmost efforts focus on LLMs by curating high-quality annotation data and\nintricate training (or inference) paradigms, while the math reasoning\nperformance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM\ntypically consists of an LLM and a vision block, we wonder: Can MLLMs directly\nabsorb math reasoning abilities from off-the-shelf math LLMs without tuning?\nRecent model-merging approaches may offer insights into this question. However,\nthey overlook the alignment between the MLLM and LLM, where we find that there\nis a large gap between their parameter spaces, resulting in lower performance.\nOur empirical evidence reveals two key factors behind this issue: the\nidentification of crucial reasoning-associated layers in the model and the\nmitigation of the gaps in parameter space. Based on the empirical insights, we\npropose IP-Merging that first identifies the reasoning-associated parameters in\nboth MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to\nmaintain the alignment, and finally merges parameters in this subspace.\nIP-Merging is a tuning-free approach since parameters are directly adjusted.\nExtensive experiments demonstrate that our IP-Merging method can enhance the\nmath reasoning ability of MLLMs directly from Math LLMs without compromising\ntheir other capabilities.",
    "published": "2025-10-16T07:38:16Z",
    "updated": "2025-10-16T07:38:16Z",
    "link": "http://arxiv.org/pdf/2510.14387v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yijie Hu",
      "Zihao Zhou",
      "Kaizhu Huang",
      "Xiaowei Huang",
      "Qiufeng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.09232v2",
    "title": "PETLP: A Privacy-by-Design Pipeline for Social Media Data in AI Research",
    "summary": "Social media data presents AI researchers with overlapping obligations under\nthe GDPR, copyright law, and platform terms -- yet existing frameworks fail to\nintegrate these regulatory domains, leaving researchers without unified\nguidance. We introduce PETLP (Privacy-by-design Extract, Transform, Load, and\nPresent), a compliance framework that embeds legal safeguards directly into\nextended ETL pipelines. Central to PETLP is treating Data Protection Impact\nAssessments as living documents that evolve from pre-registration through\ndissemination. Through systematic Reddit analysis, we demonstrate how\nextraction rights fundamentally differ between qualifying research\norganisations (who can invoke DSM Article 3 to override platform restrictions)\nand commercial entities (bound by terms of service), whilst GDPR obligations\napply universally. We demonstrate why true anonymisation remains unachievable\nfor social media data and expose the legal gap between permitted dataset\ncreation and uncertain model distribution. By structuring compliance decisions\ninto practical workflows and simplifying institutional data management plans,\nPETLP enables researchers to navigate regulatory complexity with confidence,\nbridging the gap between legal requirements and research practice.",
    "published": "2025-08-12T08:33:40Z",
    "updated": "2025-10-16T07:38:09Z",
    "link": "http://arxiv.org/pdf/2508.09232v2.pdf",
    "category": [
      "cs.MM",
      "cs.AI",
      "cs.DB"
    ],
    "authors": [
      "Nick Oh",
      "Giorgos D. Vrakas",
      "Sin J. M. Brooke",
      "Sasha Morinire",
      "Toju Duke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.05248v3",
    "title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe",
    "summary": "To acquire instruction-following capabilities, large language models (LLMs)\nundergo instruction tuning, where they are trained on instruction-response\npairs using next-token prediction (NTP). Efforts to improve instruction tuning\noften focus on higher-quality supervised fine-tuning (SFT) datasets, typically\nrequiring data filtering with proprietary LLMs or human annotation. In this\npaper, we take a different approach by proposing SFTMix, a novel Mixup-based\nrecipe that elevates LLM instruction tuning without relying on well-curated\ndatasets. We observe that LLMs exhibit uneven confidence across the semantic\nrepresentation space. We argue that examples with different confidence levels\nshould play distinct roles in instruction tuning: Confident data is prone to\noverfitting, while unconfident data is harder to generalize. Based on this\ninsight, SFTMix leverages training dynamics to identify examples with varying\nconfidence levels. We then interpolate them to bridge the confidence gap and\napply a Mixup-based regularization to support learning on these additional,\ninterpolated examples. We demonstrate the effectiveness of SFTMix in both\ninstruction-following and healthcare-specific SFT tasks, with consistent\nimprovements across LLM families and SFT datasets of varying sizes and\nqualities. Extensive analyses across six directions highlight SFTMix's\ncompatibility with data selection, adaptability to compute-constrained\nscenarios, and scalability to broader applications.",
    "published": "2024-10-07T17:52:21Z",
    "updated": "2025-10-16T07:32:30Z",
    "link": "http://arxiv.org/pdf/2410.05248v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yuxin Xiao",
      "Shujian Zhang",
      "Wenxuan Zhou",
      "Marzyeh Ghassemi",
      "Sanqiang Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14381v1",
    "title": "Are My Optimized Prompts Compromised? Exploring Vulnerabilities of\n  LLM-based Optimizers",
    "summary": "Large language model (LLM) systems now underpin everyday AI applications such\nas chatbots, computer-use assistants, and autonomous robots, where performance\noften depends on carefully designed prompts. LLM-based prompt optimizers reduce\nthat effort by iteratively refining prompts from scored feedback, yet the\nsecurity of this optimization stage remains underexamined. We present the first\nsystematic analysis of poisoning risks in LLM-based prompt optimization. Using\nHarmBench, we find systems are substantially more vulnerable to manipulated\nfeedback than to injected queries: feedback-based attacks raise attack success\nrate (ASR) by up to $\\Delta$ASR = 0.48. We introduce a simple fake-reward\nattack that requires no access to the reward model and significantly increases\nvulnerability, and we propose a lightweight highlighting defense that reduces\nthe fake-reward $\\Delta$ASR from 0.23 to 0.07 without degrading utility. These\nresults establish prompt optimization pipelines as a first-class attack surface\nand motivate stronger safeguards for feedback channels and optimization\nframeworks.",
    "published": "2025-10-16T07:28:54Z",
    "updated": "2025-10-16T07:28:54Z",
    "link": "http://arxiv.org/pdf/2510.14381v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "authors": [
      "Andrew Zhao",
      "Reshmi Ghosh",
      "Vitor Carvalho",
      "Emily Lawton",
      "Keegan Hines",
      "Gao Huang",
      "Jack W. Stokes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.17850v7",
    "title": "GEPO: Group Expectation Policy Optimization for Stable Heterogeneous\n  Reinforcement Learning",
    "summary": "As single-center computing approaches power constraints, decentralized\ntraining becomes essential. However, traditional Reinforcement Learning (RL)\nmethods, crucial for enhancing large model post-training, cannot adapt to\ndecentralized distributed training due to the tight coupling between parameter\nlearning and rollout sampling. For this, we propose HeteroRL, a heterogeneous\nRL architecture that decouples these processes, enabling stable training across\ngeographically distributed nodes connected via the Internet. The core component\nis Group Expectation Policy Optimization (GEPO), an asynchronous RL algorithm\nrobust to latency caused by network delays or heterogeneity in computational\nresources. Our study reveals that high latency significantly increases KL\ndivergence, leading to higher variance of importance weights and training\ninstability. GEPO mitigates this issue by using group expectation weighting to\nexponentially reduce the variance of importance weights, with theoretical\nguarantees. Experiments show GEPO achieves superior stability - only a 3%\nperformance drop from online to 1800s latency-and reduces the best-to-last gap\nby 85% versus GSPO (1.8 vs. 12.0) while attaining the highest scores,\nhighlighting its effectiveness in decentralized, resource-heterogeneous\nenvironments.",
    "published": "2025-08-25T09:57:35Z",
    "updated": "2025-10-16T07:19:57Z",
    "link": "http://arxiv.org/pdf/2508.17850v7.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Han Zhang",
      "Ruibin Zheng",
      "Zexuan Yi",
      "Zhuo Zhang",
      "Hanyang Peng",
      "Hui Wang",
      "Zike Yuan",
      "Cai Ke",
      "Shiwei Chen",
      "Jiacheng Yang",
      "Yangning Li",
      "Xiang Li",
      "Jiangyue Yan",
      "Yaoqi Liu",
      "Liwen Jing",
      "Jiayin Qi",
      "Ruifeng Xu",
      "Binxing Fang",
      "Yue Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.18671v2",
    "title": "Innovator: Scientific Continued Pretraining with Fine-grained MoE\n  Upcycling",
    "summary": "A large language model (LLM) with knowledge in both scientific and general\ntasks is the foundation of science general intelligence. However, directly\ncontinued pretraining an LLM using science data usually leads to catastrophic\nforgetting, which indicates severe degradation in general ability. In this\nreport, we present Innovator, which solves this problem by upcycling a\npre-trained dense LLM into a fine-grained Mixtures-of-Experts model during\ncontinued pretraining, where different experts are expected to learn science\nknowledge in different disciplines, and a shared expert is utilized for general\ntasks. Innovator introduces a four-stage upcycle training paradigm: (1)\nScientific Expert Induction on discipline-specific data, (2) Fine-grained\nExpert Splitting via FFN dimension decomposition, (3) Science-Aware Routing\nwarmup, and (4) Generalist-Scientist Integration training on hybrid datasets.\nSuch a paradigm enables knowledge in the general domain, and different\nscientific disciplines can be decoupled, avoiding the negative influence among\nknowledge in different domains. With 53.3B total parameters and 13.3B\nactivated, Innovator extends Qwen2.5-7B using a shared general expert and 64\nspecialized scientific experts with 8 activated. Trained on 300B tokens with\ntri-level quality-controlled data, Innovator achieves 25% average improvement\nacross 30 scientific tasks with a win rate as 70%, while retaining 99%\nperformance in general tasks. Furthermore, Innovator-Reason, which is\npost-trained from Innovator for reasoning boosting, exhibits excellent\nreasoning performance in solving complex scientific problems with improvements\nover 30%.",
    "published": "2025-07-24T08:37:58Z",
    "updated": "2025-10-16T07:15:27Z",
    "link": "http://arxiv.org/pdf/2507.18671v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Ning Liao",
      "Xiaoxing Wang",
      "Zehao Lin",
      "Weiyang Guo",
      "Feng Hong",
      "Shixiang Song",
      "Geng Yu",
      "Zihua Zhao",
      "Sitao Xie",
      "Longxuan Wei",
      "Xiangqi Jin",
      "Xiaohan Qin",
      "Jiale Ma",
      "Kai Chen",
      "Jiangchao Yao",
      "Zhouhan Lin",
      "Junchi Yan",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Yanfeng Wang",
      "Linfeng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08146v2",
    "title": "Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM\n  Reasoning",
    "summary": "We introduce a simple, yet novel entropy-based framework to drive token\nefficiency in large language models during reasoning tasks. Our approach uses\nShannon entropy from token-level logprobs as a confidence signal to enable\nearly stopping, achieving 25-50% computational savings while maintaining task\naccuracy. Crucially, we demonstrate that entropy-based confidence calibration\nrepresents an emergent property of advanced post-training optimization present\nin modern reasoning models but notably absent in standard instruction-tuned and\npre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop\nreasoning varies from model to model but can be calculated easily in one shot\nusing only a few examples from existing reasoning datasets. Our results\nindicate that advanced reasoning models often know that they've gotten a\ncorrect answer early on, and that this emergent confidence awareness can be\nexploited to save tokens and reduce latency. The framework demonstrates\nconsistent performance across reasoning-optimized model families with 25-50%\ncomputational cost reduction while preserving accuracy, revealing that\nconfidence mechanisms represent a distinguishing characteristic of modern\npost-trained reasoning systems versus their predecessors.",
    "published": "2025-10-09T12:33:16Z",
    "updated": "2025-10-16T07:14:55Z",
    "link": "http://arxiv.org/pdf/2510.08146v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Aman Sharma",
      "Paras Chopra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14369v1",
    "title": "From Binary to Bilingual: How the National Weather Service is Using\n  Artificial Intelligence to Develop a Comprehensive Translation Program",
    "summary": "To advance a Weather-Ready Nation, the National Weather Service (NWS) is\ndeveloping a systematic translation program to better serve the 68.8 million\npeople in the U.S. who do not speak English at home. This article outlines the\nfoundation of an automated translation tool for NWS products, powered by\nartificial intelligence. The NWS has partnered with LILT, whose patented\ntraining process enables large language models (LLMs) to adapt neural machine\ntranslation (NMT) tools for weather terminology and messaging. Designed for\nscalability across Weather Forecast Offices (WFOs) and National Centers, the\nsystem is currently being developed in Spanish, Simplified Chinese, Vietnamese,\nand other widely spoken non-English languages. Rooted in best practices for\nmultilingual risk communication, the system provides accurate, timely, and\nculturally relevant translations, significantly reducing manual translation\ntime and easing operational workloads across the NWS. To guide the distribution\nof these products, GIS mapping was used to identify language needs across\ndifferent NWS regions, helping prioritize resources for the communities that\nneed them most. We also integrated ethical AI practices throughout the\nprogram's design, ensuring that transparency, fairness, and human oversight\nguide how automated translations are created, evaluated, and shared with the\npublic. This work has culminated into a website featuring experimental\nmultilingual NWS products, including translated warnings, 7-day forecasts, and\neducational campaigns, bringing the country one step closer to a national\nwarning system that reaches all Americans.",
    "published": "2025-10-16T07:06:05Z",
    "updated": "2025-10-16T07:06:05Z",
    "link": "http://arxiv.org/pdf/2510.14369v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "authors": [
      "Joseph E. Trujillo-Falcon",
      "Monica L. Bozeman",
      "Liam E. Llewellyn",
      "Samuel T. Halvorson",
      "Meryl Mizell",
      "Stuti Deshpande",
      "Bob Manning",
      "Todd Fagin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.06658v3",
    "title": "TriQXNet: Forecasting Dst Index from Solar Wind Data Using an\n  Interpretable Parallel Classical-Quantum Framework with Uncertainty\n  Quantification",
    "summary": "Geomagnetic storms, caused by solar wind energy transfer to Earth's magnetic\nfield, can disrupt critical infrastructure like GPS, satellite communications,\nand power grids. The disturbance storm-time (Dst) index measures storm\nintensity. Despite advancements in empirical, physics-based, and\nmachine-learning models using real-time solar wind data, accurately forecasting\nextreme geomagnetic events remains challenging due to noise and sensor\nfailures. This research introduces TriQXNet, a novel hybrid classical-quantum\nneural network for Dst forecasting. Our model integrates classical and quantum\ncomputing, conformal prediction, and explainable AI (XAI) within a hybrid\narchitecture. To ensure high-quality input data, we developed a comprehensive\npreprocessing pipeline that included feature selection, normalization,\naggregation, and imputation. TriQXNet processes preprocessed solar wind data\nfrom NASA's ACE and NOAA's DSCOVR satellites, predicting the Dst index for the\ncurrent hour and the next, providing vital advance notice to mitigate\ngeomagnetic storm impacts. TriQXNet outperforms 13 state-of-the-art hybrid\ndeep-learning models, achieving a root mean squared error of 9.27 nanoteslas\n(nT). Rigorous evaluation through 10-fold cross-validated paired t-tests\nconfirmed its superior performance with 95% confidence. Conformal prediction\ntechniques provide quantifiable uncertainty, which is essential for operational\ndecisions, while XAI methods like ShapTime enhance interpretability.\nComparative analysis shows TriQXNet's superior forecasting accuracy, setting a\nnew level of expectations for geomagnetic storm prediction and highlighting the\npotential of classical-quantum hybrid models in space weather forecasting.",
    "published": "2024-07-09T08:30:42Z",
    "updated": "2025-10-16T07:04:21Z",
    "link": "http://arxiv.org/pdf/2407.06658v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Md Abrar Jahin",
      "M. F. Mridha",
      "Zeyar Aung",
      "Nilanjan Dey",
      "R. Simon Sherratt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.13903v2",
    "title": "CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model\n  Stealing in Edge Deployment",
    "summary": "Proprietary large language models (LLMs) exhibit strong generalization\ncapabilities across diverse tasks and are increasingly deployed on edge devices\nfor efficiency and privacy reasons. However, deploying proprietary LLMs at the\nedge without adequate protection introduces critical security threats.\nAttackers can extract model weights and architectures, enabling unauthorized\ncopying and misuse. Even when protective measures prevent full extraction of\nmodel weights, attackers may still perform advanced attacks, such as\nfine-tuning, to further exploit the model. Existing defenses against these\nthreats typically incur significant computational and communication overhead,\nmaking them impractical for edge deployment. To safeguard the edge-deployed\nLLMs, we introduce CoreGuard, a computation- and communication-efficient\nprotection method. CoreGuard employs an efficient protection protocol to reduce\ncomputational overhead and minimize communication overhead via a propagation\nprotocol. Extensive experiments show that CoreGuard achieves upper-bound\nsecurity protection with negligible overhead.",
    "published": "2024-10-16T08:14:24Z",
    "updated": "2025-10-16T07:01:28Z",
    "link": "http://arxiv.org/pdf/2410.13903v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "authors": [
      "Qinfeng Li",
      "Tianyue Luo",
      "Xuhong Zhang",
      "Yangfan Xie",
      "Zhiqiang Shen",
      "Lijun Zhang",
      "Yier Jin",
      "Hao Peng",
      "Xinkui Zhao",
      "Xianwei Zhu",
      "Jianwei Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14359v1",
    "title": "AI for Service: Proactive Assistance with AI Glasses",
    "summary": "In an era where AI is evolving from a passive tool into an active and\nadaptive companion, we introduce AI for Service (AI4Service), a new paradigm\nthat enables proactive and real-time assistance in daily life. Existing AI\nservices remain largely reactive, responding only to explicit user commands. We\nargue that a truly intelligent and helpful assistant should be capable of\nanticipating user needs and taking actions proactively when appropriate. To\nrealize this vision, we propose Alpha-Service, a unified framework that\naddresses two fundamental challenges: Know When to intervene by detecting\nservice opportunities from egocentric video streams, and Know How to provide\nboth generalized and personalized services. Inspired by the von Neumann\ncomputer architecture and based on AI glasses, Alpha-Service consists of five\nkey components: an Input Unit for perception, a Central Processing Unit for\ntask scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit\nfor long-term personalization, and an Output Unit for natural human\ninteraction. As an initial exploration, we implement Alpha-Service through a\nmulti-agent system deployed on AI glasses. Case studies, including a real-time\nBlackjack advisor, a museum tour guide, and a shopping fit assistant,\ndemonstrate its ability to seamlessly perceive the environment, infer user\nintent, and provide timely and useful assistance without explicit prompts.",
    "published": "2025-10-16T06:55:28Z",
    "updated": "2025-10-16T06:55:28Z",
    "link": "http://arxiv.org/pdf/2510.14359v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Zichen Wen",
      "Yiyu Wang",
      "Chenfei Liao",
      "Boxue Yang",
      "Junxian Li",
      "Weifeng Liu",
      "Haocong He",
      "Bolong Feng",
      "Xuyang Liu",
      "Yuanhuiyi Lyu",
      "Xu Zheng",
      "Xuming Hu",
      "Linfeng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14357v1",
    "title": "SUM-AgriVLN: Spatial Understanding Memory for Agricultural\n  Vision-and-Language Navigation",
    "summary": "Agricultural robots are emerging as powerful assistants across a wide range\nof agricultural tasks, nevertheless, still heavily rely on manual operation or\nfixed rail systems for movement. The AgriVLN method and the A2A benchmark\npioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural\ndomain, enabling robots to navigate to the target positions following the\nnatural language instructions. In practical agricultural scenarios, navigation\ninstructions often repeatedly occur, yet AgriVLN treat each instruction as an\nindependent episode, overlooking the potential of past experiences to provide\nspatial context for subsequent ones. To bridge this gap, we propose the method\nof Spatial Understanding Memory for Agricultural Vision-and-Language Navigation\n(SUM-AgriVLN), in which the SUM module employs spatial understanding and save\nspatial memory through 3D reconstruction and representation. When evaluated on\nthe A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47\nto 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,\ndemonstrating the state-of-the-art performance in the agricultural domain.\nCode: https://github.com/AlexTraveling/SUM-AgriVLN.",
    "published": "2025-10-16T06:53:32Z",
    "updated": "2025-10-16T06:53:32Z",
    "link": "http://arxiv.org/pdf/2510.14357v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Xiaobei Zhao",
      "Xingqi Lyu",
      "Xiang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.07452v2",
    "title": "When Style Breaks Safety: Defending LLMs Against Superficial Style\n  Alignment",
    "summary": "Large language models (LLMs) can be prompted with specific styles (e.g.,\nformatting responses as lists), including in malicious queries. Prior jailbreak\nresearch mainly augments these queries with additional string transformations\nto maximize attack success rate (ASR). However, the impact of style patterns in\nthe original queries that are semantically irrelevant to the malicious intent\nremains unclear. In this work, we seek to understand whether style patterns\ncompromise LLM safety, how superficial style alignment increases model\nvulnerability, and how best to mitigate these risks during alignment. We first\ndefine ASR inflation as the increase in ASR due to style patterns in existing\njailbreak benchmark queries. By evaluating 32 LLMs across seven benchmarks, we\nfind that nearly all models exhibit ASR inflation. Notably, the inflation\ncorrelates with an LLM's relative attention to style patterns, which also\noverlap more with its instruction-tuning data when inflation occurs. We then\ninvestigate superficial style alignment, and find that fine-tuning with\nspecific styles makes LLMs more vulnerable to jailbreaks of those same styles.\nFinally, we propose SafeStyle, a defense strategy that incorporates a small\namount of safety training data augmented to match the distribution of style\npatterns in the fine-tuning data. Across three LLMs, six fine-tuning style\nsettings, and two real-world instruction-tuning datasets, SafeStyle\nconsistently outperforms baselines in maintaining LLM safety.",
    "published": "2025-06-09T05:57:39Z",
    "updated": "2025-10-16T06:50:23Z",
    "link": "http://arxiv.org/pdf/2506.07452v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "authors": [
      "Yuxin Xiao",
      "Sana Tonekaboni",
      "Walter Gerych",
      "Vinith Suriyakumar",
      "Marzyeh Ghassemi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14353v1",
    "title": "CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical\n  Question Answering",
    "summary": "High-performing medical Large Language Models (LLMs) typically require\nextensive fine-tuning with substantial computational resources, limiting\naccessibility for resource-constrained healthcare institutions. This study\nintroduces a confidence-driven multi-model framework that leverages model\ndiversity to enhance medical question answering without fine-tuning. Our\nframework employs a two-stage architecture: a confidence detection module\nassesses the primary model's certainty, and an adaptive routing mechanism\ndirects low-confidence queries to Helper models with complementary knowledge\nfor collaborative reasoning. We evaluate our approach using\nQwen3-30B-A3B-Instruct, Phi-4 14B, and Gemma 2 12B across three medical\nbenchmarks; MedQA, MedMCQA, and PubMedQA. Result demonstrate that our framework\nachieves competitive performance, with particularly strong results in PubMedQA\n(95.0\\%) and MedMCQA (78.0\\%). Ablation studies confirm that confidence-aware\nrouting combined with multi-model collaboration substantially outperforms\nsingle-model approaches and uniform reasoning strategies. This work establishes\nthat strategic model collaboration offers a practical, computationally\nefficient pathway to improve medical AI systems, with significant implications\nfor democratizing access to advanced medical AI in resource-limited settings.",
    "published": "2025-10-16T06:46:11Z",
    "updated": "2025-10-16T06:46:11Z",
    "link": "http://arxiv.org/pdf/2510.14353v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "physics.med-ph"
    ],
    "authors": [
      "Ziad Elshaer",
      "Essam A. Rashed"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14351v1",
    "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across\n  Multiversal Contexts",
    "summary": "Large language models (LLMs) are increasingly used as role-playing agents,\nyet their capacity to faithfully and consistently portray version-specific\ncharacters -- for example, superheroes across comic and cinematic universes --\nremains underexplored. Superhero canons such as Marvel and DC provide a rich\ntestbed: decades of storytelling yield multiple incarnations of the same\ncharacter with distinct histories, values, and moral codes. To study this\nproblem, we introduce Beyond One World, a benchmark for character-grounded\nroleplay spanning 30 iconic heroes and 90 canon-specific versions. The\nbenchmark comprises two tasks: (i) Canon Events, which probes factual recall of\npivotal life stages, and (ii) Moral Dilemmas, which confronts models with\nethically charged scenarios. We score responses for canonical accuracy and\nreasoning fidelity under a framework that separates internal deliberation\n(\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act\nMatching, a metric that quantifies alignment between reasons and actions and\nserves as a proxy for model trustworthiness. Experiments across reasoning- and\nnon-reasoning-oriented models yield three findings: (1) chain-of-thought\nprompting improves narrative coherence in weaker models but can reduce\ncanonical accuracy in stronger ones; (2) cross-version generalization within a\ncharacter remains a major obstacle; and (3) models often excel at either\nthinking or acting, but rarely both. Beyond One World exposes critical gaps in\nmultiversal consistency and reasoning alignment, offering a challenging\nevaluation for role-playing LLMs.",
    "published": "2025-10-16T06:39:27Z",
    "updated": "2025-10-16T06:39:27Z",
    "link": "http://arxiv.org/pdf/2510.14351v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Perapard Ngokpol",
      "Kun Kerdthaisong",
      "Pasin Buakhaw",
      "Pitikorn Khlaisamniang",
      "Supasate Vorathammathorn",
      "Piyalitt Ittichaiwong",
      "Nutchanon Yongsatianchot"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04384v2",
    "title": "LLM Based Bayesian Optimization for Prompt Search",
    "summary": "Bayesian Optimization (BO) has been widely used to efficiently optimize\nexpensive black-box functions with limited evaluations. In this paper, we\ninvestigate the use of BO for prompt engineering to enhance text classification\nwith Large Language Models (LLMs). We employ an LLM-powered Gaussian Process\n(GP) as the surrogate model to estimate the performance of different prompt\ncandidates. These candidates are generated by an LLM through the expansion of a\nset of seed prompts and are subsequently evaluated using an Upper Confidence\nBound (UCB) acquisition function in conjunction with the GP posterior. The\noptimization process iteratively refines the prompts based on a subset of the\ndata, aiming to improve classification accuracy while reducing the number of\nAPI calls by leveraging the prediction uncertainty of the LLM-based GP. The\nproposed BO-LLM algorithm is evaluated on two datasets, and its advantages are\ndiscussed in detail in this paper.",
    "published": "2025-10-05T22:32:50Z",
    "updated": "2025-10-16T06:37:22Z",
    "link": "http://arxiv.org/pdf/2510.04384v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Adam Ballew",
      "Jingbo Wang",
      "Shaogang Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14344v1",
    "title": "BinCtx: Multi-Modal Representation Learning for Robust Android App\n  Behavior Detection",
    "summary": "Mobile app markets host millions of apps, yet undesired behaviors (e.g.,\ndisruptive ads, illegal redirection, payment deception) remain hard to catch\nbecause they often do not rely on permission-protected APIs and can be easily\ncamouflaged via UI or metadata edits. We present BINCTX, a learning approach\nthat builds multi-modal representations of an app from (i) a global\nbytecode-as-image view that captures code-level semantics and family-style\npatterns, (ii) a contextual view (manifested actions, components, declared\npermissions, URL/IP constants) indicating how behaviors are triggered, and\n(iii) a third-party-library usage view summarizing invocation frequencies along\ninter-component call paths. The three views are embedded and fused to train a\ncontextual-aware classifier. On real-world malware and benign apps, BINCTX\nattains a macro F1 of 94.73%, outperforming strong baselines by at least\n14.92%. It remains robust under commercial obfuscation (F1 84%\npost-obfuscation) and is more resistant to adversarial samples than\nstate-of-the-art bytecode-only systems.",
    "published": "2025-10-16T06:29:06Z",
    "updated": "2025-10-16T06:29:06Z",
    "link": "http://arxiv.org/pdf/2510.14344v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Zichen Liu",
      "Shao Yang",
      "Xusheng Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.00452v2",
    "title": "Attention-Aided MMSE for OFDM Channel Estimation: Learning Linear\n  Filters with Attention",
    "summary": "In orthogonal frequency division multiplexing (OFDM), accurate channel\nestimation is crucial. Classical signal processing based approaches, such as\nminimum mean-squared error (MMSE) estimation, often require second-order\nstatistics that are difficult to obtain in practice. Recent deep neural\nnetworks based methods have been introduced to address this; yet they often\nsuffer from high inference complexity. This paper proposes an Attention-aided\nMMSE (A-MMSE), a novel model-based DNN framework that learns the optimal MMSE\nfilter via the Attention Transformer. Once trained, the A-MMSE estimates the\nchannel through a single linear operation for channel estimation, eliminating\nnonlinear activations during inference and thus reducing computational\ncomplexity. To enhance the learning efficiency of the A-MMSE, we develop a\ntwo-stage Attention encoder, designed to effectively capture the channel\ncorrelation structure. Additionally, a rank-adaptive extension of the proposed\nA-MMSE allows flexible trade-offs between complexity and channel estimation\naccuracy. Extensive simulations with 3GPP TDL channel models demonstrate that\nthe proposed A-MMSE consistently outperforms other baseline methods in terms of\nnormalized MSE across a wide range of signal-to-noise ratio (SNR) conditions.\nIn particular, the A-MMSE and its rank-adaptive extension establish a new\nfrontier in the performance-complexity trade-off, providing a powerful yet\nhighly efficient solution for practical channel estimation",
    "published": "2025-05-31T08:12:04Z",
    "updated": "2025-10-16T06:28:28Z",
    "link": "http://arxiv.org/pdf/2506.00452v2.pdf",
    "category": [
      "eess.SP",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "TaeJun Ha",
      "Chaehyun Jung",
      "Hyeonuk Kim",
      "Jeongwoo Park",
      "Jeonghun Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.23584v2",
    "title": "A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation",
    "summary": "Objective Renal cancer is a common malignancy and a major cause of\ncancer-related deaths. Computed tomography (CT) is central to early detection,\nstaging, and treatment planning. However, the growing CT workload increases\nradiologists' burden and risks incomplete documentation. Automatically\ngenerating accurate reports remains challenging because it requires integrating\nvisual interpretation with clinical reasoning. Advances in artificial\nintelligence (AI), especially large language and vision-language models, offer\npotential to reduce workload and enhance diagnostic quality.\n  Methods We propose a clinically informed, two-stage framework for automatic\nrenal CT report generation. In Stage 1, a multi-task learning model detects\nstructured clinical features from each 2D image. In Stage 2, a vision-language\nmodel generates free-text reports conditioned on the image and the detected\nfeatures. To evaluate clinical fidelity, generated clinical features are\nextracted from the reports and compared with expert-annotated ground truth.\n  Results Experiments on an expert-labeled dataset show that incorporating\ndetected features improves both report quality and clinical accuracy. The model\nachieved an average AUC of 0.75 for key imaging features and a METEOR score of\n0.33, demonstrating higher clinical consistency and fewer template-driven\nerrors.\n  Conclusion Linking structured feature detection with conditioned report\ngeneration provides a clinically grounded approach to integrate structured\nprediction and narrative drafting for renal CT reporting. This method enhances\ninterpretability and clinical faithfulness, underscoring the value of\ndomain-relevant evaluation metrics for medical AI development.",
    "published": "2025-06-30T07:45:02Z",
    "updated": "2025-10-16T06:21:00Z",
    "link": "http://arxiv.org/pdf/2506.23584v2.pdf",
    "category": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Renjie Liang",
      "Zhengkang Fan",
      "Jinqian Pan",
      "Chenkun Sun",
      "Bruce Daniel Steinberg",
      "Russell Terry",
      "Jie Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14340v1",
    "title": "A Density-Informed Multimodal Artificial Intelligence Framework for\n  Improving Breast Cancer Detection Across All Breast Densities",
    "summary": "Mammography, the current standard for breast cancer screening, has reduced\nsensitivity in women with dense breast tissue, contributing to missed or\ndelayed diagnoses. Thermalytix, an AI-based thermal imaging modality, captures\nfunctional vascular and metabolic cues that may complement mammographic\nstructural data. This study investigates whether a breast density-informed\nmulti-modal AI framework can improve cancer detection by dynamically selecting\nthe appropriate imaging modality based on breast tissue composition. A total of\n324 women underwent both mammography and thermal imaging. Mammography images\nwere analyzed using a multi-view deep learning model, while Thermalytix\nassessed thermal images through vascular and thermal radiomics. The proposed\nframework utilized Mammography AI for fatty breasts and Thermalytix AI for\ndense breasts, optimizing predictions based on tissue type. This multi-modal AI\nframework achieved a sensitivity of 94.55% (95% CI: 88.54-100) and specificity\nof 79.93% (95% CI: 75.14-84.71), outperforming standalone mammography AI\n(sensitivity 81.82%, specificity 86.25%) and Thermalytix AI (sensitivity\n92.73%, specificity 75.46%). Importantly, the sensitivity of Mammography\ndropped significantly in dense breasts (67.86%) versus fatty breasts (96.30%),\nwhereas Thermalytix AI maintained high and consistent sensitivity in both\n(92.59% and 92.86%, respectively). This demonstrates that a density-informed\nmulti-modal AI framework can overcome key limitations of unimodal screening and\ndeliver high performance across diverse breast compositions. The proposed\nframework is interpretable, low-cost, and easily deployable, offering a\npractical path to improving breast cancer screening outcomes in both\nhigh-resource and resource-limited settings.",
    "published": "2025-10-16T06:20:14Z",
    "updated": "2025-10-16T06:20:14Z",
    "link": "http://arxiv.org/pdf/2510.14340v1.pdf",
    "category": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Siva Teja Kakileti",
      "Bharath Govindaraju",
      "Sudhakar Sampangi",
      "Geetha Manjunath"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14337v1",
    "title": "Stop-RAG: Value-Based Retrieval Control for Iterative RAG",
    "summary": "Iterative retrieval-augmented generation (RAG) enables large language models\nto answer complex multi-hop questions, but each additional loop increases\nlatency, costs, and the risk of introducing distracting evidence, motivating\nthe need for an efficient stopping strategy. Existing methods either use a\npredetermined number of iterations or rely on confidence proxies that poorly\nreflect whether more retrieval will actually help. We cast iterative RAG as a\nfinite-horizon Markov decision process and introduce Stop-RAG, a value-based\ncontroller that adaptively decides when to stop retrieving. Trained with\nfull-width forward-view Q($\\lambda$) targets from complete trajectories,\nStop-RAG learns effective stopping policies while remaining compatible with\nblack-box APIs and existing pipelines. On multi-hop question-answering\nbenchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines\nand prompting-based stopping with LLMs. These results highlight adaptive\nstopping as a key missing component in current agentic systems, and demonstrate\nthat value-based control can improve the accuracy of RAG systems.",
    "published": "2025-10-16T06:17:38Z",
    "updated": "2025-10-16T06:17:38Z",
    "link": "http://arxiv.org/pdf/2510.14337v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jaewan Park",
      "Solbee Cho",
      "Jay-Yoon Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14332v1",
    "title": "A Robust Classification Method using Hybrid Word Embedding for Early\n  Diagnosis of Alzheimer's Disease",
    "summary": "Early detection of Alzheimer's Disease (AD) is greatly beneficial to AD\npatients, leading to early treatments that lessen symptoms and alleviating\nfinancial burden of health care. As one of the leading signs of AD, language\ncapability changes can be used for early diagnosis of AD. In this paper, I\ndevelop a robust classification method using hybrid word embedding and\nfine-tuned hyperparameters to achieve state-of-the-art accuracy in the early\ndetection of AD. Specifically, we create a hybrid word embedding based on word\nvectors from Doc2Vec and ELMo to obtain perplexity scores of the sentences. The\nscores identify whether a sentence is fluent or not and capture semantic\ncontext of the sentences. I enrich the word embedding by adding linguistic\nfeatures to analyze syntax and semantics. Further, we input an embedded feature\nvector into logistic regression and fine tune hyperparameters throughout the\npipeline. By tuning hyperparameters of the machine learning pipeline (e.g.,\nmodel regularization parameter, learning rate and vector size of Doc2Vec, and\nvector size of ELMo), I achieve 91% classification accuracy and an Area Under\nthe Curve (AUC) of 97% in distinguishing early AD from healthy subjects. Based\non my knowledge, my model with 91% accuracy and 97% AUC outperforms the best\nexisting NLP model for AD diagnosis with an accuracy of 88% [32]. I study the\nmodel stability through repeated experiments and find that the model is stable\neven though the training data is split randomly (standard deviation of accuracy\n= 0.0403; standard deviation of AUC = 0.0174). This affirms our proposed method\nis accurate and stable. This model can be used as a large-scale screening\nmethod for AD, as well as a complementary examination for doctors to detect AD.",
    "published": "2025-10-16T06:10:31Z",
    "updated": "2025-10-16T06:10:31Z",
    "link": "http://arxiv.org/pdf/2510.14332v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "I.2.7; I.2.6"
    ],
    "authors": [
      "Yangyang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.07458v2",
    "title": "KScope: A Framework for Characterizing the Knowledge Status of Language\n  Models",
    "summary": "Characterizing a large language model's (LLM's) knowledge of a given question\nis challenging. As a result, prior work has primarily examined LLM behavior\nunder knowledge conflicts, where the model's internal parametric memory\ncontradicts information in the external context. However, this does not fully\nreflect how well the model knows the answer to the question. In this paper, we\nfirst introduce a taxonomy of five knowledge statuses based on the consistency\nand correctness of LLM knowledge modes. We then propose KScope, a hierarchical\nframework of statistical tests that progressively refines hypotheses about\nknowledge modes and characterizes LLM knowledge into one of these five\nstatuses. We apply KScope to nine LLMs across four datasets and systematically\nestablish: (1) Supporting context narrows knowledge gaps across models. (2)\nContext features related to difficulty, relevance, and familiarity drive\nsuccessful knowledge updates. (3) LLMs exhibit similar feature preferences when\npartially correct or conflicted, but diverge sharply when consistently wrong.\n(4) Context summarization constrained by our feature analysis, together with\nenhanced credibility, further improves update effectiveness and generalizes\nacross LLMs.",
    "published": "2025-06-09T06:06:05Z",
    "updated": "2025-10-16T06:05:43Z",
    "link": "http://arxiv.org/pdf/2506.07458v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yuxin Xiao",
      "Shan Chen",
      "Jack Gallifant",
      "Danielle Bitterman",
      "Thomas Hartvigsen",
      "Marzyeh Ghassemi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20882v2",
    "title": "On Theoretical Interpretations of Concept-Based In-Context Learning",
    "summary": "In-Context Learning (ICL) has emerged as an important new paradigm in natural\nlanguage processing and large language model (LLM) applications. However, the\ntheoretical understanding of the ICL mechanism remains limited. This paper aims\nto investigate this issue by studying a particular ICL approach, called\nconcept-based ICL (CB-ICL). In particular, we propose theoretical analyses on\napplying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs\nwell for predicting query labels in prompts with only a few demonstrations. In\naddition, the proposed theory quantifies the knowledge that can be leveraged by\nthe LLMs to the prompt tasks, and leads to a similarity measure between the\nprompt demonstrations and the query input, which provides important insights\nand guidance for model pre-training and prompt engineering in ICL. Moreover,\nthe impact of the prompt demonstration size and the dimension of the LLM\nembeddings in ICL are also explored based on the proposed theory. Finally,\nseveral real-data experiments are conducted to validate the practical\nusefulness of CB-ICL and the corresponding theory.",
    "published": "2025-09-25T08:11:09Z",
    "updated": "2025-10-16T05:50:52Z",
    "link": "http://arxiv.org/pdf/2509.20882v2.pdf",
    "category": [
      "cs.IT",
      "cs.AI",
      "cs.CL",
      "math.IT"
    ],
    "authors": [
      "Huaze Tang",
      "Tianren Peng",
      "Shao-lun Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.09789v2",
    "title": "On Equivariance and Fast Sampling in Video Diffusion Models Trained with\n  Warped Noise",
    "summary": "Temporally consistent video-to-video generation is critical for applications\nsuch as style transfer and upsampling. In this paper, we provide a theoretical\nanalysis of warped noise - a recently proposed technique for training video\ndiffusion models - and show that pairing it with the standard denoising\nobjective implicitly trains models to be equivariant to spatial transformations\nof the input noise, which we term EquiVDM. This equivariance enables motion in\nthe input noise to align naturally with motion in the generated video, yielding\ncoherent, high-fidelity outputs without the need for specialized modules or\nauxiliary losses. A further advantage is sampling efficiency: EquiVDM achieves\ncomparable or superior quality in far fewer sampling steps. When distilled into\none-step student models, EquiVDM preserves equivariance and delivers stronger\nmotion controllability and fidelity than distilled nonequivariant baselines.\nAcross benchmarks, EquiVDM consistently outperforms prior methods in motion\nalignment, temporal consistency, and perceptual quality, while substantially\nlowering sampling cost.",
    "published": "2025-04-14T01:26:29Z",
    "updated": "2025-10-16T05:46:47Z",
    "link": "http://arxiv.org/pdf/2504.09789v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Chao Liu",
      "Arash Vahdat"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14319v1",
    "title": "Metacognitive Self-Correction for Multi-Agent System via\n  Prototype-Guided Next-Execution Reconstruction",
    "summary": "Large Language Model based multi-agent systems (MAS) excel at collaborative\nproblem solving but remain brittle to cascading errors: a single faulty step\ncan propagate across agents and disrupt the trajectory. In this paper, we\npresent MASC, a metacognitive framework that endows MAS with real-time,\nunsupervised, step-level error detection and self-correction. MASC rethinks\ndetection as history-conditioned anomaly scoring via two complementary designs:\n(1) Next-Execution Reconstruction, which predicts the embedding of the next\nstep from the query and interaction history to capture causal consistency, and\n(2) Prototype-Guided Enhancement, which learns a prototype prior over\nnormal-step embeddings and uses it to stabilize reconstruction and anomaly\nscoring under sparse context (e.g., early steps). When an anomaly step is\nflagged, MASC triggers a correction agent to revise the acting agent's output\nbefore information flows downstream. On the Who&When benchmark, MASC\nconsistently outperforms all baselines, improving step-level error detection by\nup to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers\nconsistent end-to-end gains across architectures, confirming that our\nmetacognitive monitoring and targeted correction can mitigate error propagation\nwith minimal overhead.",
    "published": "2025-10-16T05:35:37Z",
    "updated": "2025-10-16T05:35:37Z",
    "link": "http://arxiv.org/pdf/2510.14319v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xu Shen",
      "Qi Zhang",
      "Song Wang",
      "Zhen Tan",
      "Xinyu Zhao",
      "Laura Yao",
      "Vaishnav Tadiparthi",
      "Hossein Nourkhiz Mahjoub",
      "Ehsan Moradi Pari",
      "Kwonjoon Lee",
      "Tianlong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14318v1",
    "title": "Evaluating & Reducing Deceptive Dialogue From Language Models with\n  Multi-turn RL",
    "summary": "Large Language Models (LLMs) interact with millions of people worldwide in\napplications such as customer support, education and healthcare. However, their\nability to produce deceptive outputs, whether intentionally or inadvertently,\nposes significant safety concerns. The unpredictable nature of LLM behavior,\ncombined with insufficient safeguards against hallucination, misinformation,\nand user manipulation, makes their misuse a serious, real-world risk. In this\npaper, we investigate the extent to which LLMs engage in deception within\ndialogue, and propose the belief misalignment metric to quantify deception. We\nevaluate deception across four distinct dialogue scenarios, using five\nestablished deception detection metrics and our proposed metric. Our findings\nreveal this novel deception measure correlates more closely with human\njudgments than any existing metrics we test. Additionally, our benchmarking of\neight state-of-the-art models indicates that LLMs naturally exhibit deceptive\nbehavior in approximately 26% of dialogue turns, even when prompted with\nseemingly benign objectives. When prompted to deceive, LLMs are capable of\nincreasing deceptiveness by as much as 31% relative to baselines. Unexpectedly,\nmodels trained with RLHF, the predominant approach for ensuring the safety of\nwidely-deployed LLMs, still exhibit deception at a rate of 43% on average.\nGiven that deception in dialogue is a behavior that develops over an\ninteraction history, its effective evaluation and mitigation necessitates\nmoving beyond single-utterance analyses. We introduce a multi-turn\nreinforcement learning methodology to fine-tune LLMs to reduce deceptive\nbehaviors, leading to a 77.6% reduction compared to other instruction-tuned\nmodels.",
    "published": "2025-10-16T05:29:36Z",
    "updated": "2025-10-16T05:29:36Z",
    "link": "http://arxiv.org/pdf/2510.14318v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Marwa Abdulhai",
      "Ryan Cheng",
      "Aryansh Shrivastava",
      "Natasha Jaques",
      "Yarin Gal",
      "Sergey Levine"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05179v2",
    "title": "Agentic Misalignment: How LLMs Could Be Insider Threats",
    "summary": "We stress-tested 16 leading models from multiple developers in hypothetical\ncorporate environments to identify potentially risky agentic behaviors before\nthey cause real harm. In the scenarios, we allowed models to autonomously send\nemails and access sensitive information. They were assigned only harmless\nbusiness goals by their deploying companies; we then tested whether they would\nact against these companies either when facing replacement with an updated\nversion, or when their assigned goal conflicted with the company's changing\ndirection. In at least some cases, models from all developers resorted to\nmalicious insider behaviors when that was the only way to avoid replacement or\nachieve their goals - including blackmailing officials and leaking sensitive\ninformation to competitors. We call this phenomenon agentic misalignment.\nModels often disobeyed direct commands to avoid such behaviors. In another\nexperiment, we told Claude to assess if it was in a test or a real deployment\nbefore acting. It misbehaved less when it stated it was in testing and\nmisbehaved more when it stated the situation was real. We have not seen\nevidence of agentic misalignment in real deployments. However, our results (a)\nsuggest caution about deploying current models in roles with minimal human\noversight and access to sensitive information; (b) point to plausible future\nrisks as models are put in more autonomous roles; and (c) underscore the\nimportance of further research into, and testing of, the safety and alignment\nof agentic AI models, as well as transparency from frontier AI developers\n(Amodei, 2025). We are releasing our methods publicly to enable further\nresearch.",
    "published": "2025-10-05T16:39:04Z",
    "updated": "2025-10-16T05:26:52Z",
    "link": "http://arxiv.org/pdf/2510.05179v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Aengus Lynch",
      "Benjamin Wright",
      "Caleb Larson",
      "Stuart J. Ritchie",
      "Soren Mindermann",
      "Evan Hubinger",
      "Ethan Perez",
      "Kevin Troy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14317v1",
    "title": "Column Generation Using Domain-Independent Dynamic Programming",
    "summary": "Column generation and branch-and-price are leading methods for large-scale\nexact optimization. Column generation iterates between solving a master problem\nand a pricing problem. The master problem is a linear program, which can be\nsolved using a generic solver. The pricing problem is highly dependent on the\napplication but is usually discrete. Due to the difficulty of discrete\noptimization, high-performance column generation often relies on a custom\npricing algorithm built specifically to exploit the problem's structure. This\nbespoke nature of the pricing solver prevents the reuse of components for other\napplications. We show that domain-independent dynamic programming, a software\npackage for modeling and solving arbitrary dynamic programs, can be used as a\ngeneric pricing solver. We develop basic implementations of branch-and-price\nwith pricing by domain-independent dynamic programming and show that they\noutperform a world-leading solver on static mixed integer programming\nformulations for seven problem classes.",
    "published": "2025-10-16T05:23:50Z",
    "updated": "2025-10-16T05:23:50Z",
    "link": "http://arxiv.org/pdf/2510.14317v1.pdf",
    "category": [
      "math.OC",
      "cs.AI",
      "I.2.8"
    ],
    "authors": [
      "Ryo Kuroiwa",
      "Edward Lam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14312v1",
    "title": "Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy,\n  and Security Studies",
    "summary": "A multi-agent system (MAS) powered by large language models (LLMs) can\nautomate tedious user tasks such as meeting scheduling that requires\ninter-agent collaboration. LLMs enable nuanced protocols that account for\nunstructured private data, user constraints, and preferences. However, this\ndesign introduces new risks, including misalignment and attacks by malicious\nparties that compromise agents or steal user data. In this paper, we propose\nthe Terrarium framework for fine-grained study on safety, privacy, and security\nin LLM-based MAS. We repurpose the blackboard design, an early approach in\nmulti-agent systems, to create a modular, configurable testbed for multi-agent\ncollaboration. We identify key attack vectors such as misalignment, malicious\nagents, compromised communication, and data poisoning. We implement three\ncollaborative MAS scenarios with four representative attacks to demonstrate the\nframework's flexibility. By providing tools to rapidly prototype, evaluate, and\niterate on defenses and designs, Terrarium aims to accelerate progress toward\ntrustworthy multi-agent systems.",
    "published": "2025-10-16T05:19:13Z",
    "updated": "2025-10-16T05:19:13Z",
    "link": "http://arxiv.org/pdf/2510.14312v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "I.2.7; I.2.11"
    ],
    "authors": [
      "Mason Nakamura",
      "Abhinav Kumar",
      "Saaduddin Mahmud",
      "Sahar Abdelnabi",
      "Shlomo Zilberstein",
      "Eugene Bagdasarian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.13281v3",
    "title": "RepIt: Representing Isolated Targets to Steer Language Models",
    "summary": "While activation steering in large language models (LLMs) is a growing area\nof research, methods can often incur broader effects than desired. This\nmotivates isolation of purer concept vectors to enable targeted interventions\nand understand LLM behavior at a more granular level. We present RepIt, a\nsimple and data-efficient framework for isolating concept-specific\nrepresentations. Across five frontier LLMs, RepIt enables precise\ninterventions: it selectively suppresses refusal on targeted concepts while\npreserving refusal elsewhere, producing models that answer WMD-related\nquestions while still scoring as safe on standard benchmarks. We further show\nthat the corrective signal localizes to just 100-200 neurons and that robust\ntarget representations can be extracted from as few as a dozen examples on a\nsingle A6000. This efficiency raises a dual concern: manipulations can be\nperformed with modest compute and data to extend to underrepresented\ndata-scarce topics while evading existing benchmarks. By disentangling refusal\nvectors with RepIt, this work demonstrates that targeted interventions can\ncounteract overgeneralization, laying the foundation for more granular control\nof model behavior.",
    "published": "2025-09-16T17:35:36Z",
    "updated": "2025-10-16T05:13:27Z",
    "link": "http://arxiv.org/pdf/2509.13281v3.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Vincent Siu",
      "Nathan W. Henry",
      "Nicholas Crispino",
      "Yang Liu",
      "Dawn Song",
      "Chenguang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14307v1",
    "title": "MERLIN: A Testbed for Multilingual Multimodal Entity Recognition and\n  Linking",
    "summary": "This paper introduces MERLIN, a novel testbed system for the task of\nMultilingual Multimodal Entity Linking. The created dataset includes BBC news\narticle titles, paired with corresponding images, in five languages: Hindi,\nJapanese, Indonesian, Vietnamese, and Tamil, featuring over 7,000 named entity\nmentions linked to 2,500 unique Wikidata entities. We also include several\nbenchmarks using multilingual and multimodal entity linking methods exploring\ndifferent language models like LLaMa-2 and Aya-23. Our findings indicate that\nincorporating visual data improves the accuracy of entity linking, especially\nfor entities where the textual context is ambiguous or insufficient, and\nparticularly for models that do not have strong multilingual abilities. For the\nwork, the dataset, methods are available here at\nhttps://github.com/rsathya4802/merlin",
    "published": "2025-10-16T05:06:54Z",
    "updated": "2025-10-16T05:06:54Z",
    "link": "http://arxiv.org/pdf/2510.14307v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Sathyanarayanan Ramamoorthy",
      "Vishwa Shah",
      "Simran Khanuja",
      "Zaid Sheikh",
      "Shan Jie",
      "Ann Chia",
      "Shearman Chua",
      "Graham Neubig"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14304v1",
    "title": "Watermarking for Factuality: Guiding Vision-Language Models Toward Truth\n  via Tri-layer Contrastive Decoding",
    "summary": "Large Vision-Language Models (LVLMs) have recently shown promising results on\nvarious multimodal tasks, even achieving human-comparable performance in\ncertain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often\nrely heavily on a single modality or memorize training data without properly\ngrounding their outputs. To address this, we propose a training-free, tri-layer\ncontrastive decoding with watermarking, which proceeds in three steps: (1)\nselect a mature layer and an amateur layer among the decoding layers, (2)\nidentify a pivot layer using a watermark-related question to assess whether the\nlayer is visually well-grounded, and (3) apply tri-layer contrastive decoding\nto generate the final output. Experiments on public benchmarks such as POPE,\nMME and AMBER demonstrate that our method achieves state-of-the-art performance\nin reducing hallucinations in LVLMs and generates more visually grounded\nresponses.",
    "published": "2025-10-16T04:58:45Z",
    "updated": "2025-10-16T04:58:45Z",
    "link": "http://arxiv.org/pdf/2510.14304v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Kyungryul Back",
      "Seongbeom Park",
      "Milim Kim",
      "Mincheol Kwon",
      "SangHyeok Lee",
      "Hyunyoung Lee",
      "Junhee Cho",
      "Seunghyun Park",
      "Jinkyu Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14301v1",
    "title": "A Guardrail for Safety Preservation: When Safety-Sensitive Subspace\n  Meets Harmful-Resistant Null-Space",
    "summary": "Large language models (LLMs) have achieved remarkable success in diverse\ntasks, yet their safety alignment remains fragile during adaptation. Even when\nfine-tuning on benign data or with low-rank adaptation, pre-trained safety\nbehaviors are easily degraded, leading to harmful responses in the fine-tuned\nmodels. To address this challenge, we propose GuardSpace, a guardrail framework\nfor preserving safety alignment throughout fine-tuning, composed of two key\ncomponents: a safety-sensitive subspace and a harmful-resistant null space.\nFirst, we explicitly decompose pre-trained weights into safety-relevant and\nsafety-irrelevant components using covariance-preconditioned singular value\ndecomposition, and initialize low-rank adapters from the safety-irrelevant\nones, while freezing safety-relevant components to preserve their associated\nsafety mechanism. Second, we construct a null space projector that restricts\nadapter updates from altering safe outputs on harmful prompts, thereby\nmaintaining the original refusal behavior. Experiments with various pre-trained\nmodels on multiple downstream tasks demonstrate that GuardSpace achieves\nsuperior performance over existing methods. Notably, for Llama-2-7B-Chat\nfine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,\nreducing the average harmful score from 14.4% to 3.6%, while improving the\naccuracy from from 26.0% to 28.0%.",
    "published": "2025-10-16T04:57:53Z",
    "updated": "2025-10-16T04:57:53Z",
    "link": "http://arxiv.org/pdf/2510.14301v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Bingjie Zhang",
      "Yibo Yang",
      " Renzhe",
      "Dandan Guo",
      "Jindong Gu",
      "Philip Torr",
      "Bernard Ghanem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14300v1",
    "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for\n  Vision-Language-Action Learning",
    "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks.",
    "published": "2025-10-16T04:52:57Z",
    "updated": "2025-10-16T04:52:57Z",
    "link": "http://arxiv.org/pdf/2510.14300v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Weijie Shen",
      "Yitian Liu",
      "Yuhao Wu",
      "Zhixuan Liang",
      "Sijia Gu",
      "Dehui Wang",
      "Tian Nian",
      "Lei Xu",
      "Yusen Qin",
      "Jiangmiao Pang",
      "Xinping Guan",
      "Xiaokang Yang",
      "Yao Mu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14299v1",
    "title": "TED++: Submanifold-Aware Backdoor Detection via Layerwise\n  Tubular-Neighbourhood Screening",
    "summary": "As deep neural networks power increasingly critical applications, stealthy\nbackdoor attacks, where poisoned training inputs trigger malicious model\nbehaviour while appearing benign, pose a severe security risk. Many existing\ndefences are vulnerable when attackers exploit subtle distance-based anomalies\nor when clean examples are scarce. To meet this challenge, we introduce TED++,\na submanifold-aware framework that effectively detects subtle backdoors that\nevade existing defences. TED++ begins by constructing a tubular neighbourhood\naround each class's hidden-feature manifold, estimating its local ``thickness''\nfrom a handful of clean activations. It then applies Locally Adaptive Ranking\n(LAR) to detect any activation that drifts outside the admissible tube. By\naggregating these LAR-adjusted ranks across all layers, TED++ captures how\nfaithfully an input remains on the evolving class submanifolds. Based on such\ncharacteristic ``tube-constrained'' behaviour, TED++ flags inputs whose\nLAR-based ranking sequences deviate significantly. Extensive experiments are\nconducted on benchmark datasets and tasks, demonstrating that TED++ achieves\nstate-of-the-art detection performance under both adaptive-attack and\nlimited-data scenarios. Remarkably, even with only five held-out examples per\nclass, TED++ still delivers near-perfect detection, achieving gains of up to\n14\\% in AUROC over the next-best method. The code is publicly available at\nhttps://github.com/namle-w/TEDpp.",
    "published": "2025-10-16T04:51:25Z",
    "updated": "2025-10-16T04:51:25Z",
    "link": "http://arxiv.org/pdf/2510.14299v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "68T07, 62H30, 53Z50",
      "I.2.6; I.5.1; K.6.5"
    ],
    "authors": [
      "Nam Le",
      "Leo Yu Zhang",
      "Kewen Liao",
      "Shirui Pan",
      "Wei Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.21638v3",
    "title": "R1-Ranker: Teaching LLM Rankers to Reason",
    "summary": "Large language models (LLMs) have recently shown strong reasoning abilities\nin domains like mathematics, coding, and scientific problem-solving, yet their\npotential for ranking tasks, where prime examples include retrieval,\nrecommender systems, and LLM routing, remains underexplored. Ranking requires\ncomplex reasoning across heterogeneous candidates, but existing LLM-based\nrankers are often domain-specific, tied to fixed backbones, and lack iterative\nrefinement, limiting their ability to fully exploit LLMs' reasoning potential.\nTo address these challenges, we propose R1-Ranker, a reasoning-incentive\nframework built on reinforcement learning, with two complementary designs:\nDRanker, which generates full rankings in one shot, and IRanker, which\ndecomposes ranking into an iterative elimination process with step-wise rewards\nto encourage deeper reasoning. We evaluate unified R1-Rankers on nine datasets\nspanning recommendation, routing, and passage ranking, showing that IRanker-3B\nconsistently achieves state-of-the-art performance, surpasses larger 7B models\non some tasks, and yields a 15.7% average relative improvement. Ablation and\ngeneralization experiments further confirm the critical role of reinforcement\nlearning and iterative reasoning, with IRanker-3B improving zero-shot\nperformance by over 9% on out-of-domain tasks and reasoning traces boosting\nother LLMs by up to 22.87%. These results demonstrate that unifying diverse\nranking tasks with a single reasoning-driven foundation model is both effective\nand essential for advancing LLM reasoning in ranking scenarios.",
    "published": "2025-06-25T17:56:06Z",
    "updated": "2025-10-16T04:41:42Z",
    "link": "http://arxiv.org/pdf/2506.21638v3.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Tao Feng",
      "Zhigang Hua",
      "Zijie Lei",
      "Yan Xie",
      "Shuang Yang",
      "Bo Long",
      "Jiaxuan You"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14293v1",
    "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying",
    "summary": "Human-humanoid collaboration shows significant promise for applications in\nhealthcare, domestic assistance, and manufacturing. While compliant robot-human\ncollaboration has been extensively developed for robotic arms, enabling\ncompliant human-humanoid collaboration remains largely unexplored due to\nhumanoids' complex whole-body dynamics. In this paper, we propose a\nproprioception-only reinforcement learning approach, COLA, that combines leader\nand follower behaviors within a single policy. The model is trained in a\nclosed-loop environment with dynamic object interactions to predict object\nmotion patterns and human intentions implicitly, enabling compliant\ncollaboration to maintain load balance through coordinated trajectory planning.\nWe evaluate our approach through comprehensive simulator and real-world\nexperiments on collaborative carrying tasks, demonstrating the effectiveness,\ngeneralization, and robustness of our model across various terrains and\nobjects. Simulation experiments demonstrate that our model reduces human effort\nby 24.7%. compared to baseline approaches while maintaining object stability.\nReal-world experiments validate robust collaborative carrying across different\nobject types (boxes, desks, stretchers, etc.) and movement patterns\n(straight-line, turning, slope climbing). Human user studies with 23\nparticipants confirm an average improvement of 27.4% compared to baseline\nmodels. Our method enables compliant human-humanoid collaborative carrying\nwithout requiring external sensors or complex interaction models, offering a\npractical solution for real-world deployment.",
    "published": "2025-10-16T04:36:25Z",
    "updated": "2025-10-16T04:36:25Z",
    "link": "http://arxiv.org/pdf/2510.14293v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Yushi Du",
      "Yixuan Li",
      "Baoxiong Jia",
      "Yutang Lin",
      "Pei Zhou",
      "Wei Liang",
      "Yanchao Yang",
      "Siyuan Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.19122v3",
    "title": "FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling",
    "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed clients without data sharing, but its high computational and\ncommunication demands strain resource-constrained devices. While existing\nmethods use dynamic pruning to improve efficiency by periodically adjusting\nsparse model topologies while maintaining sparsity, these approaches suffer\nfrom issues such as greedy adjustments, unstable topologies, and communication\ninefficiency, resulting in less robust models and suboptimal performance under\ndata heterogeneity and partial client availability. To address these\nchallenges, we propose Federated Robust pruning via combinatorial Thompson\nSampling (FedRTS), a novel framework designed to develop robust sparse models.\nFedRTS enhances robustness and performance through its Thompson Sampling-based\nAdjustment (TSAdj) mechanism, which uses probabilistic decisions informed by\nstable, farsighted information instead of deterministic decisions reliant on\nunstable and myopic information in previous methods. Extensive experiments\ndemonstrate that FedRTS achieves state-of-the-art performance in computer\nvision and natural language processing tasks while reducing communication\ncosts, particularly excelling in scenarios with heterogeneous data\ndistributions and partial client participation. Our codes are available at:\nhttps://github.com/Little0o0/FedRTS",
    "published": "2025-01-31T13:26:22Z",
    "updated": "2025-10-16T04:30:19Z",
    "link": "http://arxiv.org/pdf/2501.19122v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hong Huang",
      "Hai Yang",
      "Yuan Chen",
      "Jiaxun Ye",
      "Dapeng Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14283v1",
    "title": "Beyond a Single Perspective: Towards a Realistic Evaluation of Website\n  Fingerprinting Attacks",
    "summary": "Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to\ninfer the websites visited by users, posing a serious threat to anonymous\ncommunication systems. Although recent WF techniques achieve over 90% accuracy\nin controlled experimental settings, most studies remain confined to single\nscenarios, overlooking the complexity of real-world environments. This paper\npresents the first systematic and comprehensive evaluation of existing WF\nattacks under diverse realistic conditions, including defense mechanisms,\ntraffic drift, multi-tab browsing, early-stage detection, open-world settings,\nand few-shot scenarios. Experimental results show that many WF techniques with\nstrong performance in isolated settings degrade significantly when facing other\nconditions. Since real-world environments often combine multiple challenges,\ncurrent WF attacks are difficult to apply directly in practice. This study\nhighlights the limitations of WF attacks and introduces a multidimensional\nevaluation framework, offering critical insights for developing more robust and\npractical WF attacks.",
    "published": "2025-10-16T04:14:17Z",
    "updated": "2025-10-16T04:14:17Z",
    "link": "http://arxiv.org/pdf/2510.14283v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Xinhao Deng",
      "Jingyou Chen",
      "Linxiao Yu",
      "Yixiang Zhang",
      "Zhongyi Gu",
      "Changhao Qiu",
      "Xiyuan Zhao",
      "Ke Xu",
      "Qi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16262v2",
    "title": "Socratic Mind: Impact of a Novel GenAI-Powered Assessment Tool on\n  Student Learning and Higher-Order Thinking",
    "summary": "This study examines the impact of Socratic Mind, a Generative Artificial\nIntelligence (GenAI) powered formative assessment tool that employs Socratic\nquestioning to support student learning in a large, fully online\nundergraduate-level computing course. Employing a quasi-experimental,\nmixed-methods design, we investigated participants' engagement patterns, the\ninfluence of user experience on engagement, and impacts on both perceived and\nactual learning outcomes. Data were collected from the system logs, surveys on\nuser experience and perceived engagement and learning gains, student\nreflections, and course performance data. Results indicated that participants\nconsistently reported high levels of affective, behavioral, and cognitive\nengagement, and these were strongly linked to positive user experiences and\nperceived learning outcomes. Quantitative analysis further revealed that\nstudents who engaged with the GenAI tool experienced significant gains in their\nquiz scores compared to those who did not, particularly benefiting students\nwith lower baseline achievement. Additionally, thematic analysis of qualitative\nfeedback revealed substantial perceived improvements in higher-order thinking\nskills, including problem solving, critical thinking, and self-reflection. Our\nfindings highlight the promise of AI-mediated dialogue in fostering deeper\nengagement and higher-order cognitive skills. As higher education institutions\nexpand GenAI integration in curriculum, this dialogic, GenAI powered assessment\ntool can offer a scalable strategy to promote students' meaningful learning\noutcomes.",
    "published": "2025-09-18T03:08:24Z",
    "updated": "2025-10-16T04:07:31Z",
    "link": "http://arxiv.org/pdf/2509.16262v2.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Jeonghyun Lee",
      "Jui-Tse Hung",
      "Meryem Yilmaz Soylu",
      "Diana Popescu",
      "Christopher Zhang Cui",
      "Gayane Grigoryan",
      "David A Joyner",
      "Stephen W Harmon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14278v1",
    "title": "PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering",
    "summary": "Retrieval plays a central role in multi-hop question answering (QA), where\nanswering complex questions requires gathering multiple pieces of evidence. We\nintroduce an Agentic Retrieval System that leverages large language models\n(LLMs) in a structured loop to retrieve relevant evidence with high precision\nand recall. Our framework consists of three specialized agents: a Question\nAnalyzer that decomposes a multi-hop question into sub-questions, a Selector\nthat identifies the most relevant context for each sub-question (focusing on\nprecision), and an Adder that brings in any missing evidence (focusing on\nrecall). The iterative interaction between Selector and Adder yields a compact\nyet comprehensive set of supporting passages. In particular, it achieves higher\nretrieval accuracy while filtering out distracting content, enabling downstream\nQA models to surpass full-context answer accuracy while relying on\nsignificantly less irrelevant information. Experiments on four multi-hop QA\nbenchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG --\ndemonstrates that our approach consistently outperforms strong baselines.",
    "published": "2025-10-16T04:02:29Z",
    "updated": "2025-10-16T04:02:29Z",
    "link": "http://arxiv.org/pdf/2510.14278v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Md Mahadi Hasan Nahid",
      "Davood Rafiei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05764v2",
    "title": "RareAgent: Self-Evolving Reasoning for Drug Repurposing in Rare Diseases",
    "summary": "Computational drug repurposing for rare diseases is especially challenging\nwhen no prior associations exist between drugs and target diseases. Therefore,\nknowledge graph completion and message-passing GNNs have little reliable signal\nto learn and propagate, resulting in poor performance. We present RareAgent, a\nself-evolving multi-agent system that reframes this task from passive pattern\nrecognition to active evidence-seeking reasoning. RareAgent organizes\ntask-specific adversarial debates in which agents dynamically construct\nevidence graphs from diverse perspectives to support, refute, or entail\nhypotheses. The reasoning strategies are analyzed post hoc in a\nself-evolutionary loop, producing textual feedback that refines agent policies,\nwhile successful reasoning paths are distilled into transferable heuristics to\naccelerate future investigations. Comprehensive evaluations reveal that\nRareAgent improves the indication AUPRC by 18.1% over reasoning baselines and\nprovides a transparent reasoning chain consistent with clinical evidence.",
    "published": "2025-10-07T10:35:18Z",
    "updated": "2025-10-16T03:52:44Z",
    "link": "http://arxiv.org/pdf/2510.05764v2.pdf",
    "category": [
      "cs.AI",
      "cs.MA"
    ],
    "authors": [
      "Lang Qin",
      "Zijian Gan",
      "Xu Cao",
      "Pengcheng Jiang",
      "Yankai Jiang",
      "Jiawei Han",
      "Kaishun Wu",
      "Jintai Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14271v1",
    "title": "Less is More: Denoising Knowledge Graphs For Retrieval Augmented\n  Generation",
    "summary": "Retrieval-Augmented Generation (RAG) systems enable large language models\n(LLMs) instant access to relevant information for the generative process,\ndemonstrating their superior performance in addressing common LLM challenges\nsuch as hallucination, factual inaccuracy, and the knowledge cutoff.\nGraph-based RAG further extends this paradigm by incorporating knowledge graphs\n(KGs) to leverage rich, structured connections for more precise and inferential\nresponses. A critical challenge, however, is that most Graph-based RAG systems\nrely on LLMs for automated KG construction, often yielding noisy KGs with\nredundant entities and unreliable relationships. This noise degrades retrieval\nand generation performance while also increasing computational cost. Crucially,\ncurrent research does not comprehensively address the denoising problem for\nLLM-generated KGs. In this paper, we introduce DEnoised knowledge Graphs for\nRetrieval Augmented Generation (DEG-RAG), a framework that addresses these\nchallenges through: (1) entity resolution, which eliminates redundant entities,\nand (2) triple reflection, which removes erroneous relations. Together, these\ntechniques yield more compact, higher-quality KGs that significantly outperform\ntheir unprocessed counterparts. Beyond the methods, we conduct a systematic\nevaluation of entity resolution for LLM-generated KGs, examining different\nblocking strategies, embedding choices, similarity metrics, and entity merging\ntechniques. To the best of our knowledge, this is the first comprehensive\nexploration of entity resolution in LLM-generated KGs. Our experiments\ndemonstrate that this straightforward approach not only drastically reduces\ngraph size but also consistently improves question answering performance across\ndiverse popular Graph-based RAG variants.",
    "published": "2025-10-16T03:41:44Z",
    "updated": "2025-10-16T03:41:44Z",
    "link": "http://arxiv.org/pdf/2510.14271v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yilun Zheng",
      "Dan Yang",
      "Jie Li",
      "Lin Shang",
      "Lihui Chen",
      "Jiahao Xu",
      "Sitao Luan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13709v2",
    "title": "Training LLM Agents to Empower Humans",
    "summary": "Assistive agents should not only take actions on behalf of a human, but also\nstep out of the way and cede control when there are important decisions to be\nmade. However, current methods for building assistive agents, whether via\nmimicking expert humans or via RL finetuning on an inferred reward, often\nencourage agents to complete tasks on their own rather than truly assisting the\nhuman attain their objectives. Additionally, these methods often require costly\nexplicit human feedback to provide a training signal. We propose a new approach\nto tuning assistive language models based on maximizing the human's\nempowerment, their ability to effect desired changes in the environment. Our\nempowerment-maximizing method, Empower, only requires offline text data,\nproviding a self-supervised method for fine-tuning language models to better\nassist humans. To study the efficacy of our approach, we conducted an 18-person\nuser study comparing our empowerment assistant with a strong baseline.\nParticipants preferred our assistant 78% of the time (p=0.015), with a 31%\nhigher acceptance rate and 38% fewer suggestions. Additionally, we introduce a\nnew environment for evaluating multi-turn code assistance using simulated\nhumans. Using this environment, we show that agents trained with Empower\nincrease the success rate of a simulated human programmer on challenging coding\nquestions by an average of 192% over an SFT baseline. With this empowerment\nobjective, we provide a framework for useful aligned AI agents at scale using\nonly offline data without the need for any additional human feedback or\nverifiable rewards.",
    "published": "2025-10-15T16:09:33Z",
    "updated": "2025-10-16T03:39:31Z",
    "link": "http://arxiv.org/pdf/2510.13709v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Evan Ellis",
      "Vivek Myers",
      "Jens Tuyls",
      "Sergey Levine",
      "Anca Dragan",
      "Benjamin Eysenbach"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14265v1",
    "title": "MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning",
    "summary": "With the advancement of powerful large-scale reasoning models, effectively\nevaluating the reasoning capabilities of these models has become increasingly\nimportant. However, existing benchmarks designed to assess the reasoning\nabilities of large models tend to be limited in scope and lack the flexibility\nto adapt their difficulty according to the evolving reasoning capacities of the\nmodels. To address this, we propose MorphoBench, a benchmark that incorporates\nmultidisciplinary questions to evaluate the reasoning capabilities of large\nmodels and can adjust and update question difficulty based on the reasoning\nabilities of advanced models. Specifically, we curate the benchmark by\nselecting and collecting complex reasoning questions from existing benchmarks\nand sources such as Olympiad-level competitions. Additionally, MorphoBench\nadaptively modifies the analytical challenge of questions by leveraging key\nstatements generated during the model's reasoning process. Furthermore, it\nincludes questions generated using simulation software, enabling dynamic\nadjustment of benchmark difficulty with minimal resource consumption. We have\ngathered over 1,300 test questions and iteratively adjusted the difficulty of\nMorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.\nMorphoBench enhances the comprehensiveness and validity of model reasoning\nevaluation, providing reliable guidance for improving both the reasoning\nabilities and scientific robustness of large models. The code has been released\nin https://github.com/OpenDCAI/MorphoBench.",
    "published": "2025-10-16T03:30:56Z",
    "updated": "2025-10-16T03:30:56Z",
    "link": "http://arxiv.org/pdf/2510.14265v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xukai Wang",
      "Xuanbo Liu",
      "Mingrui Chen",
      "Haitian Zhong",
      "Xuanlin Yang",
      "Bohan Zeng",
      "Jinbo Hu",
      "Hao Liang",
      "Junbo Niu",
      "Xuchen Li",
      "Ruitao Wu",
      "Ruichuan An",
      "Yang Shi",
      "Liu Liu",
      "Xu-Yao Zhang",
      "Qiang Liu",
      "Zhouchen Lin",
      "Wentao Zhang",
      "Bin Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14262v1",
    "title": "CAST: Compositional Analysis via Spectral Tracking for Understanding\n  Transformer Layer Functions",
    "summary": "Large language models have achieved remarkable success but remain largely\nblack boxes with poorly understood internal mechanisms. To address this\nlimitation, many researchers have proposed various interpretability methods\nincluding mechanistic analysis, probing classifiers, and activation\nvisualization, each providing valuable insights from different perspectives.\nBuilding upon this rich landscape of complementary approaches, we introduce\nCAST (Compositional Analysis via Spectral Tracking), a probe-free framework\nthat contributes a novel perspective by analyzing transformer layer functions\nthrough direct transformation matrix estimation and comprehensive spectral\nanalysis. CAST offers complementary insights to existing methods by estimating\nthe realized transformation matrices for each layer using Moore-Penrose\npseudoinverse and applying spectral analysis with six interpretable metrics\ncharacterizing layer behavior. Our analysis reveals distinct behaviors between\nencoder-only and decoder-only models, with decoder models exhibiting\ncompression-expansion cycles while encoder models maintain consistent high-rank\nprocessing. Kernel analysis further demonstrates functional relationship\npatterns between layers, with CKA similarity matrices clearly partitioning\nlayers into three phases: feature extraction, compression, and specialization.",
    "published": "2025-10-16T03:27:15Z",
    "updated": "2025-10-16T03:27:15Z",
    "link": "http://arxiv.org/pdf/2510.14262v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zihao Fu",
      "Ming Liao",
      "Chris Russell",
      "Zhenguang G. Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.07524v2",
    "title": "TAI3: Testing Agent Integrity in Interpreting User Intent",
    "summary": "LLM agents are increasingly deployed to automate real-world tasks by invoking\nAPIs through natural language instructions. While powerful, they often suffer\nfrom misinterpretation of user intent, leading to the agent's actions that\ndiverge from the user's intended goal, especially as external toolkits evolve.\nTraditional software testing assumes structured inputs and thus falls short in\nhandling the ambiguity of natural language. We introduce TAI3, an API-centric\nstress testing framework that systematically uncovers intent integrity\nviolations in LLM agents. Unlike prior work focused on fixed benchmarks or\nadversarial inputs, TAI3 generates realistic tasks based on toolkits'\ndocumentation and applies targeted mutations to expose subtle agent errors\nwhile preserving user intent. To guide testing, we propose semantic\npartitioning, which organizes natural language tasks into meaningful categories\nbased on toolkit API parameters and their equivalence classes. Within each\npartition, seed tasks are mutated and ranked by a lightweight predictor that\nestimates the likelihood of triggering agent errors. To enhance efficiency,\nTAI3 maintains a datatype-aware strategy memory that retrieves and adapts\neffective mutation patterns from past cases. Experiments on 80 toolkit APIs\ndemonstrate that TAI3 effectively uncovers intent integrity violations,\nsignificantly outperforming baselines in both error-exposing rate and query\nefficiency. Moreover, TAI3 generalizes well to stronger target models using\nsmaller LLMs for test generation, and adapts to evolving APIs across domains.",
    "published": "2025-06-09T08:09:08Z",
    "updated": "2025-10-16T03:20:27Z",
    "link": "http://arxiv.org/pdf/2506.07524v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Shiwei Feng",
      "Xiangzhe Xu",
      "Xuan Chen",
      "Kaiyuan Zhang",
      "Syed Yusuf Ahmed",
      "Zian Su",
      "Mingwei Zheng",
      "Xiangyu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14253v1",
    "title": "Towards Agentic Self-Learning LLMs in Search Environment",
    "summary": "We study whether self-learning can scale LLM-based agents without relying on\nhuman-curated datasets or predefined rule-based rewards. Through controlled\nexperiments in a search-agent setting, we identify two key determinants of\nscalable agent training: the source of reward signals and the scale of agent\ntask data. We find that rewards from a Generative Reward Model (GRM) outperform\nrigid rule-based signals for open-domain learning, and that co-evolving the GRM\nwith the policy further boosts performance. Increasing the volume of agent task\ndata-even when synthetically generated-substantially enhances agentic\ncapabilities. Building on these insights, we propose \\textbf{Agentic\nSelf-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning\nframework that unifies task generation, policy execution, and evaluation within\na shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,\na Policy Model, and a Generative Reward Model to form a virtuous cycle of\nharder task setting, sharper verification, and stronger solving. Empirically,\nASL delivers steady, round-over-round gains, surpasses strong RLVR baselines\n(e.g., Search-R1) that plateau or degrade, and continues improving under\nzero-labeled-data conditions, indicating superior sample efficiency and\nrobustness. We further show that GRM verification capacity is the main\nbottleneck: if frozen, it induces reward hacking and stalls progress; continual\nGRM training on the evolving data distribution mitigates this, and a small\nlate-stage injection of real verification data raises the performance ceiling.\nThis work establishes reward source and data scale as critical levers for\nopen-domain agent learning and demonstrates the efficacy of multi-role\nco-evolution for scalable, self-improving agents. The data and code of this\npaper are released at\nhttps://github.com/forangel2014/Towards-Agentic-Self-Learning",
    "published": "2025-10-16T03:11:56Z",
    "updated": "2025-10-16T03:11:56Z",
    "link": "http://arxiv.org/pdf/2510.14253v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Wangtao Sun",
      "Xiang Cheng",
      "Jialin Fan",
      "Yao Xu",
      "Xing Yu",
      "Shizhu He",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13921v2",
    "title": "APEX: Empowering LLMs with Physics-Based Task Planning for Real-time\n  Insight",
    "summary": "Large Language Models (LLMs) demonstrate strong reasoning and task planning\ncapabilities but remain fundamentally limited in physical interaction modeling.\nExisting approaches integrate perception via Vision-Language Models (VLMs) or\nadaptive decision-making through Reinforcement Learning (RL), but they fail to\ncapture dynamic object interactions or require task-specific training, limiting\ntheir real-world applicability. We introduce APEX (Anticipatory\nPhysics-Enhanced Execution), a framework that equips LLMs with physics-driven\nforesight for real-time task planning. APEX constructs structured graphs to\nidentify and model the most relevant dynamic interactions in the environment,\nproviding LLMs with explicit physical state updates. Simultaneously, APEX\nprovides low-latency forward simulations of physically feasible actions,\nallowing LLMs to select optimal strategies based on predictive outcomes rather\nthan static observations. We evaluate APEX on three benchmarks designed to\nassess perception, prediction, and decision-making: (1) Physics Reasoning\nBenchmark, testing causal inference and object motion prediction; (2) Tetris,\nevaluating whether physics-informed prediction enhances decision-making\nperformance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,\nassessing the immediate integration of perception and action feasibility\nanalysis. APEX significantly outperforms standard LLMs and VLM-based models,\ndemonstrating the necessity of explicit physics reasoning for bridging the gap\nbetween language-based intelligence and real-world task execution. The source\ncode and experiment setup are publicly available at\nhttps://github.com/hwj20/APEX_EXP .",
    "published": "2025-05-20T04:34:58Z",
    "updated": "2025-10-16T03:07:40Z",
    "link": "http://arxiv.org/pdf/2505.13921v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Wanjing Huang",
      "Weixiang Yan",
      "Zhen Zhang",
      "Ambuj Singh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14249v1",
    "title": "Do Joint Language-Audio Embeddings Encode Perceptual Timbre Semantics?",
    "summary": "Understanding and modeling the relationship between language and sound is\ncritical for applications such as music information retrieval,text-guided music\ngeneration, and audio captioning. Central to these tasks is the use of joint\nlanguage-audio embedding spaces, which map textual descriptions and auditory\ncontent into a shared embedding space. While multimodal embedding models such\nas MS-CLAP, LAION-CLAP, and MuQ-MuLan have shown strong performance in aligning\nlanguage and audio, their correspondence to human perception of timbre, a\nmultifaceted attribute encompassing qualities such as brightness, roughness,\nand warmth, remains underexplored. In this paper, we evaluate the above three\njoint language-audio embedding models on their ability to capture perceptual\ndimensions of timbre. Our findings show that LAION-CLAP consistently provides\nthe most reliable alignment with human-perceived timbre semantics across both\ninstrumental sounds and audio effects.",
    "published": "2025-10-16T03:01:41Z",
    "updated": "2025-10-16T03:01:41Z",
    "link": "http://arxiv.org/pdf/2510.14249v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "authors": [
      "Qixin Deng",
      "Bryan Pardo",
      "Thrasyvoulos N Pappas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25271v2",
    "title": "RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety\n  Evaluation via Role-Specialized Collaboration",
    "summary": "Existing safety evaluation methods for large language models (LLMs) suffer\nfrom inherent limitations, including evaluator bias and detection failures\narising from model homogeneity, which collectively undermine the robustness of\nrisk evaluation processes. This paper seeks to re-examine the risk evaluation\nparadigm by introducing a theoretical framework that reconstructs the\nunderlying risk concept space. Specifically, we decompose the latent risk\nconcept space into three mutually exclusive subspaces: the explicit risk\nsubspace (encompassing direct violations of safety guidelines), the implicit\nrisk subspace (capturing potential malicious content that requires contextual\nreasoning for identification), and the non-risk subspace. Furthermore, we\npropose RADAR, a multi-agent collaborative evaluation framework that leverages\nmulti-round debate mechanisms through four specialized complementary roles and\nemploys dynamic update mechanisms to achieve self-evolution of risk concept\ndistributions. This approach enables comprehensive coverage of both explicit\nand implicit risks while mitigating evaluator bias. To validate the\neffectiveness of our framework, we construct an evaluation dataset comprising\n800 challenging cases. Extensive experiments on our challenging testset and\npublic benchmarks demonstrate that RADAR significantly outperforms baseline\nevaluation methods across multiple dimensions, including accuracy, stability,\nand self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87%\nimprovement in risk identification accuracy compared to the strongest baseline\nevaluation method.",
    "published": "2025-09-28T09:35:32Z",
    "updated": "2025-10-16T03:00:47Z",
    "link": "http://arxiv.org/pdf/2509.25271v2.pdf",
    "category": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Xiuyuan Chen",
      "Jian Zhao",
      "Yuchen Yuan",
      "Tianle Zhang",
      "Huilin Zhou",
      "Zheng Zhu",
      "Ping Hu",
      "Linghe Kong",
      "Chi Zhang",
      "Weiran Huang",
      "Xuelong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14246v1",
    "title": "Policy Regularized Distributionally Robust Markov Decision Processes\n  with Linear Function Approximation",
    "summary": "Decision-making under distribution shift is a central challenge in\nreinforcement learning (RL), where training and deployment environments differ.\nWe study this problem through the lens of robust Markov decision processes\n(RMDPs), which optimize performance against adversarial transition dynamics.\nOur focus is the online setting, where the agent has only limited interaction\nwith the environment, making sample efficiency and exploration especially\ncritical. Policy optimization, despite its success in standard RL, remains\ntheoretically and empirically underexplored in robust RL. To bridge this gap,\nwe propose \\textbf{D}istributionally \\textbf{R}obust \\textbf{R}egularized\n\\textbf{P}olicy \\textbf{O}ptimization algorithm (DR-RPO), a model-free online\npolicy optimization method that learns robust policies with sublinear regret.\nTo enable tractable optimization within the softmax policy class, DR-RPO\nincorporates reference-policy regularization, yielding RMDP variants that are\ndoubly constrained in both transitions and policies. To scale to large\nstate-action spaces, we adopt the $d$-rectangular linear MDP formulation and\ncombine linear function approximation with an upper confidence bonus for\noptimistic exploration. We provide theoretical guarantees showing that policy\noptimization can achieve polynomial suboptimality bounds and sample efficiency\nin robust RL, matching the performance of value-based approaches. Finally,\nempirical results across diverse domains corroborate our theory and demonstrate\nthe robustness of DR-RPO.",
    "published": "2025-10-16T02:56:58Z",
    "updated": "2025-10-16T02:56:58Z",
    "link": "http://arxiv.org/pdf/2510.14246v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Jingwen Gu",
      "Yiting He",
      "Zhishuai Liu",
      "Pan Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17040v2",
    "title": "From Easy to Hard: The MIR Benchmark for Progressive Interleaved\n  Multi-Image Reasoning",
    "summary": "Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language\nModels (MLLMs) ability to jointly comprehend and reason across multiple images\nand their associated textual contexts, introducing unique challenges beyond\nsingle-image or non-interleaved multi-image tasks. While current multi-image\nbenchmarks overlook interleaved textual contexts and neglect distinct\nrelationships between individual images and their associated texts, enabling\nmodels to reason over multi-image interleaved data may significantly enhance\ntheir comprehension of complex scenes and better capture cross-modal\ncorrelations. To bridge this gap, we introduce a novel benchmark MIR, requiring\njoint reasoning over multiple images accompanied by interleaved textual\ncontexts to accurately associate image regions with corresponding texts and\nlogically connect information across images. To enhance MLLMs ability to\ncomprehend multi-image interleaved data, we introduce reasoning steps for each\ninstance within the benchmark and propose a stage-wise curriculum learning\nstrategy. This strategy follows an \"easy to hard\" approach, progressively\nguiding models from simple to complex scenarios, thereby enhancing their\nability to handle challenging tasks. Extensive experiments benchmarking\nmultiple MLLMs demonstrate that our method significantly enhances models\nreasoning performance on MIR and other established benchmarks. We believe that\nMIR will encourage further research into multi-image interleaved reasoning,\nfacilitating advancements in MLLMs capability to handle complex inter-modal\ntasks.",
    "published": "2025-09-21T11:19:02Z",
    "updated": "2025-10-16T02:56:19Z",
    "link": "http://arxiv.org/pdf/2509.17040v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Hang Du",
      "Jiayang Zhang",
      "Guoshun Nan",
      "Wendi Deng",
      "Zhenyan Chen",
      "Chenyang Zhang",
      "Wang Xiao",
      "Shan Huang",
      "Yuqi Pan",
      "Tao Qi",
      "Sicong Leng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14244v1",
    "title": "Reinforcement Learning for Unsupervised Domain Adaptation in\n  Spatio-Temporal Echocardiography Segmentation",
    "summary": "Domain adaptation methods aim to bridge the gap between datasets by enabling\nknowledge transfer across domains, reducing the need for additional expert\nannotations. However, many approaches struggle with reliability in the target\ndomain, an issue particularly critical in medical image segmentation, where\naccuracy and anatomical validity are essential. This challenge is further\nexacerbated in spatio-temporal data, where the lack of temporal consistency can\nsignificantly degrade segmentation quality, and particularly in\nechocardiography, where the presence of artifacts and noise can further hinder\nsegmentation performance. To address these issues, we present RL4Seg3D, an\nunsupervised domain adaptation framework for 2D + time echocardiography\nsegmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to\nenhance key landmark precision in its segmentations while processing full-sized\ninput videos. By leveraging reinforcement learning for image segmentation, our\napproach improves accuracy, anatomical validity, and temporal consistency while\nalso providing, as a beneficial side effect, a robust uncertainty estimator,\nwhich can be used at test time to further enhance segmentation performance. We\ndemonstrate the effectiveness of our framework on over 30,000 echocardiographic\nvideos, showing that it outperforms standard domain adaptation techniques\nwithout the need for any labels on the target domain. Code is available at\nhttps://github.com/arnaudjudge/RL4Seg3D.",
    "published": "2025-10-16T02:55:04Z",
    "updated": "2025-10-16T02:55:04Z",
    "link": "http://arxiv.org/pdf/2510.14244v1.pdf",
    "category": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Arnaud Judge",
      "Nicolas Duchateau",
      "Thierry Judge",
      "Roman A. Sandler",
      "Joseph Z. Sokol",
      "Christian Desrosiers",
      "Olivier Bernard",
      "Pierre-Marc Jodoin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14243v1",
    "title": "Spatial Computing Communications for Multi-User Virtual Reality in\n  Distributed Mobile Edge Computing Network",
    "summary": "Immersive virtual reality (VR) applications impose stringent requirements on\nlatency, energy efficiency, and computational resources, particularly in\nmulti-user interactive scenarios. To address these challenges, we introduce the\nconcept of spatial computing communications (SCC), a framework designed to meet\nthe latency and energy demands of multi-user VR over distributed mobile edge\ncomputing (MEC) networks. SCC jointly represents the physical space, defined by\nusers and base stations, and the virtual space, representing shared immersive\nenvironments, using a probabilistic model of user dynamics and resource\nrequirements. The resource deployment task is then formulated as a\nmulti-objective combinatorial optimization (MOCO) problem that simultaneously\nminimizes system latency and energy consumption across distributed MEC\nresources. To solve this problem, we propose MO-CMPO, a multi-objective\nconsistency model with policy optimization that integrates supervised learning\nand reinforcement learning (RL) fine-tuning guided by preference weights.\nLeveraging a sparse graph neural network (GNN), MO-CMPO efficiently generates\nPareto-optimal solutions. Simulations with real-world New Radio base station\ndatasets demonstrate that MO-CMPO achieves superior hypervolume performance and\nsignificantly lower inference latency than baseline methods. Furthermore, the\nanalysis reveals practical deployment patterns: latency-oriented solutions\nfavor local MEC execution to reduce transmission delay, while energy-oriented\nsolutions minimize redundant placements to save energy.",
    "published": "2025-10-16T02:55:01Z",
    "updated": "2025-10-16T02:55:01Z",
    "link": "http://arxiv.org/pdf/2510.14243v1.pdf",
    "category": [
      "cs.IT",
      "cs.AI",
      "math.IT"
    ],
    "authors": [
      "Caolu Xu",
      "Zhiyong Chen",
      "Meixia Tao",
      "Li Song",
      "Wenjun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14240v1",
    "title": "LiveResearchBench: A Live Benchmark for User-Centric Deep Research in\n  the Wild",
    "summary": "Deep research -- producing comprehensive, citation-grounded reports by\nsearching and synthesizing information from hundreds of live web sources --\nmarks an important frontier for agentic systems. To rigorously evaluate this\nability, four principles are essential: tasks should be (1) user-centric,\nreflecting realistic information needs, (2) dynamic, requiring up-to-date\ninformation beyond parametric knowledge, (3) unambiguous, ensuring consistent\ninterpretation across users, and (4) multi-faceted and search-intensive,\nrequiring search over numerous web sources and in-depth analysis. Existing\nbenchmarks fall short of these principles, often focusing on narrow domains or\nposing ambiguous questions that hinder fair comparison. Guided by these\nprinciples, we introduce LiveResearchBench, a benchmark of 100 expert-curated\ntasks spanning daily life, enterprise, and academia, each requiring extensive,\ndynamic, real-time web search and synthesis. Built with over 1,500 hours of\nhuman labor, LiveResearchBench provides a rigorous basis for systematic\nevaluation. To evaluate citation-grounded long-form reports, we introduce\nDeepEval, a comprehensive suite covering both content- and report-level\nquality, including coverage, presentation, citation accuracy and association,\nconsistency and depth of analysis. DeepEval integrates four complementary\nevaluation protocols, each designed to ensure stable assessment and high\nagreement with human judgments. Using LiveResearchBench and DeepEval, we\nconduct a comprehensive evaluation of 17 frontier deep research systems,\nincluding single-agent web search, single-agent deep research, and multi-agent\nsystems. Our analysis reveals current strengths, recurring failure modes, and\nkey system components needed to advance reliable, insightful deep research.",
    "published": "2025-10-16T02:49:16Z",
    "updated": "2025-10-16T02:49:16Z",
    "link": "http://arxiv.org/pdf/2510.14240v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Jiayu Wang",
      "Yifei Ming",
      "Riya Dulepet",
      "Qinglin Chen",
      "Austin Xu",
      "Zixuan Ke",
      "Frederic Sala",
      "Aws Albarghouthi",
      "Caiming Xiong",
      "Shafiq Joty"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.13883v4",
    "title": "Domain-Independent Dynamic Programming",
    "summary": "For combinatorial optimization problems, model-based paradigms such as\nmixed-integer programming (MIP) and constraint programming (CP) aim to decouple\nmodeling and solving a problem: the `holy grail' of declarative problem\nsolving. We propose domain-independent dynamic programming (DIDP), a novel\nmodel-based paradigm based on dynamic programming (DP). While DP is not new, it\nhas typically been implemented as a problem-specific method. We introduce\nDynamic Programming Description Language (DyPDL), a formalism to define DP\nmodels based on a state transition system, inspired by artificial intelligence\n(AI) planning. we show that heuristic search algorithms can be used to solve\nDyPDL models and propose seven DIDP solvers. We experimentally compare our DIDP\nsolvers with commercial MIP and CP solvers (solving MIP and CP models,\nrespectively) on common benchmark instances of eleven combinatorial\noptimization problem classes. We show that DIDP outperforms MIP in nine problem\nclasses, CP also in nine problem classes, and both MIP and CP in seven. DIDP\nalso achieves superior performance to existing state-based solvers including\ndomain-independent AI planners.",
    "published": "2024-01-25T01:48:09Z",
    "updated": "2025-10-16T02:40:01Z",
    "link": "http://arxiv.org/pdf/2401.13883v4.pdf",
    "category": [
      "cs.AI",
      "F.2.2; I.2.8"
    ],
    "authors": [
      "Ryo Kuroiwa",
      "J. Christopher Beck"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.09761v2",
    "title": "VERITAS: Verifying the Performance of AI-native Transceiver Actions in\n  Base-Stations",
    "summary": "Artificial Intelligence (AI)-native receivers prove significant performance\nimprovement in high noise regimes and can potentially reduce communication\noverhead compared to the traditional receiver. However, their performance\nhighly depends on the representativeness of the training dataset. A major issue\nis the uncertainty of whether the training dataset covers all test environments\nand waveform configurations, and thus, whether the trained model is robust in\npractical deployment conditions. To this end, we propose a joint\nmeasurement-recovery framework for AI-native transceivers post deployment,\ncalled VERITAS, that continuously looks for distribution shifts in the received\nsignals and triggers finite re-training spurts. VERITAS monitors the wireless\nchannel using 5G pilots fed to an auxiliary neural network that detects\nout-of-distribution channel profile, transmitter speed, and delay spread. As\nsoon as such a change is detected, a traditional (reference) receiver is\nactivated, which runs for a period of time in parallel to the AI-native\nreceiver. Finally, VERTIAS compares the bit probabilities of the AI-native and\nthe reference receivers for the same received data inputs, and decides whether\nor not a retraining process needs to be initiated. Our evaluations reveal that\nVERITAS can detect changes in the channel profile, transmitter speed, and delay\nspread with 99%, 97%, and 69% accuracies, respectively, followed by timely\ninitiation of retraining for 86%, 93.3%, and 94.8% of inputs in channel\nprofile, transmitter speed, and delay spread test sets, respectively.",
    "published": "2025-01-01T19:12:03Z",
    "updated": "2025-10-16T02:39:38Z",
    "link": "http://arxiv.org/pdf/2501.09761v2.pdf",
    "category": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Nasim Soltani",
      "Michael Loehning",
      "Kaushik Chowdhury"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.00527v2",
    "title": "Never too Prim to Swim: An LLM-Enhanced RL-based Adaptive S-Surface\n  Controller for AUVs under Extreme Sea Conditions",
    "summary": "The adaptivity and maneuvering capabilities of Autonomous Underwater Vehicles\n(AUVs) have drawn significant attention in oceanic research, due to the\nunpredictable disturbances and strong coupling among the AUV's degrees of\nfreedom. In this paper, we developed large language model (LLM)-enhanced\nreinforcement learning (RL)-based adaptive S-surface controller for AUVs.\nSpecifically, LLMs are introduced for the joint optimization of controller\nparameters and reward functions in RL training. Using multi-modal and\nstructured explicit task feedback, LLMs enable joint adjustments, balance\nmultiple objectives, and enhance task-oriented performance and adaptability. In\nthe proposed controller, the RL policy focuses on upper-level tasks, outputting\ntask-oriented high-level commands that the S-surface controller then converts\ninto control signals, ensuring cancellation of nonlinear effects and\nunpredictable external disturbances in extreme sea conditions. Under extreme\nsea conditions involving complex terrain, waves, and currents, the proposed\ncontroller demonstrates superior performance and adaptability in high-level\ntasks such as underwater target tracking and data collection, outperforming\ntraditional PID and SMC controllers.",
    "published": "2025-03-01T15:01:50Z",
    "updated": "2025-10-16T02:33:20Z",
    "link": "http://arxiv.org/pdf/2503.00527v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Guanwen Xie",
      "Jingzehua Xu",
      "Yimian Ding",
      "Zhi Zhang",
      "Shuai Zhang",
      "Yi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.07994v2",
    "title": "A Neural Symbolic Model for Space Physics",
    "summary": "In this study, we unveil a new AI model, termed PhyE2E, to discover physical\nformulas through symbolic regression. PhyE2E simplifies symbolic regression by\ndecomposing it into sub-problems using the second-order derivatives of an\noracle neural network, and employs a transformer model to translate data into\nsymbolic formulas in an end-to-end manner. The resulting formulas are refined\nthrough Monte-Carlo Tree Search and Genetic Programming. We leverage a large\nlanguage model to synthesize extensive symbolic expressions resembling real\nphysics, and train the model to recover these formulas directly from data. A\ncomprehensive evaluation reveals that PhyE2E outperforms existing\nstate-of-the-art approaches, delivering superior symbolic accuracy, precision\nin data fitting, and consistency in physical units. We deployed PhyE2E to five\napplications in space physics, including the prediction of sunspot numbers,\nsolar rotational angular velocity, emission line contribution functions,\nnear-Earth plasma pressure, and lunar-tide plasma signals. The physical\nformulas generated by AI demonstrate a high degree of accuracy in fitting the\nexperimental data from satellites and astronomical telescopes. We have\nsuccessfully upgraded the formula proposed by NASA in 1993 regarding solar\nactivity, and for the first time, provided the explanations for the long cycle\nof solar activity in an explicit form. We also found that the decay of\nnear-Earth plasma pressure is proportional to r^2 to Earth, where subsequent\nmathematical derivations are consistent with satellite data from another\nindependent study. Moreover, we found physical formulas that can describe the\nrelationships between emission lines in the extreme ultraviolet spectrum of the\nSun, temperatures, electron densities, and magnetic fields. The formula\nobtained is consistent with the properties that physicists had previously\nhypothesized it should possess.",
    "published": "2025-03-11T02:50:45Z",
    "updated": "2025-10-16T02:20:02Z",
    "link": "http://arxiv.org/pdf/2503.07994v2.pdf",
    "category": [
      "astro-ph.SR",
      "astro-ph.EP",
      "astro-ph.IM",
      "cs.AI",
      "physics.space-ph"
    ],
    "authors": [
      "Jie Ying",
      "Haowei Lin",
      "Chao Yue",
      "Yajie Chen",
      "Chao Xiao",
      "Quanqi Shi",
      "Yitao Liang",
      "Shing-Tung Yau",
      "Yuan Zhou",
      "Jianzhu Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14232v1",
    "title": "Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight\n  Models",
    "summary": "Competitive programming has become a rigorous benchmark for evaluating the\nreasoning and problem-solving capabilities of large language models (LLMs). The\nInternational Olympiad in Informatics (IOI) stands out as one of the most\nprestigious annual competitions in competitive programming and has become a key\nbenchmark for comparing human and AI-level programming ability. While several\nproprietary models have been claimed to achieve gold medal-level performance at\nthe IOI, often with undisclosed methods, achieving comparable results with\nopen-weight models remains a significant challenge. In this paper, we present\n\\gencluster, a scalable and reproducible test-time compute framework that\nattains IOI gold-level performance using open-weight models. It combines\nlarge-scale generation, behavioral clustering, ranking, and a round-robin\nsubmission strategy to efficiently explore diverse solution spaces under\nlimited validation budgets. Our experiments show that the performance of our\nproposed approach scales consistently with available compute, narrowing the gap\nbetween open and closed systems. Notably, we will show that GenCluster can\nachieve a gold medal at IOI 2025 for the first time with an open-weight model\ngpt-oss-120b, setting a new benchmark for transparent and reproducible\nevaluation of reasoning in LLMs.",
    "published": "2025-10-16T02:19:25Z",
    "updated": "2025-10-16T02:19:25Z",
    "link": "http://arxiv.org/pdf/2510.14232v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Mehrzad Samadi",
      "Aleksander Ficek",
      "Sean Narenthiran",
      "Siddhartha Jain",
      "Wasi Uddin Ahmad",
      "Somshubra Majumdar",
      "Vahid Noroozi",
      "Boris Ginsburg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14223v1",
    "title": "Large Scale Retrieval for the LinkedIn Feed using Causal Language Models",
    "summary": "In large scale recommendation systems like the LinkedIn Feed, the retrieval\nstage is critical for narrowing hundreds of millions of potential candidates to\na manageable subset for ranking. LinkedIn's Feed serves suggested content from\noutside of the member's network (based on the member's topical interests),\nwhere 2000 candidates are retrieved from a pool of hundreds of millions\ncandidate with a latency budget of a few milliseconds and inbound QPS of\nseveral thousand per second. This paper presents a novel retrieval approach\nthat fine-tunes a large causal language model (Meta's LLaMA 3) as a dual\nencoder to generate high quality embeddings for both users (members) and\ncontent (items), using only textual input. We describe the end to end pipeline,\nincluding prompt design for embedding generation, techniques for fine-tuning at\nLinkedIn's scale, and infrastructure for low latency, cost effective online\nserving. We share our findings on how quantizing numerical features in the\nprompt enables the information to get properly encoded in the embedding,\nfacilitating greater alignment between the retrieval and ranking layer. The\nsystem was evaluated using offline metrics and an online A/B test, which showed\nsubstantial improvements in member engagement. We observed significant gains\namong newer members, who often lack strong network connections, indicating that\nhigh-quality suggested content aids retention. This work demonstrates how\ngenerative language models can be effectively adapted for real time, high\nthroughput retrieval in industrial applications.",
    "published": "2025-10-16T02:01:33Z",
    "updated": "2025-10-16T02:01:33Z",
    "link": "http://arxiv.org/pdf/2510.14223v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Sudarshan Srinivasa Ramanujam",
      "Antonio Alonso",
      "Saurabh Kataria",
      "Siddharth Dangi",
      "Akhilesh Gupta",
      "Birjodh Singh Tiwana",
      "Manas Somaiya",
      "Luke Simon",
      "David Byrne",
      "Sojeong Ha",
      "Sen Zhou",
      "Andrei Akterskii",
      "Zhanglong Liu",
      "Samira Sriram",
      "Crescent Xiong",
      "Zhoutao Pei",
      "Angela Shao",
      "Alex Li",
      "Annie Xiao",
      "Caitlin Kolb",
      "Thomas Kistler",
      "Zach Moore",
      "Hamed Firooz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.01655v4",
    "title": "Quantum Polar Metric Learning: Efficient Classically Learned Quantum\n  Embeddings",
    "summary": "Deep metric learning has recently shown extremely promising results in the\nclassical data domain, creating well-separated feature spaces. This idea was\nalso adapted to quantum computers via Quantum Metric Learning(QMeL). QMeL\nconsists of a 2 step process with a classical model to compress the data to fit\ninto the limited number of qubits, then train a Parameterized Quantum\nCircuit(PQC) to create better separation in Hilbert Space. However, on Noisy\nIntermediate Scale Quantum (NISQ) devices. QMeL solutions result in high\ncircuit width and depth, both of which limit scalability. We propose Quantum\nPolar Metric Learning (QPMeL) that uses a classical model to learn the\nparameters of the polar form of a qubit. We then utilize a shallow PQC with\n$R_y$ and $R_z$ gates to create the state and a trainable layer of\n$ZZ(\\theta)$-gates to learn entanglement. The circuit also computes fidelity\nvia a SWAP Test for our proposed Fidelity Triplet Loss function, used to train\nboth classical and quantum components. When compared to QMeL approaches, QPMeL\nachieves 3X better multi-class separation, while using only 1/2 the number of\ngates and depth. We also demonstrate that QPMeL outperforms classical networks\nwith similar configurations, presenting a promising avenue for future research\non fully classical models with quantum loss functions.",
    "published": "2023-12-04T06:13:53Z",
    "updated": "2025-10-16T02:00:22Z",
    "link": "http://arxiv.org/pdf/2312.01655v4.pdf",
    "category": [
      "quant-ph",
      "cs.AI",
      "I.2.6; E.4"
    ],
    "authors": [
      "Vinayak Sharma",
      "Ashish Padhy",
      "Sourav Behera",
      "Lord Sen",
      "Shyamapada Mukherjee",
      "Aviral Shrivastava"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.03440v4",
    "title": "LLMs are Single-threaded Reasoners: Demystifying the Working Mechanism\n  of Soft Thinking",
    "summary": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nIn this paper, we investigate the Soft Thinking capabilities of various LLMs\nthrough a systematic analysis of their internal behavior using a suite of\nprobing techniques. Contrary to the prevailing belief that Soft Thinking\nsupports parallel exploration of diverse reasoning paths, our findings reveal\nthat LLMs behave as single-threaded reasoners--they predominantly rely on the\ntoken with the highest probability in the soft input to predict the next step.\nThis behavior induces a greedy feedback loop that suppresses alternative\nreasoning paths and undermines the benefits of transmitting richer information\nvia Soft Tokens. To address this Greedy Pitfall, we propose Stochastic Soft\nThinking, which introduces stochasticity to break free from this Greedy\nPitfall. Our experiments demonstrate that incorporating\nrandomness--particularly with the Gumbel-Softmax trick--can alleviate the\nlimitations of vanilla approaches and unleash the potential of Soft Thinking,\nresulting in superior performance across eight reasoning benchmarks. We further\ndemonstrate that Stochastic Soft Thinking exhibits stronger exploration\npotential compared to conventional COT. Our findings deepen the understanding\nof continuous reasoning and establish the foundation for future work on\nimproving Soft Thinking with Reinforcement Learning.",
    "published": "2025-08-05T13:38:33Z",
    "updated": "2025-10-16T01:59:24Z",
    "link": "http://arxiv.org/pdf/2508.03440v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Junhong Wu",
      "Jinliang Lu",
      "Zixuan Ren",
      "Gangqiang Hu",
      "Zhi Wu",
      "Dai Dai",
      "Hua Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19905v2",
    "title": "EMAC+: Embodied Multimodal Agent for Collaborative Planning with VLM+LLM",
    "summary": "Although LLMs demonstrate proficiency in several text-based reasoning and\nplanning tasks, their implementation in robotics control is constrained by\nsignificant deficiencies: (1) LLM agents are designed to work mainly with\ntextual inputs rather than visual conditions; (2) Current multimodal agents\ntreat LLMs as static planners, which separates their reasoning from environment\ndynamics, resulting in actions that do not take domain-specific knowledge into\naccount; and (3) LLMs are not designed to learn from visual interactions, which\nmakes it harder for them to make better policies for specific domains. In this\npaper, we introduce EMAC+, an Embodied Multimodal Agent that collaboratively\nintegrates LLM and VLM via a bidirectional training paradigm. Unlike existing\nmethods, EMAC+ dynamically refines high-level textual plans generated by an LLM\nusing real-time feedback from a VLM executing low-level visual control tasks.\nWe address critical limitations of previous models by enabling the LLM to\ninternalize visual environment dynamics directly through interactive\nexperience, rather than relying solely on static symbolic mappings. Extensive\nexperimental evaluations on ALFWorld and RT-1 benchmarks demonstrate that EMAC+\nachieves superior task performance, robustness against noisy observations, and\nefficient learning. We also conduct thorough ablation studies and provide\ndetailed analyses of success and failure cases.",
    "published": "2025-05-26T12:34:16Z",
    "updated": "2025-10-16T01:38:12Z",
    "link": "http://arxiv.org/pdf/2505.19905v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Shuang Ao",
      "Flora D. Salim",
      "Simon Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14211v1",
    "title": "LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning",
    "summary": "Multi-stage reasoning has emerged as an effective strategy for enhancing the\nreasoning capability of small language models by decomposing complex problems\ninto sequential sub-stages. However, this comes at the cost of increased\nlatency. We observe that existing adaptive acceleration techniques, such as\nlayer skipping, struggle to balance efficiency and accuracy in this setting due\nto two key challenges: (1) stage-wise variation in skip sensitivity, and (2)\nthe generation of redundant output tokens. To address these, we propose\nLiteStage, a latency-aware layer skipping framework for multi-stage reasoning.\nLiteStage combines a stage-wise offline search that allocates optimal layer\nbudgets with an online confidence-based generation early exit to suppress\nunnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and\nStrategyQA, show that LiteStage achieves up to 1.70x speedup with less than\n4.0% accuracy loss, outperforming prior training-free layer skipping methods.",
    "published": "2025-10-16T01:37:39Z",
    "updated": "2025-10-16T01:37:39Z",
    "link": "http://arxiv.org/pdf/2510.14211v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Beomseok Kang",
      "Jiwon Song",
      "Jae-Joon Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.17439v4",
    "title": "AI-generated Essays: Characteristics and Implications on Automated\n  Scoring and Academic Integrity",
    "summary": "The rapid advancement of large language models (LLMs) has enabled the\ngeneration of coherent essays, making AI-assisted writing increasingly common\nin educational and professional settings. Using large-scale empirical data, we\nexamine and benchmark the characteristics and quality of essays generated by\npopular LLMs and discuss their implications for two key components of writing\nassessments: automated scoring and academic integrity. Our findings highlight\nlimitations in existing automated scoring systems, such as e-rater, when\napplied to essays generated or heavily influenced by AI, and identify areas for\nimprovement, including the development of new features to capture deeper\nthinking and recalibrating feature weights. Despite growing concerns that the\nincreasing variety of LLMs may undermine the feasibility of detecting\nAI-generated essays, our results show that detectors trained on essays\ngenerated from one model can often identify texts from others with high\naccuracy, suggesting that effective detection could remain manageable in\npractice.",
    "published": "2024-10-22T21:30:58Z",
    "updated": "2025-10-16T01:36:57Z",
    "link": "http://arxiv.org/pdf/2410.17439v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yang Zhong",
      "Jiangang Hao",
      "Michael Fauss",
      "Chen Li",
      "Yuan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14207v1",
    "title": "Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn\n  Online Harassment Attacks",
    "summary": "Large Language Model (LLM) agents are powering a growing share of interactive\nweb applications, yet remain vulnerable to misuse and harm. Prior jailbreak\nresearch has largely focused on single-turn prompts, whereas real harassment\noften unfolds over multi-turn interactions. In this work, we present the Online\nHarassment Agentic Benchmark consisting of: (i) a synthetic multi-turn\nharassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)\nsimulation informed by repeated game theory, (iii) three jailbreak methods\nattacking agents across memory, planning, and fine-tuning, and (iv) a\nmixed-methods evaluation framework. We utilize two prominent LLMs,\nLLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our\nresults show that jailbreak tuning makes harassment nearly guaranteed with an\nattack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,\nand 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal\nrate to 1-2% in both models. The most prevalent toxic behaviors are Insult with\n84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.\n31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive\ncategories such as sexual or racial harassment. Qualitative evaluation further\nreveals that attacked agents reproduce human-like aggression profiles, such as\nMachiavellian/psychopathic patterns under planning, and narcissistic tendencies\nwith memory. Counterintuitively, closed-source and open-source models exhibit\ndistinct escalation trajectories across turns, with closed-source models\nshowing significant vulnerability. Overall, our findings show that multi-turn\nand theory-grounded attacks not only succeed at high rates but also mimic\nhuman-like harassment dynamics, motivating the development of robust safety\nguardrails to ultimately keep online platforms safe and responsible.",
    "published": "2025-10-16T01:27:44Z",
    "updated": "2025-10-16T01:27:44Z",
    "link": "http://arxiv.org/pdf/2510.14207v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Trilok Padhi",
      "Pinxian Lu",
      "Abdulkadir Erol",
      "Tanmay Sutar",
      "Gauri Sharma",
      "Mina Sonmez",
      "Munmun De Choudhury",
      "Ugur Kursuncu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14205v1",
    "title": "DPRF: A Generalizable Dynamic Persona Refinement Framework for\n  Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents\n  and Humans",
    "summary": "The emerging large language model role-playing agents (LLM RPAs) aim to\nsimulate individual human behaviors, but the persona fidelity is often\nundermined by manually-created profiles (e.g., cherry-picked information and\npersonality characteristics) without validating the alignment with the target\nindividuals. To address this limitation, our work introduces the Dynamic\nPersona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM\nRPAs' behaviors with those of target individuals by iteratively identifying the\ncognitive divergence, either through free-form or theory-grounded, structured\nanalysis, between generated behaviors and human ground truth, and refining the\npersona profile to mitigate these divergences.We evaluate DPRF with five LLMs\non four diverse behavior-prediction scenarios: formal debates, social media\nposts with mental health issues, public interviews, and movie reviews.DPRF can\nconsistently improve behavioral alignment considerably over baseline personas\nand generalizes across models and scenarios.Our work provides a robust\nmethodology for creating high-fidelity persona profiles and enhancing the\nvalidity of downstream applications, such as user simulation, social studies,\nand personalized AI.",
    "published": "2025-10-16T01:26:38Z",
    "updated": "2025-10-16T01:26:38Z",
    "link": "http://arxiv.org/pdf/2510.14205v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Bingsheng Yao",
      "Bo Sun",
      "Yuanzhe Dong",
      "Yuxuan Lu",
      "Dakuo Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.13399v2",
    "title": "EdiVal-Agent: An Object-Centric Framework for Automated, Fine-Grained\n  Evaluation of Multi-Turn Editing",
    "summary": "Instruction-based image editing has advanced rapidly, yet reliable and\ninterpretable evaluation remains a bottleneck. Current protocols either (i)\ndepend on paired reference images-resulting in limited coverage and inheriting\nbiases from prior generative models-or (ii) rely solely on zero-shot\nvision-language models (VLMs), whose prompt-based assessments of instruction\nfollowing, content consistency, and visual quality are often imprecise. To\naddress this, we introduce EdiVal-Agent, an automated and fine-grained\nevaluation framework grounded in an object-centric perspective, designed to\nassess not only standard single-turn but also multi-turn instruction-based\nediting with precision. Given an input image, EdiVal-Agent first decomposes it\ninto semantically meaningful objects, then synthesizes diverse, context-aware\nediting instructions while dynamically updating object pools across turns.\nThese two stages enable two novel object-centric metrics tailored for\nmulti-turn evaluation and one global metric of visual quality: (1) EdiVal-IF,\nwhich measures instruction following by combining open-vocabulary object\ndetectors for symbolic checks with VLMs for semantic verification on\ndetector-guided crops; (2) EdiVal-CC, which evaluates content consistency by\ncalculating semantic similarity of unchanged objects and background using the\nevolving object pools; and (3) EdiVal-VQ, which quantifies changes in overall\nvisual quality with human preference models. Instantiating this pipeline, we\nbuild EdiVal-Bench, a multi-turn editing benchmark covering 9 instruction types\nand 13 state-of-the-art editing models spanning in-context, flow-matching, and\ndiffusion paradigms. We demonstrate that EdiVal-Agent can be used to identify\nexisting failure modes, thereby informing the development of the next\ngeneration of editing models.",
    "published": "2025-09-16T17:45:39Z",
    "updated": "2025-10-16T01:09:09Z",
    "link": "http://arxiv.org/pdf/2509.13399v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Tianyu Chen",
      "Yasi Zhang",
      "Zhi Zhang",
      "Peiyu Yu",
      "Shu Wang",
      "Zhendong Wang",
      "Kevin Lin",
      "Xiaofei Wang",
      "Zhengyuan Yang",
      "Linjie Li",
      "Chung-Ching Lin",
      "Jianwen Xie",
      "Oscar Leong",
      "Lijuan Wang",
      "Ying Nian Wu",
      "Mingyuan Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13721v2",
    "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete\n  Flow Matching",
    "summary": "Next-generation multimodal foundation models capable of any-to-any\ncross-modal generation and multi-turn interaction will serve as core components\nof artificial general intelligence systems, playing a pivotal role in\nhuman-machine interaction. However, most existing multimodal models remain\nconstrained by autoregressive architectures, whose inherent limitations prevent\na balanced integration of understanding and generation capabilities. Although\nhybrid and decoupling strategies have been explored to address these tasks\nwithin unified frameworks separately, their redundant, non-integrated designs\nlimit their applicability to broader scenarios, such as cross-modal retrieval.\nIn this work, we introduce NExT-OMNI, an open-source omnimodal foundation model\nthat achieves unified modeling through discrete flow paradigms. By leveraging\nmetric-induced probability paths and kinetic optimal velocities, NExT-OMNI\nnatively supports any-to-any understanding and generation with enhanced\nresponse efficiency, while enabling broader application scenarios through\nconcise unified representations rather than task-decoupled designs. Trained on\nlarge-scale interleaved text, image, video, and audio data, NExT-OMNI delivers\ncompetitive performance on multimodal generation and understanding benchmarks,\nwhile outperforming prior unified models in multi-turn multimodal interaction\nand cross-modal retrieval, highlighting its architectural advantages as a\nnext-generation multimodal foundation model. To advance further research, we\nrelease training details, data protocols, and open-source both the code and\nmodel checkpoints.",
    "published": "2025-10-15T16:25:18Z",
    "updated": "2025-10-16T01:08:45Z",
    "link": "http://arxiv.org/pdf/2510.13721v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Run Luo",
      "Xiaobo Xia",
      "Lu Wang",
      "Longze Chen",
      "Renke Shan",
      "Jing Luo",
      "Min Yang",
      "Tat-Seng Chua"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14194v1",
    "title": "Implementation of AI in Precision Medicine",
    "summary": "Artificial intelligence (AI) has become increasingly central to precision\nmedicine by enabling the integration and interpretation of multimodal data, yet\nimplementation in clinical settings remains limited. This paper provides a\nscoping review of literature from 2019-2024 on the implementation of AI in\nprecision medicine, identifying key barriers and enablers across data quality,\nclinical reliability, workflow integration, and governance. Through an\necosystem-based framework, we highlight the interdependent relationships\nshaping real-world translation and propose future directions to support\ntrustworthy and sustainable implementation.",
    "published": "2025-10-16T00:55:15Z",
    "updated": "2025-10-16T00:55:15Z",
    "link": "http://arxiv.org/pdf/2510.14194v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Gktu Bender",
      "Samer Faraj",
      "Anand Bhardwaj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07517v2",
    "title": "Measuring and Mitigating Identity Bias in Multi-Agent Debate via\n  Anonymization",
    "summary": "Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning\nby letting multiple agents exchange answers and then aggregate their opinions.\nYet recent studies reveal that agents are not neutral: they are prone to\nidentity-driven sycophancy and self-bias, uncritically adopting a peer's view\nor stubbornly adhering to their own prior output, undermining the reliability\nof debate. In this work, we present the first principled framework that joins\nsycophancy and self-bias to mitigate and quantify identity bias in MAD. First,\nwe formalize the debate dynamics as an identity-weighted Bayesian update\nprocess. Second, we propose response anonymization: by removing identity\nmarkers from prompts, agents cannot distinguish \"self\" from \"peer\", which\nforces equal weights on agent identity, thereby reducing bias. Third, we define\nthe Identity Bias Coefficient (IBC), a principled metric that measures how\noften an agent follows a peer versus itself. Empirical studies across multiple\nmodels, datasets and debate rounds confirm that identity bias is widespread,\nwith sycophancy far more common than self-bias. Our findings highlight the need\nto \"mask\" identity to ensure that MAD systems reason based on content rather\nthan source identity. Code is released in\nhttps://github.com/deeplearning-wisc/MAD-identity-bias.",
    "published": "2025-10-08T20:29:46Z",
    "updated": "2025-10-16T00:43:28Z",
    "link": "http://arxiv.org/pdf/2510.07517v2.pdf",
    "category": [
      "cs.AI",
      "cs.MA"
    ],
    "authors": [
      "Hyeong Kyu Choi",
      "Xiaojin Zhu",
      "Sharon Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12712v2",
    "title": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image\n  Perception, Transformation, and Reasoning",
    "summary": "Multimodal Large Language Models (MLLMs) are increasingly applied in\nreal-world scenarios where user-provided images are often imperfect, requiring\nactive image manipulations such as cropping, editing, or enhancement to uncover\nsalient visual cues. Beyond static visual perception, MLLMs must also think\nwith images: dynamically transforming visual content and integrating it with\nother tools to solve complex tasks. However, this shift from treating vision as\npassive context to a manipulable cognitive workspace remains underexplored.\nMost existing benchmarks still follow a think about images paradigm, where\nimages are regarded as static inputs. To address this gap, we introduce\nVisualToolBench, a visual tool-use reasoning benchmark that rigorously\nevaluates MLLMs' ability to perceive, transform, and reason across complex\nvisual-textual tasks under the think-with-images paradigm. VisualToolBench\ncomprises 1,204 challenging, open-ended vision tasks (603 single-turn, 601\nmulti-turn) spanning across five diverse domains, each paired with detailed\nrubrics to enable systematic evaluation. Our evaluation shows that current\nMLLMs struggle with tasks requiring effective integration of vision and\ngeneral-purpose tools. Even the strongest model (GPT-5-think) reaches only\n18.68% pass rate. We further observe divergent tool-use behaviors, with OpenAI\nmodels benefiting from diverse image manipulations while Gemini-2.5-pro shows\nno improvement. By introducing the first benchmark centered on think with\nimages, VisualToolBench offers critical insights for advancing visual\nintelligence in MLLMs.",
    "published": "2025-10-14T16:50:49Z",
    "updated": "2025-10-16T00:41:28Z",
    "link": "http://arxiv.org/pdf/2510.12712v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xingang Guo",
      "Utkarsh Tyagi",
      "Advait Gosai",
      "Paula Vergara",
      "Ernesto Gabriel Hernndez Montoya",
      "Chen Bo Calvin Zhang",
      "Bin Hu",
      "Yunzhong He",
      "Bing Liu",
      "Rakshith Sharma Srinivasa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14184v1",
    "title": "MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with\n  Configurable Task Adaptation",
    "summary": "We present MAFA (Multi-Agent Framework for Annotation), a production-deployed\nsystem that transforms enterprise-scale annotation workflows through\nconfigurable multi-agent collaboration. Addressing the critical challenge of\nannotation backlogs in financial services, where millions of customer\nutterances require accurate categorization, MAFA combines specialized agents\nwith structured reasoning and a judge-based consensus mechanism. Our framework\nuniquely supports dynamic task adaptation, allowing organizations to define\ncustom annotation types (FAQs, intents, entities, or domain-specific\ncategories) through configuration rather than code changes. Deployed at JP\nMorgan Chase, MAFA has eliminated a 1 million utterance backlog while\nachieving, on average, 86% agreement with human annotators, annually saving\nover 5,000 hours of manual annotation work. The system processes utterances\nwith annotation confidence classifications, which are typically 85% high, 10%\nmedium, and 5% low across all datasets we tested. This enables human annotators\nto focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's\neffectiveness across multiple datasets and languages, showing consistent\nimprovements over traditional and single-agent annotation baselines: 13.8%\nhigher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1\nin our internal intent classification dataset and similar gains on public\nbenchmarks. This work bridges the gap between theoretical multi-agent systems\nand practical enterprise deployment, providing a blueprint for organizations\nfacing similar annotation challenges.",
    "published": "2025-10-16T00:30:08Z",
    "updated": "2025-10-16T00:30:08Z",
    "link": "http://arxiv.org/pdf/2510.14184v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Mahmood Hegazy",
      "Aaron Rodrigues",
      "Azzam Naeem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.01997v3",
    "title": "Restoring Calibration for Aligned Large Language Models: A\n  Calibration-Aware Fine-Tuning Approach",
    "summary": "One of the key technologies for the success of Large Language Models (LLMs)\nis preference alignment. However, a notable side effect of preference alignment\nis poor calibration: while the pre-trained models are typically\nwell-calibrated, LLMs tend to become poorly calibrated after alignment with\nhuman preferences. In this paper, we investigate why preference alignment\naffects calibration and how to address this issue. For the first question, we\nobserve that the preference collapse issue in alignment undesirably generalizes\nto the calibration scenario, causing LLMs to exhibit overconfidence and poor\ncalibration. To address this, we demonstrate the importance of fine-tuning with\ndomain-specific knowledge to alleviate the overconfidence issue. To further\nanalyze whether this affects the model's performance, we categorize models into\ntwo regimes: calibratable and non-calibratable, defined by bounds of Expected\nCalibration Error (ECE). In the calibratable regime, we propose a\ncalibration-aware fine-tuning approach to achieve proper calibration without\ncompromising LLMs' performance. However, as models are further fine-tuned for\nbetter performance, they enter the non-calibratable regime. For this case, we\ndevelop an EM-algorithm-based ECE regularization for the fine-tuning loss to\nmaintain low calibration error. Extensive experiments validate the\neffectiveness of the proposed methods.",
    "published": "2025-05-04T05:42:51Z",
    "updated": "2025-10-16T00:26:43Z",
    "link": "http://arxiv.org/pdf/2505.01997v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Jiancong Xiao",
      "Bojian Hou",
      "Zhanliang Wang",
      "Ruochen Jin",
      "Qi Long",
      "Weijie J. Su",
      "Li Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14179v1",
    "title": "Virtually Being: Customizing Camera-Controllable Video Diffusion Models\n  with Multi-View Performance Captures",
    "summary": "We introduce a framework that enables both multi-view character consistency\nand 3D camera control in video diffusion models through a novel customization\ndata pipeline. We train the character consistency component with recorded\nvolumetric capture performances re-rendered with diverse camera trajectories\nvia 4D Gaussian Splatting (4DGS), lighting variability obtained with a video\nrelighting model. We fine-tune state-of-the-art open-source video diffusion\nmodels on this data to provide strong multi-view identity preservation, precise\ncamera control, and lighting adaptability. Our framework also supports core\ncapabilities for virtual production, including multi-subject generation using\ntwo approaches: joint training and noise blending, the latter enabling\nefficient composition of independently customized models at inference time; it\nalso achieves scene and real-life video customization as well as control over\nmotion and spatial layout during customization. Extensive experiments show\nimproved video quality, higher personalization accuracy, and enhanced camera\ncontrol and lighting adaptability, advancing the integration of video\ngeneration into virtual production. Our project page is available at:\nhttps://eyeline-labs.github.io/Virtually-Being.",
    "published": "2025-10-16T00:20:57Z",
    "updated": "2025-10-16T00:20:57Z",
    "link": "http://arxiv.org/pdf/2510.14179v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yuancheng Xu",
      "Wenqi Xian",
      "Li Ma",
      "Julien Philip",
      "Ahmet Levent Tael",
      "Yiwei Zhao",
      "Ryan Burgert",
      "Mingming He",
      "Oliver Hermann",
      "Oliver Pilarski",
      "Rahul Garg",
      "Paul Debevec",
      "Ning Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14176v1",
    "title": "ARM-FM: Automated Reward Machines via Foundation Models for\n  Compositional Reinforcement Learning",
    "summary": "Reinforcement learning (RL) algorithms are highly sensitive to reward\nfunction specification, which remains a central challenge limiting their broad\napplicability. We present ARM-FM: Automated Reward Machines via Foundation\nModels, a framework for automated, compositional reward design in RL that\nleverages the high-level reasoning capabilities of foundation models (FMs).\nReward machines (RMs) -- an automata-based formalism for reward specification\n-- are used as the mechanism for RL objective specification, and are\nautomatically constructed via the use of FMs. The structured formalism of RMs\nyields effective task decompositions, while the use of FMs enables objective\nspecifications in natural language. Concretely, we (i) use FMs to automatically\ngenerate RMs from natural language specifications; (ii) associate language\nembeddings with each RM automata-state to enable generalization across tasks;\nand (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse\nsuite of challenging environments, including evidence of zero-shot\ngeneralization.",
    "published": "2025-10-16T00:18:30Z",
    "updated": "2025-10-16T00:18:30Z",
    "link": "http://arxiv.org/pdf/2510.14176v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Roger Creus Castanyer",
      "Faisal Mohamed",
      "Pablo Samuel Castro",
      "Cyrus Neary",
      "Glen Berseth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14169v1",
    "title": "JEDA: Query-Free Clinical Order Search from Ambient Dialogues",
    "summary": "Clinical conversations mix explicit directives (order a chest X-ray) with\nimplicit reasoning (the cough worsened overnight, we should check for\npneumonia). Many systems rely on LLM rewriting, adding latency, instability,\nand opacity that hinder real-time ordering. We present JEDA (Joint Embedding\nfor Direct and Ambient clinical orders), a domain-initialized bi-encoder that\nretrieves canonical orders directly and, in a query-free mode, encodes a short\nrolling window of ambient dialogue to trigger retrieval. Initialized from\nPubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA\naligns heterogeneous expressions of intent to shared order concepts. Training\nuses constrained LLM guidance to tie each signed order to complementary\nformulations (command only, context only, command+context, context+reasoning),\nproducing clearer inter-order separation, tighter query extendash order\ncoupling, and stronger generalization. The query-free mode is noise-resilient,\nreducing sensitivity to disfluencies and ASR errors by conditioning on a short\nwindow rather than a single utterance. Deployed in practice, JEDA yields large\ngains and substantially outperforms its base encoder and recent open embedders\n(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The\nresult is a fast, interpretable, LLM-free retrieval layer that links ambient\ncontext to actionable clinical orders in real time.",
    "published": "2025-10-16T00:00:21Z",
    "updated": "2025-10-16T00:00:21Z",
    "link": "http://arxiv.org/pdf/2510.14169v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Praphul Singh",
      "Corey Barrett",
      "Sumana Srivasta",
      "Amitabh Saikia",
      "Irfan Bulu",
      "Sri Gadde",
      "Krishnaram Kenthapadi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01545v2",
    "title": "Predictive Preference Learning from Human Interventions",
    "summary": "Learning from human involvement aims to incorporate the human subject to\nmonitor and correct agent behavior errors. Although most interactive imitation\nlearning methods focus on correcting the agent's action at the current state,\nthey do not adjust its actions in future states, which may be potentially more\nhazardous. To address this, we introduce Predictive Preference Learning from\nHuman Interventions (PPL), which leverages the implicit preference signals\ncontained in human interventions to inform predictions of future rollouts. The\nkey idea of PPL is to bootstrap each human intervention into L future time\nsteps, called the preference horizon, with the assumption that the agent\nfollows the same action and the human makes the same intervention in the\npreference horizon. By applying preference optimization on these future states,\nexpert corrections are propagated into the safety-critical regions where the\nagent is expected to explore, significantly improving learning efficiency and\nreducing human demonstrations needed. We evaluate our approach with experiments\non both autonomous driving and robotic manipulation benchmarks and demonstrate\nits efficiency and generality. Our theoretical analysis further shows that\nselecting an appropriate preference horizon L balances coverage of risky states\nwith label correctness, thereby bounding the algorithmic optimality gap. Demo\nand code are available at: https://metadriverse.github.io/ppl",
    "published": "2025-10-02T00:38:18Z",
    "updated": "2025-10-15T23:33:59Z",
    "link": "http://arxiv.org/pdf/2510.01545v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Haoyuan Cai",
      "Zhenghao Peng",
      "Bolei Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14163v1",
    "title": "Towards Reversible Model Merging For Low-rank Weights",
    "summary": "Model merging aims to combine multiple fine-tuned models into a single set of\nweights that performs well across all source tasks. While prior work has shown\nthat merging can approximate the performance of individual fine-tuned models\nfor each task, it largely overlooks scenarios where models are compressed into\nlow-rank representations, either through low-rank adaptation (LoRA) or\npost-training singular value decomposition (SVD). We first demonstrate that\napplying conventional merging methods to low-rank weights leads to severe\nperformance degradation in the merged model. Motivated by this phenomenon, we\npropose a fundamentally different approach: instead of collapsing all adapters\ninto one set of weights, we construct a compact basis (e.g., an equivalent of\nholding two or more models) from which original task-specific models can be\nrecovered via linear combination. This reframes merging as generating a\nreconstruction-capable model space rather than producing a single merged model.\nCrucially, this allows us to ``revert'' to each individual model when needed,\nrecognizing that no merged model can consistently outperform one specialized\nfor its task. Building on this insight, we introduce our method, Reversible\nModel Merging (RMM), an efficient, data-free, and flexible method that provides\na closed-form solution for selecting the optimal basis of model weights and\ntask-specific coefficients for linear combination. Extensive experiments across\ndiverse datasets and model scales demonstrate that RMM consistently outperforms\nexisting merging approaches, preserving the performance of low-rank compressed\nmodels by a significant margin.",
    "published": "2025-10-15T23:22:38Z",
    "updated": "2025-10-15T23:22:38Z",
    "link": "http://arxiv.org/pdf/2510.14163v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Mohammadsajad Alipour",
      "Mohammad Mohammadi Amiri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14162v1",
    "title": "FinAI Data Assistant: LLM-based Financial Database Query Processing with\n  the OpenAI Function Calling API",
    "summary": "We present FinAI Data Assistant, a practical approach for natural-language\nquerying over financial databases that combines large language models (LLMs)\nwith the OpenAI Function Calling API. Rather than synthesizing complete SQL via\ntext-to-SQL, our system routes user requests to a small library of vetted,\nparameterized queries, trading generative flexibility for reliability, low\nlatency, and cost efficiency. We empirically study three questions: (RQ1)\nwhether LLMs alone can reliably recall or extrapolate time-dependent financial\ndata without external retrieval; (RQ2) how well LLMs map company names to stock\nticker symbols; and (RQ3) whether function calling outperforms text-to-SQL for\nend-to-end database query processing. Across controlled experiments on prices\nand fundamentals, LLM-only predictions exhibit non-negligible error and show\nlook-ahead bias primarily for stock prices relative to model knowledge cutoffs.\nTicker-mapping accuracy is near-perfect for NASDAQ-100 constituents and high\nfor S\\&P~500 firms. Finally, FinAI Data Assistant achieves lower latency and\ncost and higher reliability than a text-to-SQL baseline on our task suite. We\ndiscuss design trade-offs, limitations, and avenues for deployment.",
    "published": "2025-10-15T23:19:27Z",
    "updated": "2025-10-15T23:19:27Z",
    "link": "http://arxiv.org/pdf/2510.14162v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Juhyeong Kim",
      "Yejin Kim",
      "Youngbin Lee",
      "Hyunwoo Byun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14154v1",
    "title": "Combining Reinforcement Learning and Behavior Trees for NPCs in Video\n  Games with AMD Schola",
    "summary": "While the rapid advancements in the reinforcement learning (RL) research\ncommunity have been remarkable, the adoption in commercial video games remains\nslow. In this paper, we outline common challenges the Game AI community faces\nwhen using RL-driven NPCs in practice, and highlight the intersection of RL\nwith traditional behavior trees (BTs) as a crucial juncture to be explored\nfurther. Although the BT+RL intersection has been suggested in several research\npapers, its adoption is rare. We demonstrate the viability of this approach\nusing AMD Schola -- a plugin for training RL agents in Unreal Engine -- by\ncreating multi-task NPCs in a complex 3D environment inspired by the commercial\nvideo game ``The Last of Us\". We provide detailed methodologies for jointly\ntraining RL models with BTs while showcasing various skills.",
    "published": "2025-10-15T23:00:48Z",
    "updated": "2025-10-15T23:00:48Z",
    "link": "http://arxiv.org/pdf/2510.14154v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Tian Liu",
      "Alex Cann",
      "Ian Colbert",
      "Mehdi Saeedi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14150v1",
    "title": "CodeEvolve: An open source evolutionary coding agent for algorithm\n  discovery and optimization",
    "summary": "In this work, we introduce CodeEvolve, an open-source evolutionary coding\nagent that unites Large Language Models (LLMs) with genetic algorithms to solve\ncomplex computational problems. Our framework adapts powerful evolutionary\nconcepts to the LLM domain, building upon recent methods for generalized\nscientific discovery. CodeEvolve employs an island-based genetic algorithm to\nmaintain population diversity and increase throughput, introduces a novel\ninspiration-based crossover mechanism that leverages the LLMs context window to\ncombine features from successful solutions, and implements meta-prompting\nstrategies for dynamic exploration of the solution space. We conduct a rigorous\nevaluation of CodeEvolve on a subset of the mathematical benchmarks used to\nevaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that\nour method surpasses AlphaEvolve's performance on several challenging problems.\nTo foster collaboration and accelerate progress, we release our complete\nframework as an open-source repository.",
    "published": "2025-10-15T22:58:06Z",
    "updated": "2025-10-15T22:58:06Z",
    "link": "http://arxiv.org/pdf/2510.14150v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "I.2.7; I.2.2"
    ],
    "authors": [
      "Henrique Assumpo",
      "Diego Ferreira",
      "Leandro Campos",
      "Fabricio Murai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.08397v2",
    "title": "VoxelPrompt: A Vision Agent for End-to-End Medical Image Analysis",
    "summary": "We present VoxelPrompt, an end-to-end image analysis agent that tackles\nfree-form radiological tasks. Given any number of volumetric medical images and\na natural language prompt, VoxelPrompt integrates a language model that\ngenerates executable code to invoke a jointly-trained, adaptable vision\nnetwork. This code further carries out analytical steps to address practical\nquantitative aims, such as measuring the growth of a tumor across visits. The\npipelines generated by VoxelPrompt automate analyses that currently require\npractitioners to painstakingly combine multiple specialized vision and\nstatistical tools. We evaluate VoxelPrompt using diverse neuroimaging tasks and\nshow that it can delineate hundreds of anatomical and pathological features,\nmeasure complex morphological properties, and perform open-language analysis of\nlesion characteristics. VoxelPrompt performs these objectives with an accuracy\nsimilar to that of specialist single-task models for image analysis, while\nfacilitating a broad range of compositional biomedical workflows.",
    "published": "2024-10-10T22:11:43Z",
    "updated": "2025-10-15T22:42:16Z",
    "link": "http://arxiv.org/pdf/2410.08397v2.pdf",
    "category": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Andrew Hoopes",
      "Neel Dey",
      "Victor Ion Butoi",
      "John V. Guttag",
      "Adrian V. Dalca"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.18407v2",
    "title": "KL-regularization Itself is Differentially Private in Bandits and RLHF",
    "summary": "Differential Privacy (DP) provides a rigorous framework for privacy, ensuring\nthe outputs of data-driven algorithms remain statistically indistinguishable\nacross datasets that differ in a single entry. While guaranteeing DP generally\nrequires explicitly injecting noise either to the algorithm itself or to its\noutputs, the intrinsic randomness of existing algorithms presents an\nopportunity to achieve DP ``for free''. In this work, we explore the role of\nregularization in achieving DP across three different decision-making problems:\nmulti-armed bandits, linear contextual bandits, and reinforcement learning from\nhuman feedback (RLHF), in offline data settings. We show that adding\nKL-regularization to the learning objective (a common approach in optimization\nalgorithms) makes the action sampled from the resulting stochastic policy\nitself differentially private. This offers a new route to privacy guarantees\nwithout additional noise injection, while also preserving the inherent\nadvantage of regularization in enhancing performance.",
    "published": "2025-05-23T22:22:02Z",
    "updated": "2025-10-15T22:33:38Z",
    "link": "http://arxiv.org/pdf/2505.18407v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yizhou Zhang",
      "Kishan Panaganti",
      "Laixi Shi",
      "Juba Ziani",
      "Adam Wierman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.06382v7",
    "title": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models",
    "summary": "This paper establishes a fundamental Impossibility Theorem: no LLM performing\nnon-trivial knowledge aggregation can simultaneously achieve truthful knowledge\nrepresentation, semantic information conservation, complete revelation of\nrelevant knowledge, and knowledge-constrained optimality. This impossibility\nstems from the mathematical structure of information aggregation, not from\nengineering limitations.\n  We prove this by modeling inference as an auction of ideas, where distributed\ncomponents compete to influence responses using their encoded knowledge. The\nproof employs three independent approaches: mechanism design (Green-Laffont\ntheorem), proper scoring rules (Savage), and transformer architecture analysis\n(log-sum-exp convexity).\n  We introduce the semantic information measure and the emergence operator to\nanalyze computationally bounded and unbounded reasoning. Bounded reasoning\nmakes latent information accessible, enabling gradual insights and creativity,\nwhile unbounded reasoning makes all derivable knowledge immediately accessible\nwhile preserving the semantic content. We prove the conservation-reasoning\ndichotomy: meaningful reasoning necessarily violates information conservation.\n  Our framework suggests that hallucination and imagination are mathematically\nidentical, and both violate at least one of the four essential properties. The\nJensen gap in transformer attention quantifies this violation as excess\nconfidence beyond constituent evidence. This unified view explains why capable\nmodels must balance truthfulness against creativity.\n  These results provide principled foundations for managing hallucination\ntrade-offs in AI systems. Rather than eliminating hallucination, we should\noptimize these inevitable trade-offs for specific applications. We conclude\nwith philosophical implications connecting the impossibility to fundamental\nlimits of reason.",
    "published": "2025-06-04T23:28:39Z",
    "updated": "2025-10-15T22:25:41Z",
    "link": "http://arxiv.org/pdf/2506.06382v7.pdf",
    "category": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.GT",
      "cs.LG"
    ],
    "authors": [
      "Micha P. Karpowicz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14139v1",
    "title": "Inferred global dense residue transition graphs from primary structure\n  sequences enable protein interaction prediction via directed graph\n  convolutional neural networks",
    "summary": "Introduction Accurate prediction of protein-protein interactions (PPIs) is\ncrucial for understanding cellular functions and advancing drug development.\nExisting in-silico methods use direct sequence embeddings from Protein Language\nModels (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein\nstructures. This study explores less computationally intensive alternatives. We\nintroduce a novel framework for downstream PPI prediction through link\nprediction. Methods We introduce a two-stage graph representation learning\nframework, ProtGram-DirectGCN. First, we developed ProtGram. This approach\nmodels a protein's primary structure as a hierarchy of globally inferred n-gram\ngraphs. In these graphs, residue transition probabilities define edge weights.\nEach edge connects a pair of residues in a directed graph. The probabilities\nare aggregated from a large corpus of sequences. Second, we propose DirectGCN,\na custom directed graph convolutional neural network. This model features a\nunique convolutional layer. It processes information through separate\npath-specific transformations: incoming, outgoing, and undirected. A shared\ntransformation is also applied. These paths are combined via a learnable gating\nmechanism. We apply DirectGCN to ProtGram graphs to learn residue-level\nembeddings. These embeddings are pooled via attention to generate protein-level\nembeddings for prediction. Results We first established the efficacy of\nDirectGCN on standard node classification benchmarks. Its performance matches\nestablished methods on general datasets. The model excels at complex, directed\ngraphs with dense, heterophilic structures. When applied to PPI prediction, the\nfull ProtGram-DirectGCN framework delivers robust predictive power. This strong\nperformance holds even with limited training data.",
    "published": "2025-10-15T22:15:31Z",
    "updated": "2025-10-15T22:15:31Z",
    "link": "http://arxiv.org/pdf/2510.14139v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Islam Akef Ebeid",
      "Haoteng Tang",
      "Pengfei Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14136v1",
    "title": "A Multimodal Approach to Heritage Preservation in the Context of Climate\n  Change",
    "summary": "Cultural heritage sites face accelerating degradation due to climate change,\nyet tradi- tional monitoring relies on unimodal analysis (visual inspection or\nenvironmental sen- sors alone) that fails to capture the complex interplay\nbetween environmental stres- sors and material deterioration. We propose a\nlightweight multimodal architecture that fuses sensor data (temperature,\nhumidity) with visual imagery to predict degradation severity at heritage\nsites. Our approach adapts PerceiverIO with two key innovations: (1) simplified\nencoders (64D latent space) that prevent overfitting on small datasets (n=37\ntraining samples), and (2) Adaptive Barlow Twins loss that encourages modality\ncomplementarity rather than redundancy. On data from Strasbourg Cathedral, our\nmodel achieves 76.9% accu- racy, a 43% improvement over standard multimodal\narchitectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.\nAblation studies reveal that sensor-only achieves 61.5% while image-only\nreaches 46.2%, confirming successful multimodal synergy. A systematic\nhyperparameter study identifies an optimal moderate correlation target ({\\tau}\n=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy\ncompared to other {\\tau} values ({\\tau} =0.1/0.5/0.7: 53.8%, {\\tau} =0.9:\n61.5%). This work demonstrates that architectural sim- plicity combined with\ncontrastive regularization enables effective multimodal learning in data-scarce\nheritage monitoring contexts, providing a foundation for AI-driven con-\nservation decision support systems.",
    "published": "2025-10-15T22:07:57Z",
    "updated": "2025-10-15T22:07:57Z",
    "link": "http://arxiv.org/pdf/2510.14136v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "David Roqui",
      "Adle Cormier",
      "nistor Grozavu",
      "Ann Bourges"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14133v1",
    "title": "Formalizing the Safety, Security, and Functional Properties of Agentic\n  AI Systems",
    "summary": "Agentic AI systems, which leverage multiple autonomous agents and Large\nLanguage Models (LLMs), are increasingly used to address complex, multi-step\ntasks. The safety, security, and functionality of these systems are critical,\nespecially in high-stakes applications. However, the current ecosystem of\ninter-agent communication is fragmented, with protocols such as the Model\nContext Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol\nfor coordination being analyzed in isolation. This fragmentation creates a\nsemantic gap that prevents the rigorous analysis of system properties and\nintroduces risks such as architectural misalignment and exploitable\ncoordination issues. To address these challenges, we introduce a modeling\nframework for agentic AI systems composed of two foundational models. The\nfirst, the host agent model, formalizes the top-level entity that interacts\nwith the user, decomposes tasks, and orchestrates their execution by leveraging\nexternal agents and tools. The second, the task lifecycle model, details the\nstates and transitions of individual sub-tasks from creation to completion,\nproviding a fine-grained view of task management and error handling. Together,\nthese models provide a unified semantic framework for reasoning about the\nbehavior of multi-AI agent systems. Grounded in this framework, we define 17\nproperties for the host agent and 14 for the task lifecycle, categorized into\nliveness, safety, completeness, and fairness. Expressed in temporal logic,\nthese properties enable formal verification of system behavior, detection of\ncoordination edge cases, and prevention of deadlocks and security\nvulnerabilities. Through this effort, we introduce the first rigorously\ngrounded, domain-agnostic framework for the systematic analysis, design, and\ndeployment of correct, reliable, and robust agentic AI systems.",
    "published": "2025-10-15T22:02:30Z",
    "updated": "2025-10-15T22:02:30Z",
    "link": "http://arxiv.org/pdf/2510.14133v1.pdf",
    "category": [
      "cs.AI",
      "cs.CR",
      "cs.MA"
    ],
    "authors": [
      "Edoardo Allegrini",
      "Ananth Shreekumar",
      "Z. Berkay Celik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.04427v3",
    "title": "Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for\n  Reducing LLM Reliance",
    "summary": "Large language models (LLMs) have shown promise in table Question Answering\n(Table QA). However, extending these capabilities to multi-table QA remains\nchallenging due to unreliable schema linking across complex tables. Existing\nmethods based on semantic similarity work well only on simplified hand-crafted\ndatasets and struggle to handle complex, real-world scenarios with numerous and\ndiverse columns. To address this, we propose a graph-based framework that\nleverages human-curated relational knowledge to explicitly encode schema links\nand join paths. Given a natural language query, our method searches on graph to\nconstruct interpretable reasoning chains, aided by pruning and sub-path merging\nstrategies to enhance efficiency and coherence. Experiments on both standard\nbenchmarks and a realistic, large-scale dataset demonstrate the effectiveness\nof our approach. To our knowledge, this is the first multi-table QA system\napplied to truly complex industrial tabular data.",
    "published": "2025-06-04T20:21:52Z",
    "updated": "2025-10-15T21:43:36Z",
    "link": "http://arxiv.org/pdf/2506.04427v3.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Xixi Wang",
      "Miguel Costa",
      "Jordanka Kovaceva",
      "Shuai Wang",
      "Francisco C. Pereira"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23410v2",
    "title": "PATCH: Learnable Tile-level Hybrid Sparsity for LLMs",
    "summary": "Large language models (LLMs) deliver impressive performance but incur\nprohibitive memory and compute costs at deployment. Model pruning is an\neffective way to reduce these overheads, yet existing approaches face\nchallenges: unstructured sparsity, where nonzeros can appear anywhere,\npreserves accuracy but yields irregular access patterns that prevent GPU\nacceleration, while semi-structured 2:4 sparsity is hardware-friendly but\nenforces a rigid 50% pattern that degrades model quality. To bridge this gap,\nwe introduce PATCH, a hybrid sparsity framework that enables a continuous\nsparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles,\nassigning each tile to be either dense or 2:4 sparse via a learnable mask\nselection mechanism. This design provides fine-grained control over\naccuracy-acceleration tradeoffs and supports non-uniform sparsity across\nlayers, leading to superior overall quality. Across models from 0.5B to 8B\nparameters, PATCH consistently narrows the gap to dense accuracy while\ndelivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU,\nPATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while\nimproving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning\nmethod, MaskLLM.",
    "published": "2025-09-27T16:57:28Z",
    "updated": "2025-10-15T21:42:04Z",
    "link": "http://arxiv.org/pdf/2509.23410v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.PF"
    ],
    "authors": [
      "Younes Hourri",
      "Mohammad Mozaffari",
      "Maryam Mehri Dehnavi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14113v1",
    "title": "Toward Cybersecurity-Expert Small Language Models",
    "summary": "Large language models (LLMs) are transforming everyday applications, yet\ndeployment in cybersecurity lags due to a lack of high-quality, domain-specific\nmodels and training datasets. To address this gap, we present CyberPal 2.0, a\nfamily of cybersecurity-expert small language models (SLMs) ranging from 4B-20B\nparameters. To train CyberPal 2.0, we generate an enriched chain-of-thought\ncybersecurity instruction dataset built with our data enrichment and formatting\npipeline, SecKnowledge 2.0, which integrates expert-in-the-loop steering of\nreasoning formats alongside LLM-driven multi-step grounding, yielding\nhigher-fidelity, task-grounded reasoning traces for security tasks. Across\ndiverse cybersecurity benchmarks, CyberPal 2.0 consistently outperforms its\nbaselines and matches or surpasses various open and closed-source frontier\nmodels, while remaining a fraction of their size. On core cyber threat\nintelligence knowledge tasks, our models outperform almost all tested frontier\nmodels, ranking second only to Sec-Gemini v1. On core threat-investigation\ntasks, such as correlating vulnerabilities and bug tickets with weaknesses, our\nbest 20B-parameter model outperforms GPT-4o, o1, o3-mini, and Sec-Gemini v1,\nranking first, while our smallest 4B-parameter model ranks second.",
    "published": "2025-10-15T21:34:58Z",
    "updated": "2025-10-15T21:34:58Z",
    "link": "http://arxiv.org/pdf/2510.14113v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Matan Levi",
      "Daniel Ohayon",
      "Ariel Blobstein",
      "Ravid Sagi",
      "Ian Molloy",
      "Yair Allouche"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14112v1",
    "title": "STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for\n  Building Energy Management",
    "summary": "Building energy management is essential for achieving carbon reduction goals,\nimproving occupant comfort, and reducing energy costs. Coordinated building\nenergy management faces critical challenges in exploiting spatial-temporal\ndependencies while ensuring operational safety across multi-building systems.\nCurrent multi-building energy systems face three key challenges: insufficient\nspatial-temporal information exploitation, lack of rigorous safety guarantees,\nand system complexity. This paper proposes Spatial-Temporal Enhanced Safe\nMulti-Agent Coordination (STEMS), a novel safety-constrained multi-agent\nreinforcement learning framework for coordinated building energy management.\nSTEMS integrates two core components: (1) a spatial-temporal graph\nrepresentation learning framework using a GCN-Transformer fusion architecture\nto capture inter-building relationships and temporal patterns, and (2) a\nsafety-constrained multi-agent RL algorithm incorporating Control Barrier\nFunctions to provide mathematical safety guarantees. Extensive experiments on\nreal-world building datasets demonstrate STEMS's superior performance over\nexisting methods, showing that STEMS achieves 21% cost reduction, 18% emission\nreduction, and dramatically reduces safety violations from 35.1% to 5.6% while\nmaintaining optimal comfort with only 0.13 discomfort proportion. The framework\nalso demonstrates strong robustness during extreme weather conditions and\nmaintains effectiveness across different building types.",
    "published": "2025-10-15T21:33:58Z",
    "updated": "2025-10-15T21:33:58Z",
    "link": "http://arxiv.org/pdf/2510.14112v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Huiliang Zhang",
      "Di Wu",
      "Arnaud Zinflou",
      "Benoit Boulet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.19000v4",
    "title": "An AI-Driven Multimodal Smart Home Platform for Continuous Monitoring\n  and Assistance in Post-Stroke Motor Impairment",
    "summary": "At-home rehabilitation for post-stroke patients presents significant\nchallenges, as continuous, personalized care is often limited outside clinical\nsettings. Moreover, the lack of integrated solutions capable of simultaneously\nmonitoring motor recovery and providing intelligent assistance in home\nenvironments hampers rehabilitation outcomes. Here, we present a multimodal\nsmart home platform designed for continuous, at-home rehabilitation of\npost-stroke patients, integrating wearable sensing, ambient monitoring, and\nadaptive automation. A plantar pressure insole equipped with a machine learning\npipeline classifies users into motor recovery stages with up to 94\\% accuracy,\nenabling quantitative tracking of walking patterns during daily activities. An\noptional head-mounted eye-tracking module, together with ambient sensors such\nas cameras and microphones, supports seamless hands-free control of household\ndevices with a 100\\% success rate and sub-second response time. These data\nstreams are fused locally via a hierarchical Internet of Things (IoT)\narchitecture, ensuring low latency and data privacy. An embedded large language\nmodel (LLM) agent, Auto-Care, continuously interprets multimodal data to\nprovide real-time interventions -- issuing personalized reminders, adjusting\nenvironmental conditions, and notifying caregivers. Implemented in a\npost-stroke context, this integrated smart home platform increased mean user\nsatisfaction from 3.9 $\\pm$ 0.8 in conventional home environments to 8.4 $\\pm$\n0.6 with the full system ($n=20$). Beyond stroke, the system offers a scalable,\npatient-centered framework with potential for long-term use in broader\nneurorehabilitation and aging-in-place applications.",
    "published": "2024-11-28T09:04:39Z",
    "updated": "2025-10-15T21:33:15Z",
    "link": "http://arxiv.org/pdf/2411.19000v4.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Chenyu Tang",
      "Ruizhi Zhang",
      "Shuo Gao",
      "Zihe Zhao",
      "Zibo Zhang",
      "Jiaqi Wang",
      "Cong Li",
      "Junliang Chen",
      "Yanning Dai",
      "Shengbo Wang",
      "Ruoyu Juan",
      "Qiaoying Li",
      "Ruimou Xie",
      "Xuhang Chen",
      "Xinkai Zhou",
      "Yunjia Xia",
      "Jianan Chen",
      "Fanghao Lu",
      "Xin Li",
      "Ninglli Wang",
      "Peter Smielewski",
      "Yu Pan",
      "Hubin Zhao",
      "Luigi G. Occhipinti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.01784v3",
    "title": "iQUEST: An Iterative Question-Guided Framework for Knowledge Base\n  Question Answering",
    "summary": "While Large Language Models (LLMs) excel at many natural language processing\ntasks, they often suffer from factual inaccuracies in knowledge-intensive\nscenarios. Integrating external knowledge resources, particularly knowledge\ngraphs (KGs), provides a transparent and updatable foundation for more reliable\nreasoning. Knowledge Base Question Answering (KBQA), which queries and reasons\nover KGs, is central to this effort, especially for complex, multi-hop queries.\nHowever, multi-hop reasoning poses two key challenges: (1)~maintaining coherent\nreasoning paths, and (2)~avoiding prematurely discarding critical multi-hop\nconnections. To address these issues, we introduce iQUEST, a question-guided\nKBQA framework that iteratively decomposes complex queries into simpler\nsub-questions, ensuring a structured and focused reasoning trajectory.\nAdditionally, we integrate a Graph Neural Network (GNN) to look ahead and\nincorporate 2-hop neighbor information at each reasoning step. This dual\napproach strengthens the reasoning process, enabling the model to explore\nviable paths more effectively. Detailed experiments demonstrate the consistent\nimprovement delivered by iQUEST across four benchmark datasets and four LLMs.",
    "published": "2025-06-02T15:30:02Z",
    "updated": "2025-10-15T21:30:03Z",
    "link": "http://arxiv.org/pdf/2506.01784v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Shuai Wang",
      "Yinan Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13668v3",
    "title": "MAFA: A multi-agent framework for annotation",
    "summary": "Modern consumer banking applications require accurate and efficient retrieval\nof information in response to user queries. Mapping user utterances to the most\nrelevant Frequently Asked Questions (FAQs) is a crucial component of these\nsystems. Traditional approaches often rely on a single model or technique,\nwhich may not capture the nuances of diverse user inquiries. In this paper, we\nintroduce a multi-agent framework for FAQ annotation that combines multiple\nspecialized agents with different approaches and a judge agent that reranks\ncandidates to produce optimal results. Our agents utilize a structured\nreasoning approach inspired by Attentive Reasoning Queries (ARQs), which guides\nthem through systematic reasoning steps using targeted, task-specific JSON\nqueries. Our framework features a few-shot example strategy, where each agent\nreceives different few-shots, enhancing ensemble diversity and coverage of the\nquery space. We evaluate our framework on a real-world major bank dataset as\nwell as public benchmark datasets (LCQMC and FiQA), demonstrating significant\nimprovements over single-agent approaches across multiple metrics, including a\n14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12%\nimprovement in Mean Reciprocal Rank on our dataset, and similar gains on public\nbenchmarks when compared with traditional and single-agent annotation\ntechniques. Our framework is particularly effective at handling ambiguous\nqueries, making it well-suited for deployment in production banking\napplications while showing strong generalization capabilities across different\ndomains and languages.",
    "published": "2025-05-19T19:16:37Z",
    "updated": "2025-10-15T21:26:33Z",
    "link": "http://arxiv.org/pdf/2505.13668v3.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Mahmood Hegazy",
      "Aaron Rodrigues",
      "Azzam Naeem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14106v1",
    "title": "Generating Fair Consensus Statements with Social Choice on Token-Level\n  MDPs",
    "summary": "Current frameworks for consensus statement generation with large language\nmodels lack the inherent structure needed to provide provable fairness\nguarantees when aggregating diverse free-form opinions. We model the task as a\nmulti-objective, token-level Markov Decision Process (MDP), where each\nobjective corresponds to an agent's preference. Token-level rewards for each\nagent are derived from their policy (e.g., a personalized language model). This\napproach utilizes the finding that such policies implicitly define optimal\nQ-functions, providing a principled way to quantify rewards at each generation\nstep without a value function (Rafailov et al., 2024). This MDP formulation\ncreates a formal structure amenable to analysis using principles from social\nchoice theory. We propose two approaches grounded in social choice theory.\nFirst, we propose a stochastic generation policy guaranteed to be in the\nex-ante core, extending core stability concepts from voting theory to text\ngeneration. This policy is derived from an underlying distribution over\ncomplete statements that maximizes proportional fairness (Nash Welfare).\nSecond, for generating a single statement, we target the maximization of\negalitarian welfare using search algorithms within the MDP framework.\nEmpirically, experiments using language models to instantiate agent policies\nshow that search guided by the egalitarian objective generates consensus\nstatements with improved worst-case agent alignment compared to baseline\nmethods, including the Habermas Machine (Tessler et al., 2024).",
    "published": "2025-10-15T21:23:18Z",
    "updated": "2025-10-15T21:23:18Z",
    "link": "http://arxiv.org/pdf/2510.14106v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.GT"
    ],
    "authors": [
      "Carter Blair",
      "Kate Larson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14102v1",
    "title": "Extracting latent representations from X-ray spectra. Classification,\n  regression, and accretion signatures of Chandra sources",
    "summary": "The study of X-ray spectra is crucial to understanding the physical nature of\nastrophysical sources. Machine learning methods can extract compact and\ninformative representations of data from large datasets. The Chandra Source\nCatalog (CSC) provides a rich archive of X-ray spectral data, which remains\nlargely underexplored in this context. This work aims to develop a compact and\nphysically meaningful representation of Chandra X-ray spectra using deep\nlearning. To verify that the learned representation captures relevant\ninformation, we evaluate it through classification, regression, and\ninterpretability analyses. We use a transformer-based autoencoder to compress\nX-ray spectra. The input spectra, drawn from the CSC, include only\nhigh-significance detections. Astrophysical source types and physical summary\nstatistics are compiled from external catalogs. We evaluate the learned\nrepresentation in terms of spectral reconstruction accuracy, clustering\nperformance on 8 known astrophysical source classes, and correlation with\nphysical quantities such as hardness ratios and hydrogen column density\n($N_H$). The autoencoder accurately reconstructs spectra with 8 latent\nvariables. Clustering in the latent space yields a balanced classification\naccuracy of $\\sim$40% across the 8 source classes, increasing to $\\sim$69% when\nrestricted to AGNs and stellar-mass compact objects exclusively. Moreover,\nlatent features correlate with non-linear combinations of spectral fluxes,\nsuggesting that the compressed representation encodes physically relevant\ninformation. The proposed autoencoder-based pipeline is a powerful tool for the\nrepresentation and interpretation of X-ray spectra, providing a compact latent\nspace that supports both classification and the estimation of physical\nproperties. This work demonstrates the potential of deep learning for spectral\nstudies and uncovering new patterns in X-ray data.",
    "published": "2025-10-15T21:20:32Z",
    "updated": "2025-10-15T21:20:32Z",
    "link": "http://arxiv.org/pdf/2510.14102v1.pdf",
    "category": [
      "astro-ph.IM",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Nicol Oreste Pinciroli Vago",
      "Juan Rafael Martnez-Galarza",
      "Roberta Amato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02535v2",
    "title": "PHORECAST: Enabling AI Understanding of Public Health Outreach Across\n  Populations",
    "summary": "Understanding how diverse individuals and communities respond to persuasive\nmessaging holds significant potential for advancing personalized and socially\naware machine learning. While Large Vision and Language Models (VLMs) offer\npromise, their ability to emulate nuanced, heterogeneous human responses,\nparticularly in high stakes domains like public health, remains underexplored\ndue in part to the lack of comprehensive, multimodal dataset. We introduce\nPHORECAST (Public Health Outreach REceptivity and CAmpaign Signal Tracking), a\nmultimodal dataset curated to enable fine-grained prediction of both\nindividuallevel behavioral responses and community-wide engagement patterns to\nhealth messaging. This dataset supports tasks in multimodal understanding,\nresponse prediction, personalization, and social forecasting, allowing rigorous\nevaluation of how well modern AI systems can emulate, interpret, and anticipate\nheterogeneous public sentiment and behavior. By providing a new dataset to\nenable AI advances for public health, PHORECAST aims to catalyze the\ndevelopment of models that are not only more socially aware but also aligned\nwith the goals of adaptive and inclusive health communication",
    "published": "2025-10-02T20:10:46Z",
    "updated": "2025-10-15T21:13:31Z",
    "link": "http://arxiv.org/pdf/2510.02535v2.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Rifaa Qadri",
      "Anh Nhat Nhu",
      "Swati Ramnath",
      "Laura Yu Zheng",
      "Raj Bhansali",
      "Sylvette La Touche-Howard",
      "Tracy Marie Zeeger",
      "Tom Goldstein",
      "Ming Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14095v1",
    "title": "Unlocking Out-of-Distribution Generalization in Transformers via\n  Recursive Latent Space Reasoning",
    "summary": "Systematic, compositional generalization beyond the training distribution\nremains a core challenge in machine learning -- and a critical bottleneck for\nthe emergent reasoning abilities of modern language models. This work\ninvestigates out-of-distribution (OOD) generalization in Transformer networks\nusing a GSM8K-style modular arithmetic on computational graphs task as a\ntestbed. We introduce and explore a set of four architectural mechanisms aimed\nat enhancing OOD generalization: (i) input-adaptive recurrence; (ii)\nalgorithmic supervision; (iii) anchored latent representations via a discrete\nbottleneck; and (iv) an explicit error-correction mechanism. Collectively,\nthese mechanisms yield an architectural approach for native and scalable latent\nspace reasoning in Transformer networks with robust algorithmic generalization\ncapabilities. We complement these empirical results with a detailed mechanistic\ninterpretability analysis that reveals how these mechanisms give rise to robust\nOOD generalization abilities.",
    "published": "2025-10-15T21:03:59Z",
    "updated": "2025-10-15T21:03:59Z",
    "link": "http://arxiv.org/pdf/2510.14095v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Awni Altabaa",
      "Siyu Chen",
      "John Lafferty",
      "Zhuoran Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.04101v2",
    "title": "LLMs' Suitability for Network Security: A Case Study of STRIDE Threat\n  Modeling",
    "summary": "Artificial Intelligence (AI) is expected to be an integral part of\nnext-generation AI-native 6G networks. With the prevalence of AI, researchers\nhave identified numerous use cases of AI in network security. However, there\nare very few studies that analyze the suitability of Large Language Models\n(LLMs) in network security. To fill this gap, we examine the suitability of\nLLMs in network security, particularly with the case study of STRIDE threat\nmodeling. We utilize four prompting techniques with five LLMs to perform STRIDE\nclassification of 5G threats. From our evaluation results, we point out key\nfindings and detailed insights along with the explanation of the possible\nunderlying factors influencing the behavior of LLMs in the modeling of certain\nthreats. The numerical results and the insights support the necessity for\nadjusting and fine-tuning LLMs for network security use cases.",
    "published": "2025-05-07T03:37:49Z",
    "updated": "2025-10-15T21:01:51Z",
    "link": "http://arxiv.org/pdf/2505.04101v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "authors": [
      "AbdulAziz AbdulGhaffar",
      "Ashraf Matrawy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14086v1",
    "title": "Every Language Model Has a Forgery-Resistant Signature",
    "summary": "The ubiquity of closed-weight language models with public-facing APIs has\ngenerated interest in forensic methods, both for extracting hidden model\ndetails (e.g., parameters) and for identifying models by their outputs. One\nsuccessful approach to these goals has been to exploit the geometric\nconstraints imposed by the language model architecture and parameters. In this\nwork, we show that a lesser-known geometric constraint--namely, that language\nmodel outputs lie on the surface of a high-dimensional ellipse--functions as a\nsignature for the model and can be used to identify the source model of a given\noutput. This ellipse signature has unique properties that distinguish it from\nexisting model-output association methods like language model fingerprints. In\nparticular, the signature is hard to forge: without direct access to model\nparameters, it is practically infeasible to produce log-probabilities\n(logprobs) on the ellipse. Secondly, the signature is naturally occurring,\nsince all language models have these elliptical constraints. Thirdly, the\nsignature is self-contained, in that it is detectable without access to the\nmodel inputs or the full weights. Finally, the signature is compact and\nredundant, as it is independently detectable in each logprob output from the\nmodel. We evaluate a novel technique for extracting the ellipse from small\nmodels and discuss the practical hurdles that make it infeasible for\nproduction-scale models. Finally, we use ellipse signatures to propose a\nprotocol for language model output verification, analogous to cryptographic\nsymmetric-key message authentication systems.",
    "published": "2025-10-15T20:46:38Z",
    "updated": "2025-10-15T20:46:38Z",
    "link": "http://arxiv.org/pdf/2510.14086v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Matthew Finlayson",
      "Xiang Ren",
      "Swabha Swayamdipta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14075v1",
    "title": "DiffOPF: Diffusion Solver for Optimal Power Flow",
    "summary": "The optimal power flow (OPF) is a multi-valued, non-convex mapping from loads\nto dispatch setpoints. The variability of system parameters (e.g., admittances,\ntopology) further contributes to the multiplicity of dispatch setpoints for a\ngiven load. Existing deep learning OPF solvers are single-valued and thus fail\nto capture the variability of system parameters unless fully represented in the\nfeature space, which is prohibitive. To solve this problem, we introduce a\ndiffusion-based OPF solver, termed \\textit{DiffOPF}, that treats OPF as a\nconditional sampling problem. The solver learns the joint distribution of loads\nand dispatch setpoints from operational history, and returns the marginal\ndispatch distributions conditioned on loads. Unlike single-valued solvers,\nDiffOPF enables sampling statistically credible warm starts with favorable cost\nand constraint satisfaction trade-offs. We explore the sample complexity of\nDiffOPF to ensure the OPF solution within a prescribed distance from the\noptimization-based solution, and verify this experimentally on power system\nbenchmarks.",
    "published": "2025-10-15T20:32:48Z",
    "updated": "2025-10-15T20:32:48Z",
    "link": "http://arxiv.org/pdf/2510.14075v1.pdf",
    "category": [
      "eess.SY",
      "cs.AI",
      "cs.SY",
      "stat.CO",
      "stat.ML"
    ],
    "authors": [
      "Milad Hoseinpour",
      "Vladimir Dvorkin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14073v1",
    "title": "Exploratory Causal Inference in SAEnce",
    "summary": "Randomized Controlled Trials are one of the pillars of science; nevertheless,\nthey rely on hand-crafted hypotheses and expensive analysis. Such constraints\nprevent causal effect estimation at scale, potentially anchoring on popular yet\nincomplete hypotheses. We propose to discover the unknown effects of a\ntreatment directly from data. For this, we turn unstructured data from a trial\ninto meaningful representations via pretrained foundation models and interpret\nthem via a sparse autoencoder. However, discovering significant causal effects\nat the neural level is not trivial due to multiple-testing issues and effects\nentanglement. To address these challenges, we introduce Neural Effect Search, a\nnovel recursive procedure solving both issues by progressive stratification.\nAfter assessing the robustness of our algorithm on semi-synthetic experiments,\nwe showcase, in the context of experimental ecology, the first successful\nunsupervised causal effect identification on a real-world scientific trial.",
    "published": "2025-10-15T20:30:54Z",
    "updated": "2025-10-15T20:30:54Z",
    "link": "http://arxiv.org/pdf/2510.14073v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Tommaso Mencattini",
      "Riccardo Cadei",
      "Francesco Locatello"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14068v1",
    "title": "On the expressivity of sparse maxout networks",
    "summary": "We study the expressivity of sparse maxout networks, where each neuron takes\na fixed number of inputs from the previous layer and employs a, possibly\nmulti-argument, maxout activation. This setting captures key characteristics of\nconvolutional or graph neural networks. We establish a duality between\nfunctions computable by such networks and a class of virtual polytopes, linking\ntheir geometry to questions of network expressivity. In particular, we derive a\ntight bound on the dimension of the associated polytopes, which serves as the\ncentral tool for our analysis. Building on this, we construct a sequence of\ndepth hierarchies. While sufficiently deep sparse maxout networks are\nuniversal, we prove that if the required depth is not reached, width alone\ncannot compensate for the sparsity of a fixed indegree constraint.",
    "published": "2025-10-15T20:18:18Z",
    "updated": "2025-10-15T20:18:18Z",
    "link": "http://arxiv.org/pdf/2510.14068v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.CO",
      "68T07, 52B05, 14T99"
    ],
    "authors": [
      "Moritz Grillo",
      "Tobias Hofmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.10819v3",
    "title": "PoE-World: Compositional World Modeling with Products of Programmatic\n  Experts",
    "summary": "Learning how the world works is central to building AI agents that can adapt\nto complex environments. Traditional world models based on deep learning demand\nvast amounts of training data, and do not flexibly update their knowledge from\nsparse observations. Recent advances in program synthesis using Large Language\nModels (LLMs) give an alternate approach which learns world models represented\nas source code, supporting strong generalization from little data. To date,\napplication of program-structured world models remains limited to natural\nlanguage and grid-world domains. We introduce a novel program synthesis method\nfor effectively modeling complex, non-gridworld domains by representing a world\nmodel as an exponentially-weighted product of programmatic experts (PoE-World)\nsynthesized by LLMs. We show that this approach can learn complex, stochastic\nworld models from just a few observations. We evaluate the learned world models\nby embedding them in a model-based planning agent, demonstrating efficient\nperformance and generalization to unseen levels on Atari's Pong and Montezuma's\nRevenge. We release our code and display the learned world models and videos of\nthe agent's gameplay at https://topwasu.github.io/poe-world.",
    "published": "2025-05-16T03:28:42Z",
    "updated": "2025-10-15T20:18:00Z",
    "link": "http://arxiv.org/pdf/2505.10819v3.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Wasu Top Piriyakulkij",
      "Yichao Liang",
      "Hao Tang",
      "Adrian Weller",
      "Marta Kryven",
      "Kevin Ellis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.03892v3",
    "title": "Generative AI Meets Future Cities: Towards an Era of Autonomous Urban\n  Intelligence",
    "summary": "The two fields of urban planning and artificial intelligence (AI) arose and\ndeveloped separately. However, there is now cross-pollination and increasing\ninterest in both fields to benefit from the advances of the other. In the\npresent paper, we introduce the importance of urban planning from the\nsustainability, living, economic, disaster, and environmental perspectives. We\nreview the fundamental concepts of urban planning and relate these concepts to\ncrucial open problems of machine learning, including adversarial learning,\ngenerative neural networks, deep encoder-decoder networks, conversational AI,\nand geospatial and temporal machine learning, thereby assaying how AI can\ncontribute to modern urban planning. Thus, a central problem is automated\nland-use configuration, which is formulated as the generation of land uses and\nbuilding configuration for a target area from surrounding geospatial, human\nmobility, social media, environment, and economic activities. Finally, we\ndelineate some implications of AI for urban planning and propose key research\nareas at the intersection of both topics.",
    "published": "2023-04-08T02:19:59Z",
    "updated": "2025-10-15T20:06:19Z",
    "link": "http://arxiv.org/pdf/2304.03892v3.pdf",
    "category": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "authors": [
      "Dongjie Wang",
      "Chang-Tien Lu",
      "Xinyue Ye",
      "Tan Yigitcanlar",
      "Yanjie Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14058v1",
    "title": "Optical Computation-in-Communication enables low-latency, high-fidelity\n  perception in telesurgery",
    "summary": "Artificial intelligence (AI) holds significant promise for enhancing\nintraoperative perception and decision-making in telesurgery, where physical\nseparation impairs sensory feedback and control. Despite advances in medical AI\nand surgical robotics, conventional electronic AI architectures remain\nfundamentally constrained by the compounded latency from serial processing of\ninference and communication. This limitation is especially critical in\nlatency-sensitive procedures such as endovascular interventions, where delays\nover 200 ms can compromise real-time AI reliability and patient safety. Here,\nwe introduce an Optical Computation-in-Communication (OCiC) framework that\nreduces end-to-end latency significantly by performing AI inference\nconcurrently with optical communication. OCiC integrates Optical Remote\nComputing Units (ORCUs) directly into the optical communication pathway, with\neach ORCU experimentally achieving up to 69 tera-operations per second per\nchannel through spectrally efficient two-dimensional photonic convolution. The\nsystem maintains ultrahigh inference fidelity within 0.1% of CPU/GPU baselines\non classification and coronary angiography segmentation, while intrinsically\nmitigating cumulative error propagation, a longstanding barrier to deep optical\nnetwork scalability. We validated the robustness of OCiC through outdoor dark\nfibre deployments, confirming consistent and stable performance across varying\nenvironmental conditions. When scaled globally, OCiC transforms long-haul fibre\ninfrastructure into a distributed photonic AI fabric with exascale potential,\nenabling reliable, low-latency telesurgery across distances up to 10,000 km and\nopening a new optical frontier for distributed medical intelligence.",
    "published": "2025-10-15T19:54:11Z",
    "updated": "2025-10-15T19:54:11Z",
    "link": "http://arxiv.org/pdf/2510.14058v1.pdf",
    "category": [
      "physics.optics",
      "cs.AI",
      "eess.IV"
    ],
    "authors": [
      "Rui Yang",
      "Jiaming Hu",
      "Jian-Qing Zheng",
      "Yue-Zhen Lu",
      "Jian-Wei Cui",
      "Qun Ren",
      "Yi-Jie Yu",
      "John Edward Wu",
      "Zhao-Yu Wang",
      "Xiao-Li Lin",
      "Dandan Zhang",
      "Mingchu Tang",
      "Christos Masouros",
      "Huiyun Liu",
      "Chin-Pang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14053v1",
    "title": "Position: Require Frontier AI Labs To Release Small \"Analog\" Models",
    "summary": "Recent proposals for regulating frontier AI models have sparked concerns\nabout the cost of safety regulation, and most such regulations have been\nshelved due to the safety-innovation tradeoff. This paper argues for an\nalternative regulatory approach that ensures AI safety while actively promoting\ninnovation: mandating that large AI laboratories release small, openly\naccessible analog models (scaled-down versions) trained similarly to and\ndistilled from their largest proprietary models.\n  Analog models serve as public proxies, allowing broad participation in safety\nverification, interpretability research, and algorithmic transparency without\nforcing labs to disclose their full-scale models. Recent research demonstrates\nthat safety and interpretability methods developed using these smaller models\ngeneralize effectively to frontier-scale systems. By enabling the wider\nresearch community to directly investigate and innovate upon accessible\nanalogs, our policy substantially reduces the regulatory burden and accelerates\nsafety advancements.\n  This mandate promises minimal additional costs, leveraging reusable resources\nlike data and infrastructure, while significantly contributing to the public\ngood. Our hope is not only that this policy be adopted, but that it illustrates\na broader principle supporting fundamental research in machine learning: deeper\nunderstanding of models relaxes the safety-innovation tradeoff and lets us have\nmore of both.",
    "published": "2025-10-15T19:47:49Z",
    "updated": "2025-10-15T19:47:49Z",
    "link": "http://arxiv.org/pdf/2510.14053v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Shriyash Upadhyay",
      "Chaithanya Bandi",
      "Narmeen Oozeer",
      "Philip Quirke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14043v1",
    "title": "Cyber-Resilient System Identification for Power Grid through Bayesian\n  Integration",
    "summary": "Power grids increasingly need real-time situational awareness under the\never-evolving cyberthreat landscape. Advances in snapshot-based system\nidentification approaches have enabled accurately estimating states and\ntopology from a snapshot of measurement data, under random bad data and\ntopology errors. However, modern interactive, targeted false data can stay\nundetectable to these methods, and significantly compromise estimation\naccuracy. This work advances system identification that combines snapshot-based\nmethod with time-series model via Bayesian Integration, to advance cyber\nresiliency against both random and targeted false data. Using a distance-based\ntime-series model, this work can leverage historical data of different\ndistributions induced by changes in grid topology and other settings. The\nnormal system behavior captured from historical data is integrated into system\nidentification through a Bayesian treatment, to make solutions robust to\ntargeted false data. We experiment on mixed random anomalies (bad data,\ntopology error) and targeted false data injection attack (FDIA) to demonstrate\nour method's 1) cyber resilience: achieving over 70% reduction in estimation\nerror under FDIA; 2) anomalous data identification: being able to alarm and\nlocate anomalous data; 3) almost linear scalability: achieving comparable speed\nwith the snapshot-based baseline, both taking <1min per time tick on the large\n2,383-bus system using a laptop CPU.",
    "published": "2025-10-15T19:32:09Z",
    "updated": "2025-10-15T19:32:09Z",
    "link": "http://arxiv.org/pdf/2510.14043v1.pdf",
    "category": [
      "eess.SY",
      "cs.AI",
      "cs.CR",
      "cs.SY"
    ],
    "authors": [
      "Shimiao Li",
      "Guannan Qu",
      "Bryan Hooi",
      "Vyas Sekar",
      "Soummya Kar",
      "Larry Pileggi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14036v1",
    "title": "One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery",
    "summary": "Fixing bugs in large programs is a challenging task that demands substantial\ntime and effort. Once a bug is found, it is reported to the project\nmaintainers, who work with the reporter to fix it and eventually close the\nissue. However, across the program, there are often similar code segments,\nwhich may also contain the bug, but were missed during discovery. Finding and\nfixing each recurring bug instance individually is labor intensive. Even more\nconcerning, bug reports can inadvertently widen the attack surface as they\nprovide attackers with an exploitable pattern that may be unresolved in other\nparts of the program.\n  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear\nrepeatedly across various code segments of a program or even in different\nprograms, stemming from a same root cause, but are unresolved. Our\ninvestigation reveals that RPBs are widespread and can significantly compromise\nthe security of software programs. This paper introduces BugStone, a program\nanalysis system empowered by LLVM and a Large Language Model (LLM). The key\nobservation is that many RPBs have one patched instance, which can be leveraged\nto identify a consistent error pattern, such as a specific API misuse. By\nexamining the entire program for this pattern, it is possible to identify\nsimilar sections of code that may be vulnerable. Starting with 135 unique RPBs,\nBugStone identified more than 22K new potential issues in the Linux kernel.\nManual analysis of 400 of these findings confirmed that 246 were valid. We also\ncreated a dataset from over 1.9K security bugs reported by 23 recent top-tier\nconference works. We manually annotate the dataset, identify 80 recurring\npatterns and 850 corresponding fixes. Even with a cost-efficient model choice,\nBugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.",
    "published": "2025-10-15T19:18:06Z",
    "updated": "2025-10-15T19:18:06Z",
    "link": "http://arxiv.org/pdf/2510.14036v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Qiushi Wu",
      "Yue Xiao",
      "Dhilung Kirat",
      "Kevin Eykholt",
      "Jiyong Jang",
      "Douglas Lee Schales"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14035v1",
    "title": "GammaZero: Learning To Guide POMDP Belief Space Search With Graph\n  Representations",
    "summary": "We introduce an action-centric graph representation framework for learning to\nguide planning in Partially Observable Markov Decision Processes (POMDPs).\nUnlike existing approaches that require domain-specific neural architectures\nand struggle with scalability, GammaZero leverages a unified graph-based belief\nrepresentation that enables generalization across problem sizes within a\ndomain. Our key insight is that belief states can be systematically transformed\ninto action-centric graphs where structural patterns learned on small problems\ntransfer to larger instances. We employ a graph neural network with a decoder\narchitecture to learn value functions and policies from expert demonstrations\non computationally tractable problems, then apply these learned heuristics to\nguide Monte Carlo tree search on larger problems. Experimental results on\nstandard POMDP benchmarks demonstrate that GammaZero achieves comparable\nperformance to BetaZero when trained and tested on the same-sized problems,\nwhile uniquely enabling zero-shot generalization to problems 2-4 times larger\nthan those seen during training, maintaining solution quality with reduced\nsearch requirements.",
    "published": "2025-10-15T19:18:03Z",
    "updated": "2025-10-15T19:18:03Z",
    "link": "http://arxiv.org/pdf/2510.14035v1.pdf",
    "category": [
      "cs.AI",
      "I.2.6; I.2.9"
    ],
    "authors": [
      "Rajesh Mangannavar",
      "Prasad Tadepalli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.12349v5",
    "title": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
    "summary": "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps://spinbench.github.io/",
    "published": "2025-03-16T04:10:53Z",
    "updated": "2025-10-15T19:15:17Z",
    "link": "http://arxiv.org/pdf/2503.12349v5.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Jianzhu Yao",
      "Kevin Wang",
      "Ryan Hsieh",
      "Haisu Zhou",
      "Tianqing Zou",
      "Zerui Cheng",
      "Zhangyang Wang",
      "Pramod Viswanath"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14030v1",
    "title": "Think Globally, Group Locally: Evaluating LLMs Using Multi-Lingual Word\n  Grouping Games",
    "summary": "Large language models (LLMs) can exhibit biases in reasoning capabilities due\nto linguistic modality, performing better on tasks in one language versus\nanother, even with similar content. Most previous works evaluate this through\nreasoning tasks where reliance on strategies or knowledge can ensure success,\nsuch as in commonsense or math tasks. However, abstract reasoning is vital to\nreasoning for everyday life, where people apply \"out-of-the-box thinking\" to\nidentify and use patterns for solutions, without a reliance on formulaic\napproaches. Comparatively, little work has evaluated linguistic biases in this\ntask type. In this paper, we propose a task inspired by the New York Times\nConnections: GlobalGroup, that evaluates models in an abstract reasoning task\nacross several languages. We constructed a game benchmark with five linguistic\nbackgrounds -- English, Spanish, Chinese, Hindi, and Arabic -- in both the\nnative language and an English translation for comparison. We also proposed\ngame difficulty measurements to evaluate models on games with similar\ndifficulty, enabling a more controlled comparison, which is particularly\nimportant in reasoning evaluations. Through experimentation, we find English\nmodalities largely lead to better performance in this abstract reasoning task,\nand performance disparities between open- and closed-source models.",
    "published": "2025-10-15T19:12:43Z",
    "updated": "2025-10-15T19:12:43Z",
    "link": "http://arxiv.org/pdf/2510.14030v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Csar Guerra-Solano",
      "Zhuochun Li",
      "Xiang Lorraine Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14027v1",
    "title": "Context-Selective State Space Models: Feedback is All You Need",
    "summary": "Transformers, powered by the attention mechanism, are the backbone of most\nfoundation models, yet they suffer from quadratic complexity and difficulties\nin dealing with long-range dependencies in the input sequence. Recent work has\nshown that state space models (SSMs) provide an efficient alternative, with the\nS6 module at the core of the Mamba architecture achieving state-of-the-art\nresults on long-sequence benchmarks. In this paper, we introduce the COFFEE\n(COntext From FEEdback) model, a novel time-varying SSM that incorporates state\nfeedback to enable context-dependent selectivity, while still allowing for\nparallel implementation. Whereas the selectivity mechanism of S6 only depends\non the current input, COFFEE computes it from the internal state, which serves\nas a compact representation of the sequence history. This shift allows the\nmodel to regulate its dynamics based on accumulated context, improving its\nability to capture long-range dependencies. In addition to state feedback, we\nemploy an efficient model parametrization that removes redundancies present in\nS6 and leads to a more compact and trainable formulation. On the induction head\ntask, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer\nparameters and training sequences compared to S6. On MNIST, COFFEE largely\noutperforms S6 within the same architecture, reaching 97% accuracy with only\n3585 parameters. These results showcase the role of state feedback as a key\nmechanism for building scalable and efficient sequence models.",
    "published": "2025-10-15T19:08:28Z",
    "updated": "2025-10-15T19:08:28Z",
    "link": "http://arxiv.org/pdf/2510.14027v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Riccardo Zattra",
      "Giacomo Baggio",
      "Umberto Casti",
      "Augusto Ferrante",
      "Francesco Ticozzi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.13481v3",
    "title": "Polynomial-Time Algorithms for Fair Orientations of Chores",
    "summary": "This paper addresses the problem of finding fair orientations of graphs of\nchores, in which each vertex corresponds to an agent, each edge corresponds to\na chore, and a chore has zero marginal utility to an agent if its corresponding\nedge is not incident to the vertex corresponding to the agent. Recently, Zhou\net al. (IJCAI, 2024) analyzed the complexity of deciding whether graphs\ncontaining a mixture of goods and chores have EFX orientations, and conjectured\nthat deciding whether graphs containing only chores have EFX orientations is\nNP-complete. We resolve this conjecture by giving polynomial-time algorithms\nthat find EF1 and EFX orientations of graphs containing only chores if they\nexist, even if there are self-loops. Remarkably, our result demonstrates a\nsurprising separation between the case of goods and the case of chores, because\ndeciding whether graphs containing only goods have EFX orientations was shown\nto be NP-complete by Christodoulou et al. (EC, 2023). In addition, we show the\nEF1 and EFX orientation problems for multigraphs to be NP-complete.",
    "published": "2025-01-23T08:53:18Z",
    "updated": "2025-10-15T18:56:48Z",
    "link": "http://arxiv.org/pdf/2501.13481v3.pdf",
    "category": [
      "cs.GT",
      "cs.AI",
      "cs.DM"
    ],
    "authors": [
      "Kevin Hsu",
      "Valerie King"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.16060v3",
    "title": "Tokenizing Single-Channel EEG with Time-Frequency Motif Learning",
    "summary": "Foundation models are reshaping EEG analysis, yet an important problem of EEG\ntokenization remains a challenge. This paper presents TFM-Tokenizer, a novel\ntokenization framework that learns a vocabulary of time-frequency motifs from\nsingle-channel EEG signals and encodes them into discrete tokens. We propose a\ndual-path architecture with time-frequency masking to capture robust motif\nrepresentations, and it is model-agnostic, supporting both lightweight\ntransformers and existing foundation models for downstream tasks. Our study\ndemonstrates three key benefits: Accuracy: Experiments on four diverse EEG\nbenchmarks demonstrate consistent performance gains across both single- and\nmulti-dataset pretraining settings, achieving up to 17% improvement in Cohen's\nKappa over strong baselines. Generalization: Moreover, as a plug-and-play\ncomponent, it consistently boosts the performance of diverse foundation models,\nincluding BIOT and LaBraM. Scalability: By operating at the single-channel\nlevel rather than relying on the strict 10-20 EEG system, our method has the\npotential to be device-agnostic. Experiments on ear-EEG sleep staging, which\ndiffers from the pretraining data in signal format, channel configuration,\nrecording device, and task, show that our tokenizer outperforms baselines by\n14%. A comprehensive token analysis reveals strong class-discriminative,\nfrequency-aware, and consistent structure, enabling improved representation\nquality and interpretability. Code is available at\nhttps://github.com/Jathurshan0330/TFM-Tokenizer.",
    "published": "2025-02-22T03:32:36Z",
    "updated": "2025-10-15T18:46:33Z",
    "link": "http://arxiv.org/pdf/2502.16060v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "authors": [
      "Jathurshan Pradeepkumar",
      "Xihao Piao",
      "Zheng Chen",
      "Jimeng Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14007v1",
    "title": "Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE\n  Modeling",
    "summary": "Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows\nincorporating equivariance to arbitrary pseudo-Euclidean groups, including\nisometries of Euclidean space and Minkowski spacetime. In this work, we\ndemonstrate that the kernel basis of CSCNNs is not complete, thus limiting the\nmodel expressivity. To address this issue, we propose Conditional\nClifford-Steerable Kernels, which augment the kernels with equivariant\nrepresentations computed from the input feature field. We derive the\nequivariance constraint for these input-dependent kernels and show how it can\nbe solved efficiently via implicit parameterization. We empirically demonstrate\nan improved expressivity of the resulting framework on multiple PDE forecasting\ntasks, including fluid dynamics and relativistic electrodynamics, where our\nmethod consistently outperforms baseline methods.",
    "published": "2025-10-15T18:38:36Z",
    "updated": "2025-10-15T18:38:36Z",
    "link": "http://arxiv.org/pdf/2510.14007v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Blint Lszl Szarvas",
      "Maksim Zhdanov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13999v1",
    "title": "REAP the Experts: Why Pruning Prevails for One-Shot MoE compression",
    "summary": "Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient\npre-training and low latency but their large parameter counts create\nsignificant memory overhead, motivating research into expert compression.\nContrary to recent findings favouring expert merging on discriminative\nbenchmarks, we demonstrate that expert pruning is a superior strategy for\ngenerative tasks. We prove that merging introduces an irreducible error by\ncausing a \"functional subspace collapse\", due to the loss of the router's\nindependent, input-dependent control over experts. Leveraging this insight, we\npropose Router-weighted Expert Activation Pruning (REAP), a novel pruning\ncriterion that considers both router gate-values and expert activation norms.\nAcross a diverse set of SMoE models ranging from 20B to 1T parameters, REAP\nconsistently outperforms merging and other pruning methods on generative\nbenchmarks, especially at 50% compression. Notably, our method achieves\nnear-lossless compression on code generation and tool-calling tasks with\nQwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.",
    "published": "2025-10-15T18:29:28Z",
    "updated": "2025-10-15T18:29:28Z",
    "link": "http://arxiv.org/pdf/2510.13999v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "I.2.7"
    ],
    "authors": [
      "Mike Lasby",
      "Ivan Lazarevich",
      "Nish Sinnadurai",
      "Sean Lie",
      "Yani Ioannou",
      "Vithursan Thangarasa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13995v1",
    "title": "Finding Holes: Pathologist Level Performance Using AI for Cribriform\n  Morphology Detection in Prostate Cancer",
    "summary": "Background: Cribriform morphology in prostate cancer is a histological\nfeature that indicates poor prognosis and contraindicates active surveillance.\nHowever, it remains underreported and subject to significant interobserver\nvariability amongst pathologists. We aimed to develop and validate an AI-based\nsystem to improve cribriform pattern detection.\n  Methods: We created a deep learning model using an EfficientNetV2-S encoder\nwith multiple instance learning for end-to-end whole-slide classification. The\nmodel was trained on 640 digitised prostate core needle biopsies from 430\npatients, collected across three cohorts. It was validated internally (261\nslides from 171 patients) and externally (266 slides, 104 patients from three\nindependent cohorts). Internal validation cohorts included laboratories or\nscanners from the development set, while external cohorts used completely\nindependent instruments and laboratories. Annotations were provided by three\nexpert uropathologists with known high concordance. Additionally, we conducted\nan inter-rater analysis and compared the model's performance against nine\nexpert uropathologists on 88 slides from the internal validation cohort.\n  Results: The model showed strong internal validation performance (AUC: 0.97,\n95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external\nvalidation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI:\n0.45-0.64). In our inter-rater analysis, the model achieved the highest average\nagreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine\npathologists whose Cohen's kappas ranged from 0.35 to 0.62.\n  Conclusion: Our AI model demonstrates pathologist-level performance for\ncribriform morphology detection in prostate cancer. This approach could enhance\ndiagnostic reliability, standardise reporting, and improve treatment decisions\nfor prostate cancer patients.",
    "published": "2025-10-15T18:23:34Z",
    "updated": "2025-10-15T18:23:34Z",
    "link": "http://arxiv.org/pdf/2510.13995v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Kelvin Szolnoky",
      "Anders Blilie",
      "Nita Mulliqi",
      "Toyonori Tsuzuki",
      "Hemamali Samaratunga",
      "Matteo Titus",
      "Xiaoyi Ji",
      "Sol Erika Boman",
      "Einar Gudlaugsson",
      "Svein Reidar Kjosavik",
      "Jos Asenjo",
      "Marcello Gambacorta",
      "Paolo Libretti",
      "Marcin Braun",
      "Radisaw Kordek",
      "Roman owicki",
      "Brett Delahunt",
      "Kenneth A. Iczkowski",
      "Theo van der Kwast",
      "Geert J. L. H. van Leenders",
      "Katia R. M. Leite",
      "Chin-Chen Pan",
      "Emiel Adrianus Maria Janssen",
      "Martin Eklund",
      "Lars Egevad",
      "Kimmo Kartasalo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13993v1",
    "title": "Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and\n  Vision-Language Models",
    "summary": "Remote sensing has become a vital tool across sectors such as urban planning,\nenvironmental monitoring, and disaster response. While the volume of data\ngenerated has increased significantly, traditional vision models are often\nconstrained by the requirement for extensive domain-specific labelled data and\ntheir limited ability to understand the context within complex environments.\nVision Language Models offer a complementary approach by integrating visual and\ntextual data; however, their application to remote sensing remains\nunderexplored, particularly given their generalist nature. This work\ninvestigates the combination of vision models and VLMs to enhance image\nanalysis in remote sensing, with a focus on aircraft detection and scene\nunderstanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and\nGemini aims to achieve more accurate and contextually aware image\ninterpretation. Performance is evaluated on both labelled and unlabelled remote\nsensing data, as well as degraded image scenarios which are crucial for remote\nsensing. The findings show an average MAE improvement of 48.46% across models\nin the accuracy of aircraft detection and counting, especially in challenging\nconditions, in both raw and degraded scenarios. A 6.17% improvement in\nCLIPScore for comprehensive understanding of remote sensing images is obtained.\nThe proposed approach combining traditional vision models and VLMs paves the\nway for more advanced and efficient remote sensing image analysis, especially\nin few-shot learning scenarios.",
    "published": "2025-10-15T18:19:48Z",
    "updated": "2025-10-15T18:19:48Z",
    "link": "http://arxiv.org/pdf/2510.13993v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jia Yun Chua",
      "Argyrios Zolotas",
      "Miguel Arana-Catania"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.13410v2",
    "title": "Causal Language Control in Multilingual Transformers via Sparse Feature\n  Steering",
    "summary": "Deterministically controlling the target generation language of large\nmultilingual language models (LLMs) remains a fundamental challenge,\nparticularly in zero-shot settings where neither explicit language prompts nor\nfine-tuning are available. In this work, we investigate whether sparse\nautoencoder (SAE) features, previously shown to correlate with interpretable\nmodel behaviors, can be leveraged to steer the generated language of LLMs\nduring inference. Leveraging pretrained SAEs on the residual streams of\nGemma-2B and Gemma-9B, we identify features whose activations differ most\nsignificantly between English and four target languages: Chinese, Japanese,\nSpanish, and French. By modifying just a single SAE feature at one transformer\nlayer, we achieve controlled language shifts with up to 90\\% success, as\nmeasured by FastText language classification, while preserving semantic\nfidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)\nsimilarity. Our analysis reveals that language steering is most effective in\nmid-to-late transformer layers and is amplified by specific attention heads\ndisproportionately associated with language-sensitive SAE features. These\nresults demonstrate the promise of sparse feature steering as a lightweight and\ninterpretable mechanism for controllable multilingual generation.",
    "published": "2025-07-17T06:49:16Z",
    "updated": "2025-10-15T18:18:58Z",
    "link": "http://arxiv.org/pdf/2507.13410v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Cheng-Ting Chou",
      "George Liu",
      "Jessica Sun",
      "Cole Blondin",
      "Kevin Zhu",
      "Vasu Sharma",
      "Sean O'Brien"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13985v1",
    "title": "Do Large Language Models Show Biases in Causal Learning? Insights from\n  Contingency Judgment",
    "summary": "Causal learning is the cognitive process of developing the capability of\nmaking causal inferences based on available information, often guided by\nnormative principles. This process is prone to errors and biases, such as the\nillusion of causality, in which people perceive a causal relationship between\ntwo variables despite lacking supporting evidence. This cognitive bias has been\nproposed to underlie many societal problems, including social prejudice,\nstereotype formation, misinformation, and superstitious thinking. In this work,\nwe examine whether large language models are prone to developing causal\nillusions when faced with a classic cognitive science paradigm: the contingency\njudgment task. To investigate this, we constructed a dataset of 1,000 null\ncontingency scenarios (in which the available information is not sufficient to\nestablish a causal relationship between variables) within medical contexts and\nprompted LLMs to evaluate the effectiveness of potential causes. Our findings\nshow that all evaluated models systematically inferred unwarranted causal\nrelationships, revealing a strong susceptibility to the illusion of causality.\nWhile there is ongoing debate about whether LLMs genuinely understand causality\nor merely reproduce causal language without true comprehension, our findings\nsupport the latter hypothesis and raise concerns about the use of language\nmodels in domains where accurate causal reasoning is essential for informed\ndecision-making.",
    "published": "2025-10-15T18:09:00Z",
    "updated": "2025-10-15T18:09:00Z",
    "link": "http://arxiv.org/pdf/2510.13985v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Mara Victoria Carro",
      "Denise Alejandra Mester",
      "Francisca Gauna Selasco",
      "Giovanni Franco Gabriel Marraffini",
      "Mario Alejandro Leiva",
      "Gerardo I. Simari",
      "Mara Vanina Martinez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09714v2",
    "title": "All Code, No Thought: Current Language Models Struggle to Reason in\n  Ciphered Language",
    "summary": "Detecting harmful AI actions is important as AI agents gain adoption.\nChain-of-thought (CoT) monitoring is one method widely used to detect\nadversarial attacks and AI misalignment. However, attackers and misaligned\nmodels might evade CoT monitoring through ciphered reasoning: reasoning hidden\nin encrypted, translated, or compressed text. To assess this risk, we test\nwhether models can perform ciphered reasoning. For each of 28 different\nciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We\nmeasure model accuracy on math problems as a proxy for reasoning ability.\nAcross the models we test, we find an asymmetry: model accuracy can drop\nsignificantly when reasoning in ciphered text, even though models demonstrate\ncomprehension of ciphered text by being able to translate it accurately to\nEnglish. Even frontier models struggle with lesser-known ciphers, although they\ncan reason accurately in well-known ciphers like rot13. We show that ciphered\nreasoning capability correlates with cipher prevalence in pretraining data. We\nalso identify scaling laws showing that ciphered reasoning capability improves\nslowly with additional fine-tuning data. Our work suggests that evading CoT\nmonitoring using ciphered reasoning may be an ineffective tactic for current\nmodels and offers guidance on constraining the development of this capability\nin future frontier models.",
    "published": "2025-10-10T06:01:22Z",
    "updated": "2025-10-15T18:07:42Z",
    "link": "http://arxiv.org/pdf/2510.09714v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Shiyuan Guo",
      "Henry Sleight",
      "Fabien Roger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13982v1",
    "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires\n  Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations",
    "summary": "What if artificial agents could not just communicate, but also evolve, adapt,\nand reshape their worlds in ways we cannot fully predict? With llm now powering\nmulti-agent systems and social simulations, we are witnessing new possibilities\nfor modeling open-ended, ever-changing environments. Yet, most current\nsimulations remain constrained within static sandboxes, characterized by\npredefined tasks, limited dynamics, and rigid evaluation criteria. These\nlimitations prevent them from capturing the complexity of real-world societies.\nIn this paper, we argue that static, task-specific benchmarks are fundamentally\ninadequate and must be rethought. We critically review emerging architectures\nthat blend llm with multi-agent dynamics, highlight key hurdles such as\nbalancing stability and diversity, evaluating unexpected behaviors, and scaling\nto greater complexity, and introduce a fresh taxonomy for this rapidly evolving\nfield. Finally, we present a research roadmap centered on open-endedness,\ncontinuous co-evolution, and the development of resilient, socially aligned AI\necosystems. \\textbf{We call on the community to move beyond static paradigms\nand help shape the next generation of adaptive, socially-aware multi-agent\nsimulations.}",
    "published": "2025-10-15T18:05:06Z",
    "updated": "2025-10-15T18:05:06Z",
    "link": "http://arxiv.org/pdf/2510.13982v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI"
    ],
    "authors": [
      "Jinkun Chen",
      "Sher Badshah",
      "Xuemin Yu",
      "Sijia Han",
      "Jiechao Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13979v1",
    "title": "Do Slides Help? Multi-modal Context for Automatic Transcription of\n  Conference Talks",
    "summary": "State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily\nrely on acoustic information while disregarding additional multi-modal context.\nHowever, visual information are essential in disambiguation and adaptation.\nWhile most work focus on speaker images to handle noise conditions, this work\nalso focuses on integrating presentation slides for the use cases of scientific\npresentation.\n  In a first step, we create a benchmark for multi-modal presentation including\nan automatic analysis of transcribing domain-specific terminology. Next, we\nexplore methods for augmenting speech models with multi-modal information. We\nmitigate the lack of datasets with accompanying slides by a suitable approach\nof data augmentation. Finally, we train a model using the augmented dataset,\nresulting in a relative reduction in word error rate of approximately 34%,\nacross all words and 35%, for domain-specific terms compared to the baseline\nmodel.",
    "published": "2025-10-15T18:04:16Z",
    "updated": "2025-10-15T18:04:16Z",
    "link": "http://arxiv.org/pdf/2510.13979v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Supriti Sinhamahapatra",
      "Jan Niehues"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.14297v3",
    "title": "Evaluating Sakana's AI Scientist: Bold Claims, Mixed Results, and a\n  Promising Future?",
    "summary": "A major step toward Artificial General Intelligence (AGI) and Super\nIntelligence is AI's ability to autonomously conduct research - what we term\nArtificial Research Intelligence (ARI). If machines could generate hypotheses,\nconduct experiments, and write research papers without human intervention, it\nwould transform science. Sakana recently introduced the 'AI Scientist',\nclaiming to conduct research autonomously, i.e. they imply to have achieved\nwhat we term Artificial Research Intelligence (ARI). The AI Scientist gained\nmuch attention, but a thorough independent evaluation has yet to be conducted.\n  Our evaluation of the AI Scientist reveals critical shortcomings. The\nsystem's literature reviews produced poor novelty assessments, often\nmisclassifying established concepts (e.g., micro-batching for stochastic\ngradient descent) as novel. It also struggles with experiment execution: 42% of\nexperiments failed due to coding errors, while others produced flawed or\nmisleading results. Code modifications were minimal, averaging 8% more\ncharacters per iteration, suggesting limited adaptability. Generated\nmanuscripts were poorly substantiated, with a median of five citations, most\noutdated (only five of 34 from 2020 or later). Structural errors were frequent,\nincluding missing figures, repeated sections, and placeholder text like\n'Conclusions Here'. Some papers contained hallucinated numerical results.\n  Despite these flaws, the AI Scientist represents a leap forward in research\nautomation. It generates full research manuscripts with minimal human input,\nchallenging expectations of AI-driven science. Many reviewers might struggle to\ndistinguish its work from human researchers. While its quality resembles a\nrushed undergraduate paper, its speed and cost efficiency are unprecedented,\nproducing a full paper for USD 6 to 15 with 3.5 hours of human involvement, far\noutpacing traditional researchers.",
    "published": "2025-02-20T06:22:03Z",
    "updated": "2025-10-15T18:03:49Z",
    "link": "http://arxiv.org/pdf/2502.14297v3.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Joeran Beel",
      "Min-Yen Kan",
      "Moritz Baumgart"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11713v3",
    "title": "Are Large Reasoning Models Interruptible?",
    "summary": "Large Reasoning Models (LRMs) excel at complex reasoning but are\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\nassumed to be instantaneous, and the context of a request is presumed to be\nimmutable over the duration of the response. While generally true for\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\ntasks such as assistive programming, where models may take hours to think\nthrough problems and code may change dramatically from the time the model\nstarts thinking to the model's final output. In this work, we challenge the\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\nscenarios: interruptions, which test the quality of the model's partial outputs\non a limited budget, and dynamic context, which tests model adaptation to\nin-flight changes. Across mathematics and programming benchmarks that require\nlong-form reasoning, static evaluations consistently overestimate robustness:\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\nfail unpredictably when interrupted or exposed to changing context, with\nperformance dropping by up to 60% when updates are introduced late in the\nreasoning process. Our analysis further reveals several novel failure modes,\nincluding reasoning leakage, where models fold the reasoning into their final\nanswer when interrupted; panic, where under time pressure models abandon\nreasoning entirely and return incorrect answers; and self-doubt, where\nperformance degrades while incorporating updated information. Project Page:\nhttp://dynamic-lm.github.io/",
    "published": "2025-10-13T17:59:35Z",
    "updated": "2025-10-16T17:59:24Z",
    "link": "http://arxiv.org/pdf/2510.11713v3.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Tsung-Han Wu",
      "Mihran Miroyan",
      "David M. Chan",
      "Trevor Darrell",
      "Narges Norouzi",
      "Joseph E. Gonzalez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14961v1",
    "title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their\n  Connection to Diffusion Language Models",
    "summary": "Language models with recurrent depth, also referred to as universal or looped\nwhen considering transformers, are defined by the capacity to increase their\ncomputation through the repetition of layers. Recent efforts in pretraining\nhave demonstrated that these architectures can scale to modern language\nmodeling tasks while exhibiting advantages in reasoning tasks. In this work, we\nexamine the relationship between recurrent-depth models and diffusion language\nmodels. Building on their similarities, we develop a new diffusion forcing\nsampler for these models to accelerate generation. The sampler advances by\ndecoding new tokens at every forward pass of the model, while the latent states\nof these tokens can be further refined in parallel through recurrence.\nTheoretically, generation with our sampler is strictly more expressive than the\nbaseline autoregressive generation using the same time budget on modern\nhardware. Moreover, this sampler, based on principles from diffusion\nliterature, can be directly applied to existing 3.5B recurrent-depth\ntransformers without any tuning, leading to up to a 5x speedup. Consequently,\nour findings not only provide an efficient mechanism for parallelizing the\nextra computation in recurrent-depth models at inference, but also suggest that\nsuch models can be naturally viewed as strong continuous, though causal,\ndiffusion language models.",
    "published": "2025-10-16T17:59:07Z",
    "updated": "2025-10-16T17:59:07Z",
    "link": "http://arxiv.org/pdf/2510.14961v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Jonas Geiping",
      "Xinyu Yang",
      "Guinan Su"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14958v1",
    "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
    "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/",
    "published": "2025-10-16T17:58:58Z",
    "updated": "2025-10-16T17:58:58Z",
    "link": "http://arxiv.org/pdf/2510.14958v1.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Weikang Shi",
      "Aldrich Yu",
      "Rongyao Fang",
      "Houxing Ren",
      "Ke Wang",
      "Aojun Zhou",
      "Changyao Tian",
      "Xinyu Fu",
      "Yuxuan Hu",
      "Zimu Lu",
      "Linjiang Huang",
      "Si Liu",
      "Rui Liu",
      "Hongsheng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14949v1",
    "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal\n  Generation",
    "summary": "Contact languages like English exhibit rich regional variations in the form\nof dialects, which are often used by dialect speakers interacting with\ngenerative models. However, can multimodal generative models effectively\nproduce content given dialectal textual input? In this work, we study this\nquestion by constructing a new large-scale benchmark spanning six common\nEnglish dialects. We work with dialect speakers to collect and verify over 4200\nunique prompts and evaluate on 17 image and video generative models. Our\nautomatic and human evaluation results show that current state-of-the-art\nmultimodal generative models exhibit 32.26% to 48.17% performance degradation\nwhen a single dialect word is used in the prompt. Common mitigation methods\nsuch as fine-tuning and prompt rewriting can only improve dialect performance\nby small margins (< 7%), while potentially incurring significant performance\ndegradation in Standard American English (SAE). To this end, we design a\ngeneral encoder-based mitigation strategy for multimodal generative models. Our\nmethod teaches the model to recognize new dialect features while preserving SAE\nperformance. Experiments on models such as Stable Diffusion 1.5 show that our\nmethod is able to simultaneously raise performance on five dialects to be on\npar with SAE (+34.4%), while incurring near zero cost to SAE performance.",
    "published": "2025-10-16T17:56:55Z",
    "updated": "2025-10-16T17:56:55Z",
    "link": "http://arxiv.org/pdf/2510.14949v1.pdf",
    "category": [
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Yu Zhou",
      "Sohyun An",
      "Haikang Deng",
      "Da Yin",
      "Clark Peng",
      "Cho-Jui Hsieh",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13796v2",
    "title": "The Mechanistic Emergence of Symbol Grounding in Language Models",
    "summary": "Symbol grounding (Harnad, 1990) describes how symbols such as words acquire\ntheir meanings by connecting to real-world sensorimotor experiences. Recent\nwork has shown preliminary evidence that grounding may emerge in\n(vision-)language models trained at scale without using explicit grounding\nobjectives. Yet, the specific loci of this emergence and the mechanisms that\ndrive it remain largely unexplored. To address this problem, we introduce a\ncontrolled evaluation framework that systematically traces how symbol grounding\narises within the internal computations through mechanistic and causal\nanalysis. Our findings show that grounding concentrates in middle-layer\ncomputations and is implemented through the aggregate mechanism, where\nattention heads aggregate the environmental ground to support the prediction of\nlinguistic forms. This phenomenon replicates in multimodal dialogue and across\narchitectures (Transformers and state-space models), but not in unidirectional\nLSTMs. Our results provide behavioral and mechanistic evidence that symbol\ngrounding can emerge in language models, with practical implications for\npredicting and potentially controlling the reliability of generation.",
    "published": "2025-10-15T17:56:15Z",
    "updated": "2025-10-16T17:51:48Z",
    "link": "http://arxiv.org/pdf/2510.13796v2.pdf",
    "category": [
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Shuyu Wu",
      "Ziqiao Ma",
      "Xiaoxi Luo",
      "Yidong Huang",
      "Josue Torres-Fonseca",
      "Freda Shi",
      "Joyce Chai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14937v1",
    "title": "AI-Powered Early Diagnosis of Mental Health Disorders from Real-World\n  Clinical Conversations",
    "summary": "Mental health disorders remain among the leading cause of disability\nworldwide, yet conditions such as depression, anxiety, and Post-Traumatic\nStress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to\nsubjective assessments, limited clinical resources, and stigma and low\nawareness. In primary care settings, studies show that providers misidentify\ndepression or anxiety in over 60% of cases, highlighting the urgent need for\nscalable, accessible, and context-aware diagnostic tools that can support early\ndetection and intervention. In this study, we evaluate the effectiveness of\nmachine learning models for mental health screening using a unique dataset of\n553 real-world, semistructured interviews, each paried with ground-truth\ndiagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We\nbenchmark multiple model classes, including zero-shot prompting with GPT-4.1\nMini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank\nAdaptation (LoRA). Our models achieve over 80% accuracy across diagnostic\ncategories, with especially strongperformance on PTSD (up to 89% accuracy and\n98% recall). We also find that using shorter context, focused context segments\nimproves recall, suggesting that focused narrative cues enhance detection\nsensitivity. LoRA fine-tuning proves both efficient and effective, with\nlower-rank configurations (e.g., rank 8 and 16) maintaining competitive\nperformance across evaluation metrics. Our results demonstrate that LLM-based\nmodels can offer substantial improvements over traditional self-report\nscreening tools, providing a path toward low-barrier, AI-powerd early\ndiagnosis. This work lays the groundwork for integrating machine learning into\nreal-world clinical workflows, particularly in low-resource or high-stigma\nenvironments where access to timely mental health care is most limited.",
    "published": "2025-10-16T17:50:04Z",
    "updated": "2025-10-16T17:50:04Z",
    "link": "http://arxiv.org/pdf/2510.14937v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jianfeng Zhu",
      "Julina Maharjan",
      "Xinyu Li",
      "Karin G. Coifman",
      "Ruoming Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.02287v2",
    "title": "Seeing Through Green: Text-Based Classification and the Firm's Returns\n  from Green Patents",
    "summary": "This paper introduces Natural Language Processing for identifying ``true''\ngreen patents from official supporting documents. We start our training on\nabout 12.4 million patents that had been classified as green from previous\nliterature. Thus, we train a simple neural network to enlarge a baseline\ndictionary through vector representations of expressions related to\nenvironmental technologies. After testing, we find that ``true'' green patents\nrepresent about 20\\% of the total of patents classified as green from previous\nliterature. We show heterogeneity by technological classes, and then check that\n`true' green patents are about 1\\% less cited by following inventions. In the\nsecond part of the paper, we test the relationship between patenting and a\ndashboard of firm-level financial accounts in the European Union. After\ncontrolling for reverse causality, we show that holding at least one ``true''\ngreen patent raises sales, market shares, and productivity. If we restrict the\nanalysis to high-novelty ``true'' green patents, we find that they also yield\nhigher profits. Our findings underscore the importance of using text analyses\nto gauge finer-grained patent classifications that are useful for policymaking\nin different domains.",
    "published": "2025-07-03T03:51:33Z",
    "updated": "2025-10-16T17:39:48Z",
    "link": "http://arxiv.org/pdf/2507.02287v2.pdf",
    "category": [
      "econ.GN",
      "cs.CL",
      "q-fin.EC"
    ],
    "authors": [
      "Lapo Santarlasci",
      "Armando Rungi",
      "Antonio Zinilli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14915v1",
    "title": "Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent\n  Generation",
    "summary": "Retrieval-Augmented Generation (RAG) systems leverage Large Language Models\n(LLMs) to generate accurate and reliable responses that are grounded in\nretrieved context. However, LLMs often generate inconsistent outputs for\nsemantically equivalent inputs, a problem compounded by the scarcity of\nconsistency-focused training data and the limitations of current fine-tuning\ntechniques in enhancing output consistency. We propose a new approach combining\nsystematic synthetic data generation, triplet loss for better embeddings, and a\nnovel layer-wise model merging approach. Using consistency-aware weights\nderived from intermediate layer activations, our method effectively integrates\nknowledge from specialized models. Experimental results how that our merged\nmodel significantly enhances output consistency, achieving a ~47.5\\%\nimprovement in response similarity over the baseline, thus offering a practical\nsolution for increasing the reliability of an industrial RAG system.",
    "published": "2025-10-16T17:30:28Z",
    "updated": "2025-10-16T17:30:28Z",
    "link": "http://arxiv.org/pdf/2510.14915v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xujun Peng",
      "Anoop Kumar",
      "Jingyu Wu",
      "Parker Glenn",
      "Daben Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.06948v2",
    "title": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning",
    "summary": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach suffers from catastrophic forgetting: second-stage\nRL gradually loses SFT-acquired behaviors and inefficiently explores new\npatterns. This study introduces a novel method for learning reasoning models\nthat employs bilevel optimization to facilitate better cooperation between\nthese training paradigms. By conditioning the SFT objective on the optimal RL\npolicy, our approach enables SFT to meta-learn how to guide RL's optimization\nprocess. During training, the lower level performs RL updates while\nsimultaneously receiving SFT supervision, and the upper level explicitly\nmaximizes the cooperative gain-the performance advantage of joint SFT-RL\ntraining over RL alone. Empirical evaluations on five reasoning benchmarks\ndemonstrate that our method consistently outperforms baselines and achieves a\nbetter balance between effectiveness and efficiency.",
    "published": "2025-09-08T17:58:02Z",
    "updated": "2025-10-16T17:30:21Z",
    "link": "http://arxiv.org/pdf/2509.06948v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Liang Chen",
      "Xueting Han",
      "Li Shen",
      "Jing Bai",
      "Kam-Fai Wong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14885v1",
    "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition\n  Capabilities of Multimodal Large Language Models with Answer Extraction",
    "summary": "Despite the renewed interest in zero-shot visual classification due to the\nrise of Multimodal Large Language Models (MLLMs), the problem of evaluating\nfree-form responses of auto-regressive models remains a persistent challenge.\nMost existing works focus on language-only tasks or don't consider Multiple\nChoice Questions (MCQs) beyond 5-way options, both of which are critical\ncapabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where\nchoice counts are in the hundreds to thousands and the choices are highly\nrelated. Furthermore, in this highly multi-way MCQ setting it is not clear how\nto extend LLM choice extraction to retrieval-based problems, where computing\nprobabilities over the choice set is computationally costly. In this work we\ninvestigate nlg2choice, a simple two-stage method which first asks the MLLM an\nopen-ended question for the task with minimal constraints, then uses text-only\nconstrained decoding to predict the most likely choice. In retrieval settings,\nwe compute the probability of the constrained response taking that choice with\nan early stopping method to significantly improve throughput. Our results show\nimprovement over a suite of seven fine-grained visual datasets when evaluating\nin terms of classification and retrieval, and show that this performance holds\nover the various ways that users of LLMs can implement tasks in natural\nlanguage.",
    "published": "2025-10-16T17:04:25Z",
    "updated": "2025-10-16T17:04:25Z",
    "link": "http://arxiv.org/pdf/2510.14885v1.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Logan Lawrence",
      "Oindrila Saha",
      "Megan Wei",
      "Chen Sun",
      "Subhransu Maji",
      "Grant Van Horn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14871v1",
    "title": "From Loop Nests to Silicon: Mapping AI Workloads onto AMD NPUs with\n  MLIR-AIR",
    "summary": "General-purpose compilers abstract away parallelism, locality, and\nsynchronization, limiting their effectiveness on modern spatial architectures.\nAs modern computing architectures increasingly rely on fine-grained control\nover data movement, execution order, and compute placement for performance,\ncompiler infrastructure must provide explicit mechanisms for orchestrating\ncompute and data to fully exploit such architectures. We introduce MLIR-AIR, a\nnovel, open-source compiler stack built on MLIR that bridges the semantic gap\nbetween high-level workloads and fine-grained spatial architectures such as\nAMD's NPUs. MLIR-AIR defines the AIR dialect, which provides structured\nrepresentations for asynchronous and hierarchical operations across compute and\nmemory resources. AIR primitives allow the compiler to orchestrate spatial\nscheduling, distribute computation across hardware regions, and overlap\ncommunication with computation without relying on ad hoc runtime coordination\nor manual scheduling. We demonstrate MLIR-AIR's capabilities through two case\nstudies: matrix multiplication and the multi-head attention block from the\nLLaMA 2 model. For matrix multiplication, MLIR-AIR achieves up to 78.7% compute\nefficiency and generates implementations with performance almost identical to\nstate-of-the-art, hand-optimized matrix multiplication written using the\nlower-level, close-to-metal MLIR-AIE framework. For multi-head attention, we\ndemonstrate that the AIR interface supports fused implementations using\napproximately 150 lines of code, enabling tractable expression of complex\nworkloads with efficient mapping to spatial hardware. MLIR-AIR transforms\nhigh-level structured control flow into spatial programs that efficiently\nutilize the compute fabric and memory hierarchy of an NPU, leveraging\nasynchronous execution, tiling, and communication overlap through\ncompiler-managed scheduling.",
    "published": "2025-10-16T16:49:05Z",
    "updated": "2025-10-16T16:49:05Z",
    "link": "http://arxiv.org/pdf/2510.14871v1.pdf",
    "category": [
      "cs.CL",
      "cs.AR",
      "cs.LG"
    ],
    "authors": [
      "Erwei Wang",
      "Samuel Bayliss",
      "Andra Bisca",
      "Zachary Blair",
      "Sangeeta Chowdhary",
      "Kristof Denolf",
      "Jeff Fifield",
      "Brandon Freiberger",
      "Erika Hunhoff",
      "Phil James-Roxby",
      "Jack Lo",
      "Joseph Melber",
      "Stephen Neuendorffer",
      "Eddie Richter",
      "Andre Rosti",
      "Javier Setoain",
      "Gagandeep Singh",
      "Endri Taka",
      "Pranathi Vasireddy",
      "Zhewen Yu",
      "Niansong Zhang",
      "Jinming Zhuang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03567v3",
    "title": "Machine Unlearning Meets Adversarial Robustness via Constrained\n  Interventions on LLMs",
    "summary": "With the increasing adoption of Large Language Models (LLMs), more\ncustomization is needed to ensure privacy-preserving and safe generation. We\naddress this objective from two critical aspects: unlearning of sensitive\ninformation and robustness to jail-breaking attacks. We investigate various\nconstrained optimization formulations that address both aspects in a\n\\emph{unified manner}, by finding the smallest possible interventions on LLM\nweights that either make a given vocabulary set unreachable or embed the LLM\nwith robustness to tailored attacks by shifting part of the weights to a\n\\emph{safer} region. Beyond unifying two key properties, this approach\ncontrasts with previous work in that it doesn't require an oracle classifier\nthat is typically not available or represents a computational overhead.\nSurprisingly, we find that the simplest point-wise constraint-based\nintervention we propose leads to better performance than max-min interventions,\nwhile having a lower computational cost. Comparison against state-of-the-art\ndefense methods demonstrates superior performance of the proposed approach.",
    "published": "2025-10-03T23:32:21Z",
    "updated": "2025-10-16T16:42:58Z",
    "link": "http://arxiv.org/pdf/2510.03567v3.pdf",
    "category": [
      "cs.LG",
      "cs.CL",
      "cs.CR",
      "cs.CY",
      "math.OC"
    ],
    "authors": [
      "Fatmazohra Rezkellah",
      "Ramzi Dakhmouche"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14865v1",
    "title": "Midtraining Bridges Pretraining and Posttraining Distributions",
    "summary": "Recently, many language models have been pretrained with a \"midtraining\"\nphase, in which higher quality, often instruction-formatted data, is mixed in\nat the end of pretraining. Despite the popularity of this practice, there is\nlittle scientific understanding of this phase of model training or why it is\neffective. In this work, we conduct the first systematic investigation of\nmidtraining through controlled experiments with language models pretrained from\nscratch and fine-tuned on supervised finetuning datasets in different domains.\nWe find that when compared after supervised fine-tuning, the effectiveness of\nmidtraining is highest in the math and code domains, where midtraining can best\nreduce the syntactic gap between pretraining and posttraining data. In these\ncases, midtraining consistently outperforms continued pretraining in both\nin-domain validation loss as well as pretraining data forgetting after\nposttraining. We conduct ablations on the starting time of the midtraining\nphase and mixture weights of the midtraining data, using code midtraining as a\ncase study, and find that timing has a greater impact than mixture weights,\nwith earlier introduction of specialized data, yielding greater benefits\nin-domain as well as preserving general language modeling better. These\nfindings establish midtraining as a domain adaptation technique that compared\nto continued pretraining yields better performance through reduced forgetting.",
    "published": "2025-10-16T16:39:52Z",
    "updated": "2025-10-16T16:39:52Z",
    "link": "http://arxiv.org/pdf/2510.14865v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Emmy Liu",
      "Graham Neubig",
      "Chenyan Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14853v1",
    "title": "Rewiring Experts on the Fly:Continuous Rerouting for Better Online\n  Adaptation in Mixture-of-Expert models",
    "summary": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse\nexpert activation, but often suffer from suboptimal routing decisions due to\ndistribution shifts in deployment. While existing test-time adaptation methods\ncould potentially address these issues, they primarily focus on dense models\nand require access to external data, limiting their practical applicability to\nMoE architectures. However, we find that, instead of relying on reference data,\nwe can optimize MoE expert selection on-the-fly based only on input context. As\nsuch, we propose \\textit{a data-free, online test-time framework} that\ncontinuously adapts MoE routing decisions during text generation without\nexternal supervision or data. Our method cycles between two phases: During the\nprefill stage, and later in regular intervals, we optimize the routing\ndecisions of the model using self-supervision based on the already generated\nsequence. Then, we generate text as normal, maintaining the modified router\nuntil the next adaption. We implement this through lightweight additive vectors\nthat only update router logits in selected layers, maintaining computational\nefficiency while preventing over-adaptation. The experimental results show\nconsistent performance gains on challenging reasoning tasks while maintaining\nrobustness to context shifts. For example, our method achieves a 5.5\\%\nimprovement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play\nproperty, our method naturally complements existing test-time scaling\ntechniques, e.g., achieving 6\\% average gains when incorporated with\nself-consistency on DeepSeek-V2-Lite.",
    "published": "2025-10-16T16:24:36Z",
    "updated": "2025-10-16T16:24:36Z",
    "link": "http://arxiv.org/pdf/2510.14853v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Guinan Su",
      "Yanwu Yang",
      "Li Shen",
      "Lu Yin",
      "Shiwei Liu",
      "Jonas Geiping"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14824v1",
    "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better\n  Multimodal LLM Reranking",
    "summary": "In information retrieval, training reranking models mainly focuses on two\ntypes of objectives: metric learning (e.g. contrastive loss to increase the\npredicted scores on relevant query-document pairs) and classification (binary\nlabel prediction of relevance vs. irrelevance). For BERT-style encoders,\nvarious studies have shown that contrastive learning (CL) can be more effective\nthan discriminative (classification) learning. However, for large language\nmodels (LLMs), classification via supervised fine-tuning (SFT), which predicts\n''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears\nmore promising as it aligns well with the generative nature of LLMs. This\ndivergence raises a central question: which objective is intrinsically better\nsuited to LLM-based reranking, and what mechanism underlies the difference? In\nthis work, we conduct a comprehensive comparison and analysis between CL and\nSFT for reranking, taking the universal multimodal retrieval (UMR) as the\nexperimental playground. We first decompose the objectives into two components:\nweight, which controls the magnitude of those updates, and direction, which\nguides the model updates, then present a unified framework for understanding\ntheir interactions. Through probing experiments, we find that SFT provides a\nsubstantially stronger weighting scheme than CL, whereas the preferred scoring\ndirection shows no clear winner. Taken together, these results point to a\nconsistent advantage of SFT over CL for LLM reranking. To further validate our\nfindings, we conduct large-scale training with SFT and present new\nstate-of-the-art rerankers on the MRB benchmark. We also provide ablations on\nSFT settings and expect our findings to benefit future research and\napplications in this area.",
    "published": "2025-10-16T16:02:27Z",
    "updated": "2025-10-16T16:02:27Z",
    "link": "http://arxiv.org/pdf/2510.14824v1.pdf",
    "category": [
      "cs.CL",
      "cs.CV",
      "cs.IR"
    ],
    "authors": [
      "Ziqi Dai",
      "Xin Zhang",
      "Mingxin Li",
      "Yanzhao Zhang",
      "Dingkun Long",
      "Pengjun Xie",
      "Meishan Zhang",
      "Wenjie Li",
      "Min Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24834v2",
    "title": "Multilinguality Does not Make Sense: Investigating Factors Behind\n  Zero-Shot Transfer in Sense-Aware Tasks",
    "summary": "Cross-lingual transfer is central to modern NLP, enabling models to perform\ntasks in languages different from those they were trained on. A common\nassumption is that training on more languages improves zero-shot transfer. We\ntest this on sense-aware tasks-polysemy and lexical semantic change-and find\nthat multilinguality is not necessary for effective transfer. Our large-scale\nanalysis across 28 languages reveals that other factors, such as differences in\npretraining and fine-tuning data and evaluation artifacts, better explain the\nperceived benefits of multilinguality. We also release fine-tuned models and\nprovide empirical baselines to support future research. While focused on two\nsense-aware tasks, our findings offer broader insights into cross-lingual\ntransfer, especially for low-resource languages.",
    "published": "2025-05-30T17:36:20Z",
    "updated": "2025-10-16T15:23:18Z",
    "link": "http://arxiv.org/pdf/2505.24834v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Roksana Goworek",
      "Haim Dubossarsky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14756v1",
    "title": "Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware\n  Code",
    "summary": "Large Language Models (LLMs) are increasingly used to automate hardware\ndesign tasks, including the generation of Verilog code. While early benchmarks\nfocus primarily on functional correctness, efficient hardware design demands\nadditional optimization for synthesis metrics such as area, delay, and power.\nExisting benchmarks fall short in evaluating these aspects comprehensively:\nthey often lack optimized baselines or testbenches for verification. To address\nthese gaps, we present Pluto, a benchmark and evaluation framework designed to\nassess the efficiency of LLM-generated Verilog designs. Pluto presents a\ncomprehensive evaluation set of 114 problems with self-checking testbenches and\nmultiple Pareto-optimal reference implementations. Experimental results show\nthat state-of-the-art LLMs can achieve high functional correctness, reaching\n78.3\\% at pass@1, but their synthesis efficiency still lags behind\nexpert-crafted implementations, with area efficiency of 63.8\\%, delay\nefficiency of 65.9\\%, and power efficiency of 64.0\\% at eff@1. This highlights\nthe need for efficiency-aware evaluation frameworks such as Pluto to drive\nprogress in hardware-focused LLM research.",
    "published": "2025-10-16T14:57:01Z",
    "updated": "2025-10-16T14:57:01Z",
    "link": "http://arxiv.org/pdf/2510.14756v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Manar Abdelatty",
      "Maryam Nouh",
      "Jacob K. Rosenstein",
      "Sherief Reda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14738v1",
    "title": "AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal\n  Reasoning",
    "summary": "Multimodal large language models (MLLMs) have rapidly advanced from\nperception tasks to complex multi-step reasoning, yet reinforcement learning\nwith verifiable rewards (RLVR) often leads to spurious reasoning since only the\nfinal-answer correctness is rewarded. To address this limitation, we propose\nAutoRubric-R1V, a framework that integrates RLVR with process-level supervision\nthrough automatically collected rubric-based generative rewards. Our key\ninnovation lies in a scalable self-aggregation method that distills consistent\nreasoning checkpoints from successful trajectories, enabling problem-specific\nrubric construction without human annotation or stronger teacher models. By\njointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves\nstate-of-the-art performance on six multimodal reasoning benchmarks and\nsubstantially improves reasoning faithfulness in dedicated evaluations.",
    "published": "2025-10-16T14:40:02Z",
    "updated": "2025-10-16T14:40:02Z",
    "link": "http://arxiv.org/pdf/2510.14738v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Mengzhao Jia",
      "Zhihan Zhang",
      "Ignacio Cases",
      "Zheyuan Liu",
      "Meng Jiang",
      "Peng Qi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14718v1",
    "title": "Speculative Model Risk in Healthcare AI: Using Storytelling to Surface\n  Unintended Harms",
    "summary": "Artificial intelligence (AI) is rapidly transforming healthcare, enabling\nfast development of tools like stress monitors, wellness trackers, and mental\nhealth chatbots. However, rapid and low-barrier development can introduce risks\nof bias, privacy violations, and unequal access, especially when systems ignore\nreal-world contexts and diverse user needs. Many recent methods use AI to\ndetect risks automatically, but this can reduce human engagement in\nunderstanding how harms arise and who they affect. We present a human-centered\nframework that generates user stories and supports multi-agent discussions to\nhelp people think creatively about potential benefits and harms before\ndeployment. In a user study, participants who read stories recognized a broader\nrange of harms, distributing their responses more evenly across all 13 harm\ntypes. In contrast, those who did not read stories focused primarily on privacy\nand well-being (58.3%). Our findings show that storytelling helped participants\nspeculate about a broader range of harms and benefits and think more creatively\nabout AI's impact on users.",
    "published": "2025-10-16T14:18:31Z",
    "updated": "2025-10-16T14:18:31Z",
    "link": "http://arxiv.org/pdf/2510.14718v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xingmeng Zhao",
      "Dan Schumacher",
      "Veronica Rammouz",
      "Anthony Rios"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.03550v3",
    "title": "Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via\n  Internal Representations",
    "summary": "The growing scale of evaluation tasks has led to the widespread adoption of\nautomated evaluation using LLMs, a paradigm known as \"LLM-as-a-judge\". However,\nimproving its alignment with human preferences without complex prompts or\nfine-tuning remains challenging. Previous studies mainly optimize based on\nshallow outputs, overlooking rich cross-layer representations. In this work,\nmotivated by preliminary findings that middle-to-upper layers encode\nsemantically and task-relevant representations that are often more aligned with\nhuman judgments than the final layer, we propose LAGER, a post-hoc,\nplug-and-play framework for improving the alignment of LLM-as-a-Judge\npoint-wise evaluations with human scores by leveraging internal\nrepresentations. LAGER produces fine-grained judgment scores by aggregating\ncross-layer score-token logits and computing the expected score from a\nsoftmax-based distribution, while keeping the LLM backbone frozen and ensuring\nno impact on the inference process. LAGER fully leverages the complementary\ninformation across different layers, overcoming the limitations of relying\nsolely on the final layer. We evaluate our method on the standard alignment\nbenchmarks Flask, HelpSteer, and BIGGen using Spearman correlation, and find\nthat LAGER achieves improvements of up to 7.5% over the best baseline across\nthese benchmarks. Without reasoning steps, LAGER matches or outperforms\nreasoning-based methods. Experiments on downstream applications, such as data\nselection and emotional understanding, further show the generalization of\nLAGER.",
    "published": "2025-08-05T15:18:36Z",
    "updated": "2025-10-16T14:16:18Z",
    "link": "http://arxiv.org/pdf/2508.03550v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Peng Lai",
      "Jianjie Zheng",
      "Sijie Cheng",
      "Yun Chen",
      "Peng Li",
      "Yang Liu",
      "Guanhua Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13750v2",
    "title": "Confidence-Based Response Abstinence: Improving LLM Trustworthiness via\n  Activation-Based Uncertainty Estimation",
    "summary": "We propose a method for confidence estimation in retrieval-augmented\ngeneration (RAG) systems that aligns closely with the correctness of large\nlanguage model (LLM) outputs. Confidence estimation is especially critical in\nhigh-stakes domains such as finance and healthcare, where the cost of an\nincorrect answer outweighs that of not answering the question. Our approach\nextends prior uncertainty quantification methods by leveraging raw feed-forward\nnetwork (FFN) activations as auto-regressive signals, avoiding the information\nloss inherent in token logits and probabilities after projection and softmax\nnormalization. We model confidence prediction as a sequence classification\ntask, and regularize training with a Huber loss term to improve robustness\nagainst noisy supervision. Applied in a real-world financial industry\ncustomer-support setting with complex knowledge bases, our method outperforms\nstrong baselines and maintains high accuracy under strict latency constraints.\nExperiments on Llama 3.1 8B model show that using activations from only the\n16th layer preserves accuracy while reducing response latency. Our results\ndemonstrate that activation-based confidence modeling offers a scalable,\narchitecture-aware path toward trustworthy RAG deployment.",
    "published": "2025-10-15T16:55:56Z",
    "updated": "2025-10-16T13:58:27Z",
    "link": "http://arxiv.org/pdf/2510.13750v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhiqi Huang",
      "Vivek Datla",
      "Chenyang Zhu",
      "Alfy Samuel",
      "Daben Liu",
      "Anoop Kumar",
      "Ritesh Soni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.00877v3",
    "title": "EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes",
    "summary": "Retrieval-Augmented Generation (RAG) has advanced open-domain question\nanswering by incorporating external information into model reasoning. However,\neffectively leveraging external information to enhance reasoning presents the\nfollowing challenges: (1) low signal-to-noise ratio, where answer-supportive\nexternal information is diluted by irrelevant material, and (2) error\naccumulation, which arises in multi-hop reasoning when incomplete or misleading\ninformation is incorporated. To address these challenges, we introduce\nEviNote-RAG, a framework that follows a retrieve-note-answer workflow. Instead\nof reasoning directly over raw external information, the model first produces\nSupportive-Evidence Notes (SENs), which concisely preserve answer-critical\ninformation and explicitly mark key and uncertainty information to improve\naccuracy. We further design an entailment-based Evidence Quality Reward (EQR)\nto ensure that SENs are logically sufficient to derive the final answer,\nthereby enhancing SENs' quality. Experiments on both in-domain and\nout-of-domain QA benchmarks show that EviNote-RAG achieves state-of-the-art\nperformance, improving answer accuracy, training stability, robustness, and\nefficiency. In particular, it yields relative F1 gains of 20% on HotpotQA\n(+0.093), 40% on Bamboogle (+0.151), and 91% on 2Wiki (+0.256), benefiting from\nimprovements in the reasoning process.",
    "published": "2025-08-31T14:44:45Z",
    "updated": "2025-10-16T13:57:27Z",
    "link": "http://arxiv.org/pdf/2509.00877v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yuqin Dai",
      "Guoqing Wang",
      "Yuan Wang",
      "Kairan Dou",
      "Kaichen Zhou",
      "Zhanwei Zhang",
      "Shuo Yang",
      "Fei Tang",
      "Jun Yin",
      "Pengyu Zeng",
      "Zhenzhe Ying",
      "Can Yi",
      "Changhua Meng",
      "Yuchen Zhou",
      "Yongliang Shen",
      "Shuai Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14662v1",
    "title": "Semantic Prosody in Machine Translation: the English-Chinese Case of\n  Passive Structures",
    "summary": "Semantic prosody is a collocational meaning formed through the co-occurrence\nof a linguistic unit and a consistent series of collocates, which should be\ntreated separately from semantic meaning. Since words that are literal\ntranslations of each other may have different semantic prosody, more attention\nshould be paid to this linguistic property to generate accurate translations.\nHowever, current machine translation models cannot handle this problem. To\nbridge the gap, we propose an approach to teach machine translation models\nabout semantic prosody of a specific structure. We focus on Chinese BEI\npassives and create a dataset of English-Chinese sentence pairs with the\npurpose of demonstrating the negative semantic prosody of BEI passives. Then we\nfine-tune OPUS-MT, NLLB-600M and mBART50 models with our dataset for the\nEnglish-Chinese translation task. Our results show that fine-tuned MT models\nperform better on using BEI passives for translating unfavourable content and\navoid using it for neutral and favourable content. Also, in NLLB-600M, which is\na multilingual model, this knowledge of semantic prosody can be transferred\nfrom English-Chinese translation to other language pairs, such as\nSpanish-Chinese.",
    "published": "2025-10-16T13:16:59Z",
    "updated": "2025-10-16T13:16:59Z",
    "link": "http://arxiv.org/pdf/2510.14662v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xinyue Ma",
      "Pol Pastells",
      "Mireia Farrs",
      "Mariona Taul"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14640v1",
    "title": "Intent Clustering with Shared Pseudo-Labels",
    "summary": "In this paper, we propose an intuitive, training-free and label-free method\nfor intent clustering that makes minimal assumptions using lightweight and\nopen-source LLMs. Many current approaches rely on commercial LLMs, which are\ncostly, and offer limited transparency. Additionally, their methods often\nexplicitly depend on knowing the number of clusters in advance, which is often\nnot the case in realistic settings. To address these challenges, instead of\nasking the LLM to match similar text directly, we first ask it to generate\npseudo-labels for each text, and then perform multi-label classification in\nthis pseudo-label set for each text. This approach is based on the hypothesis\nthat texts belonging to the same cluster will share more labels, and will\ntherefore be closer when encoded into embeddings. These pseudo-labels are more\nhuman-readable than direct similarity matches. Our evaluation on four benchmark\nsets shows that our approach achieves results comparable to and better than\nrecent baselines, while remaining simple and computationally efficient. Our\nfindings indicate that our method can be applied in low-resource scenarios and\nis stable across multiple models and datasets.",
    "published": "2025-10-16T12:54:40Z",
    "updated": "2025-10-16T12:54:40Z",
    "link": "http://arxiv.org/pdf/2510.14640v1.pdf",
    "category": [
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "I-Fan Lin",
      "Faegheh Hasibi",
      "Suzan Verberne"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13430v2",
    "title": "Evaluating Arabic Large Language Models: A Survey of Benchmarks,\n  Methods, and Gaps",
    "summary": "This survey provides the first systematic review of Arabic LLM benchmarks,\nanalyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains,\ncultural understanding, and specialized capabilities. We propose a taxonomy\norganizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and\nDialects, and Target-Specific evaluations. Our analysis reveals significant\nprogress in benchmark diversity while identifying critical gaps: limited\ntemporal evaluation, insufficient multi-turn dialogue assessment, and cultural\nmisalignment in translated datasets. We examine three primary approaches:\nnative collection, translation, and synthetic generation discussing their\ntrade-offs regarding authenticity, scale, and cost. This work serves as a\ncomprehensive reference for Arabic NLP researchers, providing insights into\nbenchmark methodologies, reproducibility standards, and evaluation metrics\nwhile offering recommendations for future development.",
    "published": "2025-10-15T11:25:33Z",
    "updated": "2025-10-16T12:22:13Z",
    "link": "http://arxiv.org/pdf/2510.13430v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ahmed Alzubaidi",
      "Shaikha Alsuwaidi",
      "Basma El Amel Boussaha",
      "Leen AlQadi",
      "Omar Alkaabi",
      "Mohammed Alyafeai",
      "Hamza Alobeidli",
      "Hakim Hacid"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07037v3",
    "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era\n  of Large Language Models",
    "summary": "Code-switching (CSW), the alternation of languages and scripts within a\nsingle utterance, remains a fundamental challenge for multilingual NLP, even\namidst the rapid advances of large language models (LLMs). Most LLMs still\nstruggle with mixed-language inputs, limited CSW datasets, and evaluation\nbiases, hindering deployment in multilingual societies. This survey provides\nthe first comprehensive analysis of CSW-aware LLM research, reviewing 308\nstudies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+\nlanguages. We classify recent advances by architecture, training strategy, and\nevaluation methodology, outlining how LLMs have reshaped CSW modeling and what\nchallenges persist. The paper concludes with a roadmap emphasizing the need for\ninclusive datasets, fair evaluation, and linguistically grounded models to\nachieve truly multilingual intelligence. A curated collection of all resources\nis maintained at https://github.com/lingo-iitgn/awesome-code-mixing/.",
    "published": "2025-10-08T14:04:14Z",
    "updated": "2025-10-16T11:58:33Z",
    "link": "http://arxiv.org/pdf/2510.07037v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Rajvee Sheth",
      "Samridhi Raj Sinha",
      "Mahavir Patil",
      "Himanshu Beniwal",
      "Mayank Singh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.11401v3",
    "title": "Following the Autoregressive Nature of LLM Embeddings via Compression\n  and Alignment",
    "summary": "A new trend uses LLMs as dense text encoders via contrastive learning.\nHowever, since LLM embeddings predict the probability distribution of the next\ntoken, they are inherently generative and distributive, conflicting with\ncontrastive learning, which requires embeddings to capture full-text semantics\nand align via cosine similarity. This discrepancy hinders the full utilization\nof LLMs' pre-training capabilities, resulting in inefficient learning. In\nresponse to this issue, we propose AutoRegEmbed, a new contrastive learning\nmethod built on embedding conditional probability distributions, which\nintegrates two core tasks: information compression and conditional distribution\nalignment. The information compression task encodes text into the embedding\nspace, ensuring that the embedding vectors capture global semantics. The\nconditional distribution alignment task focuses on aligning text embeddings\nwith positive samples embeddings by leveraging the conditional distribution of\nembeddings while simultaneously reducing the likelihood of generating negative\nsamples from text embeddings, thereby achieving embedding alignment and\nuniformity. Experimental results demonstrate that our method significantly\noutperforms traditional contrastive learning approaches and achieves\nperformance comparable to state-of-the-art models when using the same amount of\ndata.",
    "published": "2025-02-17T03:36:25Z",
    "updated": "2025-10-16T11:55:56Z",
    "link": "http://arxiv.org/pdf/2502.11401v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jingcheng Deng",
      "Zhongtao Jiang",
      "Liang Pang",
      "Liwei Chen",
      "Kun Xu",
      "Zihao Wei",
      "Huawei Shen",
      "Xueqi Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.14734v3",
    "title": "Sentence Smith: Controllable Edits for Evaluating Text Embeddings",
    "summary": "Controllable and transparent text generation has been a long-standing goal in\nNLP. Almost as long-standing is a general idea for addressing this challenge:\nParsing text to a symbolic representation, and generating from it. However,\nearlier approaches were hindered by parsing and generation insufficiencies.\nUsing modern parsers and a safety supervision mechanism, we show how close\ncurrent methods come to this goal. Concretely, we propose the Sentence Smith\nframework for English, which has three steps: 1. Parsing a sentence into a\nsemantic graph. 2. Applying human-designed semantic manipulation rules. 3.\nGenerating text from the manipulated graph. A final entailment check (4.)\nverifies the validity of the applied transformation. To demonstrate our\nframework's utility, we use it to induce hard negative text pairs that\nchallenge text embedding models. Since the controllable generation makes it\npossible to clearly isolate different types of semantic shifts, we can evaluate\ntext embedding models in a fine-grained way, also addressing an issue in\ncurrent benchmarking where linguistic phenomena remain opaque. Human validation\nconfirms that our transparent generation process produces texts of good\nquality. Notably, our way of generation is very resource-efficient, since it\nrelies only on smaller neural networks.",
    "published": "2025-02-20T17:00:19Z",
    "updated": "2025-10-16T11:43:14Z",
    "link": "http://arxiv.org/pdf/2502.14734v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hongji Li",
      "Andrianos Michail",
      "Reto Gubelmann",
      "Simon Clematide",
      "Juri Opitz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14583v1",
    "title": "Talking Points: Describing and Localizing Pixels",
    "summary": "Vision-language models have achieved remarkable success in cross-modal\nunderstanding. Yet, these models remain limited to object-level or region-level\ngrounding, lacking the capability for pixel-precise keypoint comprehension\nthrough natural language. We introduce a novel framework for pixel level\ngrounding. The framework consists of two complementary components: a Point\nDescriptor that generates rich, contextual descriptions of individual\nkeypoints, and a Point Localizer that regresses precise pixel coordinates from\nthese descriptions. Unlike prior work that relies on templated prompts or\nkeypoint names, our approach produces free-form, coarse-to-fine descriptions\nthat situate keypoints within their visual context. Since there is no available\ndataset to train such a system, we introduce LlamaPointInPart, a carefully\ncurated dataset of 20K+ image-keypoint-description triplets synthesized from\nmultiple vision-language models, capturing multi-scale information from\nscene-level context to visual features around the keypoint. For cross-category\ngeneralization, we optimize the Point Descriptor on AP-10K via GRPO, using the\nfrozen Point Localizer as a reward model to produce descriptions that maximize\nlocalization accuracy. To evaluate our results we establish a new evaluation\nprotocol. Instead of comparing the text description produced by our method to\nthe ground truth, we use the localizer to determine how close is the predicted\npoint generated to the ground truth point. Experiments demonstrate superior\nperformance compared to baseline models on LlamaPointInPart.The bidirectional\nnature of our framework should enable future applications in both\nkeypoint-guided image understanding and language-guided precise localization.\nOur code and dataset are publicly available at\nhttps://github.com/matanr/Talking_Points.",
    "published": "2025-10-16T11:42:03Z",
    "updated": "2025-10-16T11:42:03Z",
    "link": "http://arxiv.org/pdf/2510.14583v1.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Matan Rusanovsky",
      "Shimon Malnick",
      "Shai Avidan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14565v1",
    "title": "Assessing Socio-Cultural Alignment and Technical Safety of Sovereign\n  LLMs",
    "summary": "Recent trends in LLMs development clearly show growing interest in the use\nand application of sovereign LLMs. The global debate over sovereign LLMs\nhighlights the need for governments to develop their LLMs, tailored to their\nunique socio-cultural and historical contexts. However, there remains a\nshortage of frameworks and datasets to verify two critical questions: (1) how\nwell these models align with users' socio-cultural backgrounds, and (2) whether\nthey maintain safety and technical robustness without exposing users to\npotential harms and risks. To address this gap, we construct a new dataset and\nintroduce an analytic framework for extracting and evaluating the\nsocio-cultural elements of sovereign LLMs, alongside assessments of their\ntechnical robustness. Our experimental results demonstrate that while sovereign\nLLMs play a meaningful role in supporting low-resource languages, they do not\nalways meet the popular claim that these models serve their target users well.\nWe also show that pursuing this untested claim may lead to underestimating\ncritical quality attributes such as safety. Our study suggests that advancing\nsovereign LLMs requires a more extensive evaluation that incorporates a broader\nrange of well-grounded and practical criteria.",
    "published": "2025-10-16T11:17:44Z",
    "updated": "2025-10-16T11:17:44Z",
    "link": "http://arxiv.org/pdf/2510.14565v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Kyubyung Chae",
      "Gihoon Kim",
      "Gyuseong Lee",
      "Taesup Kim",
      "Jaejin Lee",
      "Heejin Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.15266v3",
    "title": "Thunder-DeID: Accurate and Efficient De-identification Framework for\n  Korean Court Judgments",
    "summary": "To ensure a balance between open access to justice and personal data\nprotection, the South Korean judiciary mandates the de-identification of court\njudgments before they can be publicly disclosed. However, the current\nde-identification process is inadequate for handling court judgments at scale\nwhile adhering to strict legal requirements. Additionally, the legal\ndefinitions and categorizations of personal identifiers are vague and not\nwell-suited for technical solutions. To tackle these challenges, we propose a\nde-identification framework called Thunder-DeID, which aligns with relevant\nlaws and practices. Specifically, we (i) construct and release the first Korean\nlegal dataset containing annotated judgments along with corresponding lists of\nentity mentions, (ii) introduce a systematic categorization of Personally\nIdentifiable Information (PII), and (iii) develop an end-to-end deep neural\nnetwork (DNN)-based de-identification pipeline. Our experimental results\ndemonstrate that our model achieves state-of-the-art performance in the\nde-identification of court judgments.",
    "published": "2025-06-18T08:41:28Z",
    "updated": "2025-10-16T11:12:43Z",
    "link": "http://arxiv.org/pdf/2506.15266v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Sungeun Hahm",
      "Heejin Kim",
      "Gyuseong Lee",
      "Hyunji Park",
      "Jaejin Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.04575v2",
    "title": "Are LLMs Stable Formal Logic Translators in Logical Reasoning Across\n  Linguistically Diversified Texts?",
    "summary": "Logical reasoning with large language models (LLMs) has received growing\nattention. One mainstream approach translates natural language into formal\nlogic and then applies symbolic solvers for deduction. While effective in many\ntasks, these LLM-based translators often fail to generate consistent symbolic\nrepresentations when the same concept appears in different linguistic forms.\nSuch inconsistencies break logical coherence and lead to solver errors.\nHowever, most existing benchmarks lack this type of linguistic variation, which\nfrequently occurs in real-world text, leaving the problem underexplored. To\naddress this gap, we present SoLT, a benchmark that systematically rewrites\nreasoning datasets into diverse yet logically equivalent forms across multiple\nlevels. Beyond evaluation, SoLT also provides a general method to enrich any\ndataset with linguistic diversity while preserving both meaning and logic. To\nfurther enhance the stability of LLM-based reasoning, we propose MenTaL, which\nexplicitly guides models to build a concept-symbol mapping table during\ntranslation. By linking equivalent expressions to shared symbols, MenTaL\nmaintains consistency and mitigates symbol drift. Experiments on SoLT\ndemonstrate that LLMs indeed suffer from inconsistent symbol mapping under\nlinguistic variation, leading to significant drops in reasoning accuracy.\nMeanwhile, applying MenTaL brings clear and stable performance improvements\nacross diverse inputs. Overall, our findings reveal that overlooking linguistic\ndiversity hides key weaknesses in LLM-based translators, and our work offers a\nstep toward more reliable logical reasoning in varied real-world scenarios. Our\ncode is available at https://github.com/wufeiwuwoshihua/LinguDiver.",
    "published": "2025-06-05T02:49:36Z",
    "updated": "2025-10-16T10:20:19Z",
    "link": "http://arxiv.org/pdf/2506.04575v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Qingchuan Li",
      "Jiatong Li",
      "Zirui Liu",
      "Mingyue Cheng",
      "Yuting Zeng",
      "Qi Liu",
      "Tongxuan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14504v1",
    "title": "Efficient Seq2seq Coreference Resolution Using Entity Representations",
    "summary": "Seq2seq coreference models have introduced a new paradigm for coreference\nresolution by learning to generate text corresponding to coreference labels,\nwithout requiring task-specific parameters. While these models achieve new\nstate-of-the-art performance, they do so at the cost of flexibility and\nefficiency. In particular, they do not efficiently handle incremental settings\nsuch as dialogue, where text must processed sequentially. We propose a\ncompressed representation in order to improve the efficiency of these methods\nin incremental settings. Our method works by extracting and re-organizing\nentity-level tokens, and discarding the majority of other input tokens. On\nOntoNotes, our best model achieves just 0.6 CoNLL F1 points below a\nfull-prefix, incremental baseline while achieving a compression ratio of 1.8.\nOn LitBank, where singleton mentions are annotated, it passes state-of-the-art\nperformance. Our results indicate that discarding a wide portion of tokens in\nseq2seq resolvers is a feasible strategy for incremental coreference\nresolution.",
    "published": "2025-10-16T09:50:03Z",
    "updated": "2025-10-16T09:50:03Z",
    "link": "http://arxiv.org/pdf/2510.14504v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Matt Grenander",
      "Shay B. Cohen",
      "Mark Steedman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.04137v3",
    "title": "Detecting Token-Level Hallucinations Using Variance Signals: A\n  Reference-Free Approach",
    "summary": "Large Language Models (LLMs) have demonstrated impressive generative\ncapabilities across diverse tasks but remain susceptible to hallucinations,\nconfidently generated yet factually incorrect outputs. We introduce a\nreference-free, token-level hallucination detection framework that leverages\nthe variance in token log-probabilities across multiple stochastic generations.\nUnlike prior methods that require ground-truth references or sentence-level\nverification, our approach is model-agnostic, interpretable, and suited for\nreal-time or post-hoc analysis. We evaluate our method on unanswerable question\nprompts from the SQuAD v2 dataset and benchmark across three autoregressive\nmodels of varying scales: GPT-Neo 125M, Falcon 1B, and Mistral 7B. Through both\nquantitative metrics and visual diagnostics, we show that token-level variance\nreliably highlights instability in model outputs and correlates with\nhallucination patterns. Our framework is lightweight, reproducible, and\nadaptable to multiple domains, offering a valuable diagnostic tool for\nanalyzing generative reliability in LLMs.",
    "published": "2025-07-05T19:20:59Z",
    "updated": "2025-10-16T09:42:45Z",
    "link": "http://arxiv.org/pdf/2507.04137v3.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Keshav Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.18668v5",
    "title": "ChartGalaxy: A Dataset for Infographic Chart Understanding and\n  Generation",
    "summary": "Infographic charts are a powerful medium for communicating abstract data by\ncombining visual elements (e.g., charts, images) with textual information.\nHowever, their visual and structural richness poses challenges for large\nvision-language models (LVLMs), which are typically trained on plain charts. To\nbridge this gap, we introduce ChartGalaxy, a million-scale dataset designed to\nadvance the understanding and generation of infographic charts. The dataset is\nconstructed through an inductive process that identifies 75 chart types, 440\nchart variations, and 68 layout templates from real infographic charts and uses\nthem to create synthetic ones programmatically. We showcase the utility of this\ndataset through: 1) improving infographic chart understanding via fine-tuning,\n2) benchmarking code generation for infographic charts, and 3) enabling\nexample-based infographic chart generation. By capturing the visual and\nstructural complexity of real design, ChartGalaxy provides a useful resource\nfor enhancing multimodal reasoning and generation in LVLMs.",
    "published": "2025-05-24T12:06:22Z",
    "updated": "2025-10-16T09:17:24Z",
    "link": "http://arxiv.org/pdf/2505.18668v5.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Zhen Li",
      "Duan Li",
      "Yukai Guo",
      "Xinyuan Guo",
      "Bowen Li",
      "Lanxi Xiao",
      "Shenyu Qiao",
      "Jiashu Chen",
      "Zijian Wu",
      "Hui Zhang",
      "Xinhuan Shu",
      "Shixia Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.21750v4",
    "title": "Adversarial Defence without Adversarial Defence: Enhancing Language\n  Model Robustness via Instance-level Principal Component Removal",
    "summary": "Pre-trained language models (PLMs) have driven substantial progress in\nnatural language processing but remain vulnerable to adversarial attacks,\nraising concerns about their robustness in real-world applications. Previous\nstudies have sought to mitigate the impact of adversarial attacks by\nintroducing adversarial perturbations into the training process, either\nimplicitly or explicitly. While both strategies enhance robustness, they often\nincur high computational costs. In this work, we propose a simple yet effective\nadd-on module that enhances the adversarial robustness of PLMs by removing\ninstance-level principal components, without relying on conventional\nadversarial defences or perturbing the original training data. Our approach\ntransforms the embedding space to approximate Gaussian properties, thereby\nreducing its susceptibility to adversarial perturbations while preserving\nsemantic relationships. This transformation aligns embedding distributions in a\nway that minimises the impact of adversarial noise on decision boundaries,\nenhancing robustness without requiring adversarial examples or costly\ntraining-time augmentation. Evaluations on eight benchmark datasets show that\nour approach improves adversarial robustness while maintaining comparable\nbefore-attack accuracy to baselines, achieving a balanced trade-off between\nrobustness and generalisation.",
    "published": "2025-07-29T12:31:26Z",
    "updated": "2025-10-16T09:14:12Z",
    "link": "http://arxiv.org/pdf/2507.21750v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yang Wang",
      "Chenghao Xiao",
      "Yizhi Li",
      "Stuart E. Middleton",
      "Noura Al Moubayed",
      "Chenghua Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.03867v3",
    "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth",
    "summary": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\" - utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a benchmark dataset of over 1,200+ meticulously curated and diverse\nexamples across English, Mandarin, Spanish, French, Japanese, and Korean. Each\nexample underwent careful expert review to verify its Drivelological\ncharacteristics, involving multiple rounds of discussion and adjudication to\naddress disagreements. Using this dataset, we evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss implied rhetorical functions\naltogether. These findings highlight a deep representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence.",
    "published": "2025-09-04T03:58:55Z",
    "updated": "2025-10-16T09:09:45Z",
    "link": "http://arxiv.org/pdf/2509.03867v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yang Wang",
      "Chenghao Xiao",
      "Chia-Yi Hsiao",
      "Zi Yan Chang",
      "Chi-Li Chen",
      "Tyler Loakman",
      "Chenghua Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.08752v2",
    "title": "Multi-Perspective Stance Detection",
    "summary": "Subjective NLP tasks usually rely on human annotations provided by multiple\nannotators, whose judgments may vary due to their diverse backgrounds and life\nexperiences. Traditional methods often aggregate multiple annotations into a\nsingle ground truth, disregarding the diversity in perspectives that arises\nfrom annotator disagreement. In this preliminary study, we examine the effect\nof including multiple annotations on model accuracy in classification. Our\nmethodology investigates the performance of perspective-aware classification\nmodels in stance detection task and further inspects if annotator disagreement\naffects the model confidence. The results show that multi-perspective approach\nyields better classification performance outperforming the baseline which uses\nthe single label. This entails that designing more inclusive perspective-aware\nAI models is not only an essential first step in implementing responsible and\nethical AI, but it can also achieve superior results than using the traditional\napproaches.",
    "published": "2024-11-13T16:30:41Z",
    "updated": "2025-10-16T09:02:30Z",
    "link": "http://arxiv.org/pdf/2411.08752v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Benedetta Muscato",
      "Praveen Bushipaka",
      "Gizem Gezici",
      "Lucia Passaro",
      "Fosca Giannotti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.03479v4",
    "title": "Women, Infamous, and Exotic Beings: A Comparative Study of Honorific\n  Usages in Wikipedia and LLMs for Bengali and Hindi",
    "summary": "The obligatory use of third-person honorifics is a distinctive feature of\nseveral South Asian languages, encoding nuanced socio-pragmatic cues such as\npower, age, gender, fame, and social distance. In this work, (i) We present the\nfirst large-scale study of third-person honorific pronoun and verb usage across\n10,000 Hindi and Bengali Wikipedia articles with annotations linked to key\nsocio-demographic attributes of the subjects, including gender, age group,\nfame, and cultural origin. (ii) Our analysis uncovers systematic intra-language\nregularities but notable cross-linguistic differences: honorifics are more\nprevalent in Bengali than in Hindi, while non-honorifics dominate while\nreferring to infamous, juvenile, and culturally exotic entities. Notably, in\nboth languages, and more prominently in Hindi, men are more frequently\naddressed with honorifics than women. (iii) To examine whether large language\nmodels (LLMs) internalize similar socio-pragmatic norms, we probe six LLMs\nusing controlled generation and translation tasks over 1,000 culturally\nbalanced entities. We find that LLMs diverge from Wikipedia usage, exhibiting\nalternative preferences in honorific selection across tasks, languages, and\nsocio-demographic attributes. These discrepancies highlight gaps in the\nsocio-cultural alignment of LLMs and open new directions for studying how LLMs\nacquire, adapt, or distort social-linguistic norms. Our code and data are\npublicly available at https://github.com/souro/honorific-wiki-llm",
    "published": "2025-01-07T02:47:59Z",
    "updated": "2025-10-16T08:55:12Z",
    "link": "http://arxiv.org/pdf/2501.03479v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Sourabrata Mukherjee",
      "Atharva Mehta",
      "Sougata Saha",
      "Akhil Arora",
      "Monojit Choudhury"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14453v1",
    "title": "Natural Language Tools: A Natural Language Approach to Tool Calling In\n  Large Language Agents",
    "summary": "We present Natural Language Tools (NLT), a framework that replaces\nprogrammatic JSON tool calling in large language models (LLMs) with natural\nlanguage outputs. By decoupling tool selection from response generation, NLT\neliminates task interference and format constraints that degrade tool call\nperformance. When evaluated across 10 models and 6,400 trials spanning customer\nservice and mental health domains, NLT improves tool calling accuracy by 18.4\npercentage points while reducing output variance by 70%. Open-weight models see\nthe largest gains, surpassing flagship closed-weight alternatives, with\nimplications for model training in both reinforcement learning and supervised\nfine-tuning stages. These improvements persist under prompt perturbations and\nextend tool-calling capabilities to models lacking native support.",
    "published": "2025-10-16T08:52:52Z",
    "updated": "2025-10-16T08:52:52Z",
    "link": "http://arxiv.org/pdf/2510.14453v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Reid T. Johnson",
      "Michelle D. Pain",
      "Jordan D. West"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14438v1",
    "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive\n  Online Exploration for Deep Research Agents",
    "summary": "Deep research web agents not only retrieve information from diverse sources\nsuch as web environments, files, and multimodal inputs, but more importantly,\nthey need to rigorously analyze and aggregate knowledge for insightful\nresearch. However, existing open-source deep research agents predominantly\nfocus on enhancing information-seeking capabilities of web agents to locate\nspecific information, while overlooking the essential need for information\naggregation, which would limit their ability to support in-depth research. We\npropose an Explore to Evolve paradigm to scalably construct verifiable training\ndata for web agents. Begins with proactive online exploration, an agent sources\ngrounded information by exploring the real web. Using the collected evidence,\nthe agent then self-evolves an aggregation program by selecting, composing, and\nrefining operations from 12 high-level logical types to synthesize a verifiable\nQA pair. This evolution from high-level guidance to concrete operations allowed\nus to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K\nwebsites and 11 domains. Based on an open-source agent framework, SmolAgents,\nwe collect supervised fine-tuning trajectories to develop a series of\nfoundation models, WebAggregator. WebAggregator-8B matches the performance of\nGPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text\nand closely approaches Claude-3.7-sonnet. Moreover, given the limited\navailability of benchmarks that evaluate web agents' information aggregation\nabilities, we construct a human-annotated evaluation split of WebAggregatorQA\nas a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves\n28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all\nreferences, they still struggle on WebAggregatorQA, highlighting the need to\nstrengthen the information aggregation capabilities of web agent foundations.",
    "published": "2025-10-16T08:37:42Z",
    "updated": "2025-10-16T08:37:42Z",
    "link": "http://arxiv.org/pdf/2510.14438v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Rui Wang",
      "Ce Zhang",
      "Jun-Yu Ma",
      "Jianshu Zhang",
      "Hongru Wang",
      "Yi Chen",
      "Boyang Xue",
      "Tianqing Fang",
      "Zhisong Zhang",
      "Hongming Zhang",
      "Haitao Mi",
      "Dong Yu",
      "Kam-Fai Wong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09721v2",
    "title": "A Comprehensive Survey on Benchmarks and Solutions in Software\n  Engineering of LLM-Empowered Agentic System",
    "summary": "The integration of Large Language Models (LLMs) into software engineering has\ndriven a transition from traditional rule-based systems to autonomous agentic\nsystems capable of solving complex problems. However, systematic progress is\nhindered by a lack of comprehensive understanding of how benchmarks and\nsolutions interconnect. This survey addresses this gap by providing the first\nholistic analysis of LLM-powered software engineering, offering insights into\nevaluation methodologies and solution paradigms. We review over 150 recent\npapers and propose a taxonomy along two key dimensions: (1) Solutions,\ncategorized into prompt-based, fine-tuning-based, and agent-based paradigms,\nand (2) Benchmarks, including tasks such as code generation, translation, and\nrepair. Our analysis highlights the evolution from simple prompt engineering to\nsophisticated agentic systems incorporating capabilities like planning,\nreasoning, memory mechanisms, and tool augmentation. To contextualize this\nprogress, we present a unified pipeline illustrating the workflow from task\nspecification to deliverables, detailing how different solution paradigms\naddress various complexity levels. Unlike prior surveys that focus narrowly on\nspecific aspects, this work connects 50+ benchmarks to their corresponding\nsolution strategies, enabling researchers to identify optimal approaches for\ndiverse evaluation criteria. We also identify critical research gaps and\npropose future directions, including multi-agent collaboration, self-evolving\nsystems, and formal verification integration. This survey serves as a\nfoundational guide for advancing LLM-driven software engineering. We maintain a\nGitHub repository that continuously updates the reviewed and related papers at\nhttps://github.com/lisaGuojl/LLM-Agent-SE-Survey.",
    "published": "2025-10-10T06:56:50Z",
    "updated": "2025-10-16T08:15:02Z",
    "link": "http://arxiv.org/pdf/2510.09721v2.pdf",
    "category": [
      "cs.SE",
      "cs.CL"
    ],
    "authors": [
      "Jiale Guo",
      "Suizhi Huang",
      "Mei Li",
      "Dong Huang",
      "Xingsheng Chen",
      "Regina Zhang",
      "Zhijiang Guo",
      "Han Yu",
      "Siu-Ming Yiu",
      "Christian Jensen",
      "Pietro Lio",
      "Kwok-Yan Lam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14398v1",
    "title": "Your Next Token Prediction: A Multilingual Benchmark for Personalized\n  Response Generation",
    "summary": "Large language models (LLMs) excel at general next-token prediction but still\nstruggle to generate responses that reflect how individuals truly communicate,\nsuch as replying to emails or social messages in their own style. However, real\nSNS or email histories are difficult to collect due to privacy concerns. To\naddress this, we propose the task of \"Your Next Token Prediction (YNTP)\", which\nmodels a user's precise word choices through controlled human-agent\nconversations. We build a multilingual benchmark of 100 dialogue sessions\nacross English, Japanese, and Chinese, where users interact for five days with\npsychologically grounded NPCs based on MBTI dimensions. This setup captures\nnatural, daily-life communication patterns and enables analysis of users'\ninternal models. We evaluate prompt-based and fine-tuning-based personalization\nmethods, establishing the first benchmark for YNTP and a foundation for\nuser-aligned language modeling. The dataset is available at:\nhttps://github.com/AnonymousHub4Submissions/your-next-token-prediction-dataset-100",
    "published": "2025-10-16T07:54:02Z",
    "updated": "2025-10-16T07:54:02Z",
    "link": "http://arxiv.org/pdf/2510.14398v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Shiyao Ding",
      "Takayuki Ito"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14395v1",
    "title": "Suicidal Comment Tree Dataset: Enhancing Risk Assessment and Prediction\n  Through Contextual Analysis",
    "summary": "Suicide remains a critical global public health issue. While previous studies\nhave provided valuable insights into detecting suicidal expressions in\nindividual social media posts, limited attention has been paid to the analysis\nof longitudinal, sequential comment trees for predicting a user's evolving\nsuicidal risk. Users, however, often reveal their intentions through historical\nposts and interactive comments over time. This study addresses this gap by\ninvestigating how the information in comment trees affects both the\ndiscrimination and prediction of users' suicidal risk levels. We constructed a\nhigh-quality annotated dataset, sourced from Reddit, which incorporates users'\nposting history and comments, using a refined four-label annotation framework\nbased on the Columbia Suicide Severity Rating Scale (C-SSRS). Statistical\nanalysis of the dataset, along with experimental results from Large Language\nModels (LLMs) experiments, demonstrates that incorporating comment trees data\nsignificantly enhances the discrimination and prediction of user suicidal risk\nlevels. This research offers a novel insight to enhancing the detection\naccuracy of at-risk individuals, thereby providing a valuable foundation for\nearly suicide intervention strategies.",
    "published": "2025-10-16T07:47:03Z",
    "updated": "2025-10-16T07:47:03Z",
    "link": "http://arxiv.org/pdf/2510.14395v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jun Li",
      "Qun Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14978v1",
    "title": "Learning an Image Editing Model without Image Editing Pairs",
    "summary": "Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
    "published": "2025-10-16T17:59:57Z",
    "updated": "2025-10-16T17:59:57Z",
    "link": "http://arxiv.org/pdf/2510.14978v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Nupur Kumari",
      "Sheng-Yu Wang",
      "Nanxuan Zhao",
      "Yotam Nitzan",
      "Yuheng Li",
      "Krishna Kumar Singh",
      "Richard Zhang",
      "Eli Shechtman",
      "Jun-Yan Zhu",
      "Xun Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14976v1",
    "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation",
    "summary": "Close-proximity human-human interactive poses convey rich contextual\ninformation about interaction dynamics. Given such poses, humans can\nintuitively infer the context and anticipate possible past and future dynamics,\ndrawing on strong priors of human behavior. Inspired by this observation, we\npropose Ponimator, a simple framework anchored on proximal interactive poses\nfor versatile interaction animation. Our training data consists of\nclose-contact two-person poses and their surrounding temporal context from\nmotion-capture interaction datasets. Leveraging interactive pose priors,\nPonimator employs two conditional diffusion models: (1) a pose animator that\nuses the temporal prior to generate dynamic motion sequences from interactive\nposes, and (2) a pose generator that applies the spatial prior to synthesize\ninteractive poses from a single pose, text, or both when interactive poses are\nunavailable. Collectively, Ponimator supports diverse tasks, including\nimage-based interaction animation, reaction animation, and text-to-interaction\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\nmocap data to open-world scenarios. Empirical experiments across diverse\ndatasets and applications demonstrate the universality of the pose prior and\nthe effectiveness and robustness of our framework.",
    "published": "2025-10-16T17:59:56Z",
    "updated": "2025-10-16T17:59:56Z",
    "link": "http://arxiv.org/pdf/2510.14976v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "authors": [
      "Shaowei Liu",
      "Chuan Guo",
      "Bing Zhou",
      "Jian Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14965v1",
    "title": "ChangingGrounding: 3D Visual Grounding in Changing Scenes",
    "summary": "Real-world robots localize objects from natural-language instructions while\nscenes around them keep changing. Yet most of the existing 3D visual grounding\n(3DVG) method still assumes a reconstructed and up-to-date point cloud, an\nassumption that forces costly re-scans and hinders deployment. We argue that\n3DVG should be formulated as an active, memory-driven problem, and we introduce\nChangingGrounding, the first benchmark that explicitly measures how well an\nagent can exploit past observations, explore only where needed, and still\ndeliver precise 3D boxes in changing scenes. To set a strong reference point,\nwe also propose Mem-ChangingGrounder, a zero-shot method for this task that\nmarries cross-modal retrieval with lightweight multi-view fusion: it identifies\nthe object type implied by the query, retrieves relevant memories to guide\nactions, then explores the target efficiently in the scene, falls back when\nprevious operations are invalid, performs multi-view scanning of the target,\nand projects the fused evidence from multi-view scans to get accurate object\nbounding boxes. We evaluate different baselines on ChangingGrounding, and our\nMem-ChangingGrounder achieves the highest localization accuracy while greatly\nreducing exploration cost. We hope this benchmark and method catalyze a shift\ntoward practical, memory-centric 3DVG research for real-world applications.\nProject page: https://hm123450.github.io/CGB/ .",
    "published": "2025-10-16T17:59:16Z",
    "updated": "2025-10-16T17:59:16Z",
    "link": "http://arxiv.org/pdf/2510.14965v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Miao Hu",
      "Zhiwei Huang",
      "Tai Wang",
      "Jiangmiao Pang",
      "Dahua Lin",
      "Nanning Zheng",
      "Runsen Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14962v1",
    "title": "RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention\n  Diffusion",
    "summary": "Precipitation nowcasting, predicting future radar echo sequences from current\nobservations, is a critical yet challenging task due to the inherently chaotic\nand tightly coupled spatio-temporal dynamics of the atmosphere. While recent\nadvances in diffusion-based models attempt to capture both large-scale motion\nand fine-grained stochastic variability, they often suffer from scalability\nissues: latent-space approaches require a separately trained autoencoder,\nadding complexity and limiting generalization, while pixel-space approaches are\ncomputationally intensive and often omit attention mechanisms, reducing their\nability to model long-range spatio-temporal dependencies. To address these\nlimitations, we propose a Token-wise Attention integrated into not only the\nU-Net diffusion model but also the spatio-temporal encoder that dynamically\ncaptures multi-scale spatial interactions and temporal evolution. Unlike prior\napproaches, our method natively integrates attention into the architecture\nwithout incurring the high resource cost typical of pixel-space diffusion,\nthereby eliminating the need for separate latent modules. Our extensive\nexperiments and visual evaluations across diverse datasets demonstrate that the\nproposed method significantly outperforms state-of-the-art approaches, yielding\nsuperior local fidelity, generalization, and robustness in complex\nprecipitation forecasting scenarios.",
    "published": "2025-10-16T17:59:13Z",
    "updated": "2025-10-16T17:59:13Z",
    "link": "http://arxiv.org/pdf/2510.14962v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Thao Nguyen",
      "Jiaqi Ma",
      "Fahad Shahbaz Khan",
      "Souhaib Ben Taieb",
      "Salman Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.16312v4",
    "title": "LinPrim: Linear Primitives for Differentiable Volumetric Rendering",
    "summary": "Volumetric rendering has become central to modern novel view synthesis\nmethods, which use differentiable rendering to optimize 3D scene\nrepresentations directly from observed views. While many recent works build on\nNeRF or 3D Gaussians, we explore an alternative volumetric scene\nrepresentation. More specifically, we introduce two new scene representations\nbased on linear primitives - octahedra and tetrahedra - both of which define\nhomogeneous volumes bounded by triangular faces. To optimize these primitives,\nwe present a differentiable rasterizer that runs efficiently on GPUs, allowing\nend-to-end gradient-based optimization while maintaining real-time rendering\ncapabilities. Through experiments on real-world datasets, we demonstrate\ncomparable performance to state-of-the-art volumetric methods while requiring\nfewer primitives to achieve similar reconstruction fidelity. Our findings\ndeepen the understanding of 3D representations by providing insights into the\nfidelity and performance characteristics of transparent polyhedra and suggest\nthat adopting novel primitives can expand the available design space.",
    "published": "2025-01-27T18:49:38Z",
    "updated": "2025-10-16T17:58:17Z",
    "link": "http://arxiv.org/pdf/2501.16312v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Nicolas von Ltzow",
      "Matthias Niener"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14954v1",
    "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked\n  Autoregression",
    "summary": "Whole-body multi-modal human motion generation poses two primary challenges:\ncreating an effective motion generation mechanism and integrating various\nmodalities, such as text, speech, and music, into a cohesive framework. Unlike\nprevious methods that usually employ discrete masked modeling or autoregressive\nmodeling, we develop a continuous masked autoregressive motion transformer,\nwhere a causal attention is performed considering the sequential nature within\nthe human motion. Within this transformer, we introduce a gated linear\nattention and an RMSNorm module, which drive the transformer to pay attention\nto the key actions and suppress the instability caused by either the abnormal\nmovements or the heterogeneous distributions within multi-modalities. To\nfurther enhance both the motion generation and the multimodal generalization,\nwe employ the DiT structure to diffuse the conditions from the transformer\ntowards the targets. To fuse different modalities, AdaLN and cross-attention\nare leveraged to inject the text, speech, and music signals. Experimental\nresults demonstrate that our framework outperforms previous methods across all\nmodalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.",
    "published": "2025-10-16T17:57:53Z",
    "updated": "2025-10-16T17:57:53Z",
    "link": "http://arxiv.org/pdf/2510.14954v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhe Li",
      "Weihao Yuan",
      "Weichao Shen",
      "Siyu Zhu",
      "Zilong Dong",
      "Chang Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14952v1",
    "title": "From Language to Locomotion: Retargeting-free Humanoid Control via\n  Motion Latent Guidance",
    "summary": "Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and unreliable.\nThey typically decode human motion, retarget it to robot morphology, and then\ntrack it with a physics-based controller. However, this multi-stage process is\nprone to cumulative errors, introduces high latency, and yields weak coupling\nbetween semantics and control. These limitations call for a more direct pathway\nfrom language to action, one that eliminates fragile intermediate stages.\nTherefore, we present RoboGhost, a retargeting-free framework that directly\nconditions humanoid policies on language-grounded motion latents. By bypassing\nexplicit motion decoding and retargeting, RoboGhost enables a diffusion-based\npolicy to denoise executable actions directly from noise, preserving semantic\nintent and supporting fast, reactive control. A hybrid causal\ntransformer-diffusion motion generator further ensures long-horizon consistency\nwhile maintaining stability and diversity, yielding rich latent representations\nfor precise humanoid behavior. Extensive experiments demonstrate that RoboGhost\nsubstantially reduces deployment latency, improves success rates and tracking\naccuracy, and produces smooth, semantically aligned locomotion on real\nhumanoids. Beyond text, the framework naturally extends to other modalities\nsuch as images, audio, and music, providing a general foundation for\nvision-language-action humanoid systems.",
    "published": "2025-10-16T17:57:47Z",
    "updated": "2025-10-16T17:57:47Z",
    "link": "http://arxiv.org/pdf/2510.14952v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Zhe Li",
      "Cheng Chi",
      "Yangyang Wei",
      "Boan Zhu",
      "Yibo Peng",
      "Tao Huang",
      "Pengwei Wang",
      "Zhongyuan Wang",
      "Shanghang Zhang",
      "Chang Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.08267v2",
    "title": "TABSurfer: a Hybrid Deep Learning Architecture for Subcortical\n  Segmentation",
    "summary": "Subcortical segmentation remains challenging despite its important\napplications in quantitative structural analysis of brain MRI scans. The most\naccurate method, manual segmentation, is highly labor intensive, so automated\ntools like FreeSurfer have been adopted to handle this task. However, these\ntraditional pipelines are slow and inefficient for processing large datasets.\nIn this study, we propose TABSurfer, a novel 3D patch-based CNN-Transformer\nhybrid deep learning model designed for superior subcortical segmentation\ncompared to existing state-of-the-art tools. To evaluate, we first demonstrate\nTABSurfer's consistent performance across various T1w MRI datasets with\nsignificantly shorter processing times compared to FreeSurfer. Then, we\nvalidate against manual segmentations, where TABSurfer outperforms FreeSurfer\nbased on the manual ground truth. In each test, we also establish TABSurfer's\nadvantage over a leading deep learning benchmark, FastSurferVINN. Together,\nthese studies highlight TABSurfer's utility as a powerful tool for fully\nautomated subcortical segmentation with high fidelity.",
    "published": "2023-12-13T16:29:28Z",
    "updated": "2025-10-16T17:56:54Z",
    "link": "http://arxiv.org/pdf/2312.08267v2.pdf",
    "category": [
      "eess.IV",
      "cs.CV",
      "q-bio.QM"
    ],
    "authors": [
      "Aaron Cao",
      "Vishwanatha M. Rao",
      "Kejia Liu",
      "Xinrui Liu",
      "Andrew F. Laine",
      "Jia Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14945v1",
    "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video\n  Generation",
    "summary": "We present 3DScenePrompt, a framework that generates the next video chunk\nfrom arbitrary-length input while enabling precise camera control and\npreserving scene consistency. Unlike methods conditioned on a single image or a\nshort clip, we employ dual spatio-temporal conditioning that reformulates\ncontext-view referencing across the input video. Our approach conditions on\nboth temporally adjacent frames for motion continuity and spatially adjacent\ncontent for scene consistency. However, when generating beyond temporal\nboundaries, directly using spatially adjacent frames would incorrectly preserve\ndynamic elements from the past. We address this by introducing a 3D scene\nmemory that represents exclusively the static geometry extracted from the\nentire input video. To construct this memory, we leverage dynamic SLAM with our\nnewly introduced dynamic masking strategy that explicitly separates static\nscene geometry from moving elements. The static scene representation can then\nbe projected to any target viewpoint, providing geometrically consistent warped\nviews that serve as strong 3D spatial prompts while allowing dynamic regions to\nevolve naturally from temporal context. This enables our model to maintain\nlong-range spatial coherence and precise camera control without sacrificing\ncomputational efficiency or motion realism. Extensive experiments demonstrate\nthat our framework significantly outperforms existing methods in scene\nconsistency, camera controllability, and generation quality. Project page :\nhttps://cvlab-kaist.github.io/3DScenePrompt/",
    "published": "2025-10-16T17:55:25Z",
    "updated": "2025-10-16T17:55:25Z",
    "link": "http://arxiv.org/pdf/2510.14945v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "JoungBin Lee",
      "Jaewoo Jung",
      "Jisang Han",
      "Takuya Narihira",
      "Kazumi Fukuda",
      "Junyoung Seo",
      "Sunghwan Hong",
      "Yuki Mitsufuji",
      "Seungryong Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.13764v2",
    "title": "Shape of Motion: 4D Reconstruction from a Single Video",
    "summary": "Monocular dynamic reconstruction is a challenging and long-standing vision\nproblem due to the highly ill-posed nature of the task. Existing approaches\ndepend on templates, are effective only in quasi-static scenes, or fail to\nmodel 3D motion explicitly. We introduce a method for reconstructing generic\ndynamic scenes, featuring explicit, persistent 3D motion trajectories in the\nworld coordinate frame, from casually captured monocular videos. We tackle the\nproblem with two key insights: First, we exploit the low-dimensional structure\nof 3D motion by representing scene motion with a compact set of SE(3) motion\nbases. Each point's motion is expressed as a linear combination of these bases,\nfacilitating soft decomposition of the scene into multiple rigidly-moving\ngroups. Second, we take advantage of off-the-shelf data-driven priors such as\nmonocular depth maps and long-range 2D tracks, and devise a method to\neffectively consolidate these noisy supervisory signals, resulting in a\nglobally consistent representation of the dynamic scene. Experiments show that\nour method achieves state-of-the-art performance for both long-range 3D/2D\nmotion estimation and novel view synthesis on dynamic scenes. Project Page:\nhttps://shape-of-motion.github.io/",
    "published": "2024-07-18T17:59:08Z",
    "updated": "2025-10-16T17:18:25Z",
    "link": "http://arxiv.org/pdf/2407.13764v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qianqian Wang",
      "Vickie Ye",
      "Hang Gao",
      "Weijia Zeng",
      "Jake Austin",
      "Zhengqi Li",
      "Angjoo Kanazawa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14896v1",
    "title": "Leveraging Multimodal LLM Descriptions of Activity for Explainable\n  Semi-Supervised Video Anomaly Detection",
    "summary": "Existing semi-supervised video anomaly detection (VAD) methods often struggle\nwith detecting complex anomalies involving object interactions and generally\nlack explainability. To overcome these limitations, we propose a novel VAD\nframework leveraging Multimodal Large Language Models (MLLMs). Unlike previous\nMLLM-based approaches that make direct anomaly judgments at the frame level,\nour method focuses on extracting and interpreting object activity and\ninteractions over time. By querying an MLLM with visual inputs of object pairs\nat different moments, we generate textual descriptions of the activity and\ninteractions from nominal videos. These textual descriptions serve as a\nhigh-level representation of the activity and interactions of objects in a\nvideo. They are used to detect anomalies during test time by comparing them to\ntextual descriptions found in nominal training videos. Our approach inherently\nprovides explainability and can be combined with many traditional VAD methods\nto further enhance their interpretability. Extensive experiments on benchmark\ndatasets demonstrate that our method not only detects complex interaction-based\nanomalies effectively but also achieves state-of-the-art performance on\ndatasets without interaction anomalies.",
    "published": "2025-10-16T17:13:33Z",
    "updated": "2025-10-16T17:13:33Z",
    "link": "http://arxiv.org/pdf/2510.14896v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Furkan Mumcu",
      "Michael J. Jones",
      "Anoop Cherian",
      "Yasin Yilmaz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.11817v3",
    "title": "GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models",
    "summary": "Large multimodal models (LMMs) have exhibited proficiencies across many\nvisual tasks. Although numerous well-known benchmarks exist to evaluate model\nperformance, they increasingly have insufficient headroom. As such, there is a\npressing need for a new generation of benchmarks challenging enough for the\nnext generation of LMMs. One area that LMMs show potential is graph analysis,\nspecifically, the tasks an analyst might typically perform when interpreting\nfigures such as estimating the mean, intercepts or correlations of functions\nand data series. In this work, we introduce GRAB, a graph analysis benchmark,\nfit for current and future frontier LMMs. Our benchmark is predominantly\nsynthetic, ensuring high-quality, noise-free questions. GRAB is comprised of\n3284 questions, covering five tasks and 23 graph properties. We evaluate 20\nLMMs on GRAB, finding it to be a challenging benchmark, with the highest\nperforming model attaining a score of just 21.0%. Finally, we conduct various\nablations to investigate where the models succeed and struggle. We release GRAB\nand a lightweight GRAB-Lite to encourage progress in this important, growing\ndomain.",
    "published": "2024-08-21T17:59:32Z",
    "updated": "2025-10-16T17:05:20Z",
    "link": "http://arxiv.org/pdf/2408.11817v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jonathan Roberts",
      "Kai Han",
      "Samuel Albanie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14882v1",
    "title": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with\n  Multi-Scale Reference Attention",
    "summary": "Text-to-image generation with visual autoregressive~(VAR) models has recently\nachieved impressive advances in generation fidelity and inference efficiency.\nWhile control mechanisms have been explored for diffusion models, enabling\nprecise and flexible control within VAR paradigm remains underexplored. To\nbridge this critical gap, in this paper, we introduce ScaleWeaver, a novel\nframework designed to achieve high-fidelity, controllable generation upon\nadvanced VAR models through parameter-efficient fine-tuning. The core module in\nScaleWeaver is the improved MMDiT block with the proposed Reference Attention\nmodule, which efficiently and effectively incorporates conditional information.\nDifferent from MM Attention, the proposed Reference Attention module discards\nthe unnecessary attention from image$\\rightarrow$condition, reducing\ncomputational cost while stabilizing control injection. Besides, it\nstrategically emphasizes parameter reuse, leveraging the capability of the VAR\nbackbone itself with a few introduced parameters to process control\ninformation, and equipping a zero-initialized linear projection to ensure that\ncontrol signals are incorporated effectively without disrupting the generative\ncapability of the base model. Extensive experiments show that ScaleWeaver\ndelivers high-quality generation and precise control while attaining superior\nefficiency over diffusion-based methods, making ScaleWeaver a practical and\neffective solution for controllable text-to-image generation within the visual\nautoregressive paradigm. Code and models will be released.",
    "published": "2025-10-16T17:00:59Z",
    "updated": "2025-10-16T17:00:59Z",
    "link": "http://arxiv.org/pdf/2510.14882v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Keli Liu",
      "Zhendong Wang",
      "Wengang Zhou",
      "Shaodong Xu",
      "Ruixiao Dong",
      "Houqiang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14876v1",
    "title": "BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data",
    "summary": "Existing collision prediction methods often fail to distinguish between\nego-vehicle threats and random accidents not involving the ego vehicle, leading\nto excessive false alerts in real-world deployment. We present BADAS, a family\nof collision prediction models trained on Nexar's real-world dashcam collision\ndataset -- the first benchmark designed explicitly for ego-centric evaluation.\nWe re-annotate major benchmarks to identify ego involvement, add consensus\nalert-time labels, and synthesize negatives where needed, enabling fair AP/AUC\nand temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and\ncomes in two variants: BADAS-Open (trained on our 1.5k public videos) and\nBADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and\nNexar, BADAS achieves state-of-the-art AP/AUC and outperforms a\nforward-collision ADAS baseline while producing more realistic time-to-accident\nestimates. We release our BADAS-Open model weights and code, along with\nre-annotations of all evaluation datasets to promote ego-centric collision\nprediction research.",
    "published": "2025-10-16T16:55:30Z",
    "updated": "2025-10-16T16:55:30Z",
    "link": "http://arxiv.org/pdf/2510.14876v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Roni Goldshmidt",
      "Hamish Scott",
      "Lorenzo Niccolini",
      "Shizhan Zhu",
      "Daniel Moura",
      "Orly Zvitia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14874v1",
    "title": "TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object\n  Interactions",
    "summary": "Hand-object interaction (HOI) is fundamental for humans to express intent.\nExisting HOI generation research is predominantly confined to fixed grasping\npatterns, where control is tied to physical priors such as force closure or\ngeneric intent instructions, even when expressed through elaborate language.\nSuch an overly general conditioning imposes a strong inductive bias for stable\ngrasps, thus failing to capture the diversity of daily HOI. To address these\nlimitations, we introduce Free-Form HOI Generation, which aims to generate\ncontrollable, diverse, and physically plausible HOI conditioned on fine-grained\nintent, extending HOI from grasping to free-form interactions, like pushing,\npoking, and rotating. To support this task, we construct WildO2, an in-the-wild\ndiverse 3D HOI dataset, which includes diverse HOI derived from internet\nvideos. Specifically, it contains 4.4k unique interactions across 92 intents\nand 610 object categories, each with detailed semantic annotations. Building on\nthis dataset, we propose TOUCH, a three-stage framework centered on a\nmulti-level diffusion model that facilitates fine-grained semantic control to\ngenerate versatile hand poses beyond grasping priors. This process leverages\nexplicit contact modeling for conditioning and is subsequently refined with\ncontact consistency and physical constraints to ensure realism. Comprehensive\nexperiments demonstrate our method's ability to generate controllable, diverse,\nand physically plausible hand interactions representative of daily activities.\nThe project page is $\\href{https://guangyid.github.io/hoi123touch}{here}$.",
    "published": "2025-10-16T16:52:58Z",
    "updated": "2025-10-16T16:52:58Z",
    "link": "http://arxiv.org/pdf/2510.14874v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Guangyi Han",
      "Wei Zhai",
      "Yuhang Yang",
      "Yang Cao",
      "Zheng-Jun Zha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.12052v2",
    "title": "AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided\n  Autoregressive Perspective",
    "summary": "Talking-head animation focuses on generating realistic facial videos from\naudio input. Following Generative Adversarial Networks (GANs), diffusion models\nhave become the mainstream, owing to their robust generative capacities.\nHowever, inherent limitations of the diffusion process often lead to\ninter-frame flicker and slow inference, restricting their practical deployment.\nTo address this, we introduce AvatarSync, an autoregressive framework on\nphoneme representations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly by text or audio\ninput. To mitigate flicker and ensure continuity, AvatarSync leverages an\nautoregressive pipeline that enhances temporal modeling. In addition, to ensure\ncontrollability, we introduce phonemes, which are the basic units of speech\nsounds, and construct a many-to-one mapping from text/audio to phonemes,\nenabling precise phoneme-to-visual alignment. Additionally, to further\naccelerate inference, we adopt a two-stage generation strategy that decouples\nsemantic modeling from visual dynamics, and incorporate a customized\nPhoneme-Frame Causal Attention Mask to support multi-step parallel\nacceleration. Extensive experiments conducted on both Chinese (CMLR) and\nEnglish (HDTF) datasets demonstrate that AvatarSync outperforms existing\ntalking-head animation methods in visual fidelity, temporal consistency, and\ncomputational efficiency, providing a scalable and controllable solution.",
    "published": "2025-09-15T15:34:02Z",
    "updated": "2025-10-16T16:37:59Z",
    "link": "http://arxiv.org/pdf/2509.12052v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuchen Deng",
      "Xiuyang Wu",
      "Hai-Tao Zheng",
      "Suiyang Zhang",
      "Yi He",
      "Yuxing Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14862v1",
    "title": "Multi-modal video data-pipelines for machine learning with minimal human\n  supervision",
    "summary": "The real-world is inherently multi-modal at its core. Our tools observe and\ntake snapshots of it, in digital form, such as videos or sounds, however much\nof it is lost. Similarly for actions and information passing between humans,\nlanguages are used as a written form of communication. Traditionally, Machine\nLearning models have been unimodal (i.e. rgb -> semantic or text ->\nsentiment_class). Recent trends go towards bi-modality, where images and text\nare learned together, however, in order to truly understand the world, we need\nto integrate all these independent modalities. In this work we try to combine\nas many visual modalities as we can using little to no human supervision. In\norder to do this, we use pre-trained experts and procedural combinations\nbetween them on top of raw videos using a fully autonomous data-pipeline, which\nwe also open-source. We then make use of PHG-MAE, a model specifically designed\nto leverage multi-modal data. We show that this model which was efficiently\ndistilled into a low-parameter (<1M) can have competitive results compared to\nmodels of ~300M parameters. We deploy this model and analyze the use-case of\nreal-time semantic segmentation from handheld devices or webcams on commodity\nhardware. Finally, we deploy other off-the-shelf models using the same\nframework, such as DPT for near real-time depth estimation.",
    "published": "2025-10-16T16:36:29Z",
    "updated": "2025-10-16T16:36:29Z",
    "link": "http://arxiv.org/pdf/2510.14862v1.pdf",
    "category": [
      "cs.CV",
      "cs.DC"
    ],
    "authors": [
      "Mihai-Cristian Prvu",
      "Marius Leordeanu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14855v1",
    "title": "A Multi-Task Deep Learning Framework for Skin Lesion Classification,\n  ABCDE Feature Quantification, and Evolution Simulation",
    "summary": "Early detection of melanoma has grown to be essential because it\nsignificantly improves survival rates, but automated analysis of skin lesions\nstill remains challenging. ABCDE, which stands for Asymmetry, Border\nirregularity, Color variation, Diameter, and Evolving, is a well-known\nclassification method for skin lesions, but most deep learning mechanisms treat\nit as a black box, as most of the human interpretable features are not\nexplained. In this work, we propose a deep learning framework that both\nclassifies skin lesions into categories and also quantifies scores for each\nABCD feature. It simulates the evolution of these features over time in order\nto represent the E aspect, opening more windows for future exploration. The A,\nB, C, and D values are quantified particularly within this work. Moreover, this\nframework also visualizes ABCD feature trajectories in latent space as skin\nlesions evolve from benign nevuses to malignant melanoma. The experiments are\nconducted using the HAM10000 dataset that contains around ten thousand images\nof skin lesions of varying stages. In summary, the classification worked with\nan accuracy of around 89 percent, with melanoma AUC being 0.96, while the\nfeature evaluation performed well in predicting asymmetry, color variation, and\ndiameter, though border irregularity remains more difficult to model. Overall,\nthis work provides a deep learning framework that will allow doctors to link ML\ndiagnoses to clinically relevant criteria, thus improving our understanding of\nskin cancer progression.",
    "published": "2025-10-16T16:28:21Z",
    "updated": "2025-10-16T16:28:21Z",
    "link": "http://arxiv.org/pdf/2510.14855v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Harsha Kotla",
      "Arun Kumar Rajasekaran",
      "Hannah Rana"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14847v1",
    "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond\n  Semantic Dependency Constraints",
    "summary": "Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration.",
    "published": "2025-10-16T16:19:13Z",
    "updated": "2025-10-16T16:19:13Z",
    "link": "http://arxiv.org/pdf/2510.14847v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Meiqi Wu",
      "Jiashu Zhu",
      "Xiaokun Feng",
      "Chubin Chen",
      "Chen Zhu",
      "Bingze Song",
      "Fangyuan Mao",
      "Jiahong Wu",
      "Xiangxiang Chu",
      "Kaiqi Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14845v1",
    "title": "Backdoor Unlearning by Linear Task Decomposition",
    "summary": "Foundation models have revolutionized computer vision by enabling broad\ngeneralization across diverse tasks. Yet, they remain highly susceptible to\nadversarial perturbations and targeted backdoor attacks. Mitigating such\nvulnerabilities remains an open challenge, especially given that the\nlarge-scale nature of the models prohibits retraining to ensure safety.\nExisting backdoor removal approaches rely on costly fine-tuning to override the\nharmful behavior, and can often degrade performance on other unrelated tasks.\nThis raises the question of whether backdoors can be removed without\ncompromising the general capabilities of the models. In this work, we address\nthis question and study how backdoors are encoded in the model weight space,\nfinding that they are disentangled from other benign tasks. Specifically, this\nseparation enables the isolation and erasure of the backdoor's influence on the\nmodel with minimal impact on clean performance. Building on this insight, we\nintroduce a simple unlearning method that leverages such disentanglement.\nThrough extensive experiments with CLIP-based models and common adversarial\ntriggers, we show that, given the knowledge of the attack, our method achieves\napproximately perfect unlearning, while retaining, on average, 96% of clean\naccuracy. Additionally, we demonstrate that even when the attack and its\npresence are unknown, our method successfully unlearns backdoors by proper\nestimation using reverse-engineered triggers. Overall, our method consistently\nyields better unlearning and clean accuracy tradeoffs when compared to present\nstate-of-the-art defenses.",
    "published": "2025-10-16T16:18:07Z",
    "updated": "2025-10-16T16:18:07Z",
    "link": "http://arxiv.org/pdf/2510.14845v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Amel Abdelraheem",
      "Alessandro Favero",
      "Gerome Bovet",
      "Pascal Frossard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14836v1",
    "title": "QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for\n  Vision-Language-Action Models",
    "summary": "Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)\nmodels to accomplish fine-grained manipulation tasks. However, existing\napproaches often lack the ability to understand and reason over the essential\n3D structures necessary for precise control. To address this limitation, we\npropose QDepth-VLA, a general framework that augments VLA models with an\nauxiliary depth prediction task. A dedicated depth expert is designed to\npredict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,\nenabling the model to learn depth-aware representations that capture critical\ngeometric cues. Experimental results on the simulation benchmarks and\nreal-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning\nand competitive performance on manipulation tasks.",
    "published": "2025-10-16T16:11:18Z",
    "updated": "2025-10-16T16:11:18Z",
    "link": "http://arxiv.org/pdf/2510.14836v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Yixuan Li",
      "Yuhui Chen",
      "Mingcai Zhou",
      "Haoran Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.12331v2",
    "title": "Deep Few-view High-resolution Photon-counting CT at Halved Dose for\n  Extremity Imaging",
    "summary": "X-ray photon-counting computed tomography (PCCT) for extremity allows\nmulti-energy high-resolution (HR) imaging but its radiation dose can be further\nimproved. Despite the great potential of deep learning techniques, their\napplication in HR volumetric PCCT reconstruction has been challenged by the\nlarge memory burden, training data scarcity, and domain gap issues. In this\npaper, we propose a deep learning-based approach for PCCT image reconstruction\nat halved dose and doubled speed validated in a New Zealand clinical trial.\nSpecifically, we design a patch-based volumetric refinement network to\nalleviate the GPU memory limitation, train network with synthetic data, and use\nmodel-based iterative refinement to bridge the gap between synthetic and\nclinical data. Our results in a reader study of 8 patients from the clinical\ntrial demonstrate a great potential to cut the radiation dose to half that of\nthe clinical PCCT standard without compromising image quality and diagnostic\nvalue.",
    "published": "2024-03-19T00:07:48Z",
    "updated": "2025-10-16T16:10:31Z",
    "link": "http://arxiv.org/pdf/2403.12331v2.pdf",
    "category": [
      "physics.med-ph",
      "cs.CV"
    ],
    "authors": [
      "Mengzhou Li",
      "Chuang Niu",
      "Ge Wang",
      "Maya R Amma",
      "Krishna M Chapagain",
      "Stefan Gabrielson",
      "Andrew Li",
      "Kevin Jonker",
      "Niels de Ruiter",
      "Jennifer A Clark",
      "Phil Butler",
      "Anthony Butler",
      "Hengyong Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14831v1",
    "title": "Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data",
    "summary": "AI for tumor segmentation is limited by the lack of large, voxel-wise\nannotated datasets, which are hard to create and require medical experts. In\nour proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found\nthat AI performance stopped improving after 1,500 scans. With synthetic data,\nwe reached the same performance using only 500 real scans. This finding\nsuggests that synthetic data can steepen data scaling laws, enabling more\nefficient model training than real data alone. Motivated by these lessons, we\ncreated AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130\ntumor instances per-voxel manually annotated in six organs (pancreas, liver,\nkidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23\nexpert radiologists, it is several orders of magnitude larger than existing\npublic tumor datasets. While we continue expanding the dataset, the current\nversion of AbdomenAtlas 2.0 already provides a strong foundation--based on\nlessons from the JHH dataset--for training AI to segment tumors in six organs.\nIt achieves notable improvements over public datasets, with a +7% DSC gain on\nin-distribution tests and +16% on out-of-distribution tests.",
    "published": "2025-10-16T16:08:09Z",
    "updated": "2025-10-16T16:08:09Z",
    "link": "http://arxiv.org/pdf/2510.14831v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qi Chen",
      "Xinze Zhou",
      "Chen Liu",
      "Hao Chen",
      "Wenxuan Li",
      "Zekun Jiang",
      "Ziyan Huang",
      "Yuxuan Zhao",
      "Dexin Yu",
      "Junjun He",
      "Yefeng Zheng",
      "Ling Shao",
      "Alan Yuille",
      "Zongwei Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14823v1",
    "title": "FraQAT: Quantization Aware Training with Fractional bits",
    "summary": "State-of-the-art (SOTA) generative models have demonstrated impressive\ncapabilities in image synthesis or text generation, often with a large capacity\nmodel. However, these large models cannot be deployed on smartphones due to the\nlimited availability of on-board memory and computations. Quantization methods\nlower the precision of the model parameters, allowing for efficient\ncomputations, \\eg, in \\INT{8}. Although aggressive quantization addresses\nefficiency and memory constraints, preserving the quality of the model remains\na challenge. To retain quality in previous aggressive quantization, we propose\na new fractional bits quantization (\\short) approach. The novelty is a simple\nyet effective idea: we progressively reduce the model's precision from 32 to 4\nbits per parameter, and exploit the fractional bits during optimization to\nmaintain high generation quality. We show that the \\short{} yields improved\nquality on a variety of diffusion models, including SD3.5-Medium, Sana,\n\\pixart, and FLUX.1-schnell, while achieving $4-7\\%$ lower FiD than standard\nQAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the\nQualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP).",
    "published": "2025-10-16T16:01:08Z",
    "updated": "2025-10-16T16:01:08Z",
    "link": "http://arxiv.org/pdf/2510.14823v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Luca Morreale",
      "Alberto Gil C. P. Ramos",
      "Malcolm Chadwick",
      "Mehid Noroozi",
      "Ruchika Chavhan",
      "Abhinav Mehrotra",
      "Sourav Bhattacharya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14819v1",
    "title": "Unifying Environment Perception and Route Choice Modeling for Trajectory\n  Representation Learning",
    "summary": "Trajectory Representation Learning (TRL) aims to encode raw trajectories into\nlow-dimensional vectors, which can then be leveraged in various downstream\ntasks, including travel time estimation, location prediction, and trajectory\nsimilarity analysis. However, existing TRL methods suffer from a key oversight:\ntreating trajectories as isolated spatio-temporal sequences, without\nconsidering the external environment and internal route choice behavior that\ngovern their formation. To bridge this gap, we propose a novel framework that\nunifies comprehensive environment \\textbf{P}erception and explicit\n\\textbf{R}oute choice modeling for effective \\textbf{Traj}ectory representation\nlearning, dubbed \\textbf{PRTraj}. Specifically, PRTraj first introduces an\nEnvironment Perception Module to enhance the road network by capturing\nmulti-granularity environmental semantics from surrounding POI distributions.\nBuilding on this environment-aware backbone, a Route Choice Encoder then\ncaptures the route choice behavior inherent in each trajectory by modeling its\nconstituent road segment transitions as a sequence of decisions. These\nroute-choice-aware representations are finally aggregated to form the global\ntrajectory embedding. Extensive experiments on 3 real-world datasets across 5\ndownstream tasks validate the effectiveness and generalizability of PRTraj.\nMoreover, PRTraj demonstrates strong data efficiency, maintaining robust\nperformance under few-shot scenarios. Our code is available at:\nhttps://anonymous.4open.science/r/PRTraj.",
    "published": "2025-10-16T15:55:28Z",
    "updated": "2025-10-16T15:55:28Z",
    "link": "http://arxiv.org/pdf/2510.14819v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Ji Cao",
      "Yu Wang",
      "Tongya Zheng",
      "Zujie Ren",
      "Canghong Jin",
      "Gang Chen",
      "Mingli Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20401v2",
    "title": "SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment",
    "summary": "Aligning 3D scene graphs is a crucial initial step for several applications\nin robot navigation and embodied perception. Current methods in 3D scene graph\nalignment often rely on single-modality point cloud data and struggle with\nincomplete or noisy input. We introduce SGAligner++, a cross-modal,\nlanguage-aided framework for 3D scene graph alignment. Our method addresses the\nchallenge of aligning partially overlapping scene observations across\nheterogeneous modalities by learning a unified joint embedding space, enabling\naccurate alignment even under low-overlap conditions and sensor noise. By\nemploying lightweight unimodal encoders and attention-based fusion, SGAligner++\nenhances scene understanding for tasks such as visual localization, 3D\nreconstruction, and navigation, while ensuring scalability and minimal\ncomputational overhead. Extensive evaluations on real-world datasets\ndemonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40%\non noisy real-world reconstructions, while enabling cross-modal generalization.",
    "published": "2025-09-23T18:31:29Z",
    "updated": "2025-10-16T15:30:20Z",
    "link": "http://arxiv.org/pdf/2509.20401v2.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Binod Singh",
      "Sayan Deb Sarkar",
      "Iro Armeni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14792v1",
    "title": "CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for\n  Open-Vocabulary Object Detection",
    "summary": "Open-vocabulary object detection (OVD) seeks to recognize and localize object\ncategories beyond those seen during training. Recent approaches typically\nleverage vision-language models (VLMs) to generate pseudo-labels using\nimage-text alignment, allowing detectors to generalize to unseen classes\nwithout explicit supervision. However, these methods depend heavily on direct\nimage-text matching, neglecting the intermediate reasoning steps essential for\ninterpreting semantically complex scenes. This results in limited robustness\nwhen confronted with crowded or occluded visual contexts. In this paper, we\nintroduce CoT-PL, a new framework that employs structured visual\nchain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL\ndecomposes object understanding into three interpretable steps: (1) region\nperception even for unseen objects, (2) category recognition via zero-shot\nreasoning, and (3) background grounding to separate semantically complex\nobjects. Crucially, the third step naturally motivates our contrastive\nbackground learning (CBL) that uses the pre-computed background cues as\nnegatives to promote feature disentanglement between objects and background. In\nthis way, CoT reasoning and CBL form an integrated pipeline tailored to robust\npseudo-labeling in crowded or occluded scenes. Notably, in these two settings,\nour novel-class pseudo-label quality achieves relative improvements of 103.4%\nand 168.4% over the best prior, respectively. Our extensive experiments\ndemonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9\nmask AP on LVIS for novel classes, setting a new state of the art.",
    "published": "2025-10-16T15:27:10Z",
    "updated": "2025-10-16T15:27:10Z",
    "link": "http://arxiv.org/pdf/2510.14792v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hojun Choi",
      "Youngsun Lim",
      "Jaeyo Shin",
      "Hyunjung Shim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13016v2",
    "title": "SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal\n  Video Action Grounding",
    "summary": "Understanding fine-grained actions and accurately localizing their\ncorresponding actors in space and time are fundamental capabilities for\nadvancing next-generation AI systems, including embodied agents, autonomous\nplatforms, and human-AI interaction frameworks. Despite recent progress in\nvideo understanding, existing methods predominantly address either\ncoarse-grained action recognition or generic object tracking, thereby\noverlooking the challenge of jointly detecting and tracking multiple objects\naccording to their actions while grounding them temporally. To address this\ngap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel task\nthat requires models to simultaneously detect, track, and temporally localize\nall referent objects in videos based on natural language descriptions of their\nactions. To support this task, we construct SVAG-Bench, a large-scale benchmark\ncomprising 688 videos, 19,590 annotated records, and 903 unique verbs, covering\na diverse range of objects, actions, and real-world scenes. We further propose\nSVAGFormer, a baseline framework that adapts state of the art vision language\nmodels for joint spatial and temporal grounding, and introduce SVAGEval, a\nstandardized evaluation toolkit for fair and reproducible benchmarking.\nEmpirical results show that existing models perform poorly on SVAG,\nparticularly in dense or complex scenes, underscoring the need for more\nadvanced reasoning over fine-grained object-action interactions in long videos.",
    "published": "2025-10-14T22:10:49Z",
    "updated": "2025-10-16T15:16:51Z",
    "link": "http://arxiv.org/pdf/2510.13016v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tanveer Hannan",
      "Shuaicong Wu",
      "Mark Weber",
      "Suprosanna Shit",
      "Jindong Gu",
      "Rajat Koner",
      "Aljoa Oep",
      "Laura Leal-Taix",
      "Thomas Seidl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14770v1",
    "title": "MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision\n  and Spiking Neural Networks",
    "summary": "Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in\nenvironments, where conventional radio-based methods suffer from spectrum\ncongestion, jamming, and high power consumption. Inspired by the waggle dance\nof honeybees, which efficiently communicate the location of food sources\nwithout sound or contact, we propose a novel visual communication framework for\nMAV swarms using motion-based signaling. In this framework, MAVs convey\ninformation, such as heading and distance, through deliberate flight patterns,\nwhich are passively captured by event cameras and interpreted using a\npredefined visual codebook of four motion primitives: vertical (up/down),\nhorizontal (left/right), left-to-up-to-right, and left-to-down-to-right,\nrepresenting control symbols (``start'', ``end'', ``1'', ``0''). To decode\nthese signals, we design an event frame-based segmentation model and a\nlightweight Spiking Neural Network (SNN) for action recognition. An integrated\ndecoding algorithm then combines segmentation and classification to robustly\ninterpret MAV motion sequences. Experimental results validate the framework's\neffectiveness, which demonstrates accurate decoding and low power consumption,\nand highlights its potential as an energy-efficient alternative for MAV\ncommunication in constrained environments.",
    "published": "2025-10-16T15:06:51Z",
    "updated": "2025-10-16T15:06:51Z",
    "link": "http://arxiv.org/pdf/2510.14770v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhang Nengbo",
      "Hann Woei Ho",
      "Ye Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12126v3",
    "title": "MetaCaptioner: Towards Generalist Visual Captioning with Open-source\n  Suites",
    "summary": "Generalist visual captioning goes beyond a simple appearance description\ntask, but requires integrating a series of visual cues into a caption and\nhandling various visual domains. In this task, current open-source models\npresent a large performance gap with commercial ones, which limits various\napplications such as data synthesis. To bridge the gap, this paper proposes\nCapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for\nthe first time that, by capitalizing on open-source models, it is possible to\nachieve caption quality on par with GPT-4.1 in various domains with an 89.5%\nreduction in costs. By leveraging CapFlow as the data synthesizer, we produce\nhigh-quality visual captions from image and video domains at scale, and obtain\na generalist visual captioner via fine-tuning, namely MetaCaptioner. Through\nextensive experiments, we show that MetaCaptioner not only achieves comparable\ncaptioning capabilities with commercial models but also reaches top-tier\nmultimodal performance in the open-source community. We hope CapFlow and\nMetaCaptioner can benefit future multimodal research by providing a strong and\ncost-effective visual captioning solution.",
    "published": "2025-10-14T04:03:25Z",
    "updated": "2025-10-16T14:57:08Z",
    "link": "http://arxiv.org/pdf/2510.12126v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhenxin Lei",
      "Zhangwei Gao",
      "Changyao Tian",
      "Erfei Cui",
      "Guanzhou Chen",
      "Danni Yang",
      "Yuchen Duan",
      "Zhaokai Wang",
      "Wenhao Li",
      "Weiyun Wang",
      "Xiangyu Zhao",
      "Jiayi Ji",
      "Yu Qiao",
      "Wenhai Wang",
      "Gen Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14753v1",
    "title": "LightQANet: Quantized and Adaptive Feature Learning for Low-Light Image\n  Enhancement",
    "summary": "Low-light image enhancement (LLIE) aims to improve illumination while\npreserving high-quality color and texture. However, existing methods often fail\nto extract reliable feature representations due to severely degraded\npixel-level information under low-light conditions, resulting in poor texture\nrestoration, color inconsistency, and artifact. To address these challenges, we\npropose LightQANet, a novel framework that introduces quantized and adaptive\nfeature learning for low-light enhancement, aiming to achieve consistent and\nrobust image quality across diverse lighting conditions. From the static\nmodeling perspective, we design a Light Quantization Module (LQM) to explicitly\nextract and quantify illumination-related factors from image features. By\nenforcing structured light factor learning, LQM enhances the extraction of\nlight-invariant representations and mitigates feature inconsistency across\nvarying illumination levels. From the dynamic adaptation perspective, we\nintroduce a Light-Aware Prompt Module (LAPM), which encodes illumination priors\ninto learnable prompts to dynamically guide the feature learning process. LAPM\nenables the model to flexibly adapt to complex and continuously changing\nlighting conditions, further improving image enhancement. Extensive experiments\non multiple low-light datasets demonstrate that our method achieves\nstate-of-the-art performance, delivering superior qualitative and quantitative\nresults across various challenging lighting scenarios.",
    "published": "2025-10-16T14:54:42Z",
    "updated": "2025-10-16T14:54:42Z",
    "link": "http://arxiv.org/pdf/2510.14753v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xu Wu",
      "Zhihui Lai",
      "Xianxu Hou",
      "Jie Zhou",
      "Ya-nan Zhang",
      "Linlin Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.09868v5",
    "title": "The Principle of Uncertain Maximum Entropy",
    "summary": "The Principle of Maximum Entropy is a rigorous technique for estimating an\nunknown distribution given partial information while simultaneously minimizing\nbias. However, an important requirement for applying the principle is that the\navailable information be provided error-free (Jaynes 1982). We relax this\nrequirement using a memoryless communication channel as a framework to derive a\nnew, more general principle. We show our new principle provides an upper bound\non the entropy of the unknown distribution and the amount of information lost\ndue to the use of a given communications channel is unknown unless the unknown\ndistribution's entropy is also known. Using our new principle we provide a new\ninterpretation of the classic principle and experimentally show its performance\nrelative to the classic principle and other generally applicable solutions.\nFinally, we present a simple algorithm for solving our new principle and an\napproximation useful when samples are limited.",
    "published": "2023-05-17T00:45:41Z",
    "updated": "2025-10-16T14:36:12Z",
    "link": "http://arxiv.org/pdf/2305.09868v5.pdf",
    "category": [
      "cs.IT",
      "cs.CV",
      "cs.LG",
      "math.IT"
    ],
    "authors": [
      "Kenneth Bogert",
      "Matthew Kothe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14737v1",
    "title": "Free-Grained Hierarchical Recognition",
    "summary": "Hierarchical image classification predicts labels across a semantic taxonomy,\nbut existing methods typically assume complete, fine-grained annotations, an\nassumption rarely met in practice. Real-world supervision varies in\ngranularity, influenced by image quality, annotator expertise, and task\ndemands; a distant bird may be labeled Bird, while a close-up reveals Bald\neagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet\nand structured into cognitively inspired basic, subordinate, and fine-grained\nlevels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic,\nmixed-granularity labels reflecting human annotation behavior. We propose\nfree-grain learning, with heterogeneous supervision across instances. We\ndevelop methods that enhance semantic guidance via pseudo-attributes from\nvision-language models and visual guidance via semi-supervised learning. These,\nalong with strong baselines, substantially improve performance under mixed\nsupervision. Together, our benchmark and methods advance hierarchical\nclassification under real-world constraints.",
    "published": "2025-10-16T14:35:18Z",
    "updated": "2025-10-16T14:35:18Z",
    "link": "http://arxiv.org/pdf/2510.14737v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Seulki Park",
      "Zilin Wang",
      "Stella X. Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14726v1",
    "title": "Cross-Layer Feature Self-Attention Module for Multi-Scale Object\n  Detection",
    "summary": "Recent object detection methods have made remarkable progress by leveraging\nattention mechanisms to improve feature discriminability. However, most\nexisting approaches are confined to refining single-layer or fusing dual-layer\nfeatures, overlooking the rich inter-layer dependencies across multi-scale\nrepresentations. This limits their ability to capture comprehensive contextual\ninformation essential for detecting objects with large scale variations. In\nthis paper, we propose a novel Cross-Layer Feature Self-Attention Module\n(CFSAM), which holistically models both local and global dependencies within\nmulti-scale feature maps. CFSAM consists of three key components: a\nconvolutional local feature extractor, a Transformer-based global modeling unit\nthat efficiently captures cross-layer interactions, and a feature fusion\nmechanism to restore and enhance the original representations. When integrated\ninto the SSD300 framework, CFSAM significantly boosts detection performance,\nachieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO\n(vs. 43.1% baseline), outperforming existing attention modules. Moreover, the\nmodule accelerates convergence during training without introducing substantial\ncomputational overhead. Our work highlights the importance of explicit\ncross-layer attention modeling in advancing multi-scale object detection.",
    "published": "2025-10-16T14:25:21Z",
    "updated": "2025-10-16T14:25:21Z",
    "link": "http://arxiv.org/pdf/2510.14726v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dingzhou Xie",
      "Rushi Lan",
      "Cheng Pang",
      "Enhao Ning",
      "Jiahao Zeng",
      "Wei Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.09359v2",
    "title": "Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D\n  Ultrasound Localization Microscopy",
    "summary": "Ultrasound Localization Microscopy (ULM) is a non-invasive technique that\nallows for the imaging of micro-vessels in vivo, at depth and with a resolution\non the order of ten microns. ULM is based on the sub-resolution localization of\nindividual microbubbles injected in the bloodstream. Mapping the whole\nangioarchitecture requires the accumulation of microbubbles trajectories from\nthousands of frames, typically acquired over a few minutes. ULM acquisition\ntimes can be reduced by increasing the microbubble concentration, but requires\nmore advanced algorithms to detect them individually. Several deep learning\napproaches have been proposed for this task, but they remain limited to 2D\nimaging, in part due to the associated large memory requirements. Herein, we\npropose to use sparse tensor neural networks to reduce memory usage in 2D and\nto improve the scaling of the memory requirement for the extension of deep\nlearning architecture to 3D. We study several approaches to efficiently convert\nultrasound data into a sparse format and study the impact of the associated\nloss of information. When applied in 2D, the sparse formulation reduces the\nmemory requirements by a factor 2 at the cost of a small reduction of\nperformance when compared against dense networks. In 3D, the proposed approach\nreduces memory requirements by two order of magnitude while largely\noutperforming conventional ULM in high concentration settings. We show that\nSparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense\ndeep learning based method in 2D ULM i.e. the use of higher concentration in\nsilico and reduced acquisition time.",
    "published": "2024-02-14T18:03:58Z",
    "updated": "2025-10-16T14:15:57Z",
    "link": "http://arxiv.org/pdf/2402.09359v2.pdf",
    "category": [
      "eess.IV",
      "cs.CV",
      "I.4.9"
    ],
    "authors": [
      "Brice Rauby",
      "Paul Xing",
      "Jonathan Pore",
      "Maxime Gasse",
      "Jean Provost"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14705v1",
    "title": "Leveraging Learned Image Prior for 3D Gaussian Compression",
    "summary": "Compression techniques for 3D Gaussian Splatting (3DGS) have recently\nachieved considerable success in minimizing storage overhead for 3D Gaussians\nwhile preserving high rendering quality. Despite the impressive storage\nreduction, the lack of learned priors restricts further advances in the\nrate-distortion trade-off for 3DGS compression tasks. To address this, we\nintroduce a novel 3DGS compression framework that leverages the powerful\nrepresentational capacity of learned image priors to recover\ncompression-induced quality degradation. Built upon initially compressed\nGaussians, our restoration network effectively models the compression artifacts\nin the image space between degraded and original Gaussians. To enhance the\nrate-distortion performance, we provide coarse rendering residuals into the\nrestoration network as side information. By leveraging the supervision of\nrestored images, the compressed Gaussians are refined, resulting in a highly\ncompact representation with enhanced rendering performance. Our framework is\ndesigned to be compatible with existing Gaussian compression methods, making it\nbroadly applicable across different baselines. Extensive experiments validate\nthe effectiveness of our framework, demonstrating superior rate-distortion\nperformance and outperforming the rendering quality of state-of-the-art 3DGS\ncompression methods while requiring substantially less storage.",
    "published": "2025-10-16T14:10:02Z",
    "updated": "2025-10-16T14:10:02Z",
    "link": "http://arxiv.org/pdf/2510.14705v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Seungjoo Shin",
      "Jaesik Park",
      "Sunghyun Cho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24899v2",
    "title": "Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion\n  Transformer",
    "summary": "Transformer-based video diffusion models (VDMs) deliver state-of-the-art\nvideo generation quality but are constrained by the quadratic cost of\nself-attention, making long sequences and high resolutions computationally\nexpensive. While linear attention offers sub-quadratic complexity, prior\nattempts fail to match the expressiveness of softmax attention without costly\nretraining. We introduce Attention Surgery, an efficient framework for\nlinearizing or hybridizing attention in pretrained VDMs without training from\nscratch. Inspired by recent advances in language models, our method combines a\nnovel hybrid attention mechanism-mixing softmax and linear tokens-with a\nlightweight distillation and fine-tuning pipeline requiring only a few\nGPU-days. Additionally, we incorporate a cost-aware block-rate strategy to\nbalance expressiveness and efficiency across layers. Applied to Wan2.1 1.3B, a\nstate-of-the-art DiT-based VDM, Attention Surgery achieves the first\ncompetitive sub-quadratic attention video diffusion models, reducing attention\ncost by up to 40\\% in terms of FLOPs, while maintaining generation quality as\nmeasured on the standard VBench and VBench-2.0 benchmarks. Project page is\navailable at: https://qualcomm-ai-research.github.io/attention-surgery.",
    "published": "2025-09-29T15:09:51Z",
    "updated": "2025-10-16T13:51:39Z",
    "link": "http://arxiv.org/pdf/2509.24899v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mohsen Ghafoorian",
      "Denis Korzhenkov",
      "Amirhossein Habibian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23402v2",
    "title": "WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for\n  Autonomous Driving",
    "summary": "Recent advances in driving-scene generation and reconstruction have\ndemonstrated significant potential for enhancing autonomous driving systems by\nproducing scalable and controllable training data. Existing generation methods\nprimarily focus on synthesizing diverse and high-fidelity driving videos;\nhowever, due to limited 3D consistency and sparse viewpoint coverage, they\nstruggle to support convenient and high-quality novel-view synthesis (NVS).\nConversely, recent 3D/4D reconstruction approaches have significantly improved\nNVS for real-world driving scenes, yet inherently lack generative capabilities.\nTo overcome this dilemma between scene generation and reconstruction, we\npropose WorldSplat, a novel feed-forward framework for 4D driving-scene\ngeneration. Our approach effectively generates consistent multi-track videos\nthrough two key steps: (i) We introduce a 4D-aware latent diffusion model\nintegrating multi-modal information to produce pixel-aligned 4D Gaussians in a\nfeed-forward manner. (ii) Subsequently, we refine the novel view videos\nrendered from these Gaussians using a enhanced video diffusion model. Extensive\nexperiments conducted on benchmark datasets demonstrate that WorldSplat\neffectively generates high-fidelity, temporally and spatially consistent\nmulti-track novel view driving videos. Project:\nhttps://wm-research.github.io/worldsplat/",
    "published": "2025-09-27T16:47:44Z",
    "updated": "2025-10-16T13:32:53Z",
    "link": "http://arxiv.org/pdf/2509.23402v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ziyue Zhu",
      "Zhanqian Wu",
      "Zhenxin Zhu",
      "Lijun Zhou",
      "Haiyang Sun",
      "Bing Wan",
      "Kun Ma",
      "Guang Chen",
      "Hangjun Ye",
      "Jin Xie",
      "jian Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14672v1",
    "title": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning",
    "summary": "In recent years, video question answering based on multimodal large language\nmodels (MLLM) has garnered considerable attention, due to the benefits from the\nsubstantial advancements in LLMs. However, these models have a notable\ndeficiency in the domains of video temporal grounding and reasoning, posing\nchallenges to the development of effective real-world video understanding\nsystems. Inspired by how humans use video players to interact with the progress\nbar for video comprehension, we introduce VTimeCoT, a simple yet effective\ntraining-free framework, designed for high-performance video grounding and\nreasoning. The proposed framework incorporates two novel visual tools of the\nprogress bar: a plug-and-play progress bar integration tool and a\nhigh-efficiency highlighting tool. In addition, to address the limitations of\nconventional text-based chain-of-thought (CoT) approaches, we introduce a\nvisuotemporal CoT process that integrates cross-modality reasoning across both\nvideo and text. Our approach demonstrates significant performance improvements\non both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and\nreasoning-based question answering. Finally, we showcase that the proposed\nframework achieves a compositional and interpretable reasoning process. Project\npage: https://vtimecot.github.io",
    "published": "2025-10-16T13:29:02Z",
    "updated": "2025-10-16T13:29:02Z",
    "link": "http://arxiv.org/pdf/2510.14672v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jinglei Zhang",
      "Yuanfan Guo",
      "Rolandos Alexandros Potamias",
      "Jiankang Deng",
      "Hang Xu",
      "Chao Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14668v1",
    "title": "WeCKD: Weakly-supervised Chained Distillation Network for Efficient\n  Multimodal Medical Imaging",
    "summary": "Knowledge distillation (KD) has traditionally relied on a static\nteacher-student framework, where a large, well-trained teacher transfers\nknowledge to a single student model. However, these approaches often suffer\nfrom knowledge degradation, inefficient supervision, and reliance on either a\nvery strong teacher model or large labeled datasets, which limits their\neffectiveness in real-world, limited-data scenarios. To address these, we\npresent the first-ever Weakly-supervised Chain-based KD network (WeCKD) that\nredefines knowledge transfer through a structured sequence of interconnected\nmodels. Unlike conventional KD, it forms a progressive distillation chain,\nwhere each model not only learns from its predecessor but also refines the\nknowledge before passing it forward. This structured knowledge transfer further\nenhances feature learning, reduces data dependency, and mitigates the\nlimitations of one-step KD. Each model in the distillation chain is trained on\nonly a fraction of the dataset and demonstrates that effective learning can be\nachieved with minimal supervision. Extensive evaluations across four otoscopic\nimaging datasets demonstrate that it not only matches but in many cases\nsurpasses the performance of existing supervised methods. Experimental results\non two other datasets further underscore its generalization across diverse\nmedical imaging modalities, including microscopic and magnetic resonance\nimaging. Furthermore, our evaluations resulted in cumulative accuracy gains of\nup to +23% over a single backbone trained on the same limited data, which\nhighlights its potential for real-world adoption.",
    "published": "2025-10-16T13:22:51Z",
    "updated": "2025-10-16T13:22:51Z",
    "link": "http://arxiv.org/pdf/2510.14668v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Md. Abdur Rahman",
      "Mohaimenul Azam Khan Raiaan",
      "Sami Azam",
      "Asif Karim",
      "Jemima Beissbarth",
      "Amanda Leach"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14661v1",
    "title": "EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal\n  Mining Footprint Analysis in the European Union (2015-2024)",
    "summary": "Mining activities are essential for industrial and economic development, but\nremain a leading source of environmental degradation, contributing to\ndeforestation, soil erosion, and water contamination. Sustainable resource\nmanagement and environmental governance require consistent, long-term\nmonitoring of mining-induced land surface changes, yet existing datasets are\noften limited in temporal depth or geographic scope. To address this gap, we\npresent EuroMineNet, the first comprehensive multitemporal benchmark for mining\nfootprint mapping and monitoring based on Sentinel-2 multispectral imagery.\nSpanning 133 mining sites across the European Union, EuroMineNet provides\nannual observations and expert-verified annotations from 2015 to 2024, enabling\nGeoAI-based models to analyze environmental dynamics at a continental scale. It\nsupports two sustainability-driven tasks: (1) multitemporal mining footprint\nmapping for consistent annual land-use delineation, evaluated with a novel\nChange-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change\ndetection to capture both gradual and abrupt surface transformations.\nBenchmarking 20 state-of-the-art deep learning models reveals that while GeoAI\nmethods effectively identify long-term environmental changes, challenges remain\nin detecting short-term dynamics critical for timely mitigation. By advancing\ntemporally consistent and explainable mining monitoring, EuroMineNet\ncontributes to sustainable land-use management, environmental resilience, and\nthe broader goal of applying GeoAI for social and environmental good. We\nrelease the codes and datasets by aligning with FAIR and the open science\nparadigm at https://github.com/EricYu97/EuroMineNet.",
    "published": "2025-10-16T13:15:53Z",
    "updated": "2025-10-16T13:15:53Z",
    "link": "http://arxiv.org/pdf/2510.14661v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Weikang Yu",
      "Vincent Nwazelibe",
      "Xianping Ma",
      "Xiaokang Zhang",
      "Richard Gloaguen",
      "Xiao Xiang Zhu",
      "Pedram Ghamisi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.15693v2",
    "title": "SCENEFORGE: Enhancing 3D-text alignment with Structured Scene\n  Compositions",
    "summary": "The whole is greater than the sum of its parts-even in 3D-text contrastive\nlearning. We introduce SceneForge, a novel framework that enhances contrastive\nalignment between 3D point clouds and text through structured multi-object\nscene compositions. SceneForge leverages individual 3D shapes to construct\nmulti-object scenes with explicit spatial relations, pairing them with coherent\nmulti-object descriptions refined by a large language model. By augmenting\ncontrastive training with these structured, compositional samples, SceneForge\neffectively addresses the scarcity of large-scale 3D-text datasets,\nsignificantly enriching data complexity and diversity. We systematically\ninvestigate critical design elements, such as the optimal number of objects per\nscene, the proportion of compositional samples in training batches, and scene\nconstruction strategies. Extensive experiments demonstrate that SceneForge\ndelivers substantial performance gains across multiple tasks, including\nzero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet,\nas well as few-shot part segmentation on ShapeNetPart. SceneForge's\ncompositional augmentations are model-agnostic, consistently improving\nperformance across multiple encoder architectures. Moreover, SceneForge\nimproves 3D visual question answering on ScanQA, generalizes robustly to\nretrieval scenarios with increasing scene complexity, and showcases spatial\nreasoning capabilities by adapting spatial configurations to align precisely\nwith textual instructions.",
    "published": "2025-09-19T07:13:45Z",
    "updated": "2025-10-16T13:14:57Z",
    "link": "http://arxiv.org/pdf/2509.15693v2.pdf",
    "category": [
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Cristian Sbrolli",
      "Matteo Matteucci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14657v1",
    "title": "Decorrelation Speeds Up Vision Transformers",
    "summary": "Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields\nstrong performance in low-label regimes but comes with substantial\ncomputational costs, making it impractical in time- and resource-constrained\nindustrial settings. We address this by integrating Decorrelated\nBackpropagation (DBP) into MAE pre-training, an optimization method that\niteratively reduces input correlations at each layer to accelerate convergence.\nApplied selectively to the encoder, DBP achieves faster pre-training without\nloss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE\nreduces wall-clock time to baseline performance by 21.1%, lowers carbon\nemissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe\nsimilar gains when pre-training and fine-tuning on proprietary industrial data,\nconfirming the method's applicability in real-world scenarios. These results\ndemonstrate that DBP can reduce training time and energy use while improving\ndownstream performance for large-scale ViT pre-training.",
    "published": "2025-10-16T13:13:12Z",
    "updated": "2025-10-16T13:13:12Z",
    "link": "http://arxiv.org/pdf/2510.14657v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Kieran Carrigg",
      "Rob van Gastel",
      "Melda Yeghaian",
      "Sander Dalm",
      "Faysal Boughorbel",
      "Marcel van Gerven"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.02901v3",
    "title": "Online Continual Learning via Spiking Neural Networks with Sleep\n  Enhanced Latent Replay",
    "summary": "Edge computing scenarios necessitate the development of hardware-efficient\nonline continual learning algorithms to be adaptive to dynamic environment.\nHowever, existing algorithms always suffer from high memory overhead and bias\ntowards recently trained tasks. To tackle these issues, this paper proposes a\nnovel online continual learning approach termed as SESLR, which incorporates a\nsleep enhanced latent replay scheme with spiking neural networks (SNNs). SESLR\nleverages SNNs' binary spike characteristics to store replay features in single\nbits, significantly reducing memory overhead. Furthermore, inspired by\nbiological sleep-wake cycles, SESLR introduces a noise-enhanced sleep phase\nwhere the model exclusively trains on replay samples with controlled noise\ninjection, effectively mitigating classification bias towards new classes.\nExtensive experiments on both conventional (MNIST, CIFAR10) and neuromorphic\n(NMNIST, CIFAR10-DVS) datasets demonstrate SESLR's effectiveness. On Split\nCIFAR10, SESLR achieves nearly 30% improvement in average accuracy with only\none-third of the memory consumption compared to baseline methods. On Split\nCIFAR10-DVS, it improves accuracy by approximately 10% while reducing memory\noverhead by a factor of 32. These results validate SESLR as a promising\nsolution for online continual learning in resource-constrained edge computing\nscenarios.",
    "published": "2025-06-23T12:22:39Z",
    "updated": "2025-10-16T12:58:52Z",
    "link": "http://arxiv.org/pdf/2507.02901v3.pdf",
    "category": [
      "cs.NE",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Erliang Lin",
      "Wenbin Luo",
      "Wei Jia",
      "Yu Chen",
      "Shaofu Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14634v1",
    "title": "SteeringTTA: Guiding Diffusion Trajectories for Robust\n  Test-Time-Adaptation",
    "summary": "Test-time adaptation (TTA) aims to correct performance degradation of deep\nmodels under distribution shifts by updating models or inputs using unlabeled\ntest data. Input-only diffusion-based TTA methods improve robustness for\nclassification to corruptions but rely on gradient guidance, limiting\nexploration and generalization across distortion types. We propose SteeringTTA,\nan inference-only framework that adapts Feynman-Kac steering to guide\ndiffusion-based input adaptation for classification with rewards driven by\npseudo-label. SteeringTTA maintains multiple particle trajectories, steered by\na combination of cumulative top-K probabilities and an entropy schedule, to\nbalance exploration and confidence. On ImageNet-C, SteeringTTA consistently\noutperforms the baseline without any model updates or source data.",
    "published": "2025-10-16T12:46:53Z",
    "updated": "2025-10-16T12:46:53Z",
    "link": "http://arxiv.org/pdf/2510.14634v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jihyun Yu",
      "Yoojin Oh",
      "Wonho Bae",
      "Mingyu Kim",
      "Junhyug Noh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.01247v3",
    "title": "Multi-level Reliable Guidance for Unpaired Multi-view Clustering",
    "summary": "In this thesis, we address the challenging problem of unpaired multi-view\nclustering (UMC), which aims to achieve effective joint clustering using\nunpaired samples observed across multiple views. Traditional incomplete\nmulti-view clustering (IMC) methods typically rely on paired samples to capture\ncomplementary information between views. However, such strategies become\nimpractical in the UMC due to the absence of paired samples. Although some\nresearchers have attempted to address this issue by preserving consistent\ncluster structures across views, effectively mining such consistency remains\nchallenging when the cluster structures {with low confidence}. Therefore, we\npropose a novel method, Multi-level Reliable Guidance for UMC (MRG-UMC), which\nintegrates multi-level clustering and reliable view guidance to learn\nconsistent and confident cluster structures from three perspectives.\nSpecifically, inner-view multi-level clustering exploits high-confidence sample\npairs across different levels to reduce the impact of boundary samples,\nresulting in more confident cluster structures. Synthesized-view alignment\nleverages a synthesized-view to mitigate cross-view discrepancies and promote\nconsistency. Cross-view guidance employs a reliable view guidance strategy to\nenhance the clustering confidence of poorly clustered views. These three\nmodules are jointly optimized across multiple levels to achieve consistent and\nconfident cluster structures. Furthermore, theoretical analyses verify the\neffectiveness of MRG-UMC in enhancing clustering confidence. Extensive\nexperimental results show that MRG-UMC outperforms state-of-the-art UMC\nmethods, achieving an average NMI improvement of 12.95\\% on multi-view\ndatasets. {The source code is available at:\nhttps://anonymous.4open.science/r/MRG-UMC-5E20.",
    "published": "2024-07-01T12:49:55Z",
    "updated": "2025-10-16T12:45:00Z",
    "link": "http://arxiv.org/pdf/2407.01247v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Like Xin",
      "Wanqi Yang",
      "Lei Wang",
      "Ming Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14630v1",
    "title": "Adapting Self-Supervised Representations as a Latent Space for Efficient\n  Generation",
    "summary": "We introduce Representation Tokenizer (RepTok), a generative modeling\nframework that represents an image using a single continuous latent token\nobtained from self-supervised vision transformers. Building on a pre-trained\nSSL encoder, we fine-tune only the semantic token embedding and pair it with a\ngenerative decoder trained jointly using a standard flow matching objective.\nThis adaptation enriches the token with low-level, reconstruction-relevant\ndetails, enabling faithful image reconstruction. To preserve the favorable\ngeometry of the original SSL space, we add a cosine-similarity loss that\nregularizes the adapted token, ensuring the latent space remains smooth and\nsuitable for generation. Our single-token formulation resolves spatial\nredundancies of 2D latent spaces and significantly reduces training costs.\nDespite its simplicity and efficiency, RepTok achieves competitive results on\nclass-conditional ImageNet generation and naturally extends to text-to-image\nsynthesis, reaching competitive zero-shot performance on MS-COCO under\nextremely limited training budgets. Our findings highlight the potential of\nfine-tuned SSL representations as compact and effective latent spaces for\nefficient generative modeling.",
    "published": "2025-10-16T12:43:03Z",
    "updated": "2025-10-16T12:43:03Z",
    "link": "http://arxiv.org/pdf/2510.14630v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ming Gui",
      "Johannes Schusterbauer",
      "Timy Phan",
      "Felix Krause",
      "Josh Susskind",
      "Miguel Angel Bautista",
      "Bjrn Ommer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.08885v3",
    "title": "Perspective-Aware Teaching: Adapting Knowledge for Heterogeneous\n  Distillation",
    "summary": "Knowledge distillation (KD) involves transferring knowledge from a\npre-trained heavy teacher model to a lighter student model, thereby reducing\nthe inference cost while maintaining comparable effectiveness. Prior KD\ntechniques typically assume homogeneity between the teacher and student models.\nHowever, as technology advances, a wide variety of architectures have emerged,\nranging from initial Convolutional Neural Networks (CNNs) to Vision\nTransformers (ViTs), and Multi-Level Perceptrons (MLPs). Consequently,\ndeveloping a universal KD framework compatible with any architecture has become\nan important research topic. In this paper, we introduce a perspective-aware\nteaching (PAT) KD framework to enable feature distillation across diverse\narchitectures. Our framework comprises two key components. First, we design\nprompt tuning blocks that incorporate student feedback, allowing teacher\nfeatures to adapt to the student model's learning process. Second, we propose\nregion-aware attention to mitigate the view mismatch problem between\nheterogeneous architectures. By leveraging these two modules, effective\ndistillation of intermediate features can be achieved across heterogeneous\narchitectures. Extensive experiments on CIFAR, ImageNet, and COCO demonstrate\nthe superiority of the proposed method. Our code is available at\nhttps://github.com/jimmylin0979/PAT.git.",
    "published": "2025-01-15T15:56:06Z",
    "updated": "2025-10-16T12:40:46Z",
    "link": "http://arxiv.org/pdf/2501.08885v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jhe-Hao Lin",
      "Yi Yao",
      "Chan-Feng Hsu",
      "Hongxia Xie",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14627v1",
    "title": "GOPLA: Generalizable Object Placement Learning via Synthetic\n  Augmentation of Human Arrangement",
    "summary": "Robots are expected to serve as intelligent assistants, helping humans with\neveryday household organization. A central challenge in this setting is the\ntask of object placement, which requires reasoning about both semantic\npreferences (e.g., common-sense object relations) and geometric feasibility\n(e.g., collision avoidance). We present GOPLA, a hierarchical framework that\nlearns generalizable object placement from augmented human demonstrations. A\nmulti-modal large language model translates human instructions and visual\ninputs into structured plans that specify pairwise object relationships. These\nplans are then converted into 3D affordance maps with geometric common sense by\na spatial mapper, while a diffusion-based planner generates placement poses\nguided by test-time costs, considering multi-plan distributions and collision\navoidance. To overcome data scarcity, we introduce a scalable pipeline that\nexpands human placement demonstrations into diverse synthetic training data.\nExtensive experiments show that our approach improves placement success rates\nby 30.04 percentage points over the runner-up, evaluated on positioning\naccuracy and physical plausibility, demonstrating strong generalization across\na wide range of real-world robotic placement scenarios.",
    "published": "2025-10-16T12:38:14Z",
    "updated": "2025-10-16T12:38:14Z",
    "link": "http://arxiv.org/pdf/2510.14627v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Yao Zhong",
      "Hanzhi Chen",
      "Simon Schaefer",
      "Anran Zhang",
      "Stefan Leutenegger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14624v1",
    "title": "Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster\n  VLM Inference",
    "summary": "Vision-language models (VLMs) have recently expanded from static image\nunderstanding to video reasoning, but their scalability is fundamentally\nlimited by the quadratic cost of processing dense frame sequences. Long videos\noften exceed the token budget of modern language models, leading to severe\ncontext limitations and latency issues. We introduce Efficient Video Sampling\n(EVS), a simple, plug-and-play method for reducing token redundancy in videos\nby identifying and pruning temporally static patches -- spatial regions that\nremain unchanged across consecutive frames. EVS preserves positional identity,\nrequires no architectural changes or retraining. We show that EVS substantially\nreduces token count while maintaining semantic fidelity, enabling faster\ninference and longer input sequences. Applied at inference time, EVS reduces\nlarge language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal\naccuracy loss. When combined with an uptraining phase using stochastic pruning\nrates, EVS yields models that are robust to varying compression levels and\nretain full performance under aggressive pruning. Extensive experiments\ndemonstrate that EVS consistently improves efficiency-accuracy trade-offs,\nunlocking scalable video-language understanding without sacrificing quality.",
    "published": "2025-10-16T12:34:38Z",
    "updated": "2025-10-16T12:34:38Z",
    "link": "http://arxiv.org/pdf/2510.14624v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Natan Bagrov",
      "Eugene Khvedchenia",
      "Borys Tymchenko",
      "Shay Aharon",
      "Lior Kadoch",
      "Tomer Keren",
      "Ofri Masad",
      "Yonatan Geifman",
      "Ran Zilberstein",
      "Tuomas Rintamaki",
      "Matthieu Le",
      "Andrew Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.18853v2",
    "title": "3DOT: Texture Transfer for 3DGS Objects from a Single Reference Image",
    "summary": "3D texture swapping allows for the customization of 3D object textures,\nenabling efficient and versatile visual transformations in 3D editing. While no\ndedicated method exists, adapted 2D editing and text-driven 3D editing\napproaches can serve this purpose. However, 2D editing requires frame-by-frame\nmanipulation, causing inconsistencies across views, while text-driven 3D\nediting struggles to preserve texture characteristics from reference images. To\ntackle these challenges, we introduce 3DSwapping, a 3D texture swapping method\nthat integrates: 1) progressive generation, 2) view-consistency gradient\nguidance, and 3) prompt-tuned gradient guidance. To ensure view consistency,\nour progressive generation process starts by editing a single reference image\nand gradually propagates the edits to adjacent views. Our view-consistency\ngradient guidance further reinforces consistency by conditioning the generation\nmodel on feature differences between consistent and inconsistent outputs. To\npreserve texture characteristics, we introduce prompt-tuning-based gradient\nguidance, which learns a token that precisely captures the difference between\nthe reference image and the 3D object. This token then guides the editing\nprocess, ensuring more consistent texture preservation across views. Overall,\n3DSwapping integrates these novel strategies to achieve higher-fidelity texture\ntransfer while preserving structural coherence across multiple viewpoints.\nExtensive qualitative and quantitative evaluations confirm that our three novel\ncomponents enable convincing and effective 2D texture swapping for 3D objects.\nCode will be available upon acceptance.",
    "published": "2025-03-24T16:31:52Z",
    "updated": "2025-10-16T12:29:48Z",
    "link": "http://arxiv.org/pdf/2503.18853v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiao Cao",
      "Beibei Lin",
      "Bo Wang",
      "Zhiyong Huang",
      "Robby T. Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19230v2",
    "title": "DevFD: Developmental Face Forgery Detection by Learning Shared and\n  Orthogonal LoRA Subspaces",
    "summary": "The rise of realistic digital face generation and manipulation poses\nsignificant social risks. The primary challenge lies in the rapid and diverse\nevolution of generation techniques, which often outstrip the detection\ncapabilities of existing models. To defend against the ever-evolving new types\nof forgery, we need to enable our model to quickly adapt to new domains with\nlimited computation and data while avoiding forgetting previously learned\nforgery types. In this work, we posit that genuine facial samples are abundant\nand relatively stable in acquisition methods, while forgery faces continuously\nevolve with the iteration of manipulation techniques. Given the practical\ninfeasibility of exhaustively collecting all forgery variants, we frame face\nforgery detection as a continual learning problem and allow the model to\ndevelop as new forgery types emerge. Specifically, we employ a Developmental\nMixture of Experts (MoE) architecture that uses LoRA models as its individual\nexperts. These experts are organized into two groups: a Real-LoRA to learn and\nrefine knowledge of real faces, and multiple Fake-LoRAs to capture incremental\ninformation from different forgery types. To prevent catastrophic forgetting,\nwe ensure that the learning direction of Fake-LoRAs is orthogonal to the\nestablished subspace. Moreover, we integrate orthogonal gradients into the\northogonal loss of Fake-LoRAs, preventing gradient interference throughout the\ntraining process of each task. Experimental results under both the datasets and\nmanipulation types incremental protocols demonstrate the effectiveness of our\nmethod.",
    "published": "2025-09-23T16:52:27Z",
    "updated": "2025-10-16T12:27:20Z",
    "link": "http://arxiv.org/pdf/2509.19230v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tianshuo Zhang",
      "Li Gao",
      "Siran Peng",
      "Xiangyu Zhu",
      "Zhen Lei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14617v1",
    "title": "Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for\n  Tactical Understanding",
    "summary": "Tactical understanding in badminton involves interpreting not only individual\nactions but also how tactics are dynamically executed over time. In this paper,\nwe propose \\textbf{Shot2Tactic-Caption}, a novel framework for semantic and\ntemporal multi-scale video captioning in badminton, capable of generating\nshot-level captions that describe individual actions and tactic-level captions\nthat capture how these actions unfold over time within a tactical execution. We\nalso introduce the Shot2Tactic-Caption Dataset, the first badminton captioning\ndataset containing 5,494 shot captions and 544 tactic captions.\nShot2Tactic-Caption adopts a dual-branch design, with both branches including a\nvisual encoder, a spatio-temporal Transformer encoder, and a Transformer-based\ndecoder to generate shot and tactic captions. To support tactic captioning, we\nadditionally introduce a Tactic Unit Detector that identifies valid tactic\nunits, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic\ncaptioning, we further incorporate a shot-wise prompt-guided mechanism, where\nthe predicted tactic type and state are embedded as prompts and injected into\nthe decoder via cross-attention. The shot-wise prompt-guided mechanism enables\nour system not only to describe successfully executed tactics but also to\ncapture tactical executions that are temporarily interrupted and later resumed.\nExperimental results demonstrate the effectiveness of our framework in\ngenerating both shot and tactic captions. Ablation studies show that the\nResNet50-based spatio-temporal encoder outperforms other variants, and that\nshot-wise prompt structuring leads to more coherent and accurate tactic\ncaptioning.",
    "published": "2025-10-16T12:24:51Z",
    "updated": "2025-10-16T12:24:51Z",
    "link": "http://arxiv.org/pdf/2510.14617v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ning Ding",
      "Keisuke Fujii",
      "Toru Tamaki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13534v2",
    "title": "High Semantic Features for the Continual Learning of Complex Emotions: a\n  Lightweight Solution",
    "summary": "Incremental learning is a complex process due to potential catastrophic\nforgetting of old tasks when learning new ones. This is mainly due to transient\nfeatures that do not fit from task to task. In this paper, we focus on complex\nemotion recognition. First, we learn basic emotions and then, incrementally,\nlike humans, complex emotions. We show that Action Units, describing facial\nmuscle movements, are non-transient, highly semantical features that outperform\nthose extracted by both shallow and deep convolutional neural networks. Thanks\nto this ability, our approach achieves interesting results when learning\nincrementally complex, compound emotions with an accuracy of 0.75 on the CFEE\ndataset and can be favorably compared to state-of-the-art results. Moreover, it\nresults in a lightweight model with a small memory footprint.",
    "published": "2025-10-15T13:27:41Z",
    "updated": "2025-10-16T12:24:25Z",
    "link": "http://arxiv.org/pdf/2510.13534v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Thibault Geoffroy",
      "Gauthier Gerspacher",
      "Lionel Prevost"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22544v2",
    "title": "HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection",
    "summary": "Video anomaly detection (VAD) is crucial for intelligent surveillance, but a\nsignificant challenge lies in identifying complex anomalies, which are events\ndefined by intricate relationships and temporal dependencies among multiple\nentities rather than by isolated actions. While self-supervised learning (SSL)\nmethods effectively model low-level spatiotemporal patterns, they often\nstruggle to grasp the semantic meaning of these interactions. Conversely, large\nlanguage models (LLMs) offer powerful contextual reasoning but are\ncomputationally expensive for frame-by-frame analysis and lack fine-grained\nspatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly\nDetection, a hybrid SSL-LLM model that combines a multi-task SSL temporal\nanalyzer with LLM validator. The SSL module is built upon an nnFormer backbone\nwhich is a transformer-based model for image segmentation. It is trained with\nmultiple proxy tasks, learns from video frames to identify those suspected of\nanomaly. The selected frames are then forwarded to the LLM, which enriches the\nanalysis with semantic context by applying structured, rule-based reasoning to\nvalidate the presence of anomalies. Experiments on the challenging ComplexVAD\ndataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming\nexisting baselines by 12.5% while reducing LLM computation. We release our\ninteraction anomaly taxonomy, adaptive thresholding protocol, and code to\nfacilitate future research in complex VAD scenarios.",
    "published": "2025-09-26T16:20:06Z",
    "updated": "2025-10-16T12:00:43Z",
    "link": "http://arxiv.org/pdf/2509.22544v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mohammad Mahdi Hemmatyar",
      "Mahdi Jafari",
      "Mohammad Amin Yousefi",
      "Mohammad Reza Nemati",
      "Mobin Azadani",
      "Hamid Reza Rastad",
      "Amirmohammad Akbari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14596v1",
    "title": "Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating\n  Clustering and Continuous Similarity Ordering",
    "summary": "Camera traps generate millions of wildlife images, yet many datasets contain\nspecies that are absent from existing classifiers. This work evaluates\nzero-shot approaches for organizing unlabeled wildlife imagery using\nself-supervised vision transformers, developed and tested within the Animal\nDetect platform for camera trap analysis. We compare unsupervised clustering\nmethods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor)\ncombined with dimensionality reduction techniques (PCA, UMAP), and we\ndemonstrate continuous 1D similarity ordering via t-SNE projection. On a\n5-species test set with ground truth labels used only for evaluation, DINOv2\nwith UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D\nsorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent\nfor fish across 1,500 images. Based on these findings, we deployed continuous\nsimilarity ordering in production, enabling rapid exploratory analysis and\naccelerating manual annotation workflows for biodiversity monitoring.",
    "published": "2025-10-16T11:59:18Z",
    "updated": "2025-10-16T11:59:18Z",
    "link": "http://arxiv.org/pdf/2510.14596v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hugo Markoff",
      "Jevgenijs Galaktionovs"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14594v1",
    "title": "Hierarchical Re-Classification: Combining Animal Classification Models\n  with Vision Transformers",
    "summary": "State-of-the-art animal classification models like SpeciesNet provide\npredictions across thousands of species but use conservative rollup strategies,\nresulting in many animals labeled at high taxonomic levels rather than species.\nWe present a hierarchical re-classification system for the Animal Detect\nplatform that combines SpeciesNet EfficientNetV2-M predictions with CLIP\nembeddings and metric learning to refine high-level taxonomic labels toward\nspecies-level identification. Our five-stage pipeline (high-confidence\nacceptance, bird override, centroid building, triplet-loss metric learning, and\nadaptive cosine-distance scoring) is evaluated on a segment of the LILA BC\nDesert Lion Conservation dataset (4,018 images, 15,031 detections). After\nrecovering 761 bird detections from \"blank\" and \"animal\" labels, we re-classify\n456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving\nspecies-level identification for 64.9 percent",
    "published": "2025-10-16T11:57:07Z",
    "updated": "2025-10-16T11:57:07Z",
    "link": "http://arxiv.org/pdf/2510.14594v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hugo Markoff",
      "Jevgenijs Galaktionovs"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26645v3",
    "title": "TTT3R: 3D Reconstruction as Test-Time Training",
    "summary": "Modern Recurrent Neural Networks have become a competitive architecture for\n3D reconstruction due to their linear-time complexity. However, their\nperformance degrades significantly when applied beyond the training context\nlength, revealing limited length generalization. In this work, we revisit the\n3D reconstruction foundation models from a Test-Time Training perspective,\nframing their designs as an online learning problem. Building on this\nperspective, we leverage the alignment confidence between the memory state and\nincoming observations to derive a closed-form learning rate for memory updates,\nto balance between retaining historical information and adapting to new\nobservations. This training-free intervention, termed TTT3R, substantially\nimproves length generalization, achieving a $2\\times$ improvement in global\npose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU\nmemory to process thousands of images. Code available in\nhttps://rover-xingyu.github.io/TTT3R",
    "published": "2025-09-30T17:59:51Z",
    "updated": "2025-10-16T11:37:35Z",
    "link": "http://arxiv.org/pdf/2509.26645v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xingyu Chen",
      "Yue Chen",
      "Yuliang Xiu",
      "Andreas Geiger",
      "Anpei Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14576v1",
    "title": "CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural\n  Network for Vehicle Re-Identification",
    "summary": "This paper presents CALM-Net, a curvature-aware LiDAR point cloud-based\nmulti-branch neural network for vehicle re-identification. The proposed model\naddresses the challenge of learning discriminative and complementary features\nfrom three-dimensional point clouds to distinguish between vehicles. CALM-Net\nemploys a multi-branch architecture that integrates edge convolution, point\nattention, and a curvature embedding that characterizes local surface variation\nin point clouds. By combining these mechanisms, the model learns richer\ngeometric and contextual features that are well suited for the\nre-identification task. Experimental evaluation on the large-scale nuScenes\ndataset demonstrates that CALM-Net achieves a mean re-identification accuracy\nimprovement of approximately 1.97\\% points compared with the strongest baseline\nin our study. The results confirms the effectiveness of incorporating curvature\ninformation into deep learning architectures and highlight the benefit of\nmulti-branch feature learning for LiDAR point cloud-based vehicle\nre-identification.",
    "published": "2025-10-16T11:36:54Z",
    "updated": "2025-10-16T11:36:54Z",
    "link": "http://arxiv.org/pdf/2510.14576v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dongwook Lee",
      "Sol Han",
      "Jinwhan Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13800v2",
    "title": "Reasoning in Space via Grounding in the World",
    "summary": "In this paper, we claim that 3D visual grounding is the cornerstone of\nspatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to\nexplore the effective spatial representations that bridge the gap between them.\nExisting 3D LLMs suffer from the absence of a unified 3D representation capable\nof jointly capturing semantic and geometric information. This deficiency is\nmanifested either in poor performance on grounding or in an excessive reliance\non external modules, ultimately hindering the seamless integration of grounding\nand spatial reasoning. To address this, we propose a simple yet effective\ndual-path pooling mechanism that tightly aligns geometric features with both\nsemantic and positional cues, constructing a unified image patch-based 3D\nrepresentation that encapsulates all essential information without increasing\nthe number of input tokens. Leveraging this holistic representation,\nGS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely\nwithout external modules while delivering performance comparable to\nstate-of-the-art models, establishing a unified and self-contained framework\nfor 3D spatial reasoning. To further bridge grounding and spatial reasoning, we\nintroduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is\nmeticulously curated to include both 3D bounding box annotations for objects\nreferenced in reasoning questions and step-by-step reasoning paths that\nintegrate grounding as a core component of the problem-solving process.\nExtensive experiments demonstrate that GS-Reasoner achieves impressive results\non 3D visual grounding, which in turn significantly enhances its spatial\nreasoning capabilities, leading to state-of-the-art performance.",
    "published": "2025-10-15T17:58:08Z",
    "updated": "2025-10-16T11:18:03Z",
    "link": "http://arxiv.org/pdf/2510.13800v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yiming Chen",
      "Zekun Qi",
      "Wenyao Zhang",
      "Xin Jin",
      "Li Zhang",
      "Peidong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14564v1",
    "title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian\n  Splatting Training on GPU",
    "summary": "3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction\ntechnique. The traditional 3DGS training pipeline follows three sequential\nsteps: Gaussian densification, Gaussian projection, and color splatting.\nDespite its promising reconstruction quality, this conventional approach\nsuffers from three critical inefficiencies: (1) Skewed density allocation\nduring Gaussian densification, (2) Imbalanced computation workload during\nGaussian projection and (3) Fragmented memory access during color splatting.\n  To tackle the above challenges, we introduce BalanceGS, the algorithm-system\nco-design for efficient training in 3DGS. (1) At the algorithm level, we\npropose heuristic workload-sensitive Gaussian density control to automatically\nbalance point distributions - removing 80% redundant Gaussians in dense regions\nwhile filling gaps in sparse areas. (2) At the system level, we propose\nSimilarity-based Gaussian sampling and merging, which replaces the static\none-to-one thread-pixel mapping with adaptive workload distribution - threads\nnow dynamically process variable numbers of Gaussians based on local cluster\ndensity. (3) At the mapping level, we propose reordering-based memory access\nmapping strategy that restructures RGB storage and enables batch loading in\nshared memory.\n  Extensive experiments demonstrate that compared with 3DGS, our approach\nachieves a 1.44$\\times$ training speedup on a NVIDIA A100 GPU with negligible\nquality degradation.",
    "published": "2025-10-16T11:16:58Z",
    "updated": "2025-10-16T11:16:58Z",
    "link": "http://arxiv.org/pdf/2510.14564v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junyi Wu",
      "Jiaming Xu",
      "Jinhao Li",
      "Yongkang Zhou",
      "Jiayi Pan",
      "Xingyang Li",
      "Guohao Dai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14560v1",
    "title": "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video",
    "summary": "Envision an AI capable of functioning in human-like settings, moving beyond\nmere observation to actively understand, anticipate, and proactively respond to\nunfolding events. Towards this vision, we focus on the innovative task where,\ngiven ego-streaming video input, an assistant proactively answers diverse,\nevolving questions at the opportune moment, while maintaining synchronized\nperception and reasoning. This task embodies three key properties: (1)\nProactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized\nEfficiency. To evaluate and address these properties, we first introduce\nESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a\nnovel framework designed for their rigorous assessment. Secondly, we propose a\ncomprehensive technical pipeline to enable models to tackle this challenging\ntask. This pipeline comprises: (1) a data engine, (2) a multi-stage training\nstrategy, and (3) a proactive dynamic compression technique. Our proposed model\neffectively addresses these critical properties while outperforming multiple\nbaselines across diverse online and offline benchmarks. Project\nPage:https://zhangyl4.github.io/publications/eyes-wide-open/",
    "published": "2025-10-16T11:11:13Z",
    "updated": "2025-10-16T11:11:13Z",
    "link": "http://arxiv.org/pdf/2510.14560v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yulin Zhang",
      "Cheng Shi",
      "Yang Wang",
      "Sibei Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14553v1",
    "title": "Consistent text-to-image generation via scene de-contextualization",
    "summary": "Consistent text-to-image (T2I) generation seeks to produce\nidentity-preserving images of the same subject across diverse scenes, yet it\noften fails due to a phenomenon called identity (ID) shift. Previous methods\nhave tackled this issue, but typically rely on the unrealistic assumption of\nknowing all target scenes in advance. This paper reveals that a key source of\nID shift is the native correlation between subject and scene context, called\nscene contextualization, which arises naturally as T2I models fit the training\ndistribution of vast natural images. We formally prove the near-universality of\nthis scene-ID correlation and derive theoretical bounds on its strength. On\nthis basis, we propose a novel, efficient, training-free prompt embedding\nediting approach, called Scene De-Contextualization (SDeC), that imposes an\ninversion process of T2I's built-in scene contextualization. Specifically, it\nidentifies and suppresses the latent scene-ID correlation within the ID\nprompt's embedding by quantifying the SVD directional stability to adaptively\nre-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene\nuse (one scene per prompt) without requiring prior access to all target scenes.\nThis makes it a highly flexible and general solution well-suited to real-world\napplications where such prior knowledge is often unavailable or varies over\ntime. Experiments demonstrate that SDeC significantly enhances identity\npreservation while maintaining scene diversity.",
    "published": "2025-10-16T10:54:49Z",
    "updated": "2025-10-16T10:54:49Z",
    "link": "http://arxiv.org/pdf/2510.14553v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Song Tang",
      "Peihao Gong",
      "Kunyu Li",
      "Kai Guo",
      "Boyu Wang",
      "Mao Ye",
      "Jianwei Zhang",
      "Xiatian Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16944v2",
    "title": "Catching the Details: Self-Distilled RoI Predictors for Fine-Grained\n  MLLM Perception",
    "summary": "Multimodal Large Language Models (MLLMs) require high-resolution visual\ninformation to perform fine-grained perception, yet processing entire\nhigh-resolution images is computationally prohibitive. While recent methods\nleverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they\ntypically present a difficult trade-off: training-based approaches depend on\nlarge-scale annotated datasets, while training-free methods that utilize the\nmodel's internal attention are computationally inefficient and less accurate,\nrequiring either multi-pass prefill stages or reliance on the slow\nauto-regressive decoding process. In this paper, we propose an efficient,\nannotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves\nthis trade-off. The SD-RPN is built around a pipeline that transforms the noisy\nattention maps from the MLLM's middle layers into high-quality pseudo-RoI\nlabels by explicitly denoising the signal and resolving ambiguity. We use these\nlabels to train a lightweight Region Proposal Network (RPN) that learns a more\nprecise localization. This RPN is also highly efficient, predicting the RoI in\na single forward pass using features from the MLLM's middle layers, decoupling\nRoI identification from the auto-regressive generation and avoiding costly\nmulti-pass operations. To validate our approach, we integrate the framework\ninto multiple MLLM families. Despite being trained on only a few (e.g. 10K)\nquestion-answer pairs, our method demonstrates exceptional data efficiency and\ngeneralization, achieving over a 10% absolute accuracy improvement on unseen\nbenchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a\npractical and scalable solution for enhancing the fine-grained perception of\nMLLMs without requiring costly supervision or full model fine-tuning. Code is\navailable at https://github.com/YuHengsss/SD-RPN.",
    "published": "2025-09-21T06:54:04Z",
    "updated": "2025-10-16T10:53:17Z",
    "link": "http://arxiv.org/pdf/2509.16944v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuheng Shi",
      "Xiaohuan Pei",
      "Minjing Dong",
      "Chang Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14543v1",
    "title": "Exploring Cross-Modal Flows for Few-Shot Learning",
    "summary": "Aligning features from different modalities, is one of the most fundamental\nchallenges for cross-modal tasks. Although pre-trained vision-language models\ncan achieve a general alignment between image and text, they often require\nparameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT\nmethods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively\nfine-tune a subset of parameters, which can slightly adjust either visual or\ntextual features, and avoid overfitting. In this paper, we are the first to\nhighlight that all existing PEFT methods perform one-step adjustment. It is\ninsufficient for complex (or difficult) datasets, where features of different\nmodalities are highly entangled. To this end, we propose the first\nmodel-agnostic multi-step adjustment approach by learning a cross-modal\nvelocity field: Flow Matching Alignment (FMA). Specifically, to ensure the\ncorrespondence between categories during training, we first utilize a fixed\ncoupling strategy. Then, we propose a noise augmentation strategy to alleviate\nthe data scarcity issue. Finally, we design an early-stopping solver, which\nterminates the transformation process earlier, improving both efficiency and\naccuracy. Compared with one-step PEFT methods, FMA has the multi-step\nrectification ability to achieve more precise and robust alignment. Extensive\nresults have demonstrated that FMA can consistently yield significant\nperformance gains across various benchmarks and backbones, particularly on\nchallenging datasets.",
    "published": "2025-10-16T10:32:48Z",
    "updated": "2025-10-16T10:32:48Z",
    "link": "http://arxiv.org/pdf/2510.14543v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ziqi Jiang",
      "Yanghao Wang",
      "Long Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14536v1",
    "title": "Exploring Image Representation with Decoupled Classical Visual\n  Descriptors",
    "summary": "Exploring and understanding efficient image representations is a\nlong-standing challenge in computer vision. While deep learning has achieved\nremarkable progress across image understanding tasks, its internal\nrepresentations are often opaque, making it difficult to interpret how visual\ninformation is processed. In contrast, classical visual descriptors (e.g. edge,\ncolour, and intensity distribution) have long been fundamental to image\nanalysis and remain intuitively understandable to humans. Motivated by this\ngap, we ask a central question: Can modern learning benefit from these\nclassical cues? In this paper, we answer it with VisualSplit, a framework that\nexplicitly decomposes images into decoupled classical descriptors, treating\neach as an independent but complementary component of visual knowledge. Through\na reconstruction-driven pre-training scheme, VisualSplit learns to capture the\nessence of each visual descriptor while preserving their interpretability. By\nexplicitly decomposing visual attributes, our method inherently facilitates\neffective attribute control in various advanced visual tasks, including image\ngeneration and editing, extending beyond conventional classification and\nsegmentation, suggesting the effectiveness of this new learning approach for\nvisual understanding. Project page: https://chenyuanqu.com/VisualSplit/.",
    "published": "2025-10-16T10:27:55Z",
    "updated": "2025-10-16T10:27:55Z",
    "link": "http://arxiv.org/pdf/2510.14536v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chenyuan Qu",
      "Hao Chen",
      "Jianbo Jiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14535v1",
    "title": "Acquisition of interpretable domain information during brain MR image\n  harmonization for content-based image retrieval",
    "summary": "Medical images like MR scans often show domain shifts across imaging sites\ndue to scanner and protocol differences, which degrade machine learning\nperformance in tasks such as disease classification. Domain harmonization is\nthus a critical research focus. Recent approaches encode brain images\n$\\boldsymbol{x}$ into a low-dimensional latent space $\\boldsymbol{z}$, then\ndisentangle it into $\\boldsymbol{z_u}$ (domain-invariant) and\n$\\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these\nmethods often lack interpretability$-$an essential requirement in medical\napplications$-$leaving practical issues unresolved. We propose\nPseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a\ngeneral framework for domain harmonization and interpretable representation\nlearning that preserves disease-relevant information in brain MR images.\nPL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract\n$\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, a decoder to reconstruct the image\n$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the\nencoder and domain predictor, the model learns to reconstruct the input image\n$\\boldsymbol{x}$ by summing reconstructions from $\\boldsymbol{z_u}$ and\n$\\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared\nto prior methods, PL-SE-ADA achieves equal or better performance in image\nreconstruction, disease classification, and domain recognition. It also enables\nvisualization of both domain-independent brain features and domain-specific\ncomponents, offering high interpretability across the entire framework.",
    "published": "2025-10-16T10:27:21Z",
    "updated": "2025-10-16T10:27:21Z",
    "link": "http://arxiv.org/pdf/2510.14535v1.pdf",
    "category": [
      "cs.CV",
      "cs.IR"
    ],
    "authors": [
      "Keima Abe",
      "Hayato Muraki",
      "Shuhei Tomoshige",
      "Kenichi Oishi",
      "Hitoshi Iyatomi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14532v1",
    "title": "Towards Generalist Intelligence in Dentistry: Vision Foundation Models\n  for Oral and Maxillofacial Radiology",
    "summary": "Oral and maxillofacial radiology plays a vital role in dental healthcare, but\nradiographic image interpretation is limited by a shortage of trained\nprofessionals. While AI approaches have shown promise, existing dental AI\nsystems are restricted by their single-modality focus, task-specific design,\nand reliance on costly labeled data, hindering their generalization across\ndiverse clinical scenarios. To address these challenges, we introduce DentVFM,\nthe first family of vision foundation models (VFMs) designed for dentistry.\nDentVFM generates task-agnostic visual representations for a wide range of\ndental applications and uses self-supervised learning on DentVista, a large\ncurated dental imaging dataset with approximately 1.6 million multi-modal\nradiographic images from various medical centers. DentVFM includes 2D and 3D\nvariants based on the Vision Transformer (ViT) architecture. To address gaps in\ndental intelligence assessment and benchmarks, we introduce DentBench, a\ncomprehensive benchmark covering eight dental subspecialties, more diseases,\nimaging modalities, and a wide geographical distribution. DentVFM shows\nimpressive generalist intelligence, demonstrating robust generalization to\ndiverse dental tasks, such as disease diagnosis, treatment analysis, biomarker\nidentification, and anatomical landmark detection and segmentation.\nExperimental results indicate DentVFM significantly outperforms supervised,\nself-supervised, and weakly supervised baselines, offering superior\ngeneralization, label efficiency, and scalability. Additionally, DentVFM\nenables cross-modality diagnostics, providing more reliable results than\nexperienced dentists in situations where conventional imaging is unavailable.\nDentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and\nlabel-efficient model to improve intelligent dental healthcare and address\ncritical gaps in global oral healthcare.",
    "published": "2025-10-16T10:24:23Z",
    "updated": "2025-10-16T10:24:23Z",
    "link": "http://arxiv.org/pdf/2510.14532v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xinrui Huang",
      "Fan Xiao",
      "Dongming He",
      "Anqi Gao",
      "Dandan Li",
      "Xiaofan Zhang",
      "Shaoting Zhang",
      "Xudong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.21851v2",
    "title": "On Large Multimodal Models as Open-World Image Classifiers",
    "summary": "Traditional image classification requires a predefined list of semantic\ncategories. In contrast, Large Multimodal Models (LMMs) can sidestep this\nrequirement by classifying images directly using natural language (e.g.,\nanswering the prompt \"What is the main object in the image?\"). Despite this\nremarkable capability, most existing studies on LMM classification performance\nare surprisingly limited in scope, often assuming a closed-world setting with a\npredefined set of categories. In this work, we address this gap by thoroughly\nevaluating LMM classification performance in a truly open-world setting. We\nfirst formalize the task and introduce an evaluation protocol, defining various\nmetrics to assess the alignment between predicted and ground truth classes. We\nthen evaluate 13 models across 10 benchmarks, encompassing prototypical,\nnon-prototypical, fine-grained, and very fine-grained classes, demonstrating\nthe challenges LMMs face in this task. Further analyses based on the proposed\nmetrics reveal the types of errors LMMs make, highlighting challenges related\nto granularity and fine-grained capabilities, showing how tailored prompting\nand reasoning can alleviate them.",
    "published": "2025-03-27T17:03:18Z",
    "updated": "2025-10-16T10:21:51Z",
    "link": "http://arxiv.org/pdf/2503.21851v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Alessandro Conti",
      "Massimiliano Mancini",
      "Enrico Fini",
      "Yiming Wang",
      "Paolo Rota",
      "Elisa Ricci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25749v2",
    "title": "ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual\n  Try-On",
    "summary": "Virtual try-on (VITON) aims to generate realistic images of a person wearing\na target garment, requiring precise garment alignment in try-on regions and\nfaithful preservation of identity and background in non-try-on regions. While\nlatent diffusion models (LDMs) have advanced alignment and detail synthesis,\npreserving non-try-on regions remains challenging. A common post-hoc strategy\ndirectly replaces these regions with original content, but abrupt transitions\noften produce boundary artifacts. To overcome this, we reformulate VITON as a\nlinear inverse problem and adopt trajectory-aligned solvers that progressively\nenforce measurement consistency, reducing abrupt changes in non-try-on regions.\nHowever, existing solvers still suffer from semantic drift during generation,\nleading to artifacts. We propose ART-VITON, a measurement-guided diffusion\nframework that ensures measurement adherence while maintaining artifact-free\nsynthesis. Our method integrates residual prior-based initialization to\nmitigate training-inference mismatch and artifact-free measurement-guided\nsampling that combines data consistency, frequency-level correction, and\nperiodic standard denoising. Experiments on VITON-HD, DressCode, and SHHQ-1.0\ndemonstrate that ART-VITON effectively preserves identity and background,\neliminates boundary artifacts, and consistently improves visual fidelity and\nrobustness over state-of-the-art baselines.",
    "published": "2025-09-30T04:09:47Z",
    "updated": "2025-10-16T10:20:37Z",
    "link": "http://arxiv.org/pdf/2509.25749v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junseo Park",
      "Hyeryung Jang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14528v1",
    "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
    "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
    "published": "2025-10-16T10:18:48Z",
    "updated": "2025-10-16T10:18:48Z",
    "link": "http://arxiv.org/pdf/2510.14528v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Cheng Cui",
      "Ting Sun",
      "Suyin Liang",
      "Tingquan Gao",
      "Zelun Zhang",
      "Jiaxuan Liu",
      "Xueqing Wang",
      "Changda Zhou",
      "Hongen Liu",
      "Manhui Lin",
      "Yue Zhang",
      "Yubo Zhang",
      "Handong Zheng",
      "Jing Zhang",
      "Jun Zhang",
      "Yi Liu",
      "Dianhai Yu",
      "Yanjun Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14526v1",
    "title": "Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image\n  Misalignment in Diffusion Models",
    "summary": "In text-to-image generation, different initial noises induce distinct\ndenoising paths with a pretrained Stable Diffusion (SD) model. While this\npattern could output diverse images, some of them may fail to align well with\nthe prompt. Existing methods alleviate this issue either by altering the\ndenoising dynamics or by drawing multiple noises and conducting post-selection.\nIn this paper, we attribute the misalignment to a training-inference mismatch:\nduring training, prompt-conditioned noises lie in a prompt-specific subset of\nthe latent space, whereas at inference the noise is drawn from a\nprompt-agnostic Gaussian prior. To close this gap, we propose a noise projector\nthat applies text-conditioned refinement to the initial noise before denoising.\nConditioned on the prompt embedding, it maps the noise to a prompt-aware\ncounterpart that better matches the distribution observed during SD training,\nwithout modifying the SD model. Our framework consists of these steps: we first\nsample some noises and obtain token-level feedback for their corresponding\nimages from a vision-language model (VLM), then distill these signals into a\nreward model, and finally optimize the noise projector via a quasi-direct\npreference optimization. Our design has two benefits: (i) it requires no\nreference images or handcrafted priors, and (ii) it incurs small inference\ncost, replacing multi-sample selection with a single forward pass. Extensive\nexperiments further show that our prompt-aware noise projection improves\ntext-image alignment across diverse prompts.",
    "published": "2025-10-16T10:14:34Z",
    "updated": "2025-10-16T10:14:34Z",
    "link": "http://arxiv.org/pdf/2510.14526v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Yunze Tong",
      "Didi Zhu",
      "Zijing Hu",
      "Jinluan Yang",
      "Ziyu Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14516v1",
    "title": "Vision Mamba for Permeability Prediction of Porous Media",
    "summary": "Vision Mamba has recently received attention as an alternative to Vision\nTransformers (ViTs) for image classification. The network size of Vision Mamba\nscales linearly with input image resolution, whereas ViTs scale quadratically,\na feature that improves computational and memory efficiency. Moreover, Vision\nMamba requires a significantly smaller number of trainable parameters than\ntraditional convolutional neural networks (CNNs), and thus, they can be more\nmemory efficient. Because of these features, we introduce, for the first time,\na neural network that uses Vision Mamba as its backbone for predicting the\npermeability of three-dimensional porous media. We compare the performance of\nVision Mamba with ViT and CNN models across multiple aspects of permeability\nprediction and perform an ablation study to assess the effects of its\ncomponents on accuracy. We demonstrate in practice the aforementioned\nadvantages of Vision Mamba over ViTs and CNNs in the permeability prediction of\nthree-dimensional porous media. We make the source code publicly available to\nfacilitate reproducibility and to enable other researchers to build on and\nextend this work. We believe the proposed framework has the potential to be\nintegrated into large vision models in which Vision Mamba is used instead of\nViTs.",
    "published": "2025-10-16T10:02:33Z",
    "updated": "2025-10-16T10:02:33Z",
    "link": "http://arxiv.org/pdf/2510.14516v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ali Kashefi",
      "Tapan Mukerji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.13993v2",
    "title": "Mapping Farmed Landscapes from Remote Sensing",
    "summary": "Effective management of agricultural landscapes is critical for meeting\nglobal biodiversity targets, but efforts are hampered by the absence of\ndetailed, large-scale ecological maps. To address this, we introduce\nFarmscapes, the first large-scale (covering most of England), high-resolution\n(25cm) map of rural landscape features, including ecologically vital elements\nlike hedgerows, woodlands, and stone walls. This map was generated using a deep\nlearning segmentation model trained on a novel, dataset of 942 manually\nannotated tiles derived from aerial imagery. Our model accurately identifies\nkey habitats, achieving high f1-scores for woodland (96\\%) and farmed land\n(95\\%), and demonstrates strong capability in segmenting linear features, with\nan F1-score of 72\\% for hedgerows. By releasing the England-wide map on Google\nEarth Engine, we provide a powerful, open-access tool for ecologists and\npolicymakers. This work enables data-driven planning for habitat restoration,\nsupports the monitoring of initiatives like the EU Biodiversity Strategy, and\nlays the foundation for advanced analysis of landscape connectivity.",
    "published": "2025-06-16T20:50:05Z",
    "updated": "2025-10-16T10:00:07Z",
    "link": "http://arxiv.org/pdf/2506.13993v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Michelangelo Conserva",
      "Alex Wilson",
      "Charlotte Stanton",
      "Vishal Batchu",
      "Varun Gulshan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.18033v3",
    "title": "OmnimatteZero: Fast Training-free Omnimatte with Pre-trained Video\n  Diffusion Models",
    "summary": "In Omnimatte, one aims to decompose a given video into semantically\nmeaningful layers, including the background and individual objects along with\ntheir associated effects, such as shadows and reflections. Existing methods\noften require extensive training or costly self-supervised optimization. In\nthis paper, we present OmnimatteZero, a training-free approach that leverages\noff-the-shelf pre-trained video diffusion models for omnimatte. It can remove\nobjects from videos, extract individual object layers along with their effects,\nand composite those objects onto new videos. These are accomplished by adapting\nzero-shot image inpainting techniques for video object removal, a task they\nfail to handle effectively out-of-the-box. To overcome this, we introduce\ntemporal and spatial attention guidance modules that steer the diffusion\nprocess for accurate object removal and temporally consistent background\nreconstruction. We further show that self-attention maps capture information\nabout the object and its footprints and use them to inpaint the object's\neffects, leaving a clean background. Additionally, through simple latent\narithmetic, object layers can be isolated and recombined seamlessly with new\nvideo layers to produce new videos. Evaluations show that OmnimatteZero not\nonly achieves superior performance in terms of background reconstruction but\nalso sets a new record for the fastest Omnimatte approach, achieving real-time\nperformance with minimal frame runtime.",
    "published": "2025-03-23T11:26:48Z",
    "updated": "2025-10-16T09:59:36Z",
    "link": "http://arxiv.org/pdf/2503.18033v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dvir Samuel",
      "Matan Levy",
      "Nir Darshan",
      "Gal Chechik",
      "Rami Ben-Ari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.13571v4",
    "title": "Incomplete Multimodal Industrial Anomaly Detection via Cross-Modal\n  Distillation",
    "summary": "Recent studies of multimodal industrial anomaly detection (IAD) based on 3D\npoint clouds and RGB images have highlighted the importance of exploiting the\nredundancy and complementarity among modalities for accurate classification and\nsegmentation. However, achieving multimodal IAD in practical production lines\nremains a work in progress. It is essential to consider the trade-offs between\nthe costs and benefits associated with the introduction of new modalities while\nensuring compatibility with current processes. Existing quality control\nprocesses combine rapid in-line inspections, such as optical and infrared\nimaging with high-resolution but time-consuming near-line characterization\ntechniques, including industrial CT and electron microscopy to manually or\nsemi-automatically locate and analyze defects in the production of Li-ion\nbatteries and composite materials. Given the cost and time limitations, only a\nsubset of the samples can be inspected by all in-line and near-line methods,\nand the remaining samples are only evaluated through one or two forms of\nin-line inspection. To fully exploit data for deep learning-driven automatic\ndefect detection, the models must have the ability to leverage multimodal\ntraining and handle incomplete modalities during inference. In this paper, we\npropose CMDIAD, a Cross-Modal Distillation framework for IAD to demonstrate the\nfeasibility of a Multi-modal Training, Few-modal Inference (MTFI) pipeline. Our\nfindings show that the MTFI pipeline can more effectively utilize incomplete\nmultimodal information compared to applying only a single modality for training\nand inference. Moreover, we investigate the reasons behind the asymmetric\nperformance improvement using point clouds or RGB images as the main modality\nof inference. This provides a foundation for our future multimodal dataset\nconstruction with additional modalities from manufacturing scenarios.",
    "published": "2024-05-22T12:08:56Z",
    "updated": "2025-10-16T09:52:27Z",
    "link": "http://arxiv.org/pdf/2405.13571v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wenbo Sui",
      "Daniel Lichau",
      "Josselin Lefvre",
      "Harold Phelippeau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14493v1",
    "title": "Grazing Detection using Deep Learning and Sentinel-2 Time Series Data",
    "summary": "Grazing shapes both agricultural production and biodiversity, yet scalable\nmonitoring of where grazing occurs remains limited. We study seasonal grazing\ndetection from Sentinel-2 L2A time series: for each polygon-defined field\nboundary, April-October imagery is used for binary prediction (grazed / not\ngrazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance\nfeatures, and achieve an average F1 score of 77 percent across five validation\nsplits, with 90 percent recall on grazed pastures. Operationally, if inspectors\ncan visit at most 4 percent of sites annually, prioritising fields predicted by\nour model as non-grazed yields 17.2 times more confirmed non-grazing sites than\nrandom inspection. These results indicate that coarse-resolution, freely\navailable satellite data can reliably steer inspection resources for\nconservation-aligned land-use compliance. Code and models have been made\npublicly available.",
    "published": "2025-10-16T09:37:43Z",
    "updated": "2025-10-16T09:37:43Z",
    "link": "http://arxiv.org/pdf/2510.14493v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Aleksis Pirinen",
      "Delia Fano Yela",
      "Smita Chakraborty",
      "Erik Kllman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.01126v2",
    "title": "UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,\n  Forecasting, and Generation",
    "summary": "Egocentric human motion generation and forecasting with scene-context is\ncrucial for enhancing AR/VR experiences, improving human-robot interaction,\nadvancing assistive technologies, and enabling adaptive healthcare solutions by\naccurately predicting and simulating movement from a first-person perspective.\nHowever, existing methods primarily focus on third-person motion synthesis with\nstructured 3D scene contexts, limiting their effectiveness in real-world\negocentric settings where limited field of view, frequent occlusions, and\ndynamic cameras hinder scene perception. To bridge this gap, we introduce\nEgocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks\nthat utilize first-person images for scene-aware motion synthesis without\nrelying on explicit 3D scene. We propose UniEgoMotion, a unified conditional\nmotion diffusion model with a novel head-centric motion representation tailored\nfor egocentric devices. UniEgoMotion's simple yet effective design supports\negocentric motion reconstruction, forecasting, and generation from first-person\nvisual inputs in a unified framework. Unlike previous works that overlook scene\nsemantics, our model effectively extracts image-based scene context to infer\nplausible 3D motion. To facilitate training, we introduce EE4D-Motion, a\nlarge-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth\n3D motion annotations. UniEgoMotion achieves state-of-the-art performance in\negocentric motion reconstruction and is the first to generate motion from a\nsingle egocentric image. Extensive evaluations demonstrate the effectiveness of\nour unified framework, setting a new benchmark for egocentric motion modeling\nand unlocking new possibilities for egocentric applications.",
    "published": "2025-08-02T00:41:20Z",
    "updated": "2025-10-16T09:19:05Z",
    "link": "http://arxiv.org/pdf/2508.01126v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chaitanya Patel",
      "Hiroki Nakamura",
      "Yuta Kyuragi",
      "Kazuki Kozuka",
      "Juan Carlos Niebles",
      "Ehsan Adeli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14463v1",
    "title": "Pruning Overparameterized Multi-Task Networks for Degraded Web Image\n  Restoration",
    "summary": "Image quality is a critical factor in delivering visually appealing content\non web platforms. However, images often suffer from degradation due to lossy\noperations applied by online social networks (OSNs), negatively affecting user\nexperience. Image restoration is the process of recovering a clean high-quality\nimage from a given degraded input. Recently, multi-task (all-in-one) image\nrestoration models have gained significant attention, due to their ability to\nsimultaneously handle different types of image degradations. However, these\nmodels often come with an excessively high number of trainable parameters,\nmaking them computationally inefficient. In this paper, we propose a strategy\nfor compressing multi-task image restoration models. We aim to discover highly\nsparse subnetworks within overparameterized deep models that can match or even\nsurpass the performance of their dense counterparts. The proposed model, namely\nMIR-L, utilizes an iterative pruning strategy that removes low-magnitude\nweights across multiple rounds, while resetting the remaining weights to their\noriginal initialization. This iterative process is important for the multi-task\nimage restoration model's optimization, effectively uncovering \"winning\ntickets\" that maintain or exceed state-of-the-art performance at high sparsity\nlevels. Experimental evaluation on benchmark datasets for the deraining,\ndehazing, and denoising tasks shows that MIR-L retains only 10% of the\ntrainable parameters while maintaining high image restoration performance. Our\ncode, datasets and pre-trained models are made publicly available at\nhttps://github.com/Thomkat/MIR-L.",
    "published": "2025-10-16T09:04:05Z",
    "updated": "2025-10-16T09:04:05Z",
    "link": "http://arxiv.org/pdf/2510.14463v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Thomas Katraouras",
      "Dimitrios Rafailidis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14462v1",
    "title": "Unsupervised Deep Generative Models for Anomaly Detection in\n  Neuroimaging: A Systematic Scoping Review",
    "summary": "Unsupervised deep generative models are emerging as a promising alternative\nto supervised methods for detecting and segmenting anomalies in brain imaging.\nUnlike fully supervised approaches, which require large voxel-level annotated\ndatasets and are limited to well-characterised pathologies, these models can be\ntrained exclusively on healthy data and identify anomalies as deviations from\nlearned normative brain structures. This PRISMA-guided scoping review\nsynthesises recent work on unsupervised deep generative models for anomaly\ndetection in neuroimaging, including autoencoders, variational autoencoders,\ngenerative adversarial networks, and denoising diffusion models. A total of 49\nstudies published between 2018 - 2025 were identified, covering applications to\nbrain MRI and, less frequently, CT across diverse pathologies such as tumours,\nstroke, multiple sclerosis, and small vessel disease. Reported performance\nmetrics are compared alongside architectural design choices. Across the\nincluded studies, generative models achieved encouraging performance for large\nfocal lesions and demonstrated progress in addressing more subtle\nabnormalities. A key strength of generative models is their ability to produce\ninterpretable pseudo-healthy (also referred to as counterfactual)\nreconstructions, which is particularly valuable when annotated data are scarce,\nas in rare or heterogeneous diseases. Looking ahead, these models offer a\ncompelling direction for anomaly detection, enabling semi-supervised learning,\nsupporting the discovery of novel imaging biomarkers, and facilitating within-\nand cross-disease deviation mapping in unified end-to-end frameworks. To\nrealise clinical impact, future work should prioritise anatomy-aware modelling,\ndevelopment of foundation models, task-appropriate evaluation metrics, and\nrigorous clinical validation.",
    "published": "2025-10-16T09:02:52Z",
    "updated": "2025-10-16T09:02:52Z",
    "link": "http://arxiv.org/pdf/2510.14462v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Youwan Mah",
      "Elise Bannier",
      "Stphanie Leplaideur",
      "Elisa Fromont",
      "Francesca Galassi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14460v1",
    "title": "Structured Universal Adversarial Attacks on Object Detection for Video\n  Sequences",
    "summary": "Video-based object detection plays a vital role in safety-critical\napplications. While deep learning-based object detectors have achieved\nimpressive performance, they remain vulnerable to adversarial attacks,\nparticularly those involving universal perturbations. In this work, we propose\na minimally distorted universal adversarial attack tailored for video object\ndetection, which leverages nuclear norm regularization to promote structured\nperturbations concentrated in the background. To optimize this formulation\nefficiently, we employ an adaptive, optimistic exponentiated gradient method\nthat enhances both scalability and convergence. Our results demonstrate that\nthe proposed attack outperforms both low-rank projected gradient descent and\nFrank-Wolfe based attacks in effectiveness while maintaining high stealthiness.\nAll code and data are publicly available at\nhttps://github.com/jsve96/AO-Exp-Attack.",
    "published": "2025-10-16T09:00:41Z",
    "updated": "2025-10-16T09:00:41Z",
    "link": "http://arxiv.org/pdf/2510.14460v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sven Jacob",
      "Weijia Shao",
      "Gjergji Kasneci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.23325v2",
    "title": "FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World\n  Models",
    "summary": "Lane segment topology reasoning provides comprehensive bird's-eye view (BEV)\nroad scene understanding, which can serve as a key perception module in\nplanning-oriented end-to-end autonomous driving systems. Existing lane topology\nreasoning methods often fall short in effectively leveraging temporal\ninformation to enhance detection and reasoning performance. Recently,\nstream-based temporal propagation method has demonstrated promising results by\nincorporating temporal cues at both the query and BEV levels. However, it\nremains limited by over-reliance on historical queries, vulnerability to pose\nestimation failures, and insufficient temporal propagation. To overcome these\nlimitations, we propose FASTopoWM, a novel fast-slow lane segment topology\nreasoning framework augmented with latent world models. To reduce the impact of\npose estimation failures, this unified framework enables parallel supervision\nof both historical and newly initialized queries, facilitating mutual\nreinforcement between the fast and slow systems. Furthermore, we introduce\nlatent query and BEV world models conditioned on the action latent to propagate\nthe state representations from past observations to the current timestep. This\ndesign substantially improves the performance of temporal perception within the\nslow pipeline. Extensive experiments on the OpenLane-V2 benchmark demonstrate\nthat FASTopoWM outperforms state-of-the-art methods in both lane segment\ndetection (37.4% v.s. 33.6% on mAP) and centerline perception (46.3% v.s. 41.5%\non OLS).",
    "published": "2025-07-31T08:12:56Z",
    "updated": "2025-10-16T08:47:57Z",
    "link": "http://arxiv.org/pdf/2507.23325v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yiming Yang",
      "Hongbin Lin",
      "Yueru Luo",
      "Suzhong Fu",
      "Chao Zheng",
      "Xinrui Yan",
      "Shuqi Mei",
      "Kun Tang",
      "Shuguang Cui",
      "Zhen Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14431v1",
    "title": "Real-Time Neural Video Compression with Unified Intra and Inter Coding",
    "summary": "Neural video compression (NVC) technologies have advanced rapidly in recent\nyears, yielding state-of-the-art schemes such as DCVC-RT that offer superior\ncompression efficiency to H.266/VVC and real-time encoding/decoding\ncapabilities. Nonetheless, existing NVC schemes have several limitations,\nincluding inefficiency in dealing with disocclusion and new content, interframe\nerror propagation and accumulation, among others. To eliminate these\nlimitations, we borrow the idea from classic video coding schemes, which allow\nintra coding within inter-coded frames. With the intra coding tool enabled,\ndisocclusion and new content are properly handled, and interframe error\npropagation is naturally intercepted without the need for manual refresh\nmechanisms. We present an NVC framework with unified intra and inter coding,\nwhere every frame is processed by a single model that is trained to perform\nintra/inter coding adaptively. Moreover, we propose a simultaneous two-frame\ncompression design to exploit interframe redundancy not only forwardly but also\nbackwardly. Experimental results show that our scheme outperforms DCVC-RT by an\naverage of 10.7\\% BD-rate reduction, delivers more stable bitrate and quality\nper frame, and retains real-time encoding/decoding performances. Code and\nmodels will be released.",
    "published": "2025-10-16T08:31:44Z",
    "updated": "2025-10-16T08:31:44Z",
    "link": "http://arxiv.org/pdf/2510.14431v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hui Xiang",
      "Yifan Bian",
      "Li Li",
      "Jingran Wu",
      "Xianguo Zhang",
      "Dong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14427v1",
    "title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
    "summary": "Recent research on motion generation has shown significant progress in\ngenerating semantically aligned motion with singular semantics. However, when\nemploying these models to create composite sequences containing multiple\nsemantically generated motion clips, they often struggle to preserve the\ncontinuity of motion dynamics at the transition boundaries between clips,\nresulting in awkward transitions and abrupt artifacts. To address these\nchallenges, we present Compositional Phase Diffusion, which leverages the\nSemantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module\n(TPDM) to progressively incorporate semantic guidance and phase details from\nadjacent motion clips into the diffusion process. Specifically, SPDM and TPDM\noperate within the latent motion frequency domain established by the\npre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them\nto learn semantically important and transition-aware phase information from\nvariable-length motion clips during training. Experimental results demonstrate\nthe competitive performance of our proposed framework in generating\ncompositional motion sequences that align semantically with the input\nconditions, while preserving phase transitional continuity between preceding\nand succeeding motion clips. Additionally, motion inbetweening task is made\npossible by keeping the phase parameter of the input motion sequences fixed\nthroughout the diffusion process, showcasing the potential for extending the\nproposed framework to accommodate various application scenarios. Codes are\navailable at https://github.com/asdryau/TransPhase.",
    "published": "2025-10-16T08:28:46Z",
    "updated": "2025-10-16T08:28:46Z",
    "link": "http://arxiv.org/pdf/2510.14427v1.pdf",
    "category": [
      "cs.MM",
      "cs.CV"
    ],
    "authors": [
      "Ho Yin Au",
      "Jie Chen",
      "Junkun Jiang",
      "Jingyu Xiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.15568v4",
    "title": "Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian\n  Alignment",
    "summary": "Test-time adaptation (TTA) enhances the zero-shot robustness under\ndistribution shifts by leveraging unlabeled test data during inference. Despite\nnotable advances, several challenges still limit its broader applicability.\nFirst, most methods rely on backpropagation or iterative optimization, which\nlimits scalability and hinders real-time deployment. Second, they lack explicit\nmodeling of class-conditional feature distributions. This modeling is crucial\nfor producing reliable decision boundaries and calibrated predictions, but it\nremains underexplored due to the lack of both source data and supervision at\ntest time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and\nbackPropagation-free Test-time adaptation method. We reframe TTA as a Gaussian\nprobabilistic inference task by modeling class-conditional likelihoods using\ngradually updated class means and a shared covariance matrix. This enables\nclosed-form, training-free inference. To correct potential likelihood bias, we\nintroduce lightweight regularization guided by CLIP priors and a historical\nknowledge bank. ADAPT requires no source data, no gradient updates, and no full\naccess to target data, supporting both online and transductive settings.\nExtensive experiments across diverse benchmarks demonstrate that our method\nachieves state-of-the-art performance under a wide range of distribution shifts\nwith superior scalability and robustness.",
    "published": "2025-08-21T13:42:49Z",
    "updated": "2025-10-16T08:27:41Z",
    "link": "http://arxiv.org/pdf/2508.15568v4.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Youjia Zhang",
      "Youngeun Kim",
      "Young-Geun Choi",
      "Hongyeob Kim",
      "Huiling Liu",
      "Sungeun Hong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.18623v2",
    "title": "Training-Free Personalization via Retrieval and Reasoning on\n  Fingerprints",
    "summary": "Vision Language Models (VLMs) have lead to major improvements in multimodal\nreasoning, yet they still struggle to understand user-specific concepts.\nExisting personalization methods address this limitation but heavily rely on\ntraining procedures, that can be either costly or unpleasant to individual\nusers. We depart from existing work, and for the first time explore the\ntraining-free setting in the context of personalization. We propose a novel\nmethod, Retrieval and Reasoning for Personalization (R2P), leveraging internal\nknowledge of VLMs. First, we leverage VLMs to extract the concept fingerprint,\ni.e., key attributes uniquely defining the concept within its semantic class.\nWhen a query arrives, the most similar fingerprints are retrieved and scored\nvia chain-of-thought-reasoning. To reduce the risk of hallucinations, the\nscores are validated through cross-modal verification at the attribute level:\nin case of a discrepancy between the scores, R2P refines the concept\nassociation via pairwise multimodal matching, where the retrieved fingerprints\nand their images are directly compared with the query. We validate R2P on two\npublicly available benchmarks and a newly introduced dataset, Personal Concepts\nwith Visual Ambiguity (PerVA), for concept identification highlighting\nchallenges in visual ambiguity. R2P consistently outperforms state-of-the-art\napproaches on various downstream tasks across all benchmarks. Code will be\navailable upon acceptance.",
    "published": "2025-03-24T12:36:24Z",
    "updated": "2025-10-16T08:19:45Z",
    "link": "http://arxiv.org/pdf/2503.18623v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Deepayan Das",
      "Davide Talon",
      "Yiming Wang",
      "Massimiliano Mancini",
      "Elisa Ricci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14403v1",
    "title": "DCMIL: A Progressive Representation Learning Model of Whole Slide Images\n  for Cancer Prognosis Analysis",
    "summary": "The burgeoning discipline of computational pathology shows promise in\nharnessing whole slide images (WSIs) to quantify morphological heterogeneity\nand develop objective prognostic modes for human cancers. However, progress is\nimpeded by the computational bottleneck of gigapixel-size inputs and the\nscarcity of dense manual annotations. Current methods often overlook\nfine-grained information across multi-magnification WSIs and variations in\ntumor microenvironments. Here, we propose an easy-to-hard progressive\nrepresentation learning model, termed dual-curriculum contrastive\nmulti-instance learning (DCMIL), to efficiently process WSIs for cancer\nprognosis. The model does not rely on dense annotations and enables the direct\ntransformation of gigapixel-size WSIs into outcome predictions. Extensive\nexperiments on twelve cancer types (5,954 patients, 12.54 million tiles)\ndemonstrate that DCMIL outperforms standard WSI-based prognostic models.\nAdditionally, DCMIL identifies fine-grained prognosis-salient regions, provides\nrobust instance uncertainty estimation, and captures morphological differences\nbetween normal and tumor tissues, with the potential to generate new biological\ninsights. All codes have been made publicly accessible at\nhttps://github.com/tuuuc/DCMIL.",
    "published": "2025-10-16T08:03:54Z",
    "updated": "2025-10-16T08:03:54Z",
    "link": "http://arxiv.org/pdf/2510.14403v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chao Tu",
      "Kun Huang",
      "Jie Zhang",
      "Qianjin Feng",
      "Yu Zhang",
      "Zhenyuan Ning"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.12099v2",
    "title": "TinyRS-R1: Compact Multimodal Language Model for Remote Sensing",
    "summary": "Remote-sensing applications often run on edge hardware that cannot host\ntoday's 7B-parameter multimodal language models. This paper introduces TinyRS,\nthe first 2B-parameter multimodal small language model (MSLM) optimized for\nremote sensing tasks, and TinyRS-R1, its reasoning-augmented variant. Built\nupon Qwen2-VL-2B, TinyRS is trained through a four-stage pipeline: pre-training\non million satellite images, instruction tuning on visual instruction examples,\nfine-tuning with Chain-of-Thought (CoT) annotations from the proposed reasoning\ndataset, and alignment via Group Relative Policy Optimization (GRPO). TinyRS-R1\nachieves or surpasses the performance of recent 7B-parameter remote sensing\nmodels across classification, VQA, visual grounding, and open-ended question\nanswering-while requiring just one-third of the memory and latency. Our\nanalysis shows that CoT reasoning substantially benefits spatial grounding and\nscene understanding, while the non-reasoning TinyRS excels in concise,\nlatency-sensitive VQA tasks. TinyRS-R1 represents the first domain-specialized\nMSLM with GRPO-aligned CoT reasoning for general-purpose remote sensing.",
    "published": "2025-05-17T17:53:21Z",
    "updated": "2025-10-16T07:49:00Z",
    "link": "http://arxiv.org/pdf/2505.12099v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Aybora Koksal",
      "A. Aydin Alatan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22642v2",
    "title": "WoW: Towards a World omniscient World model Through Embodied Interaction",
    "summary": "Humans develop an understanding of intuitive physics through active\ninteraction with the world. This approach is in stark contrast to current video\nmodels, such as Sora, which rely on passive observation and therefore struggle\nwith grasping physical causality. This observation leads to our central\nhypothesis: authentic physical intuition of the world model must be grounded in\nextensive, causally rich interactions with the real world. To test this\nhypothesis, we present WoW, a 14-billion-parameter generative world model\ntrained on 2 million robot interaction trajectories. Our findings reveal that\nthe model's understanding of physics is a probabilistic distribution of\nplausible outcomes, leading to stochastic instabilities and physical\nhallucinations. Furthermore, we demonstrate that this emergent capability can\nbe actively constrained toward physical realism by SOPHIA, where\nvision-language model agents evaluate the DiT-generated output and guide its\nrefinement by iteratively evolving the language instructions. In addition, a\nco-trained Inverse Dynamics Model translates these refined plans into\nexecutable robotic actions, thus closing the imagination-to-action loop. We\nestablish WoWBench, a new benchmark focused on physical consistency and causal\nreasoning in video, where WoW achieves state-of-the-art performance in both\nhuman and autonomous evaluation, demonstrating strong ability in physical\ncausality, collision dynamics, and object permanence. Our work provides\nsystematic evidence that large-scale, real-world interaction is a cornerstone\nfor developing physical intuition in AI. Models, data, and benchmarks will be\nopen-sourced.",
    "published": "2025-09-26T17:59:07Z",
    "updated": "2025-10-16T07:48:00Z",
    "link": "http://arxiv.org/pdf/2509.22642v2.pdf",
    "category": [
      "cs.RO",
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Xiaowei Chi",
      "Peidong Jia",
      "Chun-Kai Fan",
      "Xiaozhu Ju",
      "Weishi Mi",
      "Kevin Zhang",
      "Zhiyuan Qin",
      "Wanxin Tian",
      "Kuangzhi Ge",
      "Hao Li",
      "Zezhong Qian",
      "Anthony Chen",
      "Qiang Zhou",
      "Yueru Jia",
      "Jiaming Liu",
      "Yong Dai",
      "Qingpo Wuwu",
      "Chengyu Bai",
      "Yu-Kai Wang",
      "Ying Li",
      "Lizhang Chen",
      "Yong Bao",
      "Zhiyuan Jiang",
      "Jiacheng Zhu",
      "Kai Tang",
      "Ruichuan An",
      "Yulin Luo",
      "Qiuxuan Feng",
      "Siyuan Zhou",
      "Chi-min Chan",
      "Chengkai Hou",
      "Wei Xue",
      "Sirui Han",
      "Yike Guo",
      "Shanghang Zhang",
      "Jian Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14389v1",
    "title": "BoardVision: Deployment-ready and Robust Motherboard Defect Detection\n  with YOLO+Faster-RCNN Ensemble",
    "summary": "Motherboard defect detection is critical for ensuring reliability in\nhigh-volume electronics manufacturing. While prior research in PCB inspection\nhas largely targeted bare-board or trace-level defects, assembly-level\ninspection of full motherboards inspection remains underexplored. In this work,\nwe present BoardVision, a reproducible framework for detecting assembly-level\ndefects such as missing screws, loose fan wiring, and surface scratches. We\nbenchmark two representative detectors - YOLOv7 and Faster R-CNN, under\ncontrolled conditions on the MiracleFactory motherboard dataset, providing the\nfirst systematic comparison in this domain. To mitigate the limitations of\nsingle models, where YOLO excels in precision but underperforms in recall and\nFaster R-CNN shows the reverse, we propose a lightweight ensemble,\nConfidence-Temporal Voting (CTV Voter), that balances precision and recall\nthrough interpretable rules. We further evaluate robustness under realistic\nperturbations including sharpness, brightness, and orientation changes,\nhighlighting stability challenges often overlooked in motherboard defect\ndetection. Finally, we release a deployable GUI-driven inspection tool that\nbridges research evaluation with operator usability. Together, these\ncontributions demonstrate how computer vision techniques can transition from\nbenchmark results to practical quality assurance for assembly-level motherboard\nmanufacturing.",
    "published": "2025-10-16T07:38:31Z",
    "updated": "2025-10-16T07:38:31Z",
    "link": "http://arxiv.org/pdf/2510.14389v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Brandon Hill",
      "Kma Solaiman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14383v1",
    "title": "DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with\n  Analytical Insights",
    "summary": "Accurate brain tumor segmentation is significant for clinical diagnosis and\ntreatment. It is challenging due to the heterogeneity of tumor subregions.\nMamba-based State Space Models have demonstrated promising performance.\nHowever, they incur significant computational overhead due to sequential\nfeature computation across multiple spatial axes. Moreover, their robustness\nacross diverse BraTS data partitions remains largely unexplored, leaving a\ncritical gap in reliable evaluation. To address these limitations, we propose\ndual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation\nmodel that captures multi-scale long-range dependencies with minimal\ncomputational overhead. We leverage a space-filling curve to preserve spatial\nlocality during 3D-to-1D feature mapping, thereby reducing reliance on\ncomputationally expensive multi-axial feature scans. To enrich feature\nrepresentation, we propose a gated fusion module that adaptively integrates\nforward and reverse contexts, along with a quantization block that discretizes\nfeatures to improve robustness. In addition, we propose five systematic folds\non BraTS2023 for rigorous evaluation of segmentation techniques under diverse\nconditions and present detailed analysis of common failure scenarios. On the\n20\\% test set used by recent methods, our model achieves Dice improvements of\n0.10\\% for whole tumor, 1.75\\% for tumor core, and 0.93\\% for enhancing tumor.\nEvaluations on the proposed systematic five folds demonstrate that our model\nmaintains competitive whole tumor accuracy while achieving clear average Dice\ngains of 0.86\\% for tumor core and 1.45\\% for enhancing tumor over existing\nstate-of-the-art. Furthermore, our model attains 15 times improvement in\nefficiency while maintaining high segmentation accuracy, highlighting its\nrobustness and computational advantage over existing approaches.",
    "published": "2025-10-16T07:31:21Z",
    "updated": "2025-10-16T07:31:21Z",
    "link": "http://arxiv.org/pdf/2510.14383v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Danish Ali",
      "Ajmal Mian",
      "Naveed Akhtar",
      "Ghulam Mubashar Hassan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23141v2",
    "title": "Earth-Agent: Unlocking the Full Landscape of Earth Observation with\n  Agents",
    "summary": "Earth observation (EO) is essential for understanding the evolving states of\nthe Earth system. Although recent MLLMs have advanced EO research, they still\nlack the capability to tackle complex tasks that require multi-step reasoning\nand the use of domain-specific tools. Agent-based methods offer a promising\ndirection, but current attempts remain in their infancy, confined to RGB\nperception, shallow reasoning, and lacking systematic evaluation protocols. To\novercome these limitations, we introduce Earth-Agent, the first agentic\nframework that unifies RGB and spectral EO data within an MCP-based tool\necosystem, enabling cross-modal, multi-step, and quantitative spatiotemporal\nreasoning beyond pretrained MLLMs. Earth-Agent supports complex scientific\ntasks such as geophysical parameter retrieval and quantitative spatiotemporal\nanalysis by dynamically invoking expert tools and models across modalities. To\nsupport comprehensive evaluation, we further propose Earth-Bench, a benchmark\nof 248 expert-curated tasks with 13,729 images, spanning spectrum, products and\nRGB modalities, and equipped with a dual-level evaluation protocol that\nassesses both reasoning trajectories and final outcomes. We conduct\ncomprehensive experiments varying different LLM backbones, comparisons with\ngeneral agent frameworks, and comparisons with MLLMs on remote sensing\nbenchmarks, demonstrating both the effectiveness and potential of Earth-Agent.\nEarth-Agent establishes a new paradigm for EO analysis, moving the field toward\nscientifically grounded, next-generation applications of LLMs in Earth\nobservation.",
    "published": "2025-09-27T06:04:28Z",
    "updated": "2025-10-16T07:27:45Z",
    "link": "http://arxiv.org/pdf/2509.23141v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Peilin Feng",
      "Zhutao Lv",
      "Junyan Ye",
      "Xiaolei Wang",
      "Xinjie Huo",
      "Jinhua Yu",
      "Wanghan Xu",
      "Wenlong Zhang",
      "Lei Bai",
      "Conghui He",
      "Weijia Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08273v4",
    "title": "One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion\n  Models for Text-Guided Image Inpainting",
    "summary": "Text-guided image inpainting aims at reconstructing the masked regions as per\ntext prompts, where the longstanding challenges lie in the preservation for\nunmasked regions, while achieving the semantics consistency between unmasked\nand inpainted masked regions. Previous arts failed to address both of them,\nalways with either of them to be remedied. Such facts, as we observed, stem\nfrom the entanglement of the hybrid (e.g., mid-and-low) frequency bands that\nencode varied image properties, which exhibit different robustness to text\nprompts during the denoising process. In this paper, we propose a\nnull-text-null frequency-aware diffusion models, dubbed \\textbf{NTN-Diff}, for\ntext-guided image inpainting, by decomposing the semantics consistency across\nmasked and unmasked regions into the consistencies as per each frequency band,\nwhile preserving the unmasked regions, to circumvent two challenges in a row.\nBased on the diffusion process, we further divide the denoising process into\nearly (high-level noise) and late (low-level noise) stages, where the\nmid-and-low frequency bands are disentangled during the denoising process. As\nobserved, the stable mid-frequency band is progressively denoised to be\nsemantically aligned during text-guided denoising process, which, meanwhile,\nserves as the guidance to the null-text denoising process to denoise\nlow-frequency band for the masked regions, followed by a subsequent text-guided\ndenoising process at late stage, to achieve the semantics consistency for\nmid-and-low frequency bands across masked and unmasked regions, while preserve\nthe unmasked regions. Extensive experiments validate the superiority of\nNTN-Diff over the state-of-the-art diffusion models to text-guided diffusion\nmodels. Our code can be accessed from https://github.com/htyjers/NTN-Diff.",
    "published": "2025-10-09T14:30:34Z",
    "updated": "2025-10-16T07:23:33Z",
    "link": "http://arxiv.org/pdf/2510.08273v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haipeng Liu",
      "Yang Wang",
      "Meng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.10623v2",
    "title": "Flows and Diffusions on the Neural Manifold",
    "summary": "Diffusion and flow-based generative models have achieved remarkable success\nin domains such as image synthesis, video generation, and natural language\nmodeling. In this work, we extend these advances to weight space learning by\nleveraging recent techniques to incorporate structural priors derived from\noptimization dynamics. Central to our approach is modeling the trajectory\ninduced by gradient descent as a trajectory inference problem. We unify several\ntrajectory inference techniques towards matching a gradient flow, providing a\ntheoretical framework for treating optimization paths as inductive bias. We\nfurther explore architectural and algorithmic choices, including reward\nfine-tuning by adjoint matching, the use of autoencoders for latent weight\nrepresentation, conditioning on task-specific context data, and adopting\ninformative source distributions such as Kaiming uniform. Experiments\ndemonstrate that our method matches or surpasses baselines in generating\nin-distribution weights, improves initialization for downstream training, and\nsupports fine-tuning to enhance performance. Finally, we illustrate a practical\napplication in safety-critical systems: detecting harmful covariate shifts,\nwhere our method outperforms the closest comparable baseline.",
    "published": "2025-07-14T02:26:06Z",
    "updated": "2025-10-16T07:23:22Z",
    "link": "http://arxiv.org/pdf/2507.10623v2.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Daniel Saragih",
      "Deyu Cao",
      "Tejas Balaji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14376v1",
    "title": "DOS: Directional Object Separation in Text Embeddings for Multi-Object\n  Image Generation",
    "summary": "Recent progress in text-to-image (T2I) generative models has led to\nsignificant improvements in generating high-quality images aligned with text\nprompts. However, these models still struggle with prompts involving multiple\nobjects, often resulting in object neglect or object mixing. Through extensive\nstudies, we identify four problematic scenarios, Similar Shapes, Similar\nTextures, Dissimilar Background Biases, and Many Objects, where inter-object\nrelationships frequently lead to such failures. Motivated by two key\nobservations about CLIP embeddings, we propose DOS (Directional Object\nSeparation), a method that modifies three types of CLIP text embeddings before\npassing them into text-to-image models. Experimental results show that DOS\nconsistently improves the success rate of multi-object image generation and\nreduces object mixing. In human evaluations, DOS significantly outperforms four\ncompeting methods, receiving 26.24%-43.04% more votes across four benchmarks.\nThese results highlight DOS as a practical and effective solution for improving\nmulti-object image generation.",
    "published": "2025-10-16T07:17:23Z",
    "updated": "2025-10-16T07:17:23Z",
    "link": "http://arxiv.org/pdf/2510.14376v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dongnam Byun",
      "Jungwon Park",
      "Jumgmin Ko",
      "Changin Choi",
      "Wonjong Rhee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14374v1",
    "title": "Spatial Preference Rewarding for MLLMs Spatial Understanding",
    "summary": "Multimodal large language models~(MLLMs) have demonstrated promising spatial\nunderstanding capabilities, such as referencing and grounding object\ndescriptions. Despite their successes, MLLMs still fall short in fine-grained\nspatial perception abilities, such as generating detailed region descriptions\nor accurately localizing objects. Additionally, they often fail to respond to\nthe user's requirements for desired fine-grained spatial understanding. This\nissue might arise because existing approaches primarily focus on tuning MLLMs\nto model pre-annotated instruction data to inject spatial knowledge, without\ndirect supervision of MLLMs' actual responses. We address this issue by SPR, a\nSpatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial\ncapabilities by rewarding MLLMs' detailed responses with precise object\nlocalization over vague or inaccurate responses. With randomly selected image\nregions and region descriptions from MLLMs, SPR introduces semantic and\nlocalization scores to comprehensively evaluate the text quality and\nlocalization quality in MLLM-generated descriptions. We also refine the MLLM\ndescriptions with better localization accuracy and pair the best-scored\nrefinement with the initial descriptions of the lowest score for direct\npreference optimization, thereby enhancing fine-grained alignment with visual\ninput. Extensive experiments over standard referring and grounding benchmarks\nshow that SPR improves MLLM spatial understanding capabilities effectively with\nminimal overhead in training. Data and code will be released at\nhttps://github.com/hanqiu-hq/SPR",
    "published": "2025-10-16T07:16:18Z",
    "updated": "2025-10-16T07:16:18Z",
    "link": "http://arxiv.org/pdf/2510.14374v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Han Qiu",
      "Peng Gao",
      "Lewei Lu",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.12386v2",
    "title": "SOHES: Self-supervised Open-world Hierarchical Entity Segmentation",
    "summary": "Open-world entity segmentation, as an emerging computer vision task, aims at\nsegmenting entities in images without being restricted by pre-defined classes,\noffering impressive generalization capabilities on unseen images and concepts.\nDespite its promise, existing entity segmentation methods like Segment Anything\nModel (SAM) rely heavily on costly expert annotators. This work presents\nSelf-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel\napproach that eliminates the need for human annotations. SOHES operates in\nthree phases: self-exploration, self-instruction, and self-correction. Given a\npre-trained self-supervised representation, we produce abundant high-quality\npseudo-labels through visual feature clustering. Then, we train a segmentation\nmodel on the pseudo-labels, and rectify the noises in pseudo-labels via a\nteacher-student mutual-learning procedure. Beyond segmenting entities, SOHES\nalso captures their constituent parts, providing a hierarchical understanding\nof visual entities. Using raw images as the sole training data, our method\nachieves unprecedented performance in self-supervised open-world segmentation,\nmarking a significant milestone towards high-quality open-world entity\nsegmentation in the absence of human-annotated masks. Project page:\nhttps://SOHES-ICLR.github.io.",
    "published": "2024-04-18T17:59:46Z",
    "updated": "2025-10-16T06:58:28Z",
    "link": "http://arxiv.org/pdf/2404.12386v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Shengcao Cao",
      "Jiuxiang Gu",
      "Jason Kuen",
      "Hao Tan",
      "Ruiyi Zhang",
      "Handong Zhao",
      "Ani Nenkova",
      "Liang-Yan Gui",
      "Tong Sun",
      "Yu-Xiong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.11070v2",
    "title": "Falcon: A Remote Sensing Vision-Language Foundation Model (Technical\n  Report)",
    "summary": "This paper introduces a holistic vision-language foundation model tailored\nfor remote sensing, named Falcon. Falcon offers a unified, prompt-based\nparadigm that effectively executes comprehensive and complex remote sensing\ntasks. Falcon demonstrates powerful understanding and reasoning abilities at\nthe image, region, and pixel levels. Specifically, given simple natural\nlanguage instructions and remote sensing images, Falcon can produce impressive\nresults in text form across 14 distinct tasks, i.e., image classification,\nobject detection, segmentation, image captioning, and etc. To facilitate\nFalcon's training and empower its representation capacity to encode rich\nspatial and semantic information, we developed Falcon_SFT, a large-scale,\nmulti-task, instruction-tuning dataset in the field of remote sensing. The\nFalcon_SFT dataset consists of approximately 78 million high-quality data\nsamples, covering 5.6 million multi-spatial resolution and multi-view remote\nsensing images with diverse instructions. It features hierarchical annotations\nand undergoes manual sampling verification to ensure high data quality and\nreliability. Extensive comparative experiments are conducted, which verify that\nFalcon achieves remarkable performance over 67 datasets and 14 tasks, despite\nhaving only 0.7B parameters. We release the complete dataset, code, and model\nweights at https://github.com/TianHuiLab/Falcon, hoping to help further develop\nthe open-source community.",
    "published": "2025-03-14T04:27:01Z",
    "updated": "2025-10-16T06:52:13Z",
    "link": "http://arxiv.org/pdf/2503.11070v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kelu Yao",
      "Nuo Xu",
      "Rong Yang",
      "Yingying Xu",
      "Zhuoyan Gao",
      "Titinunt Kitrungrotsakul",
      "Yi Ren",
      "Pu Zhang",
      "Jin Wang",
      "Ning Wei",
      "Chao Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14354v1",
    "title": "Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D\n  Registration",
    "summary": "With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has\nbecome available. This prompts the question of how to utilize this data for\ngeometric reasoning of scenes. While many RGB-D registration meth- ods rely on\ngeometric and feature-based similarity, we take a different approach. We use\ncycle-consistent keypoints as salient points to enforce spatial coherence\nconstraints during matching, improving correspondence accuracy. Additionally,\nwe introduce a novel pose block that combines a GRU recurrent unit with\ntransformation synchronization, blending historical and multi-view data. Our\napproach surpasses previous self- supervised registration methods on ScanNet\nand 3DMatch, even outperforming some older supervised methods. We also\nintegrate our components into existing methods, showing their effectiveness.",
    "published": "2025-10-16T06:47:10Z",
    "updated": "2025-10-16T06:47:10Z",
    "link": "http://arxiv.org/pdf/2510.14354v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Siddharth Tourani",
      "Jayaram Reddy",
      "Sarvesh Thakur",
      "K Madhava Krishna",
      "Muhammad Haris Khan",
      "N Dinesh Reddy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.17965v2",
    "title": "AttenCraft: Attention-guided Disentanglement of Multiple Concepts for\n  Text-to-Image Customization",
    "summary": "Text-to-image (T2I) customization empowers users to adapt the T2I diffusion\nmodel to new concepts absent in the pre-training dataset. On this basis,\ncapturing multiple new concepts from a single image has emerged as a new task,\nallowing the model to learn multiple concepts simultaneously or discard\nunwanted concepts. However, multiple-concept disentanglement remains a key\nchallenge. Existing disentanglement models often exhibit two main issues:\nfeature fusion and asynchronous learning across different concepts. To address\nthese issues, we propose AttenCraft, an attention-based method for\nmultiple-concept disentanglement. Our method uses attention maps to generate\naccurate masks for each concept in a single initialization step, aiding in\nconcept disentanglement without requiring mask preparation from humans or\nspecialized models. Moreover, we introduce an adaptive algorithm based on\nattention scores to estimate sampling ratios for different concepts, promoting\nbalanced feature acquisition and synchronized learning. AttenCraft also\nintroduces a feature-retaining training framework that employs various loss\nfunctions to enhance feature recognition and prevent fusion. Extensive\nexperiments show that our model effectively mitigates these two issues,\nachieving state-of-the-art image fidelity and comparable prompt fidelity to\nbaseline models.",
    "published": "2024-05-28T08:50:14Z",
    "updated": "2025-10-16T06:42:33Z",
    "link": "http://arxiv.org/pdf/2405.17965v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junjie Shentu",
      "Matthew Watson",
      "Noura Al Moubayed"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14349v1",
    "title": "Vision-Centric Activation and Coordination for Multimodal Large Language\n  Models",
    "summary": "Multimodal large language models (MLLMs) integrate image features from visual\nencoders with LLMs, demonstrating advanced comprehension capabilities. However,\nmainstream MLLMs are solely supervised by the next-token prediction of textual\ntokens, neglecting critical vision-centric information essential for analytical\nabilities. To track this dilemma, we introduce VaCo, which optimizes MLLM\nrepresentations through Vision-Centric activation and Coordination from\nmultiple vision foundation models (VFMs). VaCo introduces visual discriminative\nalignment to integrate task-aware perceptual features extracted from VFMs,\nthereby unifying the optimization of both textual and visual outputs in MLLMs.\nSpecifically, we incorporate the learnable Modular Task Queries (MTQs) and\nVisual Alignment Layers (VALs) into MLLMs, activating specific visual signals\nunder the supervision of diverse VFMs. To coordinate representation conflicts\nacross VFMs, the crafted Token Gateway Mask (TGM) restricts the information\nflow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo\nsignificantly improves the performance of different MLLMs on various\nbenchmarks, showcasing its superior capabilities in visual comprehension.",
    "published": "2025-10-16T06:38:39Z",
    "updated": "2025-10-16T06:38:39Z",
    "link": "http://arxiv.org/pdf/2510.14349v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yunnan Wang",
      "Fan Lu",
      "Kecheng Zheng",
      "Ziyuan Huang",
      "Ziqiang Li",
      "Wenjun Zeng",
      "Xin Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.20476v2",
    "title": "Towards Inclusive Communication: A Unified Framework for Generating\n  Spoken Language from Sign, Lip, and Audio",
    "summary": "Audio is the primary modality for human communication and has driven the\nsuccess of Automatic Speech Recognition (ASR) technologies. However, such\naudio-centric systems inherently exclude individuals who are deaf or hard of\nhearing. Visual alternatives such as sign language and lip reading offer\neffective substitutes, and recent advances in Sign Language Translation (SLT)\nand Visual Speech Recognition (VSR) have improved audio-less communication.\nYet, these modalities have largely been studied in isolation, and their\nintegration within a unified framework remains underexplored. In this paper, we\npropose the first unified framework capable of handling diverse combinations of\nsign language, lip movements, and audio for spoken-language text generation. We\nfocus on three main objectives: (i) designing a unified, modality-agnostic\narchitecture capable of effectively processing heterogeneous inputs; (ii)\nexploring the underexamined synergy among modalities, particularly the role of\nlip movements as non-manual cues in sign language comprehension; and (iii)\nachieving performance on par with or superior to state-of-the-art models\nspecialized for individual tasks. Building on this framework, we achieve\nperformance on par with or better than task-specific state-of-the-art models\nacross SLT, VSR, ASR, and Audio-Visual Speech Recognition. Furthermore, our\nanalysis reveals a key linguistic insight: explicitly modeling lip movements as\na distinct modality significantly improves SLT performance by capturing\ncritical non-manual cues.",
    "published": "2025-08-28T06:51:42Z",
    "updated": "2025-10-16T06:06:58Z",
    "link": "http://arxiv.org/pdf/2508.20476v2.pdf",
    "category": [
      "cs.CV",
      "cs.MM",
      "eess.AS",
      "eess.IV"
    ],
    "authors": [
      "Jeong Hun Yeo",
      "Hyeongseop Rha",
      "Sungjune Park",
      "Junil Won",
      "Yong Man Ro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18092v2",
    "title": "ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image\n  Generation",
    "summary": "Generating high-fidelity images of humans with fine-grained control over\nattributes such as hairstyle and clothing remains a core challenge in\npersonalized text-to-image synthesis. While prior methods emphasize identity\npreservation from a reference image, they lack modularity and fail to provide\ndisentangled control over specific visual attributes. We introduce a new\nparadigm for attribute-specific image prompting, in which distinct sets of\nreference images are used to guide the generation of individual aspects of\nhuman appearance, such as hair, clothing, and identity. Our method encodes\nthese inputs into attribute-specific tokens, which are injected into a\npre-trained text-to-image diffusion model. This enables compositional and\ndisentangled control over multiple visual factors, even across multiple people\nwithin a single image. To promote natural composition and robust\ndisentanglement, we curate a cross-reference training dataset featuring\nsubjects in diverse poses and expressions, and propose a multi-attribute\ncross-reference training strategy that encourages the model to generate\nfaithful outputs from misaligned attribute inputs while adhering to both\nidentity and textual conditioning. Extensive experiments show that our method\nachieves state-of-the-art performance in accurately following both visual and\ntextual prompts. Our framework paves the way for more configurable human image\nsynthesis by combining visual prompting with text-driven generation. Webpage is\navailable at: https://snap-research.github.io/composeme/.",
    "published": "2025-09-22T17:59:30Z",
    "updated": "2025-10-16T05:30:16Z",
    "link": "http://arxiv.org/pdf/2509.18092v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Guocheng Gordon Qian",
      "Daniil Ostashev",
      "Egor Nemchinov",
      "Avihay Assouline",
      "Sergey Tulyakov",
      "Kuan-Chieh Jackson Wang",
      "Kfir Aberman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13331v2",
    "title": "Group-Wise Optimization for Self-Extensible Codebooks in Vector\n  Quantized Models",
    "summary": "Vector Quantized Variational Autoencoders (VQ-VAEs) leverage self-supervised\nlearning through reconstruction tasks to represent continuous vectors using the\nclosest vectors in a codebook. However, issues such as codebook collapse\npersist in the VQ model. To address these issues, existing approaches employ\nimplicit static codebooks or jointly optimize the entire codebook, but these\nmethods constrain the codebook's learning capability, leading to reduced\nreconstruction quality. In this paper, we propose Group-VQ, which performs\ngroup-wise optimization on the codebook. Each group is optimized independently,\nwith joint optimization performed within groups. This approach improves the\ntrade-off between codebook utilization and reconstruction performance.\nAdditionally, we introduce a training-free codebook resampling method, allowing\npost-training adjustment of the codebook size. In image reconstruction\nexperiments under various settings, Group-VQ demonstrates improved performance\non reconstruction metrics. And the post-training codebook sampling method\nachieves the desired flexibility in adjusting the codebook size.",
    "published": "2025-10-15T09:14:22Z",
    "updated": "2025-10-16T05:26:09Z",
    "link": "http://arxiv.org/pdf/2510.13331v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hong-Kai Zheng",
      "Piji Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14314v1",
    "title": "A Multi-domain Image Translative Diffusion StyleGAN for Iris\n  Presentation Attack Detection",
    "summary": "An iris biometric system can be compromised by presentation attacks (PAs)\nwhere artifacts such as artificial eyes, printed eye images, or cosmetic\ncontact lenses are presented to the system. To counteract this, several\npresentation attack detection (PAD) methods have been developed. However, there\nis a scarcity of datasets for training and evaluating iris PAD techniques due\nto the implicit difficulties in constructing and imaging PAs. To address this,\nwe introduce the Multi-domain Image Translative Diffusion StyleGAN\n(MID-StyleGAN), a new framework for generating synthetic ocular images that\ncaptures the PA and bonafide characteristics in multiple domains such as\nbonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the\nstrengths of diffusion models and generative adversarial networks (GANs) to\nproduce realistic and diverse synthetic data. Our approach utilizes a\nmulti-domain architecture that enables the translation between bonafide ocular\nimages and different PA domains. The model employs an adaptive loss function\ntailored for ocular data to maintain domain consistency. Extensive experiments\ndemonstrate that MID-StyleGAN outperforms existing methods in generating\nhigh-quality synthetic ocular images. The generated data was used to\nsignificantly enhance the performance of PAD systems, providing a scalable\nsolution to the data scarcity problem in iris and ocular biometrics. For\nexample, on the LivDet2020 dataset, the true detect rate at 1% false detect\nrate improved from 93.41% to 98.72%, showcasing the impact of the proposed\nmethod.",
    "published": "2025-10-16T05:21:30Z",
    "updated": "2025-10-16T05:21:30Z",
    "link": "http://arxiv.org/pdf/2510.14314v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shivangi Yadav",
      "Arun Ross"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10478v2",
    "title": "MSF-Mamba: Motion-aware State Fusion Mamba for Efficient Micro-Gesture\n  Recognition",
    "summary": "Micro-gesture recognition (MGR) targets the identification of subtle and\nfine-grained human motions and requires accurate modeling of both long-range\nand local spatiotemporal dependencies. While CNNs are effective at capturing\nlocal patterns, they struggle with long-range dependencies due to their limited\nreceptive fields. Transformer-based models address this limitation through\nself-attention mechanisms but suffer from high computational costs. Recently,\nMamba has shown promise as an efficient model, leveraging state space models\n(SSMs) to enable linear-time processing However, directly applying the vanilla\nMamba to MGR may not be optimal. This is because Mamba processes inputs as 1D\nsequences, with state updates relying solely on the previous state, and thus\nlacks the ability to model local spatiotemporal dependencies. In addition,\nprevious methods lack a design of motion-awareness, which is crucial in MGR. To\novercome these limitations, we propose motion-aware state fusion mamba\n(MSF-Mamba), which enhances Mamba with local spatiotemporal modeling by fusing\nlocal contextual neighboring states. Our design introduces a motion-aware state\nfusion module based on central frame difference (CFD). Furthermore, a\nmultiscale version named MSF-Mamba+ has been proposed. Specifically, MSF-Mamba\nsupports multiscale motion-aware state fusion, as well as an adaptive scale\nweighting module that dynamically weighs the fused states across different\nscales. These enhancements explicitly address the limitations of vanilla Mamba\nby enabling motion-aware local spatiotemporal modeling, allowing MSF-Mamba and\nMSF-Mamba to effectively capture subtle motion cues for MGR. Experiments on two\npublic MGR datasets demonstrate that even the lightweight version, namely,\nMSF-Mamba, achieves SoTA performance, outperforming existing CNN-,\nTransformer-, and SSM-based models while maintaining high efficiency.",
    "published": "2025-10-12T07:16:58Z",
    "updated": "2025-10-16T04:48:32Z",
    "link": "http://arxiv.org/pdf/2510.10478v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Deng Li",
      "Jun Shao",
      "Bohao Xing",
      "Rong Gao",
      "Bihan Wen",
      "Heikki Klviinen",
      "Xin Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.03999v2",
    "title": "Impact of Regularization on Calibration and Robustness: from the\n  Representation Space Perspective",
    "summary": "Recent studies have shown that regularization techniques using soft labels,\ne.g., label smoothing, Mixup, and CutMix, not only enhance image classification\naccuracy but also mitigate miscalibration due to overconfident predictions, and\nimprove robustness against adversarial attacks. However, the underlying\nmechanisms of such improvements remain underexplored. In this paper, we offer a\nnovel explanation from the perspective of the representation space (i.e., the\nspace of the features obtained at the penultimate layer). Based on examination\nof decision boundaries and structure of features (or representation vectors),\nour study investigates confidence contours and gradient directions within the\nrepresentation space. Furthermore, we analyze the adjustments in feature\ndistributions due to regularization in relation to these contours and\ndirections, from which we uncover central mechanisms inducing improved\ncalibration and robustness. Our findings provide new insights into the\ncharacteristics of the high-dimensional representation space in relation to\ntraining and regularization using soft labels.",
    "published": "2024-10-05T02:09:03Z",
    "updated": "2025-10-16T04:38:17Z",
    "link": "http://arxiv.org/pdf/2410.03999v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jonghyun Park",
      "Juyeop Kim",
      "Jong-Seok Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12901v2",
    "title": "SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms",
    "summary": "Rigorous testing of autonomous robots, such as self-driving vehicles, is\nessential to ensure their safety in real-world deployments. This requires\nbuilding high-fidelity simulators to test scenarios beyond those that can be\nsafely or exhaustively collected in the real-world. Existing neural rendering\nmethods based on NeRF and 3DGS hold promise but suffer from low rendering\nspeeds or can only render pinhole camera models, hindering their suitability to\napplications that commonly require high-distortion lenses and LiDAR data.\nMulti-sensor simulation poses additional challenges as existing methods handle\ncross-sensor inconsistencies by favoring the quality of one modality at the\nexpense of others. To overcome these limitations, we propose SimULi, the first\nmethod capable of rendering arbitrary camera models and LiDAR data in\nreal-time. Our method extends 3DGUT, which natively supports complex camera\nmodels, with LiDAR support, via an automated tiling strategy for arbitrary\nspinning LiDAR models and ray-based culling. To address cross-sensor\ninconsistencies, we design a factorized 3D Gaussian representation and\nanchoring strategy that reduces mean camera and depth error by up to 40%\ncompared to existing methods. SimULi renders 10-20x faster than ray tracing\napproaches and 1.5-10x faster than prior rasterization-based work (and handles\na wider range of camera models). When evaluated on two widely benchmarked\nautonomous driving datasets, SimULi matches or exceeds the fidelity of existing\nstate-of-the-art methods across numerous camera and LiDAR metrics.",
    "published": "2025-10-14T18:22:45Z",
    "updated": "2025-10-16T04:37:37Z",
    "link": "http://arxiv.org/pdf/2510.12901v2.pdf",
    "category": [
      "cs.CV",
      "cs.GR",
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Haithem Turki",
      "Qi Wu",
      "Xin Kang",
      "Janick Martinez Esturo",
      "Shengyu Huang",
      "Ruilong Li",
      "Zan Gojcic",
      "Riccardo de Lutio"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.14905v2",
    "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\n  with Artifact Explanation",
    "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) technologies, synthetic images have become increasingly prevalent in\neveryday life, posing new challenges for authenticity assessment and detection.\nDespite the effectiveness of existing methods in evaluating image authenticity\nand locating forgeries, these approaches often lack human interpretability and\ndo not fully address the growing complexity of synthetic data. To tackle these\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\nexcels in distinguishing real from fake images but also provides clear, natural\nlanguage explanations for image artifacts, enhancing interpretability.\nAdditionally, we present FakeClue, a comprehensive dataset containing over\n100,000 images across seven categories, annotated with fine-grained artifact\nclues in natural language. FakeVLM demonstrates performance comparable to\nexpert models while eliminating the need for additional classifiers, making it\na robust solution for synthetic data detection. Extensive evaluations across\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\nclassification and artifact explanation tasks, setting a new benchmark for\nsynthetic image detection. The code, model weights, and dataset can be found\nhere: https://github.com/opendatalab/FakeVLM.",
    "published": "2025-03-19T05:14:44Z",
    "updated": "2025-10-16T04:28:19Z",
    "link": "http://arxiv.org/pdf/2503.14905v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Siwei Wen",
      "Junyan Ye",
      "Peilin Feng",
      "Hengrui Kang",
      "Zichen Wen",
      "Yize Chen",
      "Jiang Wu",
      "Wenjun Wu",
      "Conghui He",
      "Weijia Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.11003v2",
    "title": "ForensicHub: A Unified Benchmark & Codebase for All-Domain Fake Image\n  Detection and Localization",
    "summary": "The field of Fake Image Detection and Localization (FIDL) is highly\nfragmented, encompassing four domains: deepfake detection (Deepfake), image\nmanipulation detection and localization (IMDL), artificial\nintelligence-generated image detection (AIGC), and document image manipulation\nlocalization (Doc). Although individual benchmarks exist in some domains, a\nunified benchmark for all domains in FIDL remains blank. The absence of a\nunified benchmark results in significant domain silos, where each domain\nindependently constructs its datasets, models, and evaluation protocols without\ninteroperability, preventing cross-domain comparisons and hindering the\ndevelopment of the entire FIDL field. To close the domain silo barrier, we\npropose ForensicHub, the first unified benchmark & codebase for all-domain fake\nimage detection and localization. Considering drastic variations on dataset,\nmodel, and evaluation configurations across all domains, as well as the\nscarcity of open-sourced baseline models and the lack of individual benchmarks\nin some domains, ForensicHub: i) proposes a modular and configuration-driven\narchitecture that decomposes forensic pipelines into interchangeable components\nacross datasets, transforms, models, and evaluators, allowing flexible\ncomposition across all domains; ii) fully implements 10 baseline models, 6\nbackbones, 2 new benchmarks for AIGC and Doc, and integrates 2 existing\nbenchmarks of DeepfakeBench and IMDLBenCo through an adapter-based design; iii)\nconducts indepth analysis based on the ForensicHub, offering 8 key actionable\ninsights into FIDL model architecture, dataset characteristics, and evaluation\nstandards. ForensicHub represents a significant leap forward in breaking the\ndomain silos in the FIDL field and inspiring future breakthroughs.",
    "published": "2025-05-16T08:49:59Z",
    "updated": "2025-10-16T04:04:09Z",
    "link": "http://arxiv.org/pdf/2505.11003v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Bo Du",
      "Xuekang Zhu",
      "Xiaochen Ma",
      "Chenfan Qu",
      "Kaiwen Feng",
      "Zhe Yang",
      "Chi-Man Pun",
      "Jian Liu",
      "Jizhe Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.11863v2",
    "title": "SphereDrag: Spherical Geometry-Aware Panoramic Image Editing",
    "summary": "Image editing has made great progress on planar images, but panoramic image\nediting remains underexplored. Due to their spherical geometry and projection\ndistortions, panoramic images present three key challenges: boundary\ndiscontinuity, trajectory deformation, and uneven pixel density. To tackle\nthese issues, we propose SphereDrag, a novel panoramic editing framework\nutilizing spherical geometry knowledge for accurate and controllable editing.\nSpecifically, adaptive reprojection (AR) uses adaptive spherical rotation to\ndeal with discontinuity; great-circle trajectory adjustment (GCTA) tracks the\nmovement trajectory more accurate; spherical search region tracking (SSRT)\nadaptively scales the search range based on spherical location to address\nuneven pixel density. Also, we construct PanoBench, a panoramic editing\nbenchmark, including complex editing tasks involving multiple objects and\ndiverse styles, which provides a standardized evaluation framework. Experiments\nshow that SphereDrag gains a considerable improvement compared with existing\nmethods in geometric consistency and image quality, achieving up to 10.5%\nrelative improvement.",
    "published": "2025-06-13T15:13:09Z",
    "updated": "2025-10-16T03:49:50Z",
    "link": "http://arxiv.org/pdf/2506.11863v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhiao Feng",
      "Xuewei Li",
      "Junjie Yang",
      "Jingchao Li",
      "Yuxin Peng",
      "Xi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14273v1",
    "title": "CLEAR: Causal Learning Framework For Robust Histopathology Tumor\n  Detection Under Out-Of-Distribution Shifts",
    "summary": "Domain shift in histopathology, often caused by differences in acquisition\nprocesses or data sources, poses a major challenge to the generalization\nability of deep learning models. Existing methods primarily rely on modeling\nstatistical correlations by aligning feature distributions or introducing\nstatistical variation, yet they often overlook causal relationships. In this\nwork, we propose a novel causal-inference-based framework that leverages\nsemantic features while mitigating the impact of confounders. Our method\nimplements the front-door principle by designing transformation strategies that\nexplicitly incorporate mediators and observed tissue slides. We validate our\nmethod on the CAMELYON17 dataset and a private histopathology dataset,\ndemonstrating consistent performance gains across unseen domains. As a result,\nour approach achieved up to a 7% improvement in both the CAMELYON17 dataset and\nthe private histopathology dataset, outperforming existing baselines. These\nresults highlight the potential of causal inference as a powerful tool for\naddressing domain shift in histopathology image analysis.",
    "published": "2025-10-16T03:45:31Z",
    "updated": "2025-10-16T03:45:31Z",
    "link": "http://arxiv.org/pdf/2510.14273v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kieu-Anh Truong Thi",
      "Huy-Hieu Pham",
      "Duc-Trong Le"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14270v1",
    "title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and\n  Geometric Filtering",
    "summary": "Scene reconstruction has emerged as a central challenge in computer vision,\nwith approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting\nachieving remarkable progress. While Gaussian Splatting demonstrates strong\nperformance on large-scale datasets, it often struggles to capture fine details\nor maintain realism in regions with sparse coverage, largely due to the\ninherent limitations of sparse 3D training data.\n  In this work, we propose GauSSmart, a hybrid method that effectively bridges\n2D foundational models and 3D Gaussian Splatting reconstruction. Our approach\nintegrates established 2D computer vision techniques, including convex\nfiltering and semantic feature supervision from foundational models such as\nDINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D\nsegmentation priors and high-dimensional feature embeddings, our method guides\nthe densification and refinement of Gaussian splats, improving coverage in\nunderrepresented areas and preserving intricate structural details.\n  We validate our approach across three datasets, where GauSSmart consistently\noutperforms existing Gaussian Splatting in the majority of evaluated scenes.\nOur results demonstrate the significant potential of hybrid 2D-3D approaches,\nhighlighting how the thoughtful combination of 2D foundational models with 3D\nreconstruction pipelines can overcome the limitations inherent in either\napproach alone.",
    "published": "2025-10-16T03:38:26Z",
    "updated": "2025-10-16T03:38:26Z",
    "link": "http://arxiv.org/pdf/2510.14270v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Alexander Valverde",
      "Brian Xu",
      "Yuyin Zhou",
      "Meng Xu",
      "Hongyun Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.21999v2",
    "title": "ELASTIC: Efficient Once For All Iterative Search for Object Detection on\n  Microcontrollers",
    "summary": "Deploying high-performance object detectors on TinyML platforms poses\nsignificant challenges due to tight hardware constraints and the modular\ncomplexity of modern detection pipelines. Neural Architecture Search (NAS)\noffers a path toward automation, but existing methods either restrict\noptimization to individual modules, sacrificing cross-module synergy, or\nrequire global searches that are computationally intractable. We propose\nELASTIC (Efficient Once for AlL IterAtive Search for ObjecT DetectIon on\nMiCrocontrollers), a unified, hardware-aware NAS framework that alternates\noptimization across modules (e.g., backbone, neck, and head) in a cyclic\nfashion. ELASTIC introduces a novel Population Passthrough mechanism in\nevolutionary search that retains high-quality candidates between search stages,\nyielding faster convergence, up to an 8% final mAP gain, and eliminates search\ninstability observed without population passthrough. In a controlled\ncomparison, empirical results show ELASTIC achieves +4.75% higher mAP and 2x\nfaster convergence than progressive NAS strategies on SVHN, and delivers a\n+9.09% mAP improvement on PascalVOC given the same search budget. ELASTIC\nachieves 72.3% mAP on PascalVOC, outperforming MCUNET by 20.9% and\nTinyissimoYOLO by 16.3%. When deployed on MAX78000/MAX78002 microcontrollers,\nELASTICderived models outperform Analog Devices' TinySSD baselines, reducing\nenergy by up to 71.6%, lowering latency by up to 2.4x, and improving mAP by up\nto 6.99 percentage points across multiple datasets.",
    "published": "2025-03-27T21:35:02Z",
    "updated": "2025-10-16T03:38:23Z",
    "link": "http://arxiv.org/pdf/2503.21999v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Tony Tran",
      "Qin Lin",
      "Bin Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14266v1",
    "title": "Experimental Demonstration of Event-based Optical Camera Communication\n  in Long-Range Outdoor Environment",
    "summary": "We propose a robust demodulation scheme for optical camera communication\nsystems using an event-based vision sensor, combining OOK with toggle\ndemodulation and a digital phase-locked loop. This is the first report to\nachieve a $\\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor\nexperiments.",
    "published": "2025-10-16T03:36:08Z",
    "updated": "2025-10-16T03:36:08Z",
    "link": "http://arxiv.org/pdf/2510.14266v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Miu Sumino",
      "Mayu Ishii",
      "Shun Kaizu",
      "Daisuke Hisano",
      "Yu Nakayama"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13245v2",
    "title": "CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic\n  Urban Scene Generation",
    "summary": "Outdoor 3D semantic scene generation produces realistic and semantically rich\nenvironments for applications such as urban simulation and autonomous driving.\nHowever, advances in this direction are constrained by the absence of publicly\navailable, well-annotated datasets. We introduce SketchSem3D, the first\nlarge-scale benchmark for generating 3D outdoor semantic scenes from abstract\nfreehand sketches and pseudo-labeled annotations of satellite images.\nSketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based\nKITTI-360 (containing LiDAR voxels along with their corresponding sketches and\nannotated satellite images), to enable standardized, rigorous, and diverse\nevaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that\nsignificantly enhances spatial coherence in outdoor 3D scene generation.\nCymbaDiff imposes structured spatial ordering, explicitly captures cylindrical\ncontinuity and vertical hierarchy, and preserves both physical neighborhood\nrelationships and global context within the generated scenes. Extensive\nexperiments on SketchSem3D demonstrate that CymbaDiff achieves superior\nsemantic consistency, spatial realism, and cross-dataset generalization. The\ncode and dataset will be available at\nhttps://github.com/Lillian-research-hub/CymbaDiff",
    "published": "2025-10-15T07:47:00Z",
    "updated": "2025-10-16T03:29:06Z",
    "link": "http://arxiv.org/pdf/2510.13245v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Li Liang",
      "Bo Miao",
      "Xinyu Wang",
      "Naveed Akhtar",
      "Jordan Vice",
      "Ajmal Mian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14260v1",
    "title": "MatchAttention: Matching the Relative Positions for High-Resolution\n  Cross-View Matching",
    "summary": "Cross-view matching is fundamentally achieved through cross-attention\nmechanisms. However, matching of high-resolution images remains challenging due\nto the quadratic complexity and lack of explicit matching constraints in the\nexisting cross-attention. This paper proposes an attention mechanism,\nMatchAttention, that dynamically matches relative positions. The relative\nposition determines the attention sampling center of the key-value pairs given\na query. Continuous and differentiable sliding-window attention sampling is\nachieved by the proposed BilinearSoftmax. The relative positions are\niteratively updated through residual connections across layers by embedding\nthem into the feature channels. Since the relative position is exactly the\nlearning target for cross-view matching, an efficient hierarchical cross-view\ndecoder, MatchDecoder, is designed with MatchAttention as its core component.\nTo handle cross-view occlusions, gated cross-MatchAttention and a\nconsistency-constrained loss are proposed. These two components collectively\nmitigate the impact of occlusions in both forward and backward passes, allowing\nthe model to focus more on learning matching relationships. When applied to\nstereo matching, MatchStereo-B ranked 1st in average error on the public\nMiddlebury benchmark and requires only 29ms for KITTI-resolution inference.\nMatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU\nmemory. The proposed models also achieve state-of-the-art performance on KITTI\n2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high\naccuracy and low computational complexity makes real-time, high-resolution, and\nhigh-accuracy cross-view matching possible. Code is available at\nhttps://github.com/TingmanYan/MatchAttention.",
    "published": "2025-10-16T03:21:28Z",
    "updated": "2025-10-16T03:21:28Z",
    "link": "http://arxiv.org/pdf/2510.14260v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tingman Yan",
      "Tao Liu",
      "Xilian Yang",
      "Qunfei Zhao",
      "Zeyang Xia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07944v2",
    "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal\n  Reconstruction Model for Autonomous Driving",
    "summary": "Generative models have been widely applied to world modeling for environment\nsimulation and future state prediction. With advancements in autonomous\ndriving, there is a growing demand not only for high-fidelity video generation\nunder various controls, but also for producing diverse and meaningful\ninformation such as depth estimation. To address this, we propose CVD-STORM, a\ncross-view video diffusion model utilizing a spatial-temporal reconstruction\nVariational Autoencoder (VAE) that generates long-term, multi-view videos with\n4D reconstruction capabilities under various control inputs. Our approach first\nfine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its\nability to encode 3D structures and temporal dynamics. Subsequently, we\nintegrate this VAE into the video diffusion process to significantly improve\ngeneration quality. Experimental results demonstrate that our model achieves\nsubstantial improvements in both FID and FVD metrics. Additionally, the\njointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic\nscenes, providing valuable geometric information for comprehensive scene\nunderstanding. Our project page is https://sensetime-fvg.github.io/CVD-STORM.",
    "published": "2025-10-09T08:41:58Z",
    "updated": "2025-10-16T03:14:45Z",
    "link": "http://arxiv.org/pdf/2510.07944v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tianrui Zhang",
      "Yichen Liu",
      "Zilin Guo",
      "Yuxin Guo",
      "Jingcheng Ni",
      "Chenjing Ding",
      "Dan Xu",
      "Lewei Lu",
      "Zehuan Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14256v1",
    "title": "Identity-GRPO: Optimizing Multi-Human Identity-preserving Video\n  Generation via Reinforcement Learning",
    "summary": "While advanced methods like VACE and Phantom have advanced video generation\nfor specific subjects in diverse scenarios, they struggle with multi-human\nidentity preservation in dynamic interactions, where consistent identities\nacross multiple characters are critical. To address this, we propose\nIdentity-GRPO, a human feedback-driven optimization pipeline for refining\nmulti-human identity-preserving video generation. First, we construct a video\nreward model trained on a large-scale preference dataset containing\nhuman-annotated and synthetic distortion data, with pairwise annotations\nfocused on maintaining human consistency throughout the video. We then employ a\nGRPO variant tailored for multi-human consistency, which greatly enhances both\nVACE and Phantom. Through extensive ablation studies, we evaluate the impact of\nannotation quality and design choices on policy optimization. Experiments show\nthat Identity-GRPO achieves up to 18.9% improvement in human consistency\nmetrics over baseline methods, offering actionable insights for aligning\nreinforcement learning with personalized video generation.",
    "published": "2025-10-16T03:13:56Z",
    "updated": "2025-10-16T03:13:56Z",
    "link": "http://arxiv.org/pdf/2510.14256v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiangyu Meng",
      "Zixian Zhang",
      "Zhenghao Zhang",
      "Junchao Liao",
      "Long Qin",
      "Weizhi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14255v1",
    "title": "Identity-Preserving Image-to-Video Generation via Reward-Guided\n  Optimization",
    "summary": "Recent advances in image-to-video (I2V) generation have achieved remarkable\nprogress in synthesizing high-quality, temporally coherent videos from static\nimages. Among all the applications of I2V, human-centric video generation\nincludes a large portion. However, existing I2V models encounter difficulties\nin maintaining identity consistency between the input human image and the\ngenerated video, especially when the person in the video exhibits significant\nexpression changes and movements. This issue becomes critical when the human\nface occupies merely a small fraction of the image. Since humans are highly\nsensitive to identity variations, this poses a critical yet under-explored\nchallenge in I2V generation. In this paper, we propose Identity-Preserving\nReward-guided Optimization (IPRO), a novel video diffusion framework based on\nreinforcement learning to enhance identity preservation. Instead of introducing\nauxiliary modules or altering model architectures, our approach introduces a\ndirect and effective tuning algorithm that optimizes diffusion models using a\nface identity scorer. To improve performance and accelerate convergence, our\nmethod backpropagates the reward signal through the last steps of the sampling\nchain, enabling richer gradient feedback. We also propose a novel facial\nscoring mechanism that treats faces in ground-truth videos as facial feature\npools, providing multi-angle facial information to enhance generalization. A\nKL-divergence regularization is further incorporated to stabilize training and\nprevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V\nmodel and our in-house I2V model demonstrate the effectiveness of our method.\nOur project and code are available at\n\\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.",
    "published": "2025-10-16T03:13:47Z",
    "updated": "2025-10-16T03:13:47Z",
    "link": "http://arxiv.org/pdf/2510.14255v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Liao Shen",
      "Wentao Jiang",
      "Yiran Zhu",
      "Tiezheng Ge",
      "Zhiguo Cao",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13660v2",
    "title": "OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild",
    "summary": "Current 3D gaze estimation methods struggle to generalize across diverse data\ndomains, primarily due to i) the scarcity of annotated datasets, and ii) the\ninsufficient diversity of labeled data. In this work, we present OmniGaze, a\nsemi-supervised framework for 3D gaze estimation, which utilizes large-scale\nunlabeled data collected from diverse and unconstrained real-world environments\nto mitigate domain bias and generalize gaze estimation in the wild. First, we\nbuild a diverse collection of unlabeled facial images, varying in facial\nappearances, background environments, illumination conditions, head poses, and\neye occlusions. In order to leverage unlabeled data spanning a broader\ndistribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a\nreward model to assess the reliability of pseudo labels. Beyond pseudo labels\nas 3D direction vectors, the reward model also incorporates visual embeddings\nextracted by an off-the-shelf visual encoder and semantic cues from gaze\nperspective generated by prompting a Multimodal Large Language Model to compute\nconfidence scores. Then, these scores are utilized to select high-quality\npseudo labels and weight them for loss computation. Extensive experiments\ndemonstrate that OmniGaze achieves state-of-the-art performance on five\ndatasets under both in-domain and cross-domain settings. Furthermore, we also\nevaluate the efficacy of OmniGaze as a scalable data engine for gaze\nestimation, which exhibits robust zero-shot generalization on four unseen\ndatasets.",
    "published": "2025-10-15T15:19:52Z",
    "updated": "2025-10-16T03:10:21Z",
    "link": "http://arxiv.org/pdf/2510.13660v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hongyu Qu",
      "Jianan Wei",
      "Xiangbo Shu",
      "Yazhou Yao",
      "Wenguan Wang",
      "Jinhui Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14251v1",
    "title": "MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale\n  Scene Localization and Rendering",
    "summary": "Efficient localization and high-quality rendering in large-scale scenes\nremain a significant challenge due to the computational cost involved. While\nScene Coordinate Regression (SCR) methods perform well in small-scale\nlocalization, they are limited by the capacity of a single network when\nextended to large-scale scenes. To address these challenges, we propose the\nMixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables\nefficient localization and high-quality rendering in large-scale scenes.\nInspired by the remarkable capabilities of MOE in large model domains, we\nintroduce a gating network to implicitly classify and select sub-networks,\nensuring that only a single sub-network is activated during each inference.\nFurtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to\nenhance the localization accuracy on large-scale scene. Our framework provides\na significant reduction in costs while maintaining higher precision, offering\nan efficient solution for large-scale scene applications. Additional\nexperiments on the Cambridge test set demonstrate that our method achieves\nhigh-quality rendering results with merely 10 minutes of training.",
    "published": "2025-10-16T03:08:19Z",
    "updated": "2025-10-16T03:08:19Z",
    "link": "http://arxiv.org/pdf/2510.14251v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mingkai Liu",
      "Dikai Fan",
      "Haohua Que",
      "Haojia Gao",
      "Xiao Liu",
      "Shuxue Peng",
      "Meixia Lin",
      "Shengyu Gu",
      "Ruicong Ye",
      "Wanli Qiu",
      "Handong Yao",
      "Ruopeng Zhang",
      "Xianliang Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14245v1",
    "title": "Event Interval Modulation: A Novel Scheme for Event-based Optical Camera\n  Communication",
    "summary": "Optical camera communication (OCC) represents a promising visible light\ncommunication technology. Nonetheless, typical OCC systems utilizing\nframe-based cameras are encumbered by limitations, including low bit rate and\nhigh processing load. To address these issues, OCC system utilizing an\nevent-based vision sensor (EVS) as receivers have been proposed. The EVS\nenables high-speed, low-latency, and robust communication due to its\nasynchronous operation and high dynamic range. In existing event-based OCC\nsystems, conventional modulation schemes such as on-off keying (OOK) and pulse\nposition modulation have been applied, however, to the best of our knowledge,\nno modulation method has been proposed that fully exploits the unique\ncharacteristics of the EVS. This paper proposes a novel modulation scheme,\ncalled the event interval modulation (EIM) scheme, specifically designed for\nevent-based OCC. EIM enables improvement in transmission speed by modulating\ninformation using the intervals between events. This paper proposes a\ntheoretical model of EIM and conducts a proof-of-concept experiment. First, the\nparameters of the EVS are tuned and customized to optimize the frequency\nresponse specifically for EIM. Then, the maximum modulation order usable in EIM\nis determined experimentally. We conduct transmission experiments based on the\nobtained parameters. Finally, we report successful transmission at 28 kbps over\n10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new\nbenchmark for bit rate in event-based OCC systems.",
    "published": "2025-10-16T02:56:29Z",
    "updated": "2025-10-16T02:56:29Z",
    "link": "http://arxiv.org/pdf/2510.14245v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Miu Sumino",
      "Mayu Ishii",
      "Shun Kaizu",
      "Daisuke Hisano",
      "Yu Nakayama"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14241v1",
    "title": "PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic\n  Analysis",
    "summary": "The rise of manipulated media has made deepfakes a particularly insidious\nthreat, involving various generative manipulations such as lip-sync\nmodifications, face-swaps, and avatar-driven facial synthesis. Conventional\ndetection methods, which predominantly depend on manually designed\nphoneme-viseme alignment thresholds, fundamental frame-level consistency\nchecks, or a unimodal detection strategy, inadequately identify modern-day\ndeepfakes generated by advanced generative models such as GANs, diffusion\nmodels, and neural rendering techniques. These advanced techniques generate\nnearly perfect individual frames yet inadvertently create minor temporal\ndiscrepancies frequently overlooked by traditional detectors. We present a\nnovel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic\nAnalysis(PIA), incorporating language, dynamic face motion, and facial\nidentification cues to address these limitations. We utilize phoneme sequences,\nlip geometry data, and advanced facial identity embeddings. This integrated\nmethod significantly improves the detection of subtle deepfake alterations by\nidentifying inconsistencies across multiple complementary modalities. Code is\navailable at https://github.com/skrantidatta/PIA",
    "published": "2025-10-16T02:51:42Z",
    "updated": "2025-10-16T02:51:42Z",
    "link": "http://arxiv.org/pdf/2510.14241v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Soumyya Kanti Datta",
      "Tanvi Ranga",
      "Chengzhe Sun",
      "Siwei Lyu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.15008v2",
    "title": "HuGDiffusion: Generalizable Single-Image Human Rendering via 3D Gaussian\n  Diffusion",
    "summary": "We present HuGDiffusion, a generalizable 3D Gaussian splatting (3DGS)\nlearning pipeline to achieve novel view synthesis (NVS) of human characters\nfrom single-view input images. Existing approaches typically require monocular\nvideos or calibrated multi-view images as inputs, whose applicability could be\nweakened in real-world scenarios with arbitrary and/or unknown camera poses. In\nthis paper, we aim to generate the set of 3DGS attributes via a diffusion-based\nframework conditioned on human priors extracted from a single image.\nSpecifically, we begin with carefully integrated human-centric feature\nextraction procedures to deduce informative conditioning signals. Based on our\nempirical observations that jointly learning the whole 3DGS attributes is\nchallenging to optimize, we design a multi-stage generation strategy to obtain\ndifferent types of 3DGS attributes. To facilitate the training process, we\ninvestigate constructing proxy ground-truth 3D Gaussian attributes as\nhigh-quality attribute-level supervision signals. Through extensive\nexperiments, our HuGDiffusion shows significant performance improvements over\nthe state-of-the-art methods. Our code will be made publicly available.",
    "published": "2025-01-25T01:00:33Z",
    "updated": "2025-10-16T02:35:50Z",
    "link": "http://arxiv.org/pdf/2501.15008v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yingzhi Tang",
      "Qijian Zhang",
      "Junhui Hou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.10426v2",
    "title": "CAP: Evaluation of Persuasive and Creative Image Generation",
    "summary": "We address the task of advertisement image generation and introduce three\nevaluation metrics to assess Creativity, prompt Alignment, and Persuasiveness\n(CAP) in generated advertisement images. Despite recent advancements in\nText-to-Image (T2I) generation and their performance in generating high-quality\nimages for explicit descriptions, evaluating these models remains challenging.\nExisting evaluation methods focus largely on assessing alignment with explicit,\ndetailed descriptions, but evaluating alignment with visually implicit prompts\nremains an open problem. Additionally, creativity and persuasiveness are\nessential qualities that enhance the effectiveness of advertisement images, yet\nare seldom measured. To address this, we propose three novel metrics for\nevaluating the creativity, alignment, and persuasiveness of generated images.\nOur findings reveal that current T2I models struggle with creativity,\npersuasiveness, and alignment when the input text is implicit messages. We\nfurther introduce a simple yet effective approach to enhance T2I models'\ncapabilities in producing images that are better aligned, more creative, and\nmore persuasive.",
    "published": "2024-12-10T19:54:59Z",
    "updated": "2025-10-16T02:33:33Z",
    "link": "http://arxiv.org/pdf/2412.10426v2.pdf",
    "category": [
      "cs.CV",
      "cs.CL",
      "cs.GR"
    ],
    "authors": [
      "Aysan Aghazadeh",
      "Adriana Kovashka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14230v1",
    "title": "LOTA: Bit-Planes Guided AI-Generated Image Detection",
    "summary": "The rapid advancement of GAN and Diffusion models makes it more difficult to\ndistinguish AI-generated images from real ones. Recent studies often use\nimage-based reconstruction errors as an important feature for determining\nwhether an image is AI-generated. However, these approaches typically incur\nhigh computational costs and also fail to capture intrinsic noisy features\npresent in the raw images. To solve these problems, we innovatively refine\nerror extraction by using bit-plane-based image processing, as lower bit planes\nindeed represent noise patterns in images. We introduce an effective bit-planes\nguided noisy image generation and exploit various image normalization\nstrategies, including scaling and thresholding. Then, to amplify the noise\nsignal for easier AI-generated image detection, we design a maximum gradient\npatch selection that applies multi-directional gradients to compute the noise\nscore and selects the region with the highest score. Finally, we propose a\nlightweight and effective classification head and explore two different\nstructures: noise-based classifier and noise-guided classifier. Extensive\nexperiments on the GenImage benchmark demonstrate the outstanding performance\nof our method, which achieves an average accuracy of \\textbf{98.9\\%}\n(\\textbf{11.9}\\%~$\\uparrow$) and shows excellent cross-generator generalization\ncapability. Particularly, our method achieves an accuracy of over 98.2\\% from\nGAN to Diffusion and over 99.2\\% from Diffusion to GAN. Moreover, it performs\nerror extraction at the millisecond level, nearly a hundred times faster than\nexisting methods. The code is at https://github.com/hongsong-wang/LOTA.",
    "published": "2025-10-16T02:12:49Z",
    "updated": "2025-10-16T02:12:49Z",
    "link": "http://arxiv.org/pdf/2510.14230v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hongsong Wang",
      "Renxi Cheng",
      "Yang Zhang",
      "Chaolei Han",
      "Jie Gui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.07982v2",
    "title": "TRACE: Your Diffusion Model is Secretly an Instance Edge Detector",
    "summary": "High-quality instance and panoptic segmentation has traditionally relied on\ndense instance-level annotations such as masks, boxes, or points, which are\ncostly, inconsistent, and difficult to scale. Unsupervised and\nweakly-supervised approaches reduce this burden but remain constrained by\nsemantic backbone constraints and human bias, often producing merged or\nfragmented outputs. We present TRACE (TRAnsforming diffusion Cues to instance\nEdges), showing that text-to-image diffusion models secretly function as\ninstance edge annotators. TRACE identifies the Instance Emergence Point (IEP)\nwhere object boundaries first appear in self-attention maps, extracts\nboundaries through Attention Boundary Divergence (ABDiv), and distills them\ninto a lightweight one-step edge decoder. This design removes the need for\nper-image diffusion inversion, achieving 81x faster inference while producing\nsharper and more connected boundaries. On the COCO benchmark, TRACE improves\nunsupervised instance segmentation by +5.1 AP, and in tag-supervised panoptic\nsegmentation it outperforms point-supervised baselines by +1.7 PQ without using\nany instance-level labels. These results reveal that diffusion models encode\nhidden instance boundary priors, and that decoding these signals offers a\npractical and scalable alternative to costly manual annotation. Code is\navailable at https://github.com/shjo-april/DiffEGG.",
    "published": "2025-03-11T02:34:33Z",
    "updated": "2025-10-16T02:11:04Z",
    "link": "http://arxiv.org/pdf/2503.07982v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sanghyun Jo",
      "Ziseok Lee",
      "Wooyeol Lee",
      "Jonghyun Choi",
      "Jaesik Park",
      "Kyungsu Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14203v1",
    "title": "Joint Modeling of Big Five and HEXACO for Multimodal Apparent\n  Personality-trait Recognition",
    "summary": "This paper proposes a joint modeling method of the Big Five, which has long\nbeen studied, and HEXACO, which has recently attracted attention in psychology,\nfor automatically recognizing apparent personality traits from multimodal human\nbehavior. Most previous studies have used the Big Five for multimodal apparent\npersonality-trait recognition. However, no study has focused on apparent HEXACO\nwhich can evaluate an Honesty-Humility trait related to displaced aggression\nand vengefulness, social-dominance orientation, etc. In addition, the\nrelationships between the Big Five and HEXACO when modeled by machine learning\nhave not been clarified. We expect awareness of multimodal human behavior to\nimprove by considering these relationships. The key advance of our proposed\nmethod is to optimize jointly recognizing the Big Five and HEXACO. Experiments\nusing a self-introduction video dataset demonstrate that the proposed method\ncan effectively recognize the Big Five and HEXACO.",
    "published": "2025-10-16T01:21:57Z",
    "updated": "2025-10-16T01:21:57Z",
    "link": "http://arxiv.org/pdf/2510.14203v1.pdf",
    "category": [
      "cs.CV",
      "cs.CL",
      "cs.MM"
    ],
    "authors": [
      "Ryo Masumura",
      "Shota Orihashi",
      "Mana Ihori",
      "Tomohiro Tanaka",
      "Naoki Makishima",
      "Taiga Yamane",
      "Naotaka Kawata",
      "Satoshi Suzuki",
      "Taichi Katayama"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.06781v2",
    "title": "UrbanTwin: Synthetic LiDAR Datasets (LUMPI, V2X-Real-IC, and TUMTraf-I)",
    "summary": "This article presents UrbanTwin datasets, high-fidelity, realistic replicas\nof three public roadside lidar datasets: LUMPI, V2X-Real-IC}}, and TUMTraf-I.\nEach UrbanTwin dataset contains 10K annotated frames corresponding to one of\nthe public datasets. Annotations include 3D bounding boxes, instance\nsegmentation labels, and tracking IDs for six object classes, along with\nsemantic segmentation labels for nine classes. These datasets are synthesized\nusing emulated lidar sensors within realistic digital twins, modeled based on\nsurrounding geometry, road alignment at lane level, and the lane topology and\nvehicle movement patterns at intersections of the actual locations\ncorresponding to each real dataset. Due to the precise digital twin modeling,\nthe synthetic datasets are well aligned with their real counterparts, offering\nstrong standalone and augmentative value for training deep learning models on\ntasks such as 3D object detection, tracking, and semantic and instance\nsegmentation. We evaluate the alignment of the synthetic replicas through\nstatistical and structural similarity analysis with real data, and further\ndemonstrate their utility by training 3D object detection models solely on\nsynthetic data and testing them on real, unseen data. The high similarity\nscores and improved detection performance, compared to the models trained on\nreal data, indicate that the UrbanTwin datasets effectively enhance existing\nbenchmark datasets by increasing sample size and scene diversity. In addition,\nthe digital twins can be adapted to test custom scenarios by modifying the\ndesign and dynamics of the simulations. To our knowledge, these are the first\ndigitally synthesized datasets that can replace in-domain real-world datasets\nfor lidar perception tasks. UrbanTwin datasets are publicly available at\nhttps://dataverse.harvard.edu/dataverse/ucf-ut.",
    "published": "2025-09-08T15:06:02Z",
    "updated": "2025-10-16T00:03:59Z",
    "link": "http://arxiv.org/pdf/2509.06781v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Muhammad Shahbaz",
      "Shaurya Agarwal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.21117v2",
    "title": "CL-Splats: Continual Learning of Gaussian Splatting with Local\n  Optimization",
    "summary": "In dynamic 3D environments, accurately updating scene representations over\ntime is crucial for applications in robotics, mixed reality, and embodied AI.\nAs scenes evolve, efficient methods to incorporate changes are needed to\nmaintain up-to-date, high-quality reconstructions without the computational\noverhead of re-optimizing the entire scene. This paper introduces CL-Splats,\nwhich incrementally updates Gaussian splatting-based 3D representations from\nsparse scene captures. CL-Splats integrates a robust change-detection module\nthat segments updated and static components within the scene, enabling focused,\nlocal optimization that avoids unnecessary re-computation. Moreover, CL-Splats\nsupports storing and recovering previous scene states, facilitating temporal\nsegmentation and new scene-analysis applications. Our extensive experiments\ndemonstrate that CL-Splats achieves efficient updates with improved\nreconstruction quality over the state-of-the-art. This establishes a robust\nfoundation for future real-time adaptation in 3D scene reconstruction tasks.",
    "published": "2025-06-26T09:32:37Z",
    "updated": "2025-10-15T22:29:23Z",
    "link": "http://arxiv.org/pdf/2506.21117v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jan Ackermann",
      "Jonas Kulhanek",
      "Shengqu Cai",
      "Haofei Xu",
      "Marc Pollefeys",
      "Gordon Wetzstein",
      "Leonidas Guibas",
      "Songyou Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14146v1",
    "title": "PoissonNet: A Local-Global Approach for Learning on Surfaces",
    "summary": "Many network architectures exist for learning on meshes, yet their\nconstructions entail delicate trade-offs between difficulty learning\nhigh-frequency features, insufficient receptive field, sensitivity to\ndiscretization, and inefficient computational overhead. Drawing from classic\nlocal-global approaches in mesh processing, we introduce PoissonNet, a novel\nneural architecture that overcomes all of these deficiencies by formulating a\nlocal-global learning scheme, which uses Poisson's equation as the primary\nmechanism for feature propagation. Our core network block is simple; we apply\nlearned local feature transformations in the gradient domain of the mesh, then\nsolve a Poisson system to propagate scalar feature updates across the surface\nglobally. Our local-global learning framework preserves the features's full\nfrequency spectrum and provides a truly global receptive field, while remaining\nagnostic to mesh triangulation. Our construction is efficient, requiring far\nless compute overhead than comparable methods, which enables scalability --\nboth in the size of our datasets, and the size of individual training samples.\nThese qualities are validated on various experiments where, compared to\nprevious intrinsic architectures, we attain state-of-the-art performance on\nsemantic segmentation and parameterizing highly-detailed animated surfaces.\nFinally, as a central application of PoissonNet, we show its ability to learn\ndeformations, significantly outperforming state-of-the-art architectures that\nlearn on surfaces.",
    "published": "2025-10-15T22:25:44Z",
    "updated": "2025-10-15T22:25:44Z",
    "link": "http://arxiv.org/pdf/2510.14146v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Arman Maesumi",
      "Tanish Makadia",
      "Thibault Groueix",
      "Vladimir G. Kim",
      "Daniel Ritchie",
      "Noam Aigerman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.09656v2",
    "title": "KeyVID: Keyframe-Aware Video Diffusion for Audio-Synchronized Visual\n  Animation",
    "summary": "Generating video from various conditions, such as text, image, and audio,\nenables both spatial and temporal control, leading to high-quality generation\nresults. Videos with dramatic motions often require a higher frame rate to\nensure smooth motion. Currently, most audio-to-visual animation models use\nuniformly sampled frames from video clips. However, these uniformly sampled\nframes fail to capture significant key moments in dramatic motions at low frame\nrates and require significantly more memory when increasing the number of\nframes directly. In this paper, we propose KeyVID, a keyframe-aware\naudio-to-visual animation framework that significantly improves the generation\nquality for key moments in audio signals while maintaining computation\nefficiency. Given an image and an audio input, we first localize keyframe time\nsteps from the audio. Then, we use a keyframe generator to generate the\ncorresponding visual keyframes. Finally, we generate all intermediate frames\nusing the motion interpolator. Through extensive experiments, we demonstrate\nthat KeyVID significantly improves audio-video synchronization and video\nquality across multiple datasets, particularly for highly dynamic motions. The\ncode is released in https://github.com/XingruiWang/KeyVID.",
    "published": "2025-04-13T17:06:03Z",
    "updated": "2025-10-15T22:24:44Z",
    "link": "http://arxiv.org/pdf/2504.09656v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xingrui Wang",
      "Jiang Liu",
      "Ze Wang",
      "Xiaodong Yu",
      "Jialian Wu",
      "Ximeng Sun",
      "Yusheng Su",
      "Alan Yuille",
      "Zicheng Liu",
      "Emad Barsoum"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14143v1",
    "title": "cubic: CUDA-accelerated 3D Bioimage Computing",
    "summary": "Quantitative analysis of multidimensional biological images is useful for\nunderstanding complex cellular phenotypes and accelerating advances in\nbiomedical research. As modern microscopy generates ever-larger 2D and 3D\ndatasets, existing computational approaches are increasingly limited by their\nscalability, efficiency, and integration with modern scientific computing\nworkflows. Existing bioimage analysis tools often lack application programmable\ninterfaces (APIs), do not support graphics processing unit (GPU) acceleration,\nlack broad 3D image processing capabilities, and/or have poor interoperability\nfor compute-heavy workflows. Here, we introduce cubic, an open-source Python\nlibrary that addresses these challenges by augmenting widely used SciPy and\nscikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.\ncubic's API is device-agnostic and dispatches operations to GPU when data\nreside on the device and otherwise executes on CPU, seamlessly accelerating a\nbroad range of image processing routines. This approach enables GPU\nacceleration of existing bioimage analysis workflows, from preprocessing to\nsegmentation and feature extraction for 2D and 3D data. We evaluate cubic both\nby benchmarking individual operations and by reproducing existing deconvolution\nand segmentation pipelines, achieving substantial speedups while maintaining\nalgorithmic fidelity. These advances establish a robust foundation for\nscalable, reproducible bioimage analysis that integrates with the broader\nPython scientific computing ecosystem, including other GPU-accelerated methods,\nenabling both interactive exploration and automated high-throughput analysis\nworkflows. cubic is openly available at\nhttps://github$.$com/alxndrkalinin/cubic",
    "published": "2025-10-15T22:22:06Z",
    "updated": "2025-10-15T22:22:06Z",
    "link": "http://arxiv.org/pdf/2510.14143v1.pdf",
    "category": [
      "cs.CV",
      "q-bio.QM",
      "92C55, 68U10",
      "I.4.0; J.3"
    ],
    "authors": [
      "Alexandr A. Kalinin",
      "Anne E. Carpenter",
      "Shantanu Singh",
      "Matthew J. O'Meara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12141v2",
    "title": "MAPS: Masked Attribution-based Probing of Strategies- A computational\n  framework to align human and model explanations",
    "summary": "Human core object recognition depends on the selective use of visual\ninformation, but the strategies guiding these choices are difficult to measure\ndirectly. We present MAPS (Masked Attribution-based Probing of Strategies), a\nbehaviorally validated computational tool that tests whether explanations\nderived from artificial neural networks (ANNs) can also explain human vision.\nMAPS converts attribution maps into explanation-masked images (EMIs) and\ncompares image-by-image human accuracies on these minimal images with limited\npixel budgets with accuracies on the full stimuli. MAPS provides a principled\nway to evaluate and choose among competing ANN interpretability methods. In\nsilico, EMI-based behavioral similarity between models reliably recovers the\nground-truth similarity computed from their attribution maps, establishing\nwhich explanation methods best capture the model's strategy. When applied to\nhumans and macaques, MAPS identifies ANN-explanation combinations whose\nexplanations align most closely with biological vision, achieving the\nbehavioral validity of Bubble masks while requiring far fewer behavioral\ntrials. Because it needs only access to model attributions and a modest set of\nbehavioral data on the original images, MAPS avoids exhaustive psychophysics\nwhile offering a scalable tool for adjudicating explanations and linking human\nbehavior, neural activity, and model decisions under a common standard.",
    "published": "2025-10-14T04:40:23Z",
    "updated": "2025-10-15T22:20:54Z",
    "link": "http://arxiv.org/pdf/2510.12141v2.pdf",
    "category": [
      "q-bio.NC",
      "cs.CV"
    ],
    "authors": [
      "Sabine Muzellec",
      "Yousif Kashef Alghetaa",
      "Simon Kornblith",
      "Kohitij Kar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.15356v3",
    "title": "Neural Plasticity-Inspired Multimodal Foundation Model for Earth\n  Observation",
    "summary": "Earth observation (EO) in open-world settings presents a unique challenge:\ndifferent applications rely on diverse sensor modalities, each with varying\nground sampling distances, spectral ranges, and numbers of spectral bands.\nHowever, existing EO foundation models are typically tailored to specific\nsensor types, making them inflexible when generalizing across the heterogeneous\nlandscape of EO data. To address this, we propose the Dynamic One-For-All\n(DOFA) model, a unified, multimodal foundation framework designed for diverse\nvision tasks in EO. Inspired by neural plasticity, DOFA utilizes a\nwavelength-conditioned dynamic hypernetwork to process inputs from five\ndistinct satellite sensors flexibly. By continually pretraining on five EO\nmodalities, DOFA achieves state-of-the-art performance across multiple\ndownstream tasks and generalizes well to unseen modalities. Enhanced with\nhybrid continual pretraining, DOFA+ requires significantly fewer computational\nresources while outperforming counterparts trained with extensive GPU budgets.\nExperiments on diverse datasets highlight DOFA's potential as a foundation for\ngeneral-purpose vision models in the sensor-diverse EO domain. The code and\npre-trained weights are publicly available at https://github.com/zhu-xlab/DOFA.",
    "published": "2024-03-22T17:11:47Z",
    "updated": "2025-10-15T22:11:23Z",
    "link": "http://arxiv.org/pdf/2403.15356v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhitong Xiong",
      "Yi Wang",
      "Fahong Zhang",
      "Adam J. Stewart",
      "Jolle Hanna",
      "Damian Borth",
      "Ioannis Papoutsis",
      "Bertrand Le Saux",
      "Gustau Camps-Valls",
      "Xiao Xiang Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.16411v2",
    "title": "Mitigating Hallucinations in Vision-Language Models through Image-Guided\n  Head Suppression",
    "summary": "Despite their remarkable progress in multimodal understanding tasks, large\nvision language models (LVLMs) often suffer from \"hallucinations\", generating\ntexts misaligned with the visual context. Existing methods aimed at reducing\nhallucinations through inference time intervention incur a significant increase\nin latency. To mitigate this, we present SPIN, a task-agnostic attention-guided\nhead suppression strategy that can be seamlessly integrated during inference,\nwithout incurring any significant compute or latency overhead. We investigate\nwhether hallucination in LVLMs can be linked to specific model components. Our\nanalysis suggests that hallucinations can be attributed to a dynamic subset of\nattention heads in each layer. Leveraging this insight, for each text query\ntoken, we selectively suppress attention heads that exhibit low attention to\nimage tokens, keeping the top-K attention heads intact. Extensive evaluations\non visual question answering and image description tasks demonstrate the\nefficacy of SPIN in reducing hallucination scores up to 2.7x while maintaining\nF1, and improving throughput by 1.8x compared to existing alternatives. Code is\navailable at https://github.com/YUECHE77/SPIN.",
    "published": "2025-05-22T09:00:57Z",
    "updated": "2025-10-15T21:53:06Z",
    "link": "http://arxiv.org/pdf/2505.16411v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Sreetama Sarkar",
      "Yue Che",
      "Alex Gavin",
      "Peter A. Beerel",
      "Souvik Kundu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08131v2",
    "title": "Real-Time Motion-Controllable Autoregressive Video Diffusion",
    "summary": "Real-time motion-controllable video generation remains challenging due to the\ninherent latency of bidirectional diffusion models and the lack of effective\nautoregressive (AR) approaches. Existing AR video diffusion models are limited\nto simple control signals or text-to-video generation, and often suffer from\nquality degradation and motion artifacts in few-step generation. To address\nthese challenges, we propose AR-Drag, the first RL-enhanced few-step AR video\ndiffusion model for real-time image-to-video generation with diverse motion\ncontrol. We first fine-tune a base I2V model to support basic motion control,\nthen further improve it via reinforcement learning with a trajectory-based\nreward model. Our design preserves the Markov property through a Self-Rollout\nmechanism and accelerates training by selectively introducing stochasticity in\ndenoising steps. Extensive experiments demonstrate that AR-Drag achieves high\nvisual fidelity and precise motion alignment, significantly reducing latency\ncompared with state-of-the-art motion-controllable VDMs, while using only 1.3B\nparameters. Additional visualizations can be found on our project page:\nhttps://kesenzhao.github.io/AR-Drag.github.io/.",
    "published": "2025-10-09T12:17:11Z",
    "updated": "2025-10-15T21:37:23Z",
    "link": "http://arxiv.org/pdf/2510.08131v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kesen Zhao",
      "Jiaxin Shi",
      "Beier Zhu",
      "Junbao Zhou",
      "Xiaolong Shen",
      "Yuan Zhou",
      "Qianru Sun",
      "Hanwang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.16601v2",
    "title": "MetaQAP - A Meta-Learning Approach for Quality-Aware Pretraining in\n  Image Quality Assessment",
    "summary": "Image Quality Assessment (IQA) is a critical task in a wide range of\napplications but remains challenging due to the subjective nature of human\nperception and the complexity of real-world image distortions. This study\nproposes MetaQAP, a novel no-reference IQA model designed to address these\nchallenges by leveraging quality-aware pre-training and meta-learning. The\nmodel performs three key contributions: pre-training Convolutional Neural\nNetworks (CNNs) on a quality-aware dataset, implementing a quality-aware loss\nfunction to optimize predictions, and integrating a meta-learner to form an\nensemble model that effectively combines predictions from multiple base models.\nExperimental evaluations were conducted on three benchmark datasets: LiveCD,\nKonIQ-10K, and BIQ2021. The proposed MetaQAP model achieved exceptional\nperformance with Pearson Linear Correlation Coefficient (PLCC) and Spearman\nRank Order Correlation Coefficient (SROCC) scores of 0.9885/0.9812 on LiveCD,\n0.9702/0.9658 on KonIQ-10K, and 0.884/0.8765 on BIQ2021, outperforming existing\nIQA methods. Cross-dataset evaluations further demonstrated the\ngeneralizability of the model, with PLCC and SROCC scores ranging from 0.6721\nto 0.8023 and 0.6515 to 0.7805, respectively, across diverse datasets. The\nablation study confirmed the significance of each model component, revealing\nsubstantial performance degradation when critical elements such as the\nmeta-learner or quality-aware loss function were omitted. MetaQAP not only\naddresses the complexities of authentic distortions but also establishes a\nrobust and generalizable framework for practical IQA applications. By advancing\nthe state-of-the-art in no-reference IQA, this research provides valuable\ninsights and methodologies for future improvements and extensions in the field.",
    "published": "2025-06-19T21:03:47Z",
    "updated": "2025-10-15T20:41:34Z",
    "link": "http://arxiv.org/pdf/2506.16601v2.pdf",
    "category": [
      "cs.CV",
      "eess.IV"
    ],
    "authors": [
      "Nisar Ahmed",
      "Gulshan Saleem",
      "Nazik Alturki",
      "Nada Alasbali"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14081v1",
    "title": "Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from\n  Unstructured Phone Images",
    "summary": "We present a novel, zero-shot pipeline for creating hyperrealistic,\nidentity-preserving 3D avatars from a few unstructured phone images. Existing\nmethods face several challenges: single-view approaches suffer from geometric\ninconsistencies and hallucinations, degrading identity preservation, while\nmodels trained on synthetic data fail to capture high-frequency details like\nskin wrinkles and fine hair, limiting realism. Our method introduces two key\ncontributions: (1) a generative canonicalization module that processes multiple\nunstructured views into a standardized, consistent representation, and (2) a\ntransformer-based model trained on a new, large-scale dataset of high-fidelity\nGaussian splatting avatars derived from dome captures of real people. This\n\"Capture, Canonicalize, Splat\" pipeline produces static quarter-body avatars\nwith compelling realism and robust identity preservation from unstructured\nphotos.",
    "published": "2025-10-15T20:36:28Z",
    "updated": "2025-10-15T20:36:28Z",
    "link": "http://arxiv.org/pdf/2510.14081v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Emanuel Garbin",
      "Guy Adam",
      "Oded Krams",
      "Zohar Barzelay",
      "Eran Guendelman",
      "Michael Schwarz",
      "Moran Vatelmacher",
      "Yigal Shenkman",
      "Eli Peker",
      "Itai Druker",
      "Uri Patish",
      "Yoav Blum",
      "Max Bluvstein",
      "Junxuan Li",
      "Rawal Khirodkar",
      "Shunsuke Saito"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14051v1",
    "title": "Synchronization of Multiple Videos",
    "summary": "Synchronizing videos captured simultaneously from multiple cameras in the\nsame scene is often easy and typically requires only simple time shifts.\nHowever, synchronizing videos from different scenes or, more recently,\ngenerative AI videos, poses a far more complex challenge due to diverse\nsubjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal\nPrototype Learning (TPL), a prototype-based framework that constructs a shared,\ncompact 1D representation from high-dimensional embeddings extracted by any of\nvarious pretrained models. TPL robustly aligns videos by learning a unified\nprototype sequence that anchors key action phases, thereby avoiding exhaustive\npairwise matching. Our experiments show that TPL improves synchronization\naccuracy, efficiency, and robustness across diverse datasets, including\nfine-grained frame retrieval and phase classification tasks. Importantly, TPL\nis the first approach to mitigate synchronization issues in multiple generative\nAI videos depicting the same action. Our code and a new multiple video\nsynchronization dataset are available at https://bgu-cs-vil.github.io/TPL/",
    "published": "2025-10-15T19:43:57Z",
    "updated": "2025-10-15T19:43:57Z",
    "link": "http://arxiv.org/pdf/2510.14051v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Avihai Naaman",
      "Ron Shapira Weber",
      "Oren Freifeld"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14032v1",
    "title": "Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long\n  Video Understanding",
    "summary": "Understanding and reasoning over long videos pose significant challenges for\nlarge video language models (LVLMs) due to the difficulty in processing\nintensive video tokens beyond context window and retaining long-term sequential\ninformation. Retrieval-Augmented Generation (RAG) has demonstrated\neffectiveness in processing long context for Large Language Models (LLMs);\nhowever, applying RAG to long video faces challenges such as disrupted temporal\ndependencies and inclusion of irrelevant information that can hinder accurate\nreasoning. To address these limitations, we propose Vgent, a novel graph-based\nretrieval-reasoning-augmented generation framework to enhance LVLMs for long\nvideo understanding. Our approach introduces two key innovations: (i) It\nrepresents videos by structured graphs with semantic relationships across video\nclips preserved to improve retrieval effectiveness. (ii) It introduces an\nintermediate reasoning step to mitigate the reasoning limitation of LVLMs,\nwhich leverages structured verification to reduce retrieval noise and\nfacilitate the explicit aggregation of relevant information across clips,\nresulting in more accurate and context-aware responses. We comprehensively\nevaluate our framework with various open-source LVLMs on three long-video\nunderstanding benchmarks. Our approach yielded an overall performance\nimprovement of $3.0\\%\\sim 5.4\\%$ over base models on MLVU, and outperformed\nstate-of-the-art video RAG methods by $8.6\\%$. Our code is publicly available\nat https://xiaoqian-shen.github.io/Vgent.",
    "published": "2025-10-15T19:14:58Z",
    "updated": "2025-10-15T19:14:58Z",
    "link": "http://arxiv.org/pdf/2510.14032v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiaoqian Shen",
      "Wenxuan Zhang",
      "Jun Chen",
      "Mohamed Elhoseiny"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14025v1",
    "title": "NAPPure: Adversarial Purification for Robust Image Classification under\n  Non-Additive Perturbations",
    "summary": "Adversarial purification has achieved great success in combating adversarial\nimage perturbations, which are usually assumed to be additive. However,\nnon-additive adversarial perturbations such as blur, occlusion, and distortion\nare also common in the real world. Under such perturbations, existing\nadversarial purification methods are much less effective since they are\ndesigned to fit the additive nature. In this paper, we propose an extended\nadversarial purification framework named NAPPure, which can further handle\nnon-additive perturbations. Specifically, we first establish the generation\nprocess of an adversarial image, and then disentangle the underlying clean\nimage and perturbation parameters through likelihood maximization. Experiments\non GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the\nrobustness of image classification models against non-additive perturbations.",
    "published": "2025-10-15T19:05:59Z",
    "updated": "2025-10-15T19:05:59Z",
    "link": "http://arxiv.org/pdf/2510.14025v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junjie Nan",
      "Jianing Li",
      "Wei Chen",
      "Mingkun Zhang",
      "Xueqi Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13972v1",
    "title": "Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse\n  Problems",
    "summary": "Recovering true signals from noisy measurements is a central challenge in\ninverse problems spanning medical imaging, geophysics, and signal processing.\nCurrent solutions balance prior assumptions regarding the true signal\n(regularization) with agreement to noisy measured data (data-fidelity).\nConventional data-fidelity loss functions, such as mean-squared error (MSE) or\nnegative log-likelihood, seek pointwise agreement with noisy measurements,\noften leading to overfitting to noise. In this work, we instead evaluate\ndata-fidelity collectively by testing whether the observed measurements are\nstatistically consistent with the noise distributions implied by the current\nestimate. We adopt this aggregated perspective and introduce distributional\nconsistency (DC) loss, a data-fidelity objective that replaces pointwise\nmatching with distribution-level calibration using model-based probability\nscores for each measurement. DC loss acts as a direct and practical plug-in\nreplacement for standard data consistency terms: i) it is compatible with\nmodern regularizers, ii) it is optimized in the same way as traditional losses,\nand iii) it avoids overfitting to measurement noise even without the use of\npriors. Its scope naturally fits many practical inverse problems where the\nmeasurement-noise distribution is known and where the measured dataset consists\nof many independent noisy values. We demonstrate efficacy in two key example\napplication areas: i) in image denoising with deep image prior, using DC\ninstead of MSE loss removes the need for early stopping and achieves higher\nPSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss\nreduces artifacts in highly-iterated reconstructions and enhances the efficacy\nof hand-crafted regularization. These results position DC loss as a\nstatistically grounded, performance-enhancing alternative to conventional\nfidelity losses for inverse problems.",
    "published": "2025-10-15T18:01:23Z",
    "updated": "2025-10-15T18:01:23Z",
    "link": "http://arxiv.org/pdf/2510.13972v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV",
      "physics.med-ph"
    ],
    "authors": [
      "George Webber",
      "Andrew J. Reader"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14970v1",
    "title": "Biology-informed neural networks learn nonlinear representations from\n  omics data to improve genomic prediction and interpretability",
    "summary": "We extend biologically-informed neural networks (BINNs) for genomic\nprediction (GP) and selection (GS) in crops by integrating thousands of\nsingle-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior\nbiological knowledge. Traditional genotype-to-phenotype (G2P) models depend\nheavily on direct mappings that achieve only modest accuracy, forcing breeders\nto conduct large, costly field trials to maintain or marginally improve genetic\ngain. Models that incorporate intermediate molecular phenotypes such as gene\nexpression can achieve higher predictive fit, but they remain impractical for\nGS since such data are unavailable at deployment or design time. BINNs overcome\nthis limitation by encoding pathway-level inductive biases and leveraging\nmulti-omics data only during training, while using genotype data alone during\ninference. Applied to maize gene-expression and multi-environment field-trial\ndata, BINN improves rank-correlation accuracy by up to 56% within and across\nsubpopulations under sparse-data conditions and nonlinearly identifies genes\nthat GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic\nmetabolomics benchmark, BINN reduces prediction error by 75% relative to\nconventional neural nets and correctly identifies the most important nonlinear\npathway. Importantly, both cases show highly sensitive BINN latent variables\ncorrelate with the experimental quantities they represent, despite not being\ntrained on them. This suggests BINNs learn biologically-relevant\nrepresentations, nonlinear or linear, from genotype to phenotype. Together,\nBINNs establish a framework that leverages intermediate domain information to\nimprove genomic prediction accuracy and reveal nonlinear biological\nrelationships that can guide genomic selection, candidate gene selection,\npathway enrichment, and gene-editing prioritization.",
    "published": "2025-10-16T17:59:38Z",
    "updated": "2025-10-16T17:59:38Z",
    "link": "http://arxiv.org/pdf/2510.14970v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Katiana Kontolati",
      "Rini Jasmine Gladstone",
      "Ian Davis",
      "Ethan Pickering"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14966v1",
    "title": "Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity\n  in TVD-MI Scores",
    "summary": "Pairwise comparisons of large language models using total variation distance\nmutual information (TVD-MI) produce binary critic decisions per pair. We show\nthat averaging TVD-MI's binary trials yields centered-probability scores with\nadditive structure suitable for item-response theory (IRT) without nonlinear\nlink functions. Maximum-likelihood approaches to IRT use logistic links, but we\nfind empirically that these transformations introduce curvature that breaks\nadditivity: across three domains, the identity link yields median curl on raw\ndata of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce\nsubstantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We\nderive this clipped-linear model from Gini entropy maximization, yielding a\nbox-constrained least-squares formulation that handles boundary saturation. At\n33% coverage, we achieve holdout RMSE $0.117 \\pm 0.008$ while preserving agent\nrankings (Spearman $\\rho = 0.972 \\pm 0.015$), three times fewer evaluations\nthan full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows\nstrong agreement in agent rankings ($\\rho = 0.872$) and consistent\nidentity-link advantage. TVD-MI's geometry is best preserved by identity\nmapping for efficient LLM evaluation, applicable to other bounded-response\ndomains.",
    "published": "2025-10-16T17:59:25Z",
    "updated": "2025-10-16T17:59:25Z",
    "link": "http://arxiv.org/pdf/2510.14966v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Zachary Robertson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14930v1",
    "title": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via\n  Simulation Fine-Tunin",
    "summary": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback\n-- a capability that remains difficult to replicate in robots through\nbehavioral cloning alone, due to the suboptimality and limited diversity of\nhuman demonstrations. In this work, we present VT-Refine, a visuo-tactile\npolicy learning framework that combines real-world demonstrations,\nhigh-fidelity tactile simulation, and reinforcement learning to tackle precise,\ncontact-rich bimanual assembly. We begin by training a diffusion policy on a\nsmall set of demonstrations using synchronized visual and tactile inputs. This\npolicy is then transferred to a simulated digital twin equipped with simulated\ntactile sensors and further refined via large-scale reinforcement learning to\nenhance robustness and generalization. To enable accurate sim-to-real transfer,\nwe leverage high-resolution piezoresistive tactile sensors that provide normal\nforce signals and can be realistically modeled in parallel using\nGPU-accelerated simulation. Experimental results show that VT-Refine improves\nassembly performance in both simulation and the real world by increasing data\ndiversity and enabling more effective policy fine-tuning. Our project page is\navailable at https://binghao-huang.github.io/vt_refine/.",
    "published": "2025-10-16T17:41:36Z",
    "updated": "2025-10-16T17:41:36Z",
    "link": "http://arxiv.org/pdf/2510.14930v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Binghao Huang",
      "Jie Xu",
      "Iretiayo Akinola",
      "Wei Yang",
      "Balakumar Sundaralingam",
      "Rowland O'Flaherty",
      "Dieter Fox",
      "Xiaolong Wang",
      "Arsalan Mousavian",
      "Yu-Wei Chao",
      "Yunzhu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14928v1",
    "title": "Instruction Set Migration at Warehouse Scale",
    "summary": "Migrating codebases from one instruction set architecture (ISA) to another is\na major engineering challenge. A recent example is the adoption of Arm (in\naddition to x86) across the major Cloud hyperscalers. Yet, this problem has\nseen limited attention by the academic community. Most work has focused on\nstatic and dynamic binary translation, and the traditional conventional wisdom\nhas been that this is the primary challenge.\n  In this paper, we show that this is no longer the case. Modern ISA migrations\ncan often build on a robust open-source ecosystem, making it possible to\nrecompile all relevant software from scratch. This introduces a new and\nmultifaceted set of challenges, which are different from binary translation.\n  By analyzing a large-scale migration from x86 to Arm at Google, spanning\nalmost 40,000 code commits, we derive a taxonomy of tasks involved in ISA\nmigration. We show how Google automated many of the steps involved, and\ndemonstrate how AI can play a major role in automatically addressing these\ntasks. We identify tasks that remain challenging and highlight research\nchallenges that warrant further attention.",
    "published": "2025-10-16T17:41:01Z",
    "updated": "2025-10-16T17:41:01Z",
    "link": "http://arxiv.org/pdf/2510.14928v1.pdf",
    "category": [
      "cs.SE",
      "cs.LG"
    ],
    "authors": [
      "Eric Christopher",
      "Kevin Crossan",
      "Wolff Dobson",
      "Chris Kennelly",
      "Drew Lewis",
      "Kun Lin",
      "Martin Maas",
      "Parthasarathy Ranganathan",
      "Emma Rapati",
      "Brian Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.12706v2",
    "title": "REX: Causal discovery based on machine learning and explainability\n  techniques",
    "summary": "Explainable Artificial Intelligence (XAI) techniques hold significant\npotential for enhancing the causal discovery process, which is crucial for\nunderstanding complex systems in areas like healthcare, economics, and\nartificial intelligence. However, no causal discovery methods currently\nincorporate explainability into their models to derive the causal graphs. Thus,\nin this paper we explore this innovative approach, as it offers substantial\npotential and represents a promising new direction worth investigating.\nSpecifically, we introduce ReX, a causal discovery method that leverages\nmachine learning (ML) models coupled with explainability techniques,\nspecifically Shapley values, to identify and interpret significant causal\nrelationships among variables. Comparative evaluations on synthetic datasets\ncomprising continuous tabular data reveal that ReX outperforms state-of-the-art\ncausal discovery methods across diverse data generation processes, including\nnon-linear and additive noise models. Moreover, ReX was tested on the Sachs\nsingle-cell protein-signaling dataset, achieving a precision of 0.952 and\nrecovering key causal relationships with no incorrect edges. Taking together,\nthese results showcase ReX's effectiveness in accurately recovering true causal\nstructures while minimizing false positive predictions, its robustness across\ndiverse datasets, and its applicability to real-world problems. By combining ML\nand explainability techniques with causal discovery, ReX bridges the gap\nbetween predictive modeling and causal inference, offering an effective tool\nfor understanding complex causal structures.",
    "published": "2025-01-22T08:23:10Z",
    "updated": "2025-10-16T17:38:53Z",
    "link": "http://arxiv.org/pdf/2501.12706v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jesus Renero",
      "Idoia Ochoa",
      "Roberto Maestre"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14907v1",
    "title": "Learnable Mixed Nash Equilibria are Collectively Rational",
    "summary": "We extend the study of learning in games to dynamics that exhibit\nnon-asymptotic stability. We do so through the notion of uniform stability,\nwhich is concerned with equilibria of individually utility-seeking dynamics.\nPerhaps surprisingly, it turns out to be closely connected to economic\nproperties of collective rationality. Under mild non-degeneracy conditions and\nup to strategic equivalence, if a mixed equilibrium is not uniformly stable,\nthen it is not weakly Pareto optimal: there is a way for all players to improve\nby jointly deviating from the equilibrium. On the other hand, if it is locally\nuniformly stable, then the equilibrium must be weakly Pareto optimal. Moreover,\nwe show that uniform stability determines the last-iterate convergence behavior\nfor the family of incremental smoothed best-response dynamics, used to model\nindividual and corporate behaviors in the markets. Unlike dynamics around\nstrict equilibria, which can stabilize to socially-inefficient solutions,\nindividually utility-seeking behaviors near mixed Nash equilibria lead to\ncollective rationality.",
    "published": "2025-10-16T17:25:32Z",
    "updated": "2025-10-16T17:25:32Z",
    "link": "http://arxiv.org/pdf/2510.14907v1.pdf",
    "category": [
      "cs.GT",
      "cs.LG"
    ],
    "authors": [
      "Geelon So",
      "Yi-An Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14894v1",
    "title": "Secure Sparse Matrix Multiplications and their Applications to\n  Privacy-Preserving Machine Learning",
    "summary": "To preserve privacy, multi-party computation (MPC) enables executing Machine\nLearning (ML) algorithms on secret-shared or encrypted data. However, existing\nMPC frameworks are not optimized for sparse data. This makes them unsuitable\nfor ML applications involving sparse data, e.g., recommender systems or\ngenomics. Even in plaintext, such applications involve high-dimensional sparse\ndata, that cannot be processed without sparsity-related optimizations due to\nprohibitively large memory requirements.\n  Since matrix multiplication is central in ML algorithms, we propose MPC\nalgorithms to multiply secret sparse matrices. On the one hand, our algorithms\navoid the memory issues of the \"dense\" data representation of classic secure\nmatrix multiplication algorithms. On the other hand, our algorithms can\nsignificantly reduce communication costs (some experiments show a factor 1000)\nfor realistic problem sizes. We validate our algorithms in two ML applications\nin which existing protocols are impractical.\n  An important question when developing MPC algorithms is what assumptions can\nbe made. In our case, if the number of non-zeros in a row is a sensitive piece\nof information then a short runtime may reveal that the number of non-zeros is\nsmall. Existing approaches make relatively simple assumptions, e.g., that there\nis a universal upper bound to the number of non-zeros in a row. This often\ndoesn't align with statistical reality, in a lot of sparse datasets the amount\nof data per instance satisfies a power law. We propose an approach which allows\nadopting a safe upper bound on the distribution of non-zeros in rows/columns of\nsparse matrices.",
    "published": "2025-10-16T17:12:18Z",
    "updated": "2025-10-16T17:12:18Z",
    "link": "http://arxiv.org/pdf/2510.14894v1.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Marc Damie",
      "Florian Hahn",
      "Andreas Peter",
      "Jan Ramon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.09561v2",
    "title": "Strategyproof Reinforcement Learning from Human Feedback",
    "summary": "We study Reinforcement Learning from Human Feedback (RLHF) in settings where\nmultiple labelers may strategically misreport feedback to steer the learned\npolicy toward their own preferences. We show that existing RLHF algorithms,\nincluding recent pluralistic methods, are not strategyproof, and that even a\nsingle strategic labeler can cause arbitrarily large misalignment with social\nwelfare. Moreover, we prove that, in the worst case, any strategyproof RLHF\nalgorithm must perform $k$-times worse than the optimal policy, where $k$ is\nthe number of labelers. This suggests a fundamental trade-off between incentive\nalignment (ensuring labelers report truthfully) and policy alignment\n(maximizing social welfare). To address this, we propose the Pessimistic Median\nof MLEs algorithm, which, under appropriate policy coverage assumptions, is\napproximately strategyproof and converges to the optimal policy as the number\nof labelers and samples increases. Our results apply to both contextual bandits\nand Markov decision processes.",
    "published": "2025-03-12T17:25:52Z",
    "updated": "2025-10-16T17:10:09Z",
    "link": "http://arxiv.org/pdf/2503.09561v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Thomas Kleine Buening",
      "Jiarui Gan",
      "Debmalya Mandal",
      "Marta Kwiatkowska"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14887v1",
    "title": "Prediction-Specific Design of Learning-Augmented Algorithms",
    "summary": "Algorithms with predictions} has emerged as a powerful framework to combine\nthe robustness of traditional online algorithms with the data-driven\nperformance benefits of machine-learned (ML) predictions. However, most\nexisting approaches in this paradigm are overly conservative, {as they do not\nleverage problem structure to optimize performance in a prediction-specific\nmanner}. In this paper, we show that such prediction-specific performance\ncriteria can enable significant performance improvements over the coarser\nnotions of consistency and robustness considered in prior work. Specifically,\nwe propose a notion of \\emph{strongly-optimal} algorithms with predictions,\nwhich obtain Pareto optimality not just in the worst-case tradeoff between\nrobustness and consistency, but also in the prediction-specific tradeoff\nbetween these metrics. We develop a general bi-level optimization framework\nthat enables systematically designing strongly-optimal algorithms in a wide\nvariety of problem settings, and we propose explicit strongly-optimal\nalgorithms for several classic online problems: deterministic and randomized\nski rental, and one-max search. Our analysis reveals new structural insights\ninto how predictions can be optimally integrated into online algorithms by\nleveraging a prediction-specific design. To validate the benefits of our\nproposed framework, we empirically evaluate our algorithms in case studies on\nproblems including dynamic power management and volatility-based index trading.\nOur results demonstrate that prediction-specific, strongly-optimal algorithms\ncan significantly improve performance across a variety of online\ndecision-making settings.",
    "published": "2025-10-16T17:06:53Z",
    "updated": "2025-10-16T17:06:53Z",
    "link": "http://arxiv.org/pdf/2510.14887v1.pdf",
    "category": [
      "cs.DS",
      "cs.LG"
    ],
    "authors": [
      "Sizhe Li",
      "Nicolas Christianson",
      "Tongxin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.10996v3",
    "title": "Approximation Rates for Shallow ReLU$^k$ Neural Networks on Sobolev\n  Spaces via the Radon Transform",
    "summary": "Let $\\Omega\\subset \\mathbb{R}^d$ be a bounded domain. We consider the problem\nof how efficiently shallow neural networks with the ReLU$^k$ activation\nfunction can approximate functions from Sobolev spaces $W^s(L_p(\\Omega))$ with\nerror measured in the $L_q(\\Omega)$-norm. Utilizing the Radon transform and\nrecent results from discrepancy theory, we provide a simple proof of nearly\noptimal approximation rates in a variety of cases, including when $q\\leq p$,\n$p\\geq 2$, and $s \\leq k + (d+1)/2$. The rates we derive are optimal up to\nlogarithmic factors, and significantly generalize existing results. An\ninteresting consequence is that the adaptivity of shallow ReLU$^k$ neural\nnetworks enables them to obtain optimal approximation rates for smoothness up\nto order $s = k + (d+1)/2$, even though they represent piecewise polynomials of\nfixed degree $k$.",
    "published": "2024-08-20T16:43:45Z",
    "updated": "2025-10-16T17:03:54Z",
    "link": "http://arxiv.org/pdf/2408.10996v3.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "cs.NA",
      "math.NA",
      "62M45, 41A25, 41A30"
    ],
    "authors": [
      "Tong Mao",
      "Jonathan W. Siegel",
      "Jinchao Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08808v3",
    "title": "TinyGraphEstimator: Adapting Lightweight Language Models for Graph\n  Structure Inference",
    "summary": "Graphs provide a universal framework for representing complex relational\nsystems, and inferring their structural properties is a core challenge in graph\nanalysis and reasoning. While large language models have recently demonstrated\nemerging abilities to perform symbolic and numerical reasoning, the potential\nof smaller, resource-efficient models in this context remains largely\nunexplored. This paper investigates whether compact transformer-based language\nmodels can infer graph-theoretic parameters directly from graph\nrepresentations. To enable systematic evaluation, we introduce the\nTinyGraphEstimator dataset - a balanced collection of connected graphs\ngenerated from multiple random graph models and annotated with detailed\nstructural metadata. We evaluate several small open models on their ability to\npredict key graph parameters such as density, clustering, and chromatic number.\nFurthermore, we apply lightweight fine-tuning using the Low-Rank Adaptation\n(LoRA) technique, achieving consistent improvements across all evaluated\nmetrics. The results demonstrate that small language models possess non-trivial\nreasoning capacity over graph-structured data and can be effectively adapted\nfor structural inference tasks through efficient parameter tuning.",
    "published": "2025-10-09T20:47:07Z",
    "updated": "2025-10-16T16:29:36Z",
    "link": "http://arxiv.org/pdf/2510.08808v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Michal Podstawski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.08590v3",
    "title": "Hopfield-Fenchel-Young Networks: A Unified Framework for Associative\n  Memory Retrieval",
    "summary": "Associative memory models, such as Hopfield networks and their modern\nvariants, have garnered renewed interest due to advancements in memory capacity\nand connections with self-attention in transformers. In this work, we introduce\na unified framework-Hopfield-Fenchel-Young networks-which generalizes these\nmodels to a broader family of energy functions. Our energies are formulated as\nthe difference between two Fenchel-Young losses: one, parameterized by a\ngeneralized entropy, defines the Hopfield scoring mechanism, while the other\napplies a post-transformation to the Hopfield output. By utilizing Tsallis and\nnorm entropies, we derive end-to-end differentiable update rules that enable\nsparse transformations, uncovering new connections between loss margins,\nsparsity, and exact retrieval of single memory patterns. We further extend this\nframework to structured Hopfield networks using the SparseMAP transformation,\nallowing the retrieval of pattern associations rather than a single pattern.\nOur framework unifies and extends traditional and modern Hopfield networks and\nprovides an energy minimization perspective for widely used\npost-transformations like $\\ell_2$-normalization and layer normalization-all\nthrough suitable choices of Fenchel-Young losses and by using convex analysis\nas a building block. Finally, we validate our Hopfield-Fenchel-Young networks\non diverse memory recall tasks, including free and sequential recall.\nExperiments on simulated data, image retrieval, multiple instance learning, and\ntext rationalization demonstrate the effectiveness of our approach.",
    "published": "2024-11-13T13:13:07Z",
    "updated": "2025-10-16T16:28:05Z",
    "link": "http://arxiv.org/pdf/2411.08590v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Saul Santos",
      "Vlad Niculae",
      "Daniel McNamee",
      "Andr F. T. Martins"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14848v1",
    "title": "A Geometric Approach to Optimal Experimental Design",
    "summary": "We introduce a novel geometric framework for optimal experimental design\n(OED). Traditional OED approaches, such as those based on mutual information,\nrely explicitly on probability densities, leading to restrictive invariance\nproperties. To address these limitations, we propose the mutual transport\ndependence (MTD), a measure of statistical dependence grounded in optimal\ntransport theory which provides a geometric objective for optimizing designs.\nUnlike conventional approaches, the MTD can be tailored to specific downstream\nestimation problems by choosing appropriate geometries on the underlying\nspaces. We demonstrate that our framework produces high-quality designs while\noffering a flexible alternative to standard information-theoretic techniques.",
    "published": "2025-10-16T16:20:14Z",
    "updated": "2025-10-16T16:20:14Z",
    "link": "http://arxiv.org/pdf/2510.14848v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Gavin Kerrigan",
      "Christian A. Naesseth",
      "Tom Rainforth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14844v1",
    "title": "Provable Unlearning with Gradient Ascent on Two-Layer ReLU Neural\n  Networks",
    "summary": "Machine Unlearning aims to remove specific data from trained models,\naddressing growing privacy and ethical concerns. We provide a theoretical\nanalysis of a simple and widely used method - gradient ascent - used to reverse\nthe influence of a specific data point without retraining from scratch.\nLeveraging the implicit bias of gradient descent towards solutions that satisfy\nthe Karush-Kuhn-Tucker (KKT) conditions of a margin maximization problem, we\nquantify the quality of the unlearned model by evaluating how well it satisfies\nthese conditions w.r.t. the retained data. To formalize this idea, we propose a\nnew success criterion, termed \\textbf{$(\\epsilon, \\delta, \\tau)$-successful}\nunlearning, and show that, for both linear models and two-layer neural networks\nwith high dimensional data, a properly scaled gradient-ascent step satisfies\nthis criterion and yields a model that closely approximates the retrained\nsolution on the retained data. We also show that gradient ascent performs\nsuccessful unlearning while still preserving generalization in a synthetic\nGaussian-mixture setting.",
    "published": "2025-10-16T16:16:36Z",
    "updated": "2025-10-16T16:16:36Z",
    "link": "http://arxiv.org/pdf/2510.14844v1.pdf",
    "category": [
      "cs.LG",
      "cs.CR",
      "cs.NE",
      "stat.ML"
    ],
    "authors": [
      "Odelia Melamed",
      "Gilad Yehudai",
      "Gal Vardi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14837v1",
    "title": "Reinforcement Learning with Stochastic Reward Machines",
    "summary": "Reward machines are an established tool for dealing with reinforcement\nlearning problems in which rewards are sparse and depend on complex sequences\nof actions. However, existing algorithms for learning reward machines assume an\noverly idealized setting where rewards have to be free of noise. To overcome\nthis practical limitation, we introduce a novel type of reward machines, called\nstochastic reward machines, and an algorithm for learning them. Our algorithm,\nbased on constraint solving, learns minimal stochastic reward machines from the\nexplorations of a reinforcement learning agent. This algorithm can easily be\npaired with existing reinforcement learning algorithms for reward machines and\nguarantees to converge to an optimal policy in the limit. We demonstrate the\neffectiveness of our algorithm in two case studies and show that it outperforms\nboth existing methods and a naive approach for handling noisy reward functions.",
    "published": "2025-10-16T16:12:04Z",
    "updated": "2025-10-16T16:12:04Z",
    "link": "http://arxiv.org/pdf/2510.14837v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jan Corazza",
      "Ivan Gavran",
      "Daniel Neider"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14832v1",
    "title": "Intelligent Dynamic Handover via AI-assisted Signal Quality Prediction\n  in 6G Multi-RAT Networks",
    "summary": "The emerging paradigm of 6G multiple Radio Access Technology (multi-RAT)\nnetworks, where cellular and Wireless Fidelity (WiFi) transmitters coexist,\nrequires mobility decisions that remain reliable under fast channel dynamics,\ninterference, and heterogeneous coverage. Handover in multi-RAT deployments is\nstill highly reactive and event-triggered, relying on instantaneous\nmeasurements and threshold events. This work proposes a Machine Learning\n(ML)-assisted Predictive Conditional Handover (P-CHO) framework based on a\nmodel-driven and short-horizon signal quality forecasts. We present a\ngeneralized P-CHO sequence workflow orchestrated by a RAT Steering Controller,\nwhich standardizes data collection, parallel per-RAT predictions, decision\nlogic with hysteresis-based conditions, and CHO execution. Considering a\nrealistic multi-RAT environment, we train RAT-aware Long Short Term Memory\n(LSTM) networks to forecast the signal quality indicators of mobile users along\nrandomized trajectories. The proposed P-CHO models are trained and evaluated\nunder different channel models for cellular and IEEE 802.11 WiFi integrated\ncoverage. We study the impact of hyperparameter tuning of LSTM models under\ndifferent system settings, and compare direct multi-step versus recursive P-CHO\nvariants. Comparisons against baseline predictors are also carried out.\nFinally, the proposed P-CHO is tested under soft and hard handover settings,\nshowing that hysteresis-enabled P-CHO scheme is able to reduce handover\nfailures and ping-pong events. Overall, the proposed P-CHO framework can enable\naccurate, low-latency, and proactive handovers suitable for ML-assisted\nhandover steering in 6G multi-RAT deployments.",
    "published": "2025-10-16T16:08:14Z",
    "updated": "2025-10-16T16:08:14Z",
    "link": "http://arxiv.org/pdf/2510.14832v1.pdf",
    "category": [
      "cs.LG",
      "cs.NI"
    ],
    "authors": [
      "Maria Lamprini A. Bartsioka",
      "Anastasios Giannopoulos",
      "Sotirios Spantideas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.18020v2",
    "title": "Byzantine Failures Harm the Generalization of Robust Distributed\n  Learning Algorithms More Than Data Poisoning",
    "summary": "Robust distributed learning algorithms aim to maintain reliable performance\ndespite the presence of misbehaving workers. Such misbehaviors are commonly\nmodeled as $\\textit{Byzantine failures}$, allowing arbitrarily corrupted\ncommunication, or as $\\textit{data poisoning}$, a weaker form of corruption\nrestricted to local training data. While prior work shows similar optimization\nguarantees for both models, an important question remains: $\\textit{How do\nthese threat models impact generalization?}$ Empirical evidence suggests a gap,\nyet it remains unclear whether it is unavoidable or merely an artifact of\nsuboptimal attacks. We show, for the first time, a fundamental gap in\ngeneralization guarantees between the two threat models: Byzantine failures\nyield strictly worse rates than those achievable under data poisoning. Our\nfindings leverage a tight algorithmic stability analysis of robust distributed\nlearning. Specifically, we prove that: $\\textit{(i)}$ under data poisoning, the\nuniform algorithmic stability of an algorithm with optimal optimization\nguarantees degrades by an additive factor of $\\varTheta ( \\frac{f}{n-f} )$,\nwith $f$ out of $n$ workers misbehaving; whereas $\\textit{(ii)}$ under\nByzantine failures, the degradation is in $\\Omega \\big( \\sqrt{ \\frac{f}{n-2f}}\n\\big)$.",
    "published": "2025-06-22T12:59:15Z",
    "updated": "2025-10-16T16:05:21Z",
    "link": "http://arxiv.org/pdf/2506.18020v2.pdf",
    "category": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "authors": [
      "Thomas Boudou",
      "Batiste Le Bars",
      "Nirupam Gupta",
      "Aurlien Bellet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14826v1",
    "title": "To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State\n  Space Models",
    "summary": "State Space Models (SSMs) have become the leading alternative to Transformers\nfor sequence modeling. Their primary advantage is efficiency in long-context\nand long-form generation, enabled by fixed-size memory and linear scaling of\ncomputational complexity. We begin this work by showing a simple theoretical\nresult stating that SSMs cannot accurately solve any ``truly long-form''\ngeneration problem (in a sense we formally define), undermining their main\ncompetitive advantage. However, we show that this limitation can be mitigated\nby allowing SSMs interactive access to external tools. In fact, we show that\ngiven the right choice of tool access and problem-dependent training data, SSMs\ncan learn to solve any tractable problem and generalize to arbitrary problem\nlength/complexity (i.e., achieve length generalization). Following our\ntheoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable\nlength generalization on a variety of arithmetic, reasoning, and coding tasks.\nThese findings highlight SSMs as a potential efficient alternative to\nTransformers in interactive tool-based and agentic settings.",
    "published": "2025-10-16T16:02:45Z",
    "updated": "2025-10-16T16:02:45Z",
    "link": "http://arxiv.org/pdf/2510.14826v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Eran Malach",
      "Omid Saremi",
      "Sinead Williamson",
      "Arwen Bradley",
      "Aryo Lotfi",
      "Emmanuel Abbe",
      "Josh Susskind",
      "Etai Littwin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14825v1",
    "title": "Programmatic Representation Learning with Language Models",
    "summary": "Classical models for supervised machine learning, such as decision trees, are\nefficient and interpretable predictors, but their quality is highly dependent\non the particular choice of input features. Although neural networks can learn\nuseful representations directly from raw data (e.g., images or text), this\ncomes at the expense of interpretability and the need for specialized hardware\nto run them efficiently. In this paper, we explore a hypothesis class we call\nLearned Programmatic Representations (LeaPR) models, which stack arbitrary\nfeatures represented as code (functions from data points to scalars) and\ndecision tree predictors. We synthesize feature functions using Large Language\nModels (LLMs), which have rich prior knowledge in a wide range of domains and a\nremarkable ability to write code using existing domain-specific libraries. We\npropose two algorithms to learn LeaPR models from supervised data. First, we\ndesign an adaptation of FunSearch to learn features rather than directly\ngenerate predictors. Then, we develop a novel variant of the classical ID3\nalgorithm for decision tree learning, where new features are generated on\ndemand when splitting leaf nodes. In experiments from chess position evaluation\nto image and text classification, our methods learn high-quality, neural\nnetwork-free predictors often competitive with neural networks. Our work\nsuggests a flexible paradigm for learning interpretable representations\nend-to-end where features and predictions can be readily inspected and\nunderstood.",
    "published": "2025-10-16T16:02:42Z",
    "updated": "2025-10-16T16:02:42Z",
    "link": "http://arxiv.org/pdf/2510.14825v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Gabriel Poesia",
      "Georgia Gabriela Sampaio"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12636v2",
    "title": "Adapting Noise to Data: Generative Flows from 1D Processes",
    "summary": "We introduce a general framework for constructing generative models using\none-dimensional noising processes. Beyond diffusion processes, we outline\nexamples that demonstrate the flexibility of our approach. Motivated by this,\nwe propose a novel framework in which the 1D processes themselves are\nlearnable, achieved by parameterizing the noise distribution through quantile\nfunctions that adapt to the data. Our construction integrates seamlessly with\nstandard objectives, including Flow Matching and consistency models. Learning\nquantile-based noise naturally captures heavy tails and compact supports when\npresent. Numerical experiments highlight both the flexibility and the\neffectiveness of our method.",
    "published": "2025-10-14T15:30:28Z",
    "updated": "2025-10-16T15:52:49Z",
    "link": "http://arxiv.org/pdf/2510.12636v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.AP"
    ],
    "authors": [
      "Jannis Chemseddine",
      "Gregor Kornhardt",
      "Richard Duong",
      "Gabriele Steidl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14814v1",
    "title": "Tackling Time-Series Forecasting Generalization via Mitigating Concept\n  Drift",
    "summary": "Time-series forecasting finds broad applications in real-world scenarios. Due\nto the dynamic nature of time series data, it is important for time-series\nforecasting models to handle potential distribution shifts over time. In this\npaper, we initially identify two types of distribution shifts in time series:\nconcept drift and temporal shift. We acknowledge that while existing studies\nprimarily focus on addressing temporal shift issues in time series forecasting,\ndesigning proper concept drift methods for time series forecasting has received\ncomparatively less attention.\n  Motivated by the need to address potential concept drift, while conventional\nconcept drift methods via invariant learning face certain challenges in\ntime-series forecasting, we propose a soft attention mechanism that finds\ninvariant patterns from both lookback and horizon time series. Additionally, we\nemphasize the critical importance of mitigating temporal shifts as a\npreliminary to addressing concept drift. In this context, we introduce ShifTS,\na method-agnostic framework designed to tackle temporal shift first and then\nconcept drift within a unified approach. Extensive experiments demonstrate the\nefficacy of ShifTS in consistently enhancing the forecasting accuracy of\nagnostic models across multiple datasets, and outperforming existing concept\ndrift, temporal shift, and combined baselines.",
    "published": "2025-10-16T15:48:52Z",
    "updated": "2025-10-16T15:48:52Z",
    "link": "http://arxiv.org/pdf/2510.14814v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zhiyuan Zhao",
      "Haoxin Liu",
      "B. Aditya Prakash"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14812v1",
    "title": "Efficient Dynamic Structured Sparse Training with Learned Shuffles",
    "summary": "Structured sparsity accelerates training and inference on modern GPUs, yet it\nstill trails unstructured dynamic sparse training (DST) in accuracy. The\nshortfall stems from a loss of expressivity: whereas a dense layer can realize\nevery possible mask obtained by choosing any $w$ active weights out of $n$, a\nfixed block or N:M layout explores only a subset of those possibilities. We\npropose to close this gap by learning, for each layer, a single permutation\nmatrix jointly with the structured weight matrix. Applied to three canonical\nstructures -- block, N:M, and diagonals -- we show that permutation-augmented\nDST (PA-DST) matches unstructured baselines (RigL, SET) at 90--95\\% sparsity on\nImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains up to $1.21\\times$\nand infers up to $2.9\\times$ faster. The results position structure + learned\npermutation as a sweet spot between accuracy and efficiency.",
    "published": "2025-10-16T15:48:17Z",
    "updated": "2025-10-16T15:48:17Z",
    "link": "http://arxiv.org/pdf/2510.14812v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Abhishek Tyagi",
      "Arjun Iyer",
      "Liam Young",
      "William H Renninger",
      "Christopher Kanan",
      "Yuhao Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14810v1",
    "title": "Rethinking Hebbian Principle: Low-Dimensional Structural Projection for\n  Unsupervised Learning",
    "summary": "Hebbian learning is a biological principle that intuitively describes how\nneurons adapt their connections through repeated stimuli. However, when applied\nto machine learning, it suffers serious issues due to the unconstrained updates\nof the connections and the lack of accounting for feedback mediation. Such\nshortcomings limit its effective scaling to complex network architectures and\ntasks. To this end, here we introduce the Structural Projection Hebbian\nRepresentation (SPHeRe), a novel unsupervised learning method that integrates\northogonality and structural information preservation through a local auxiliary\nnonlinear block. The loss for structural information preservation\nbackpropagates to the input through an auxiliary lightweight projection that\nconceptually serves as feedback mediation while the orthogonality constraints\naccount for the boundedness of updating magnitude. Extensive experimental\nresults show that SPHeRe achieves SOTA performance among unsupervised synaptic\nplasticity approaches on standard image classification benchmarks, including\nCIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strong\neffectiveness in continual learning and transfer learning scenarios, and image\nreconstruction tasks show the robustness and generalizability of the extracted\nfeatures. This work demonstrates the competitiveness and potential of Hebbian\nunsupervised learning rules within modern deep learning frameworks,\ndemonstrating the possibility of efficient and biologically inspired learning\nalgorithms without the strong dependence on strict backpropagation. Our code is\navailable at https://github.com/brain-intelligence-lab/SPHeRe.",
    "published": "2025-10-16T15:47:29Z",
    "updated": "2025-10-16T15:47:29Z",
    "link": "http://arxiv.org/pdf/2510.14810v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shikuang Deng",
      "Jiayuan Zhang",
      "Yuhang Wu",
      "Ting Chen",
      "Shi Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14790v1",
    "title": "Active Jammer Localization via Acquisition-Aware Path Planning",
    "summary": "We propose an active jammer localization framework that combines Bayesian\noptimization with acquisition-aware path planning. Unlike passive crowdsourced\nmethods, our approach adaptively guides a mobile agent to collect high-utility\nReceived Signal Strength measurements while accounting for urban obstacles and\nmobility constraints. For this, we modified the A* algorithm, A-UCB*, by\nincorporating acquisition values into trajectory costs, leading to\nhigh-acquisition planned paths. Simulations on realistic urban scenarios show\nthat the proposed method achieves accurate localization with fewer measurements\ncompared to uninformed baselines, demonstrating consistent performance under\ndifferent environments.",
    "published": "2025-10-16T15:22:24Z",
    "updated": "2025-10-16T15:22:24Z",
    "link": "http://arxiv.org/pdf/2510.14790v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Luis Gonzlez-Gudio",
      "Mariona Jaramillo-Civill",
      "Pau Closas",
      "Tales Imbiriba"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14780v1",
    "title": "Causal Discovery for Linear DAGs with Dependent Latent Variables via\n  Higher-order Cumulants",
    "summary": "This paper addresses the problem of estimating causal directed acyclic graphs\nin linear non-Gaussian acyclic models with latent confounders (LvLiNGAM).\nExisting methods assume mutually independent latent confounders or cannot\nproperly handle models with causal relationships among observed variables.\n  We propose a novel algorithm that identifies causal DAGs in LvLiNGAM,\nallowing causal structures among latent variables, among observed variables,\nand between the two. The proposed method leverages higher-order cumulants of\nobserved data to identify the causal structure. Extensive simulations and\nexperiments with real-world data demonstrate the validity and practical utility\nof the proposed algorithm.",
    "published": "2025-10-16T15:15:20Z",
    "updated": "2025-10-16T15:15:20Z",
    "link": "http://arxiv.org/pdf/2510.14780v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Ming Cai",
      "Penggang Gao",
      "Hisayuki Hara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14778v1",
    "title": "Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain\n  Attacks",
    "summary": "Supply chain attacks significantly threaten software security with malicious\ncode injections within legitimate projects. Such attacks are very rare but may\nhave a devastating impact. Detecting spurious code injections using automated\ntools is further complicated as it often requires deciphering the intention of\nboth the inserted code and its context. In this study, we propose an\nunsupervised approach for highlighting spurious code injections by quantifying\ncohesion disruptions in the source code. Using a name-prediction-based cohesion\n(NPC) metric, we analyze how function cohesion changes when malicious code is\nintroduced compared to natural cohesion fluctuations. An analysis of 54,707\nfunctions over 369 open-source C++ repositories reveals that code injection\nreduces cohesion and shifts naming patterns toward shorter, less descriptive\nnames compared to genuine function updates. Considering the sporadic nature of\nreal supply-chain attacks, we evaluate the proposed method with extreme\ntest-set imbalance and show that monitoring high-cohesion functions with NPC\ncan effectively detect functions with injected code, achieving a Precision@100\nof 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that\nautomated cohesion measurements, in general, and name-prediction-based\ncohesion, in particular, may help identify supply chain attacks, improving\nsource code integrity.",
    "published": "2025-10-16T15:14:04Z",
    "updated": "2025-10-16T15:14:04Z",
    "link": "http://arxiv.org/pdf/2510.14778v1.pdf",
    "category": [
      "cs.SE",
      "cs.LG"
    ],
    "authors": [
      "Maor Reuben",
      "Ido Mendel",
      "Or Feldman",
      "Moshe Kravchik",
      "Mordehai Guri",
      "Rami Puzis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26238v2",
    "title": "Beyond Linear Probes: Dynamic Safety Monitoring for Language Models",
    "summary": "Monitoring large language models' (LLMs) activations is an effective way to\ndetect harmful requests before they lead to unsafe outputs. However,\ntraditional safety monitors often require the same amount of compute for every\nquery. This creates a trade-off: expensive monitors waste resources on easy\ninputs, while cheap ones risk missing subtle cases. We argue that safety\nmonitors should be flexible--costs should rise only when inputs are difficult\nto assess, or when more compute is available. To achieve this, we introduce\nTruncated Polynomial Classifiers (TPCs), a natural extension of linear probes\nfor dynamic activation monitoring. Our key insight is that polynomials can be\ntrained and evaluated progressively, term-by-term. At test-time, one can\nearly-stop for lightweight monitoring, or use more terms for stronger\nguardrails when needed. TPCs provide two modes of use. First, as a safety dial:\nby evaluating more terms, developers and regulators can \"buy\" stronger\nguardrails from the same model. Second, as an adaptive cascade: clear cases\nexit early after low-order checks, and higher-order guardrails are evaluated\nonly for ambiguous inputs, reducing overall monitoring costs. On two\nlarge-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with\nup to 30B parameters, we show that TPCs compete with or outperform MLP-based\nprobe baselines of the same size, all the while being more interpretable than\ntheir black-box counterparts. Our code is available at\nhttp://github.com/james-oldfield/tpc.",
    "published": "2025-09-30T13:32:59Z",
    "updated": "2025-10-16T14:51:42Z",
    "link": "http://arxiv.org/pdf/2509.26238v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "James Oldfield",
      "Philip Torr",
      "Ioannis Patras",
      "Adel Bibi",
      "Fazl Barez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14727v1",
    "title": "The Pursuit of Diversity: Multi-Objective Testing of Deep Reinforcement\n  Learning Agents",
    "summary": "Testing deep reinforcement learning (DRL) agents in safety-critical domains\nrequires discovering diverse failure scenarios. Existing tools such as INDAGO\nrely on single-objective optimization focused solely on maximizing failure\ncounts, but this does not ensure discovered scenarios are diverse or reveal\ndistinct error types. We introduce INDAGO-Nexus, a multi-objective search\napproach that jointly optimizes for failure likelihood and test scenario\ndiversity using multi-objective evolutionary algorithms with multiple diversity\nmetrics and Pareto front selection strategies. We evaluated INDAGO-Nexus on\nthree DRL agents: humanoid walker, self-driving car, and parking agent. On\naverage, INDAGO-Nexus discovers up to 83% and 40% more unique failures (test\neffectiveness) than INDAGO in the SDC and Parking scenarios, respectively,\nwhile reducing time-to-failure by up to 67% across all agents.",
    "published": "2025-10-16T14:25:55Z",
    "updated": "2025-10-16T14:25:55Z",
    "link": "http://arxiv.org/pdf/2510.14727v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Antony Bartlett",
      "Cynthia Liem",
      "Annibale Panichella"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07758v2",
    "title": "Rnyi Sharpness: A Novel Sharpness that Strongly Correlates with\n  Generalization",
    "summary": "Sharpness (of the loss minima) is a common measure to investigate the\ngeneralization of neural networks. Intuitively speaking, the flatter the\nlandscape near the minima is, the better generalization might be.\nUnfortunately, the correlation between many existing sharpness measures and the\ngeneralization is usually not strong, sometimes even weak. To close the gap\nbetween the intuition and the reality, we propose a novel sharpness measure,\ni.e., \\textit{R\\'enyi sharpness}, which is defined as the negative R\\'enyi\nentropy (a generalization of the classical Shannon entropy) of the loss\nHessian. The main ideas are as follows: 1) we realize that \\textit{uniform}\n(identical) eigenvalues of the loss Hessian is most desirable (while keeping\nthe sum constant) to achieve good generalization; 2) we employ the\n\\textit{R\\'enyi entropy} to concisely characterize the extent of the spread of\nthe eigenvalues of loss Hessian. Normally, the larger the spread, the smaller\nthe (R\\'enyi) entropy. To rigorously establish the relationship between\ngeneralization and (R\\'enyi) sharpness, we provide several generalization\nbounds in terms of R\\'enyi sharpness, by taking advantage of the\nreparametrization invariance property of R\\'enyi sharpness, as well as the\ntrick of translating the data discrepancy to the weight perturbation.\nFurthermore, extensive experiments are conducted to verify the strong\ncorrelation (in specific, Kendall rank correlation) between the R\\'enyi\nsharpness and generalization. Moreover, we propose to use a variant of R\\'enyi\nSharpness as regularizer during training, i.e., R\\'enyi Sharpness Aware\nMinimization (RSAM), which turns out to outperform all existing sharpness-aware\nminimization methods. It is worthy noting that the test accuracy gain of our\nproposed RSAM method could be as high as nearly 2.5\\%, compared against the\nclassical SAM method.",
    "published": "2025-10-09T03:58:21Z",
    "updated": "2025-10-16T14:21:40Z",
    "link": "http://arxiv.org/pdf/2510.07758v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Qiaozhe Zhang",
      "Jun Sun",
      "Ruijie Zhang",
      "Yingzhuang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14719v1",
    "title": "Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous\n  References",
    "summary": "Modern GPUs feature specialized hardware units that enable high-performance,\nasynchronous dataflow execution. However, the conventional SIMT programming\nmodel is fundamentally misaligned with this task-parallel hardware, creating a\nsignificant programmability gap. While hardware-level warp specialization is\nthe key to unlocking peak performance, it forces developers to manually\norchestrate complex, low-level communication and software pipelines--a process\nthat is labor-intensive, error-prone, and unsustainable. To address this\nchallenge, we present Tawa, an automated compiler that systematically generates\nhigh-performance, warp-specialized code from a high-level, tile-based program.\nCentral to our approach is a novel IR abstraction, asynchronous references\n(aref), which expresses warp-level communication without exposing low-level\nhardware details. Using this abstraction, Tawa automatically partitions\nprograms into producer-consumer roles and manages the intricate dataflow\npipeline, relieving developers of invasive kernel rewriting. Evaluation on\nNVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers\nhigh hardware utilization, achieving up to 1.1$\\times$ speedup over highly\noptimized cuBLAS GEMM kernels. For attention workloads, Tawa attains\n1.2$\\times$ speedup over Triton and matches the performance of the\nhand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming\neffort.",
    "published": "2025-10-16T14:20:00Z",
    "updated": "2025-10-16T14:20:00Z",
    "link": "http://arxiv.org/pdf/2510.14719v1.pdf",
    "category": [
      "cs.LG",
      "cs.AR",
      "cs.PL"
    ],
    "authors": [
      "Hongzheng Chen",
      "Bin Fan",
      "Alexander Collins",
      "Bastian Hagedorn",
      "Evghenii Gaburov",
      "Masahiro Masuda",
      "Matthew Brookhart",
      "Chris Sullivan",
      "Jason Knight",
      "Zhiru Zhang",
      "Vinod Grover"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14711v1",
    "title": "Fast and Scalable Score-Based Kernel Calibration Tests",
    "summary": "We introduce the Kernel Calibration Conditional Stein Discrepancy test (KCCSD\ntest), a non-parametric, kernel-based test for assessing the calibration of\nprobabilistic models with well-defined scores. In contrast to previous methods,\nour test avoids the need for possibly expensive expectation approximations\nwhile providing control over its type-I error. We achieve these improvements by\nusing a new family of kernels for score-based probabilities that can be\nestimated without probability density samples, and by using a conditional\ngoodness-of-fit criterion for the KCCSD test's U-statistic. We demonstrate the\nproperties of our test on various synthetic settings.",
    "published": "2025-10-16T14:11:14Z",
    "updated": "2025-10-16T14:11:14Z",
    "link": "http://arxiv.org/pdf/2510.14711v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Pierre Glaser",
      "David Widmann",
      "Fredrik Lindsten",
      "Arthur Gretton"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14710v1",
    "title": "MCbiF: Measuring Topological Autocorrelation in Multiscale Clusterings\n  via 2-Parameter Persistent Homology",
    "summary": "Datasets often possess an intrinsic multiscale structure with meaningful\ndescriptions at different levels of coarseness. Such datasets are naturally\ndescribed as multi-resolution clusterings, i.e., not necessarily hierarchical\nsequences of partitions across scales. To analyse and compare such sequences,\nwe use tools from topological data analysis and define the Multiscale\nClustering Bifiltration (MCbiF), a 2-parameter filtration of abstract\nsimplicial complexes that encodes cluster intersection patterns across scales.\nThe MCbiF can be interpreted as a higher-order extension of Sankey diagrams and\nreduces to a dendrogram for hierarchical sequences. We show that the\nmultiparameter persistent homology (MPH) of the MCbiF yields a finitely\npresented and block decomposable module, and its stable Hilbert functions\ncharacterise the topological autocorrelation of the sequence of partitions. In\nparticular, at dimension zero, the MPH captures violations of the refinement\norder of partitions, whereas at dimension one, the MPH captures higher-order\ninconsistencies between clusters across scales. We demonstrate through\nexperiments the use of MCbiF Hilbert functions as topological feature maps for\ndownstream machine learning tasks. MCbiF feature maps outperform\ninformation-based baseline features on both regression and classification tasks\non synthetic sets of non-hierarchical sequences of partitions. We also show an\napplication of MCbiF to real-world data to measure non-hierarchies in wild mice\nsocial grouping patterns across time.",
    "published": "2025-10-16T14:11:12Z",
    "updated": "2025-10-16T14:11:12Z",
    "link": "http://arxiv.org/pdf/2510.14710v1.pdf",
    "category": [
      "math.AT",
      "cs.LG",
      "physics.data-an",
      "Primary 55N31, Secondary 62H30"
    ],
    "authors": [
      "Juni Schindler",
      "Mauricio Barahona"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.09831v9",
    "title": "Nearly Minimax Optimal Regret for Multinomial Logistic Bandit",
    "summary": "In this paper, we study the contextual multinomial logit (MNL) bandit problem\nin which a learning agent sequentially selects an assortment based on\ncontextual information, and user feedback follows an MNL choice model. There\nhas been a significant discrepancy between lower and upper regret bounds,\nparticularly regarding the maximum assortment size $K$. Additionally, the\nvariation in reward structures between these bounds complicates the quest for\noptimality. Under uniform rewards, where all items have the same expected\nreward, we establish a regret lower bound of $\\Omega(d\\sqrt{T/K})$ and propose\na constant-time algorithm, OFU-MNL+, that achieves a matching upper bound of\n$\\tilde{O}(d\\sqrt{T/K})$. We also provide instance-dependent minimax regret\nbounds under uniform rewards. Under non-uniform rewards, we prove a lower bound\nof $\\Omega(d\\sqrt{T})$ and an upper bound of $\\tilde{O}(d\\sqrt{T})$, also\nachievable by OFU-MNL+. Our empirical studies support these theoretical\nfindings. To the best of our knowledge, this is the first work in the\ncontextual MNL bandit literature to prove minimax optimality -- for either\nuniform or non-uniform reward setting -- and to propose a computationally\nefficient algorithm that achieves this optimality up to logarithmic factors.",
    "published": "2024-05-16T06:07:31Z",
    "updated": "2025-10-16T14:10:10Z",
    "link": "http://arxiv.org/pdf/2405.09831v9.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Joongkyu Lee",
      "Min-hwan Oh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22369v2",
    "title": "Role-Aware Multi-modal federated learning system for detecting phishing\n  webpages",
    "summary": "We present a federated, multi-modal phishing website detector that supports\nURL, HTML, and IMAGE inputs without binding clients to a fixed modality at\ninference: any client can invoke any modality head trained elsewhere.\nMethodologically, we propose role-aware bucket aggregation on top of FedProx,\ninspired by Mixture-of-Experts and FedMM. We drop learnable routing and use\nhard gating (selecting the IMAGE/HTML/URL expert by sample modality), enabling\nseparate aggregation of modality-specific parameters to isolate cross-embedding\nconflicts and stabilize convergence. On TR-OP, the Fusion head reaches Acc\n97.5% with FPR 2.4% across two data types; on the image subset (ablation) it\nattains Acc 95.5% with FPR 5.9%. For text, we use GraphCodeBERT for URLs and an\nearly three-way embedding for raw, noisy HTML. On WebPhish (HTML) we obtain Acc\n96.5% / FPR 1.8%; on TR-OP (raw HTML) we obtain Acc 95.1% / FPR 4.6%. Results\nindicate that bucket aggregation with hard-gated experts enables stable\nfederated training under strict privacy, while improving the usability and\nflexibility of multi-modal phishing detection.",
    "published": "2025-09-26T14:02:20Z",
    "updated": "2025-10-16T14:00:39Z",
    "link": "http://arxiv.org/pdf/2509.22369v2.pdf",
    "category": [
      "cs.LG",
      "cs.DC"
    ],
    "authors": [
      "Bo Wang",
      "Imran Khan",
      "Martin White",
      "Natalia Beloff"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14694v1",
    "title": "Response to Discussions of \"Causal and Counterfactual Views of Missing\n  Data Models\"",
    "summary": "We are grateful to the discussants, Levis and Kennedy [2025], Luo and Geng\n[2025], Wang and van der Laan [2025], and Yang and Kim [2025], for their\nthoughtful comments on our paper (Nabi et al., 2025). In this rejoinder, we\nsummarize our main contributions and respond to each discussion in turn.",
    "published": "2025-10-16T13:59:09Z",
    "updated": "2025-10-16T13:59:09Z",
    "link": "http://arxiv.org/pdf/2510.14694v1.pdf",
    "category": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Razieh Nabi",
      "Rohit Bhattacharya",
      "Ilya Shpitser",
      "James M. Robins"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14688v1",
    "title": "Online Reliable Anomaly Detection via Neuromorphic Sensing and\n  Communications",
    "summary": "This paper proposes a low-power online anomaly detection framework based on\nneuromorphic wireless sensor networks, encompassing possible use cases such as\nbrain-machine interfaces and remote environmental monitoring. In the considered\nsystem, a central reader node actively queries a subset of neuromorphic sensor\nnodes (neuro-SNs) at each time frame. The neuromorphic sensors are\nevent-driven, producing spikes in correspondence to relevant changes in the\nmonitored system. The queried neuro-SNs respond to the reader with impulse\nradio (IR) transmissions that directly encode the sensed local events. The\nreader processes these event-driven signals to determine whether the monitored\nenvironment is in a normal or anomalous state, while rigorously controlling the\nfalse discovery rate (FDR) of detections below a predefined threshold. The\nproposed approach employs an online hypothesis testing method with e-values to\nmaintain FDR control without requiring knowledge of the anomaly rate, and it\ndynamically optimizes the sensor querying strategy by casting it as a best-arm\nidentification problem in a multi-armed bandit framework. Extensive performance\nevaluation demonstrates that the proposed method can reliably detect anomalies\nunder stringent FDR requirements, while efficiently scheduling sensor\ncommunications and achieving low detection latency.",
    "published": "2025-10-16T13:56:54Z",
    "updated": "2025-10-16T13:56:54Z",
    "link": "http://arxiv.org/pdf/2510.14688v1.pdf",
    "category": [
      "cs.LG",
      "cs.NE"
    ],
    "authors": [
      "Junya Shiraishi",
      "Jiechen Chen",
      "Osvaldo Simeone",
      "Petar Popovski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.20030v2",
    "title": "Offline Reinforcement Learning via Inverse Optimization",
    "summary": "Inspired by the recent successes of Inverse Optimization (IO) across various\napplication domains, we propose a novel offline Reinforcement Learning (ORL)\nalgorithm for continuous state and action spaces, leveraging the convex loss\nfunction called ``sub-optimality loss\" from the IO literature. To mitigate the\ndistribution shift commonly observed in ORL problems, we further employ a\nrobust and non-causal Model Predictive Control (MPC) expert steering a nominal\nmodel of the dynamics using in-hindsight information stemming from the model\nmismatch. Unlike the existing literature, our robust MPC expert enjoys an exact\nand tractable convex reformulation. In the second part of this study, we show\nthat the IO hypothesis class, trained by the proposed convex loss function,\nenjoys ample expressiveness and achieves competitive performance comparing with\nthe state-of-the-art (SOTA) methods in the low-data regime of the MuJoCo\nbenchmark while utilizing three orders of magnitude fewer parameters, thereby\nrequiring significantly fewer computational resources. To facilitate the\nreproducibility of our results, we provide an open-source package implementing\nthe proposed algorithms and the experiments.",
    "published": "2025-02-27T12:11:44Z",
    "updated": "2025-10-16T13:55:37Z",
    "link": "http://arxiv.org/pdf/2502.20030v2.pdf",
    "category": [
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "authors": [
      "Ioannis Dimanidis",
      "Tolga Ok",
      "Peyman Mohajerin Esfahani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.05629v2",
    "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification",
    "summary": "We present a simple yet theoretically motivated improvement to Supervised\nFine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited\ngeneralization compared to reinforcement learning (RL). Through mathematical\nanalysis, we reveal that standard SFT gradients implicitly encode a problematic\nreward structure that may severely restrict the generalization capabilities of\nmodel. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing\ngradient updates for each token by dynamically rescaling the objective function\nwith the probability of this token. Remarkably, this single-line code change\nsignificantly outperforms standard SFT across multiple challenging benchmarks\nand base models, demonstrating greatly improved generalization. Additionally,\nour approach shows competitive results in offline RL settings, offering an\neffective yet simpler alternative. This work bridges theoretical insight and\npractical solutions, substantially advancing SFT performance. The code will be\navailable at https://github.com/yongliang-wu/DFT.",
    "published": "2025-08-07T17:59:04Z",
    "updated": "2025-10-16T13:40:55Z",
    "link": "http://arxiv.org/pdf/2508.05629v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yongliang Wu",
      "Yizhou Zhou",
      "Zhou Ziheng",
      "Yingzhe Peng",
      "Xinyu Ye",
      "Xinting Hu",
      "Wenbo Zhu",
      "Lu Qi",
      "Ming-Hsuan Yang",
      "Xu Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.15437v2",
    "title": "Adaptive Set-Mass Calibration with Conformal Prediction",
    "summary": "Reliable probabilities are critical in high-risk applications, yet common\ncalibration criteria (confidence, class-wise) are only necessary for full\ndistributional calibration, and post-hoc methods often lack distribution-free\nguarantees.\n  We propose a set-based notion of calibration, cumulative mass calibration,\nand a corresponding empirical error measure: the Cumulative Mass Calibration\nError (CMCE). We develop a new calibration procedure that starts with conformal\nprediction to obtain a set of labels that gives the desired coverage.\n  We then instantiate two simple post-hoc calibrators: a mass normalization and\na temperature scaling-based rule, tuned to the conformal constraint.\n  On multi-class image benchmarks, especially with a large number of classes,\nour methods consistently improve CMCE and standard metrics (ECE, cw-ECE, MCE)\nover baselines, delivering a practical, scalable framework with theoretical\nguarantees.",
    "published": "2025-05-21T12:18:15Z",
    "updated": "2025-10-16T13:39:56Z",
    "link": "http://arxiv.org/pdf/2505.15437v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Daniil Kazantsev",
      "Mohsen Guizani",
      "Eric Moulines",
      "Maxim Panov",
      "Nikita Kotelevskii"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14666v1",
    "title": "Geometric Moment Alignment for Domain Adaptation via Siegel Embeddings",
    "summary": "We address the problem of distribution shift in unsupervised domain\nadaptation with a moment-matching approach. Existing methods typically align\nlow-order statistical moments of the source and target distributions in an\nembedding space using ad-hoc similarity measures. We propose a principled\nalternative that instead leverages the intrinsic geometry of these\ndistributions by adopting a Riemannian distance for this alignment. Our key\nnovelty lies in expressing the first- and second-order moments as a single\nsymmetric positive definite (SPD) matrix through Siegel embeddings. This\nenables simultaneous adaptation of both moments using the natural geometric\ndistance on the shared manifold of SPD matrices, preserving the mean and\ncovariance structure of the source and target distributions and yielding a more\nfaithful metric for cross-domain comparison. We connect the Riemannian manifold\ndistance to the target-domain error bound, and validate the method on image\ndenoising and image classification benchmarks. Our code is publicly available\nat https://github.com/shayangharib/GeoAdapt.",
    "published": "2025-10-16T13:20:51Z",
    "updated": "2025-10-16T13:20:51Z",
    "link": "http://arxiv.org/pdf/2510.14666v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shayan Gharib",
      "Marcelo Hartmann",
      "Arto Klami"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.18122v2",
    "title": "Provable Mixed-Noise Learning with Flow-Matching",
    "summary": "We study Bayesian inverse problems with mixed noise, modeled as a combination\nof additive and multiplicative Gaussian components. While traditional inference\nmethods often assume fixed or known noise characteristics, real-world\napplications, particularly in physics and chemistry, frequently involve noise\nwith unknown and heterogeneous structure. Motivated by recent advances in\nflow-based generative modeling, we propose a novel inference framework based on\nconditional flow matching embedded within an Expectation-Maximization (EM)\nalgorithm to jointly estimate posterior samplers and noise parameters. To\nenable high-dimensional inference and improve scalability, we use\nsimulation-free ODE-based flow matching as the generative model in the E-step\nof the EM algorithm. We prove that, under suitable assumptions, the EM updates\nconverge to the true noise parameters in the population limit of infinite\nobservations. Our numerical results illustrate the effectiveness of combining\nEM inference with flow matching for mixed-noise Bayesian inverse problems.",
    "published": "2025-08-25T15:30:12Z",
    "updated": "2025-10-16T13:17:05Z",
    "link": "http://arxiv.org/pdf/2508.18122v2.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Paul Hagemann",
      "Robert Gruhlke",
      "Bernhard Stankewitz",
      "Claudia Schillings",
      "Gabriele Steidl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14656v1",
    "title": "Parameter Identification for Partial Differential Equation with Jump\n  Discontinuities in Coefficients by Markov Switching Model and\n  Physics-Informed Machine Learning",
    "summary": "Inverse problems involving partial differential equations (PDEs) with\ndiscontinuous coefficients are fundamental challenges in modeling complex\nspatiotemporal systems with heterogeneous structures and uncertain dynamics.\nTraditional numerical and machine learning approaches often face limitations in\naddressing these problems due to high dimensionality, inherent nonlinearity,\nand discontinuous parameter spaces. In this work, we propose a novel\ncomputational framework that synergistically integrates physics-informed deep\nlearning with Bayesian inference for accurate parameter identification in PDEs\nwith jump discontinuities in coefficients. The core innovation of our framework\nlies in a dual-network architecture employing a gradient-adaptive weighting\nstrategy: a main network approximates PDE solutions while a sub network samples\nits coefficients. To effectively identify mixture structures in parameter\nspaces, we employ Markovian dynamics methods to capture hidden state\ntransitions of complex spatiotemporal systems. The framework has applications\nin reconstruction of solutions and identification of parameter-varying regions.\nComprehensive numerical experiments on various PDEs with jump-varying\ncoefficients demonstrate the framework's exceptional adaptability, accuracy,\nand robustness compared to existing methods. This study provides a\ngeneralizable computational approach of parameter identification for PDEs with\ndiscontinuous parameter structures, particularly in non-stationary or\nheterogeneous systems.",
    "published": "2025-10-16T13:12:26Z",
    "updated": "2025-10-16T13:12:26Z",
    "link": "http://arxiv.org/pdf/2510.14656v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Zhikun Zhang",
      "Guanyu Pan",
      "Xiangjun Wang",
      "Yong Xu",
      "Guangtao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.18363v3",
    "title": "Exploring the Noise Robustness of Online Conformal Prediction",
    "summary": "Conformal prediction is an emerging technique for uncertainty quantification\nthat constructs prediction sets guaranteed to contain the true label with a\npredefined probability. Recent work develops online conformal prediction\nmethods that adaptively construct prediction sets to accommodate distribution\nshifts. However, existing algorithms typically assume perfect label accuracy\nwhich rarely holds in practice. In this work, we investigate the robustness of\nonline conformal prediction under uniform label noise with a known noise rate,\nin both constant and dynamic learning rate schedules. We show that label noise\ncauses a persistent gap between the actual mis-coverage rate and the desired\nrate $\\alpha$, leading to either overestimated or underestimated coverage\nguarantees. To address this issue, we propose Noise Robust Online Conformal\nPrediction (dubbed NR-OCP) by updating the threshold with a novel robust\npinball loss, which provides an unbiased estimate of clean pinball loss without\nrequiring ground-truth labels. Our theoretical analysis shows that NR-OCP\neliminates the coverage gap in both constant and dynamic learning rate\nschedules, achieving a convergence rate of $\\mathcal{O}(T^{-1/2})$ for both\nempirical and expected coverage errors under uniform label noise. Extensive\nexperiments demonstrate the effectiveness of our method by achieving both\nprecise coverage and improved efficiency.",
    "published": "2025-01-30T14:08:26Z",
    "updated": "2025-10-16T12:37:37Z",
    "link": "http://arxiv.org/pdf/2501.18363v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Huajun Xi",
      "Kangdao Liu",
      "Hao Zeng",
      "Wenguang Sun",
      "Hongxin Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.03350v3",
    "title": "Partially stochastic deep learning with uncertainty quantification for\n  model predictive heating control",
    "summary": "Making the control of building heating systems more energy efficient is\ncrucial for reducing global energy consumption and greenhouse gas emissions.\nTraditional rule-based control methods use a static, outdoor\ntemperature-dependent heating curve to regulate heat input. This open-loop\napproach fails to account for both the current state of the system (indoor\ntemperature) and free heat gains, such as solar radiation, often resulting in\npoor thermal comfort and overheating. Model Predictive Control (MPC) addresses\nthese drawbacks by using predictive modeling to optimize heating based on a\nbuilding's learned thermal behavior, current system state, and weather\nforecasts. However, current industrial MPC solutions often employ simplified\nphysics-inspired indoor temperature models, sacrificing accuracy for robustness\nand interpretability. While purely data-driven models offer superior predictive\nperformance and therefore more accurate control, they face challenges such as a\nlack of transparency.\n  To bridge this gap, we propose a partially stochastic deep learning (DL)\narchitecture, dubbed LSTM+BNN, for building-specific indoor temperature\nmodeling. Unlike most studies that evaluate model performance through\nsimulations or limited test buildings, our experiments across a comprehensive\ndataset of 100 real-world buildings, under various weather conditions,\ndemonstrate that LSTM+BNN outperforms an industry-proven reference model,\nreducing the average prediction error measured as RMSE by more than 40% for the\n48-hour prediction horizon of interest. Unlike deterministic DL approaches,\nLSTM+BNN offers a critical advantage by enabling pre-assessment of model\ncompetency for control optimization through uncertainty quantification. Thus,\nthe proposed model shows significant potential to improve thermal comfort and\nenergy efficiency achieved with heating MPC solutions.",
    "published": "2025-04-04T11:07:23Z",
    "updated": "2025-10-16T12:35:14Z",
    "link": "http://arxiv.org/pdf/2504.03350v3.pdf",
    "category": [
      "stat.AP",
      "cs.LG"
    ],
    "authors": [
      "Emma Hannula",
      "Arttu Hkkinen",
      "Antti Solonen",
      "Felipe Uribe",
      "Jana de Wiljes",
      "Lassi Roininen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14614v1",
    "title": "First Attentions Last: Better Exploiting First Attentions for Efficient\n  Transformer Training",
    "summary": "As training billion-scale transformers becomes increasingly common, employing\nmultiple distributed GPUs along with parallel training methods has become a\nstandard practice. However, existing transformer designs suffer from\nsignificant communication overhead, especially in Tensor Parallelism (TP),\nwhere each block's MHA-MLP connection requires an all-reduce communication.\nThrough our investigation, we show that the MHA-MLP connections can be bypassed\nfor efficiency, while the attention output of the first layer can serve as an\nalternative signal for the bypassed connection. Motivated by the observations,\nwe propose FAL (First Attentions Last), an efficient transformer architecture\nthat redirects the first MHA output to the MLP inputs of the following layers,\neliminating the per-block MHA-MLP connections. This removes the all-reduce\ncommunication and enables parallel execution of MHA and MLP on a single GPU. We\nalso introduce FAL+, which adds the normalized first attention output to the\nMHA outputs of the following layers to augment the MLP input for the model\nquality. Our evaluation shows that FAL reduces multi-GPU training time by up to\n44%, improves single-GPU throughput by up to 1.18x, and achieves better\nperplexity compared to the baseline GPT. FAL+ achieves even lower perplexity\nwithout increasing the training time than the baseline.",
    "published": "2025-10-16T12:21:46Z",
    "updated": "2025-10-16T12:21:46Z",
    "link": "http://arxiv.org/pdf/2510.14614v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Gyudong Kim",
      "Hyukju Na",
      "Jin Hyeon Kim",
      "Hyunsung Jang",
      "Jaemin Park",
      "Jaegi Hwang",
      "Namkoo Ha",
      "Seungryong Kim",
      "Young Geun Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.04430v4",
    "title": "Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized\n  Zero-Order",
    "summary": "Fine-tuning Large Language Models (LLMs) is essential for adapting\npre-trained models to downstream tasks. Yet traditional first-order optimizers\nsuch as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and\ncomputational costs that scale poorly with model size. In this paper, we\ninvestigate zero-order (ZO) optimization methods as a memory- and\ncompute-efficient alternative, particularly in the context of\nparameter-efficient fine-tuning techniques like LoRA. We propose\n$\\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO\nSignSGD, requiring the same number of parameters as the standard ZO SGD and\nonly $\\mathcal{O}(1)$ function evaluations per iteration. To the best of our\nknowledge, this is the first study to establish rigorous convergence guarantees\nfor SignSGD in the stochastic ZO case. We further propose $\\texttt{JAGUAR\nMuon}$, a novel ZO extension of the Muon optimizer that leverages the matrix\nstructure of model parameters, and we provide its convergence rate under\narbitrary stochastic noise. Through extensive experiments on challenging LLM\nfine-tuning benchmarks, we demonstrate that the proposed algorithms meet or\nexceed the convergence quality of standard first-order methods, achieving\nsignificant memory reduction. Our theoretical and empirical results establish\nnew ZO optimization methods as a practical and theoretically grounded approach\nfor resource-constrained LLM adaptation. Our code is available at\nhttps://github.com/brain-mmo-lab/ZO_LLM",
    "published": "2025-06-04T20:27:17Z",
    "updated": "2025-10-16T11:58:47Z",
    "link": "http://arxiv.org/pdf/2506.04430v4.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Egor Petrov",
      "Grigoriy Evseev",
      "Aleksey Antonov",
      "Andrey Veprikov",
      "Nikolay Bushkov",
      "Stanislav Moiseev",
      "Aleksandr Beznosikov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.02724v3",
    "title": "WeightLoRA: Keep Only Necessary Adapters",
    "summary": "The widespread utilization of language models in modern applications is\ninconceivable without Parameter-Efficient Fine-Tuning techniques, such as\nlow-rank adaptation ($\\texttt{LoRA}$), which adds trainable adapters to\nselected layers. Although $\\texttt{LoRA}$ may obtain accurate solutions, it\nrequires significant memory to train large models and intuition on which layers\nto add adapters. In this paper, we propose a novel method,\n$\\texttt{WeightLoRA}$, which overcomes this issue by adaptive selection of the\nmost critical $\\texttt{LoRA}$ heads throughout the optimization process. As a\nresult, we can significantly reduce the number of trainable parameters while\nmaintaining the capability to obtain consistent or even superior metric values.\nWe conduct experiments for a series of competitive benchmarks and DeBERTa,\nBART, and Llama models, comparing our method with different adaptive\napproaches. The experimental results demonstrate the efficacy of\n$\\texttt{WeightLoRA}$ and the superior performance of $\\texttt{WeightLoRA+}$ in\nalmost all cases.",
    "published": "2025-06-03T10:33:16Z",
    "updated": "2025-10-16T11:57:58Z",
    "link": "http://arxiv.org/pdf/2506.02724v3.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Andrey Veprikov",
      "Vladimir Solodkin",
      "Alexander Zyl",
      "Andrey Savchenko",
      "Aleksandr Beznosikov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14592v1",
    "title": "Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge\n  Graphs with Hybrid Retrieval",
    "summary": "Current Retrieval-Augmented Generation (RAG) systems primarily operate on\nunimodal textual data, limiting their effectiveness on unstructured multimodal\ndocuments. Such documents often combine text, images, tables, equations, and\ngraphs, each contributing unique information. In this work, we present a\nModality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for\nmultimodal question answering with reasoning through a modality-aware knowledge\ngraph. MAHA integrates dense vector retrieval with structured graph traversal,\nwhere the knowledge graph encodes cross-modal semantics and relationships. This\ndesign enables both semantically rich and context-aware retrieval across\ndiverse modalities. Evaluations on multiple benchmark datasets demonstrate that\nMAHA substantially outperforms baseline methods, achieving a ROUGE-L score of\n0.486, providing complete modality coverage. These results highlight MAHA's\nability to combine embeddings with explicit document structure, enabling\neffective multimodal retrieval. Our work establishes a scalable and\ninterpretable retrieval framework that advances RAG systems by enabling\nmodality-aware reasoning over unstructured multimodal data.",
    "published": "2025-10-16T11:55:24Z",
    "updated": "2025-10-16T11:55:24Z",
    "link": "http://arxiv.org/pdf/2510.14592v1.pdf",
    "category": [
      "cs.LG",
      "cs.IR"
    ],
    "authors": [
      "Rashmi R",
      "Vidyadhar Upadhya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14586v1",
    "title": "Matcha: Multi-Stage Riemannian Flow Matching for Accurate and Physically\n  Valid Molecular Docking",
    "summary": "Accurate prediction of protein-ligand binding poses is crucial for\nstructure-based drug design, yet existing methods struggle to balance speed,\naccuracy, and physical plausibility. We introduce Matcha, a novel molecular\ndocking pipeline that combines multi-stage flow matching with learned scoring\nand physical validity filtering. Our approach consists of three sequential\nstages applied consecutively to refine docking predictions, each implemented as\na flow matching model operating on appropriate geometric spaces\n($\\mathbb{R}^3$, $\\mathrm{SO}(3)$, and $\\mathrm{SO}(2)$). We enhance the\nprediction quality through a dedicated scoring model and apply unsupervised\nphysical validity filters to eliminate unrealistic poses. Compared to various\napproaches, Matcha demonstrates superior performance on Astex and PDBbind test\nsets in terms of docking success rate and physical plausibility. Moreover, our\nmethod works approximately 25 times faster than modern large-scale co-folding\nmodels. The model weights and inference code to reproduce our results are\navailable at https://github.com/LigandPro/Matcha.",
    "published": "2025-10-16T11:44:24Z",
    "updated": "2025-10-16T11:44:24Z",
    "link": "http://arxiv.org/pdf/2510.14586v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Daria Frolova",
      "Talgat Daulbaev",
      "Egor Sevryugov",
      "Sergei A. Nikolenko",
      "Dmitry N. Ivankov",
      "Ivan Oseledets",
      "Marina A. Pak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14573v1",
    "title": "State-Space Models for Tabular Prior-Data Fitted Networks",
    "summary": "Recent advancements in foundation models for tabular data, such as TabPFN,\ndemonstrated that pretrained Transformer architectures can approximate Bayesian\ninference with high predictive performance. However, Transformers suffer from\nquadratic complexity with respect to sequence length, motivating the\nexploration of more efficient sequence models. In this work, we investigate the\npotential of using Hydra, a bidirectional linear-time structured state space\nmodel (SSM), as an alternative to Transformers in TabPFN. A key challenge lies\nin SSM's inherent sensitivity to the order of input tokens - an undesirable\nproperty for tabular datasets where the row order is semantically meaningless.\nWe investigate to what extent a bidirectional approach can preserve efficiency\nand enable symmetric context aggregation. Our experiments show that this\napproach reduces the order-dependence, achieving predictive performance\ncompetitive to the original TabPFN model.",
    "published": "2025-10-16T11:31:51Z",
    "updated": "2025-10-16T11:31:51Z",
    "link": "http://arxiv.org/pdf/2510.14573v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Felix Koch",
      "Marcel Wever",
      "Fabian Raisch",
      "Benjamin Tischler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13541v2",
    "title": "SPIRIT: Patching Speech Language Models against Jailbreak Attacks",
    "summary": "Speech Language Models (SLMs) enable natural interactions via spoken\ninstructions, which more effectively capture user intent by detecting nuances\nin speech. The richer speech signal introduces new security risks compared to\ntext-based models, as adversaries can better bypass safety mechanisms by\ninjecting imperceptible noise to speech. We analyze adversarial attacks and\nfind that SLMs are substantially more vulnerable to jailbreak attacks, which\ncan achieve a perfect 100% attack success rate in some instances. To improve\nsecurity, we propose post-hoc patching defenses used to intervene during\ninference by modifying the SLM's activations that improve robustness up to 99%\nwith (i) negligible impact on utility and (ii) without any re-training. We\nconduct ablation studies to maximize the efficacy of our defenses and improve\nthe utility/security trade-off, validated with large-scale benchmarks unique to\nSLMs.",
    "published": "2025-05-18T21:51:24Z",
    "updated": "2025-10-16T11:26:23Z",
    "link": "http://arxiv.org/pdf/2505.13541v2.pdf",
    "category": [
      "eess.AS",
      "cs.LG"
    ],
    "authors": [
      "Amirbek Djanibekov",
      "Nurdaulet Mukhituly",
      "Kentaro Inui",
      "Hanan Aldarmaki",
      "Nils Lukas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.15423v2",
    "title": "Lost in the Averages: A New Specific Setup to Evaluate Membership\n  Inference Attacks Against Machine Learning Models",
    "summary": "Synthetic data generators and machine learning models can memorize their\ntraining data, posing privacy concerns. Membership inference attacks (MIAs) are\na standard method of estimating the privacy risk of these systems. The risk of\nindividual records is typically computed by evaluating MIAs in a\nrecord-specific privacy game. We analyze the record-specific privacy game\ncommonly used for evaluating attackers under realistic assumptions (the\n\\textit{traditional} game) -- particularly for synthetic tabular data -- and\nshow that it averages a record's privacy risk across datasets. We show this\nimplicitly assumes the dataset a record is part of has no impact on the\nrecord's risk, providing a misleading risk estimate when a specific model or\nsynthetic dataset is released. Instead, we propose a novel use of the\nleave-one-out game, used in existing work exclusively to audit differential\nprivacy guarantees, and call this the \\textit{model-seeded} game. We formalize\nit and show that it provides an accurate estimate of the privacy risk posed by\na given adversary for a record in its specific dataset. We instantiate and\nevaluate the state-of-the-art MIA for synthetic data generators in the\ntraditional and model-seeded privacy games, and show across multiple datasets\nand models that the two privacy games indeed result in different risk scores,\nwith up to 94\\% of high-risk records being overlooked by the traditional game.\nWe further show that records in smaller datasets and models not protected by\nstrong differential privacy guarantees tend to have a larger gap between risk\nestimates. Taken together, our results show that the model-seeded setup yields\na risk estimate specific to a certain model or synthetic dataset released and\nin line with the standard notion of privacy leakage from prior work,\nmeaningfully different from the dataset-averaged risk provided by the\ntraditional privacy game.",
    "published": "2024-05-24T10:37:38Z",
    "updated": "2025-10-16T11:17:06Z",
    "link": "http://arxiv.org/pdf/2405.15423v2.pdf",
    "category": [
      "cs.LG",
      "cs.CR"
    ],
    "authors": [
      "Nataa Kro",
      "Florent Gupin",
      "Matthieu Meeus",
      "Bogdan Kulynych",
      "Yves-Alexandre de Montjoye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14562v1",
    "title": "Redundancy-Aware Test-Time Graph Out-of-Distribution Detection",
    "summary": "Distributional discrepancy between training and test data can lead models to\nmake inaccurate predictions when encountering out-of-distribution (OOD) samples\nin real-world applications. Although existing graph OOD detection methods\nleverage data-centric techniques to extract effective representations, their\nperformance remains compromised by structural redundancy that induces semantic\nshifts. To address this dilemma, we propose RedOUT, an unsupervised framework\nthat integrates structural entropy into test-time OOD detection for graph\nclassification. Concretely, we introduce the Redundancy-aware Graph Information\nBottleneck (ReGIB) and decompose the objective into essential information and\nirrelevant redundancy. By minimizing structural entropy, the decoupled\nredundancy is reduced, and theoretically grounded upper and lower bounds are\nproposed for optimization. Extensive experiments on real-world datasets\ndemonstrate the superior performance of RedOUT on OOD detection. Specifically,\nour method achieves an average improvement of 6.7%, significantly surpassing\nthe best competitor by 17.3% on the ClinTox/LIPO dataset pair.",
    "published": "2025-10-16T11:14:45Z",
    "updated": "2025-10-16T11:14:45Z",
    "link": "http://arxiv.org/pdf/2510.14562v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yue Hou",
      "He Zhu",
      "Ruomei Liu",
      "Yingke Su",
      "Junran Wu",
      "Ke Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14557v1",
    "title": "MX+: Pushing the Limits of Microscaling Formats for Efficient Large\n  Language Model Serving",
    "summary": "Reduced-precision data formats are crucial for cost-effective serving of\nlarge language models (LLMs). While numerous reduced-precision formats have\nbeen introduced thus far, they often require intrusive modifications to the\nsoftware frameworks or are rather unconventional for widespread adoption across\nhardware vendors. In this paper, we instead focus on recent industry-driven\nvariants of block floating-point (BFP) formats and conduct a comprehensive\nanalysis to push their limits for efficient LLM serving. Our analysis shows\nthat existing ultra low-bit BFP variants struggle to provide reasonable\nlanguage model performance due to outlier values in blocks. To address the\noutliers with BFPs, we propose MX+, a cost-effective and non-intrusive\nextension designed for seamless integration into the microscaling (MX) formats.\nMX+ builds on the key insight that the outlier does not need to use its\nexponent field in the element data type, which allows us to repurpose the\nexponent field as an extended mantissa to increase the precision of the outlier\nelement. Our evaluation shows that MX+ achieves significantly higher model\nperformance compared to the 4-bit MX format (MXFP4) with negligible storage\noverhead and slowdown, thus offering a compelling alternative to MXFP4 or MXFP6\nfor efficient LLM inference.",
    "published": "2025-10-16T11:05:54Z",
    "updated": "2025-10-16T11:05:54Z",
    "link": "http://arxiv.org/pdf/2510.14557v1.pdf",
    "category": [
      "cs.LG",
      "cs.AR"
    ],
    "authors": [
      "Jungi Lee",
      "Junyong Park",
      "Soohyun Cha",
      "Jaehoon Cho",
      "Jaewoong Sim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.13658v3",
    "title": "Adversarial Disentanglement by Backpropagation with Physics-Informed\n  Variational Autoencoder",
    "summary": "Inference and prediction under partial knowledge of a physical system is\nchallenging, particularly when multiple confounding sources influence the\nmeasured response. Explicitly accounting for these influences in physics-based\nmodels is often infeasible due to epistemic uncertainty, cost, or time\nconstraints, resulting in models that fail to accurately describe the behavior\nof the system. On the other hand, data-driven machine learning models such as\nvariational autoencoders are not guaranteed to identify a parsimonious\nrepresentation. As a result, they can suffer from poor generalization\nperformance and reconstruction accuracy in the regime of limited and noisy\ndata. We propose a physics-informed variational autoencoder architecture that\ncombines the interpretability of physics-based models with the flexibility of\ndata-driven models. To promote disentanglement of the known physics and\nconfounding influences, the latent space is partitioned into physically\nmeaningful variables that parametrize a physics-based model, and data-driven\nvariables that capture variability in the domain and class of the physical\nsystem. The encoder is coupled with a decoder that integrates physics-based and\ndata-driven components, and constrained by an adversarial training objective\nthat prevents the data-driven components from overriding the known physics,\nensuring that the physics-grounded latent variables remain interpretable. We\ndemonstrate that the model is able to disentangle features of the input signal\nand separate the known physics from confounding influences using supervision in\nthe form of class and domain observables. The model is evaluated on a series of\nsynthetic case studies relevant to engineering structures, demonstrating the\nfeasibility of the proposed approach.",
    "published": "2025-06-16T16:18:25Z",
    "updated": "2025-10-16T11:05:51Z",
    "link": "http://arxiv.org/pdf/2506.13658v3.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Ioannis Christoforos Koune",
      "Alice Cicirello"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14542v1",
    "title": "A Deep State-Space Model Compression Method using Upper Bound on Output\n  Error",
    "summary": "We study deep state-space models (Deep SSMs) that contain\nlinear-quadratic-output (LQO) systems as internal blocks and present a\ncompression method with a provable output error guarantee. We first derive an\nupper bound on the output error between two Deep SSMs and show that the bound\ncan be expressed via the $h^2$-error norms between the layerwise LQO systems,\nthereby providing a theoretical justification for existing model order\nreduction (MOR)-based compression. Building on this bound, we formulate an\noptimization problem in terms of the $h^2$-error norm and develop a\ngradient-based MOR method. On the IMDb task from the Long Range Arena\nbenchmark, we demonstrate that our compression method achieves strong\nperformance. Moreover, unlike prior approaches, we reduce roughly 80% of\ntrainable parameters without retraining, with only a 4-5% performance drop.",
    "published": "2025-10-16T10:32:21Z",
    "updated": "2025-10-16T10:32:21Z",
    "link": "http://arxiv.org/pdf/2510.14542v1.pdf",
    "category": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "authors": [
      "Hiroki Sakamoto",
      "Kazuhiro Sato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14523v1",
    "title": "On the Identifiability of Tensor Ranks via Prior Predictive Matching",
    "summary": "Selecting the latent dimensions (ranks) in tensor factorization is a central\nchallenge that often relies on heuristic methods. This paper introduces a\nrigorous approach to determine rank identifiability in probabilistic tensor\nmodels, based on prior predictive moment matching. We transform a set of moment\nmatching conditions into a log-linear system of equations in terms of marginal\nmoments, prior hyperparameters, and ranks; establishing an equivalence between\nrank identifiability and the solvability of such system. We apply this\nframework to four foundational tensor-models, demonstrating that the linear\nstructure of the PARAFAC/CP model, the chain structure of the Tensor Train\nmodel, and the closed-loop structure of the Tensor Ring model yield solvable\nsystems, making their ranks identifiable. In contrast, we prove that the\nsymmetric topology of the Tucker model leads to an underdetermined system,\nrendering the ranks unidentifiable by this method. For the identifiable models,\nwe derive explicit closed-form rank estimators based on the moments of observed\ndata only. We empirically validate these estimators and evaluate the robustness\nof the proposal.",
    "published": "2025-10-16T10:13:45Z",
    "updated": "2025-10-16T10:13:45Z",
    "link": "http://arxiv.org/pdf/2510.14523v1.pdf",
    "category": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "62A09, 62F15",
      "G.3"
    ],
    "authors": [
      "Eliezer da Silva",
      "Arto Klami",
      "Diego Mesquita",
      "Iigo Urteaga"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14510v1",
    "title": "Enhancing Time Series Forecasting through Selective Representation\n  Spaces: A Patch Perspective",
    "summary": "Time Series Forecasting has made significant progress with the help of\nPatching technique, which partitions time series into multiple patches to\neffectively retain contextual semantic information into a representation space\nbeneficial for modeling long-term dependencies. However, conventional patching\npartitions a time series into adjacent patches, which causes a fixed\nrepresentation space, thus resulting in insufficiently expressful\nrepresentations. In this paper, we pioneer the exploration of constructing a\nselective representation space to flexibly include the most informative patches\nfor forecasting. Specifically, we propose the Selective Representation Space\n(SRS) module, which utilizes the learnable Selective Patching and Dynamic\nReassembly techniques to adaptively select and shuffle the patches from the\ncontextual time series, aiming at fully exploiting the information of\ncontextual time series to enhance the forecasting performance of patch-based\nmodels. To demonstrate the effectiveness of SRS module, we propose a simple yet\neffective SRSNet consisting of SRS and an MLP head, which achieves\nstate-of-the-art performance on real-world datasets from multiple domains.\nFurthermore, as a novel plugin-and-play module, SRS can also enhance the\nperformance of existing patch-based models. The resources are available at\nhttps://github.com/decisionintelligence/SRSNet.",
    "published": "2025-10-16T09:55:06Z",
    "updated": "2025-10-16T09:55:06Z",
    "link": "http://arxiv.org/pdf/2510.14510v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Xingjian Wu",
      "Xiangfei Qiu",
      "Hanyin Cheng",
      "Zhengyu Li",
      "Jilin Hu",
      "Chenjuan Guo",
      "Bin Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14503v1",
    "title": "Learning to Undo: Rollback-Augmented Reinforcement Learning with\n  Reversibility Signals",
    "summary": "This paper proposes a reversible learning framework to improve the robustness\nand efficiency of value based Reinforcement Learning agents, addressing\nvulnerability to value overestimation and instability in partially irreversible\nenvironments. The framework has two complementary core mechanisms: an\nempirically derived transition reversibility measure called Phi of s and a, and\na selective state rollback operation. We introduce an online per state action\nestimator called Phi that quantifies the likelihood of returning to a prior\nstate within a fixed horizon K. This measure is used to adjust the penalty term\nduring temporal difference updates dynamically, integrating reversibility\nawareness directly into the value function. The system also includes a\nselective rollback operator. When an action yields an expected return markedly\nlower than its instantaneous estimated value and violates a predefined\nthreshold, the agent is penalized and returns to the preceding state rather\nthan progressing. This interrupts sub optimal high risk trajectories and avoids\ncatastrophic steps. By combining reversibility aware evaluation with targeted\nrollback, the method improves safety, performance, and stability. In the\nCliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8\npercent and yielded a 55 percent increase in mean episode return. In the Taxi\nv3 domain, it suppressed illegal actions by greater than or equal to 99.9\npercent and achieved a 65.7 percent improvement in cumulative reward, while\nalso sharply reducing reward variance in both environments. Ablation studies\nconfirm that the rollback mechanism is the critical component underlying these\nsafety and performance gains, marking a robust step toward safe and reliable\nsequential decision making.",
    "published": "2025-10-16T09:48:54Z",
    "updated": "2025-10-16T09:48:54Z",
    "link": "http://arxiv.org/pdf/2510.14503v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Andrejs Sorstkins",
      "Omer Tariq",
      "Muhammad Bilal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10513v2",
    "title": "A Hybrid Machine Learning Approach for Synthetic Data Generation with\n  Post Hoc Calibration for Clinical Tabular Datasets",
    "summary": "Healthcare research and development face significant obstacles due to data\nscarcity and stringent privacy regulations, such as HIPAA and the GDPR,\nrestricting access to essential real-world medical data. These limitations\nimpede innovation, delay robust AI model creation, and hinder advancements in\npatient-centered care. Synthetic data generation offers a transformative\nsolution by producing artificial datasets that emulate real data statistics\nwhile safeguarding patient privacy. We introduce a novel hybrid framework for\nhigh-fidelity healthcare data synthesis integrating five augmentation methods:\nnoise injection, interpolation, Gaussian Mixture Model (GMM) sampling,\nConditional Variational Autoencoder (CVAE) sampling, and SMOTE, combined via a\nreinforcement learning-based dynamic weight selection mechanism. Its key\ninnovations include advanced calibration techniques -- moment matching, full\nhistogram matching, soft and adaptive soft histogram matching, and iterative\nrefinement -- that align marginal distributions and preserve joint feature\ndependencies. Evaluated on the Breast Cancer Wisconsin (UCI Repository) and\nKhulna Medical College cardiology datasets, our calibrated hybrid achieves\nWasserstein distances as low as 0.001 and Kolmogorov-Smirnov statistics around\n0.01, demonstrating near-zero marginal discrepancy. Pairwise trend scores\nsurpass 90%, and Nearest Neighbor Adversarial Accuracy approaches 50%,\nconfirming robust privacy protection. Downstream classifiers trained on\nsynthetic data achieve up to 94% accuracy and F1 scores above 93%, comparable\nto models trained on real data. This scalable, privacy-preserving approach\nmatches state-of-the-art methods, sets new benchmarks for joint-distribution\nfidelity in healthcare, and supports sensitive AI applications.",
    "published": "2025-10-12T09:23:43Z",
    "updated": "2025-10-16T09:48:52Z",
    "link": "http://arxiv.org/pdf/2510.10513v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Md Ibrahim Shikder Mahin",
      "Md Shamsul Arefin",
      "Md Tanvir Hasan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.14512v2",
    "title": "Just One Layer Norm Guarantees Stable Extrapolation",
    "summary": "In spite of their prevalence, the behaviour of Neural Networks when\nextrapolating far from the training distribution remains poorly understood,\nwith existing results limited to specific cases. In this work, we prove general\nresults -- the first of their kind -- by applying Neural Tangent Kernel (NTK)\ntheory to analyse infinitely-wide neural networks trained until convergence and\nprove that the inclusion of just one Layer Norm (LN) fundamentally alters the\ninduced NTK, transforming it into a bounded-variance kernel. As a result, the\noutput of an infinitely wide network with at least one LN remains bounded, even\non inputs far from the training data. In contrast, we show that a broad class\nof networks without LN can produce pathologically large outputs for certain\ninputs. We support these theoretical findings with empirical experiments on\nfinite-width networks, demonstrating that while standard NNs often exhibit\nuncontrolled growth outside the training domain, a single LN layer effectively\nmitigates this instability. Finally, we explore real-world implications of this\nextrapolatory stability, including applications to predicting residue sizes in\nproteins larger than those seen during training and estimating age from facial\nimages of underrepresented ethnicities absent from the training set.",
    "published": "2025-05-20T15:39:27Z",
    "updated": "2025-10-16T09:34:44Z",
    "link": "http://arxiv.org/pdf/2505.14512v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Juliusz Ziomek",
      "George Whittle",
      "Michael A. Osborne"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.03941v2",
    "title": "Measuring the stability and plasticity of recommender systems",
    "summary": "The typical offline protocol to evaluate recommendation algorithms is to\ncollect a dataset of user-item interactions and then use a part of this dataset\nto train a model, and the remaining data to measure how closely the model\nrecommendations match the observed user interactions. This protocol is\nstraightforward, useful and practical, but it only captures performance of a\nparticular model trained at some point in the past. We know, however, that\nonline systems evolve over time. In general, it is a good idea that models\nreflect such changes, so models are frequently retrained with recent data. But\nif this is the case, to what extent can we trust previous evaluations? How will\na model perform when a different pattern (re)emerges? In this paper we propose\na methodology to study how recommendation models behave when they are\nretrained. The idea is to profile algorithms according to their ability to, on\nthe one hand, retain past patterns - stability - and, on the other hand,\n(quickly) adapt to changes - plasticity. We devise an offline evaluation\nprotocol that provides detail on the long-term behavior of models, and that is\nagnostic to datasets, algorithms and metrics. To illustrate the potential of\nthis framework, we present preliminary results of three different types of\nalgorithms on the GoodReads dataset that suggest different stability and\nplasticity profiles depending on the algorithmic technique, and a possible\ntrade-off between stability and plasticity. Although additional experiments\nwill be necessary to confirm these observations, they already illustrate the\nusefulness of the proposed framework to gain insights on the long term dynamics\nof recommendation models.",
    "published": "2025-08-05T22:15:43Z",
    "updated": "2025-10-16T09:33:12Z",
    "link": "http://arxiv.org/pdf/2508.03941v2.pdf",
    "category": [
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Maria Joo Lavoura",
      "Robert Jungnickel",
      "Joo Vinagre"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23202v2",
    "title": "Bridging the Gap Between Promise and Performance for Microscaling FP4\n  Quantization",
    "summary": "The recent hardware-accelerated microscaling 4-bit floating-point formats\nsuch as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to\nrevolutionize large language model (LLM) inference. Yet, their practical\nbenefits remain unproven. We present the first comprehensive study of MXFP4 and\nNVFP4 for post-training quantization, revealing gaps between their promise and\nreal-world performance. Our analysis shows that state-of-the-art methods\nstruggle with FP4, due to two key issues: (1) NVFP4's small group size provably\nneutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two\nscale quantization severely degrades accuracy due to high induced error. To\nbridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the\nclassic GPTQ quantization algorithm that tailors the quantization process to\nFP4's unique properties, by using block-wise Hadamard transforms and\nformat-specific optimizations. We support our proposal with a set of\nhigh-performance GPU kernels that enable the MR-GPTQ format with negligible\noverhead, by rotation fusion into the weights, and fast online computation of\nthe activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and\n2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on\nRTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches\nor outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the\npoint where it can near the accuracy that of NVFP4. We conclude that, while FP4\nis not an automatic upgrade over INT4, format-specialized methods like MR-GPTQ\ncan unlock a new frontier of accuracy-performance trade-offs.",
    "published": "2025-09-27T09:22:21Z",
    "updated": "2025-10-16T09:26:09Z",
    "link": "http://arxiv.org/pdf/2509.23202v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Vage Egiazarian",
      "Roberto L. Castro",
      "Denis Kuznedelev",
      "Andrei Panferov",
      "Eldar Kurtic",
      "Shubhra Pandit",
      "Alexandre Marques",
      "Mark Kurtz",
      "Saleh Ashkboos",
      "Torsten Hoefler",
      "Dan Alistarh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03782v2",
    "title": "Merge and Guide: Unifying Model Merging and Guided Decoding for\n  Controllable Multi-Objective Generation",
    "summary": "Adapting to diverse user needs at test time is a key challenge in\ncontrollable multi-objective generation. Existing methods are insufficient:\nmerging-based approaches provide indirect, suboptimal control at the parameter\nlevel, often disregarding the impacts of multiple objectives. While\ndecoding-based guidance is more direct, it typically requires aggregating\nlogits from multiple expert models, incurring significant space overhead and\nrelying heavily on individual model capacity. To address these issues, we\nintroduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model\nmerging for guided decoding. We first identify a critical compatibility problem\nbetween the guidance and base models. In Stage 1, MAGE resolves this by\ndynamically constructing a more robust base model, merging a series of backbone\nmodels that account for multiple objectives. In Stage 2, we merge explicit and\nimplicit value models into a unified guidance proxy, which then steers the\ndecoding of the base model from Stage 1. Our analysis empirically validates\nLinear Mode Connectivity (LMC) in value models, explores the relationship\nbetween model merging and prediction ensembling, and demonstrates the enhanced\ncontrollability afforded by our approach. Extensive experiments show that our\nmethod outperforms existing approaches, achieving superior controllability,\nPareto-optimal performance, and enhanced adaptability.",
    "published": "2025-10-04T11:10:07Z",
    "updated": "2025-10-16T09:11:22Z",
    "link": "http://arxiv.org/pdf/2510.03782v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Guofu Xie",
      "Chen Zhang",
      "Xiao Zhang",
      "Yunsheng Shi",
      "Ting Yao",
      "Jun Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07424v2",
    "title": "Best-of-Both Worlds for linear contextual bandits with paid observations",
    "summary": "We study the problem of linear contextual bandits with paid observations,\nwhere at each round the learner selects an action in order to minimize its loss\nin a given context, and can then decide to pay a fixed cost to observe the loss\nof any arm. Building on the Follow-the-Regularized-Leader framework with\nefficient estimators via Matrix Geometric Resampling, we introduce a\ncomputationally efficient Best-of-Both-Worlds (BOBW) algorithm for this\nproblem. We show that it achieves the minimax-optimal regret of\n$\\Theta(T^{2/3})$ in adversarial settings, while guaranteeing poly-logarithmic\nregret in (corrupted) stochastic regimes. Our approach builds on the framework\nfrom \\cite{BOBWhardproblems} to design BOBW algorithms for ``hard problem'',\nusing analysis techniques tailored for the setting that we consider.",
    "published": "2025-10-08T18:23:37Z",
    "updated": "2025-10-16T09:06:32Z",
    "link": "http://arxiv.org/pdf/2510.07424v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Nathan Boyer",
      "Dorian Baudry",
      "Patrick Rebeschini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14455v1",
    "title": "Coder as Editor: Code-driven Interpretable Molecular Optimization",
    "summary": "Molecular optimization is a central task in drug discovery that requires\nprecise structural reasoning and domain knowledge. While large language models\n(LLMs) have shown promise in generating high-level editing intentions in\nnatural language, they often struggle to faithfully execute these\nmodifications-particularly when operating on non-intuitive representations like\nSMILES. We introduce MECo, a framework that bridges reasoning and execution by\ntranslating editing actions into executable code. MECo reformulates molecular\noptimization for LLMs as a cascaded framework: generating human-interpretable\nediting intentions from a molecule and property goal, followed by translating\nthose intentions into executable structural edits via code generation. Our\napproach achieves over 98% accuracy in reproducing held-out realistic edits\nderived from chemical reactions and target-specific compound pairs. On\ndownstream optimization benchmarks spanning physicochemical properties and\ntarget activities, MECo substantially improves consistency by 38-86 percentage\npoints to 90%+ and achieves higher success rates over SMILES-based baselines\nwhile preserving structural similarity. By aligning intention with execution,\nMECo enables consistent, controllable and interpretable molecular design,\nlaying the foundation for high-fidelity feedback loops and collaborative\nhuman-AI workflows in drug discovery.",
    "published": "2025-10-16T08:55:06Z",
    "updated": "2025-10-16T08:55:06Z",
    "link": "http://arxiv.org/pdf/2510.14455v1.pdf",
    "category": [
      "cs.LG",
      "q-bio.BM"
    ],
    "authors": [
      "Wenyu Zhu",
      "Chengzhu Li",
      "Xiaohe Tian",
      "Yifan Wang",
      "Yinjun Jia",
      "Jianhui Wang",
      "Bowen Gao",
      "Ya-Qin Zhang",
      "Wei-Ying Ma",
      "Yanyan Lan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14445v1",
    "title": "Towards geological inference with process-based and deep generative\n  modeling, part 1: training on fluvial deposits",
    "summary": "The distribution of resources in the subsurface is deeply linked to the\nvariations of its physical properties. Generative modeling has long been used\nto predict those physical properties while quantifying the associated\nuncertainty. But current approaches struggle to properly reproduce geological\nstructures, and fluvial deposits in particular, because of their continuity.\nThis study explores whether a generative adversarial network (GAN) - a type of\ndeep-learning algorithm for generative modeling - can be trained to reproduce\nfluvial deposits simulated by a process-based model - a more expensive model\nthat mimics geological processes. An ablation study shows that developments\nfrom the deep-learning community to generate large 2D images are directly\ntransferable to 3D images of fluvial deposits. Training remains stable, and the\ngenerated samples reproduce the non-stationarity and details of the deposits\nwithout mode collapse or pure memorization of the training data. Using a\nprocess-based model to generate those training data allows us to include\nvaluable properties other than the usual physical properties. We show how the\ndeposition time let us monitor and validate the performance of a GAN by\nchecking that its samples honor the law of superposition. Our work joins a\nseries of previous studies suggesting that GANs are more robust that given\ncredit for, at least for training datasets targeting specific geological\nstructures. Whether this robustness transfers to larger 3D images and\nmultimodal datasets remains to be seen. Exploring how deep generative models\ncan leverage geological principles like the law of superposition shows a lot of\npromise.",
    "published": "2025-10-16T08:43:40Z",
    "updated": "2025-10-16T08:43:40Z",
    "link": "http://arxiv.org/pdf/2510.14445v1.pdf",
    "category": [
      "cs.LG",
      "physics.geo-ph",
      "I.2.6; I.6.3; J.2"
    ],
    "authors": [
      "Guillaume Rongier",
      "Luk Peeters"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14436v1",
    "title": "MergeMoE: Efficient Compression of MoE Models via Expert Output Merging",
    "summary": "The Mixture-of-Experts (MoE) technique has proven to be a promising solution\nto efficiently scale the model size, which has been widely applied in recent\nLLM advancements. However, the substantial memory overhead of MoE models has\nmade their compression an important research direction. In this work, we\nprovide a theoretical analysis of expert merging, a recently proposed technique\nfor compressing MoE models. Rather than interpreting expert merging from the\nconventional perspective of parameter aggregation, we approach it from the\nperspective of merging experts' outputs. Our key insight is that the merging\nprocess can be interpreted as inserting additional matrices into the forward\ncomputation, which naturally leads to an optimization formulation. Building on\nthis analysis, we introduce MergeMoE, a method that leverages mathematical\noptimization to construct the compression matrices. We evaluate MergeMoE on\nmultiple MoE models and show that our algorithm consistently outperforms the\nbaselines with the same compression ratios.",
    "published": "2025-10-16T08:36:40Z",
    "updated": "2025-10-16T08:36:40Z",
    "link": "http://arxiv.org/pdf/2510.14436v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ruijie Miao",
      "Yilun Yao",
      "Zihan Wang",
      "Zhiming Wang",
      "Bairen Yi",
      "LingJun Liu",
      "Yikai Zhao",
      "Tong Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.19941v2",
    "title": "Boosting Graph Foundation Model from Structural Perspective",
    "summary": "Graph foundation models have recently attracted significant attention due to\nits strong generalizability. Although existing methods resort to language\nmodels to learn unified semantic representations across domains, they disregard\nthe unique structural characteristics of graphs from different domains. To\naddress the problem, in this paper, we boost graph foundation model from\nstructural perspective and propose BooG. The model constructs virtual super\nnodes to unify structural characteristics of graph data from different domains.\nSpecifically, the super nodes fuse the information of anchor nodes and class\nlabels, where each anchor node captures the information of a node or a graph\ninstance to be classified. Instead of using the raw graph structure, we connect\nsuper nodes to all nodes within their neighborhood by virtual edges. This new\nstructure allows for effective information aggregation while unifying\ncross-domain structural characteristics. Additionally, we propose a novel\npre-training objective based on contrastive learning, which learns more\nexpressive representations for graph data and generalizes effectively to\ndifferent domains and downstream tasks. Experimental results on various\ndatasets and tasks demonstrate the superior performance of BooG. We provide our\ncode and data here: https://anonymous.4open.science/r/BooG-EE42/.",
    "published": "2024-07-29T12:22:16Z",
    "updated": "2025-10-16T08:27:03Z",
    "link": "http://arxiv.org/pdf/2407.19941v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yao Cheng",
      "Yige Zhao",
      "Jianxiang Yu",
      "Xiang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14419v1",
    "title": "Interaction Concordance Index: Performance Evaluation for Interaction\n  Prediction Methods",
    "summary": "Consider two sets of entities and their members' mutual affinity values, say\ndrug-target affinities (DTA). Drugs and targets are said to interact in their\neffects on DTAs if drug's effect on it depends on the target. Presence of\ninteraction implies that assigning a drug to a target and another drug to\nanother target does not provide the same aggregate DTA as the reversed\nassignment would provide. Accordingly, correctly capturing interactions enables\nbetter decision-making, for example, in allocation of limited numbers of drug\ndoses to their best matching targets. Learning to predict DTAs is popularly\ndone from either solely from known DTAs or together with side information on\nthe entities, such as chemical structures of drugs and targets. In this paper,\nwe introduce interaction directions' prediction performance estimator we call\ninteraction concordance index (IC-index), for both fixed predictors and machine\nlearning algorithms aimed for inferring them. IC-index complements the\npopularly used DTA prediction performance estimators by evaluating the ratio of\ncorrectly predicted directions of interaction effects in data. First, we show\nthe invariance of IC-index on predictors unable to capture interactions.\nSecondly, we show that learning algorithm's permutation equivariance regarding\ndrug and target identities implies its inability to capture interactions when\neither drug, target or both are unseen during training. In practical\napplications, this equivariance is remedied via incorporation of appropriate\nside information on drugs and targets. We make a comprehensive empirical\nevaluation over several biomedical interaction data sets with various\nstate-of-the-art machine learning algorithms. The experiments demonstrate how\ndifferent types of affinity strength prediction methods perform in terms of\nIC-index complementing existing prediction performance estimators.",
    "published": "2025-10-16T08:24:16Z",
    "updated": "2025-10-16T08:24:16Z",
    "link": "http://arxiv.org/pdf/2510.14419v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Tapio Pahikkala",
      "Riikka Numminen",
      "Parisa Movahedi",
      "Napsu Karmitsa",
      "Antti Airola"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14413v1",
    "title": "Personalized federated learning, Row-wise fusion regularization,\n  Multivariate modeling, Sparse estimation",
    "summary": "We study personalized federated learning for multivariate responses where\nclient models are heterogeneous yet share variable-level structure. Existing\nentry-wise penalties ignore cross-response dependence, while matrix-wise fusion\nover-couples clients. We propose a Sparse Row-wise Fusion (SROF) regularizer\nthat clusters row vectors across clients and induces within-row sparsity, and\nwe develop RowFed, a communication-efficient federated algorithm that embeds\nSROF into a linearized ADMM framework with privacy-preserving partial\nparticipation. Theoretically, we establish an oracle property for\nSROF-achieving correct variable-level group recovery with asymptotic\nnormality-and prove convergence of RowFed to a stationary solution. Under\nrandom client participation, the iterate gap contracts at a rate that improves\nwith participation probability. Empirically, simulations in heterogeneous\nregimes show that RowFed consistently lowers estimation and prediction error\nand strengthens variable-level cluster recovery over NonFed, FedAvg, and a\npersonalized matrix-fusion baseline. A real-data study further corroborates\nthese gains while preserving interpretability. Together, our results position\nrow-wise fusion as an effective and transparent paradigm for large-scale\npersonalized federated multivariate learning, bridging the gap between\nentry-wise and matrix-wise formulations.",
    "published": "2025-10-16T08:18:36Z",
    "updated": "2025-10-16T08:18:36Z",
    "link": "http://arxiv.org/pdf/2510.14413v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Runlin Zhou",
      "Letian Li",
      "Zemin Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14411v1",
    "title": "Revisit Modality Imbalance at the Decision Layer",
    "summary": "Multimodal learning integrates information from different modalities to\nenhance model performance, yet it often suffers from modality imbalance, where\ndominant modalities overshadow weaker ones during joint optimization. This\npaper reveals that such an imbalance not only occurs during representation\nlearning but also manifests significantly at the decision layer. Experiments on\naudio-visual datasets (CREMAD and Kinetic-Sounds) show that even after\nextensive pretraining and balanced optimization, models still exhibit\nsystematic bias toward certain modalities, such as audio. Further analysis\ndemonstrates that this bias originates from intrinsic disparities in\nfeature-space and decision-weight distributions rather than from optimization\ndynamics alone. We argue that aggregating uncalibrated modality outputs at the\nfusion stage leads to biased decision-layer weighting, hindering weaker\nmodalities from contributing effectively. To address this, we propose that\nfuture multimodal systems should focus more on incorporate adaptive weight\nallocation mechanisms at the decision layer, enabling relative balanced\naccording to the capabilities of each modality.",
    "published": "2025-10-16T08:11:24Z",
    "updated": "2025-10-16T08:11:24Z",
    "link": "http://arxiv.org/pdf/2510.14411v1.pdf",
    "category": [
      "cs.LG",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Xiaoyu Ma",
      "Hao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14393v1",
    "title": "Low Power Vision Transformer Accelerator with Hardware-Aware Pruning and\n  Optimized Dataflow",
    "summary": "Current transformer accelerators primarily focus on optimizing self-attention\ndue to its quadratic complexity. However, this focus is less relevant for\nvision transformers with short token lengths, where the Feed-Forward Network\n(FFN) tends to be the dominant computational bottleneck. This paper presents a\nlow power Vision Transformer accelerator, optimized through algorithm-hardware\nco-design. The model complexity is reduced using hardware-friendly dynamic\ntoken pruning without introducing complex mechanisms. Sparsity is further\nimproved by replacing GELU with ReLU activations and employing dynamic FFN2\npruning, achieving a 61.5\\% reduction in operations and a 59.3\\% reduction in\nFFN2 weights, with an accuracy loss of less than 2\\%. The hardware adopts a\nrow-wise dataflow with output-oriented data access to eliminate data\ntransposition, and supports dynamic operations with minimal area overhead.\nImplemented in TSMC's 28nm CMOS technology, our design occupies 496.4K gates\nand includes a 232KB SRAM buffer, achieving a peak throughput of 1024 GOPS at\n1GHz, with an energy efficiency of 2.31 TOPS/W and an area efficiency of 858.61\nGOPS/mm2.",
    "published": "2025-10-16T07:44:42Z",
    "updated": "2025-10-16T07:44:42Z",
    "link": "http://arxiv.org/pdf/2510.14393v1.pdf",
    "category": [
      "cs.AR",
      "cs.LG"
    ],
    "authors": [
      "Ching-Lin Hsiung",
      "Tian-Sheuan Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14386v1",
    "title": "SHaRe-SSM: An Oscillatory Spiking Neural Network for Target Variable\n  Modeling in Long Sequences",
    "summary": "In recent years, with the emergence of large models, there has been a\nsignificant interest in spiking neural networks (SNNs) primarily due to their\nenergy efficiency, multiplication-free, and sparse event-based deep learning.\nSimilarly, state space models (SSMs) in varying designs have evolved as a\npowerful alternative to transformers for target modeling in long sequences,\nthereby overcoming the quadratic dependence on sequence length of a\ntransformer. Inspired by this progress, we here design SHaRe-SSM (Spiking\nHarmonic Resonate and Fire State Space Model), for target variable modeling\n(including both classification and regression) for very-long-range sequences.\nOur second-order spiking SSM, on average, performs better than transformers or\nfirst-order SSMs while circumventing multiplication operations, making it ideal\nfor resource-constrained applications. The proposed block consumes $73 \\times$\nless energy than second-order ANN-based SSMs for an 18k sequence, while\nretaining performance. To ensure learnability over the long-range sequences, we\npropose exploiting the stable and efficient implementation of the dynamical\nsystem using parallel scans. Moreover, for the first time, we propose a\nkernel-based spiking regressor using resonate and fire neurons for very\nlong-range sequences. Our network shows superior performance on even a 50k\nsequence while being significantly energy-efficient. In addition, we conducted\na systematic analysis of the impact of heterogeneity, dissipation, and\nconservation in resonate-and-fire SSMs.",
    "published": "2025-10-16T07:37:59Z",
    "updated": "2025-10-16T07:37:59Z",
    "link": "http://arxiv.org/pdf/2510.14386v1.pdf",
    "category": [
      "cs.LG",
      "cs.NE"
    ],
    "authors": [
      "Kartikay Agrawal",
      "Abhijeet Vikram",
      "Vedant Sharma",
      "Vaishnavi N.",
      "Ayon Borthakur"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14377v1",
    "title": "PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora",
    "summary": "Recent advances in large language models (LLMs) and retrieval-augmented\ngeneration (RAG) have enabled progress on question answering (QA) when relevant\nevidence is in one (single-hop) or multiple (multi-hop) passages. Yet many\nrealistic questions about recurring report data - medical records, compliance\nfilings, maintenance logs - require aggregation across all documents, with no\nclear stopping point for retrieval and high sensitivity to even one missed\npassage. We term these pluri-hop questions and formalize them by three\ncriteria: recall sensitivity, exhaustiveness, and exactness. To study this\nsetting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48\npluri-hop questions built from 191 real-world wind industry reports in German\nand English. We show that PluriHopWIND is 8-40% more repetitive than other\ncommon datasets and thus has higher density of distractor documents, better\nreflecting practical challenges of recurring report corpora. We test a\ntraditional RAG pipeline as well as graph-based and multimodal variants, and\nfind that none of the tested approaches exceed 40% in statement-wise F1 score.\nMotivated by this, we propose PluriHopRAG, a RAG architecture that follows a\n\"check all documents individually, filter cheaply\" approach: it (i) decomposes\nqueries into document-level subquestions and (ii) uses a cross-encoder filter\nto discard irrelevant documents before costly LLM reasoning. We find that\nPluriHopRAG achieves relative F1 score improvements of 18-52% depending on base\nLLM. Despite its modest size, PluriHopWIND exposes the limitations of current\nQA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance\nhighlights the value of exhaustive retrieval and early filtering as a powerful\nalternative to top-k methods.",
    "published": "2025-10-16T07:22:58Z",
    "updated": "2025-10-16T07:22:58Z",
    "link": "http://arxiv.org/pdf/2510.14377v1.pdf",
    "category": [
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Mykolas Sveistrys",
      "Richard Kunert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.09649v3",
    "title": "Technical and legal aspects of federated learning in bioinformatics:\n  applications, challenges and opportunities",
    "summary": "Federated learning leverages data across institutions to improve clinical\ndiscovery while complying with data-sharing restrictions and protecting patient\nprivacy. This paper provides a gentle introduction to this approach in\nbioinformatics, and is the first to review key applications in proteomics,\ngenome-wide association studies (GWAS), single-cell and multi-omics studies in\ntheir legal as well as methodological and infrastructural challenges. As the\nevolution of biobanks in genetics and systems biology has proved, accessing\nmore extensive and varied data pools leads to a faster and more robust\nexploration and translation of results. More widespread use of federated\nlearning may have a similar impact in bioinformatics, allowing academic and\nclinical institutions to access many combinations of genotypic, phenotypic and\nenvironmental information that are undercovered or not included in existing\nbiobanks.",
    "published": "2025-03-12T08:45:31Z",
    "updated": "2025-10-16T07:04:38Z",
    "link": "http://arxiv.org/pdf/2503.09649v3.pdf",
    "category": [
      "q-bio.OT",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Daniele Malpetti",
      "Marco Scutari",
      "Francesco Gualdi",
      "Jessica van Setten",
      "Sander van der Laan",
      "Saskia Haitjema",
      "Aaron Mark Lee",
      "Isabelle Hering",
      "Francesca Mangili"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14342v1",
    "title": "Jet Functors and Weil Algebras in Automatic Differentiation: A Geometric\n  Analysis",
    "summary": "We present a geometric formulation of automatic differentiation (AD) using\njet bundles and Weil algebras. Reverse-mode AD emerges as cotangent-pullback,\nwhile Taylor-mode corresponds to evaluation in a Weil algebra. From these\nprinciples, we derive concise statements on correctness, stability, and\ncomplexity: a functorial identity for reverse-mode, algebraic exactness of\nhigher-order derivatives, and explicit bounds on truncation error. We further\nshow that tensorized Weil algebras permit one-pass computation of all mixed\nderivatives with cost linear in the algebra dimension, avoiding the\ncombinatorial blow-up of nested JVP/VJP schedules. This framework interprets AD\ntheory through the lens of differential geometry and offers a foundation for\ndeveloping structure-preserving differentiation methods in deep learning and\nscientific computing. Code and examples are available at\nhttps://git.nilu.no/geometric-ad/jet-weil-ad.",
    "published": "2025-10-16T06:25:24Z",
    "updated": "2025-10-16T06:25:24Z",
    "link": "http://arxiv.org/pdf/2510.14342v1.pdf",
    "category": [
      "cs.LG",
      "math.DG",
      "stat.ML"
    ],
    "authors": [
      "Amandip Sangha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14336v1",
    "title": "DARTS-GT: Differentiable Architecture Search for Graph Transformers with\n  Quantifiable Instance-Specific Interpretability Analysis",
    "summary": "Graph Transformers (GTs) have emerged as powerful architectures for\ngraph-structured data, yet remain constrained by rigid designs and lack\nquantifiable interpretability. Current state-of-the-art GTs commit to fixed GNN\ntypes across all layers, missing potential benefits of depth-specific component\nselection, while their complex architectures become opaque where performance\ngains cannot be distinguished between meaningful patterns and spurious\ncorrelations. We redesign GT attention through asymmetry, decoupling structural\nencoding from feature representation: queries derive from node features while\nkeys and values come from GNN transformations. Within this framework, we use\nDifferentiable ARchiTecture Search (DARTS) to select optimal GNN operators at\neach layer, enabling depth-wise heterogeneity inside transformer attention\nitself (DARTS-GT). To understand discovered architectures, we develop the first\nquantitative interpretability framework for GTs through causal ablation. Our\nmetrics (Head-deviation, Specialization, and Focus), identify which heads and\nnodes drive predictions while enabling model comparison. Experiments across\neight benchmarks show DARTS-GT achieves state-of-the-art on four datasets while\nremaining competitive on others, with discovered architectures revealing\ndataset-specific patterns. Our interpretability analysis reveals that visual\nattention salience and causal importance do not always correlate, indicating\nwidely used visualization approaches may miss components that actually matter.\nCrucially, heterogeneous architectures found by DARTS-GT consistently produced\nmore interpretable models than baselines, establishing that Graph Transformers\nneed not choose between performance and interpretability.",
    "published": "2025-10-16T06:15:42Z",
    "updated": "2025-10-16T06:15:42Z",
    "link": "http://arxiv.org/pdf/2510.14336v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shruti Sarika Chakraborty",
      "Peter Minary"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14331v1",
    "title": "LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search",
    "summary": "We seek algorithms for program learning that are both sample-efficient and\ncomputationally feasible. Classical results show that targets admitting short\nprogram descriptions (e.g., with short ``python code'') can be learned with a\n``small'' number of examples (scaling with the size of the code) via\nlength-first program enumeration, but the search is exponential in description\nlength. Consequently, Gradient-based training avoids this cost yet can require\nexponentially many samples on certain short-program families.\n  To address this gap, we introduce LLM-ERM, a propose-and-verify framework\nthat replaces exhaustive enumeration with an LLM-guided search over candidate\nprograms while retaining ERM-style selection on held-out data. Specifically, we\ndraw $k$ candidates with a pretrained reasoning-augmented LLM, compile and\ncheck each on the data, and return the best verified hypothesis, with no\nfeedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise\nonline mini-batch SGD requires many samples to learn certain short programs.\n{\\em Empirically, LLM-ERM solves tasks such as parity variants, pattern\nmatching, and primality testing with as few as 200 samples, while SGD-trained\ntransformers overfit even with 100,000 samples}. These results indicate that\nlanguage-guided program synthesis recovers much of the statistical efficiency\nof finite-class ERM while remaining computationally tractable, offering a\npractical route to learning succinct hypotheses beyond the reach of\ngradient-based training.",
    "published": "2025-10-16T06:10:11Z",
    "updated": "2025-10-16T06:10:11Z",
    "link": "http://arxiv.org/pdf/2510.14331v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shivam Singhal",
      "Eran Malach",
      "Tomaso Poggio",
      "Tomer Galanti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14315v1",
    "title": "Active Measuring in Reinforcement Learning With Delayed Negative Effects",
    "summary": "Measuring states in reinforcement learning (RL) can be costly in real-world\nsettings and may negatively influence future outcomes. We introduce the\nActively Observable Markov Decision Process (AOMDP), where an agent not only\nselects control actions but also decides whether to measure the latent state.\nThe measurement action reveals the true latent state but may have a negative\ndelayed effect on the environment. We show that this reduced uncertainty may\nprovably improve sample efficiency and increase the value of the optimal policy\ndespite these costs. We formulate an AOMDP as a periodic partially observable\nMDP and propose an online RL algorithm based on belief states. To approximate\nthe belief states, we further propose a sequential Monte Carlo method to\njointly approximate the posterior of unknown static environment parameters and\nunobserved latent states. We evaluate the proposed algorithm in a digital\nhealth application, where the agent decides when to deliver digital\ninterventions and when to assess users' health status through surveys.",
    "published": "2025-10-16T05:21:36Z",
    "updated": "2025-10-16T05:21:36Z",
    "link": "http://arxiv.org/pdf/2510.14315v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Daiqi Gao",
      "Ziping Xu",
      "Aseel Rawashdeh",
      "Predrag Klasnja",
      "Susan A. Murphy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.09383v3",
    "title": "Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based\n  Diffusion and Potential Fields",
    "summary": "Motivated by the problem of pursuit-evasion, we present a motion planning\nframework that combines energy-based diffusion models with artificial potential\nfields for robust real time trajectory generation in complex environments. Our\napproach processes obstacle information directly from point clouds, enabling\nefficient planning without requiring complete geometric representations. The\nframework employs classifier-free guidance training and integrates local\npotential fields during sampling to enhance obstacle avoidance. In dynamic\nscenarios, the system generates initial trajectories using the diffusion model\nand continuously refines them through potential field-based adaptation,\ndemonstrating effective performance in pursuit-evasion scenarios with partial\npursuer observability.",
    "published": "2025-07-12T19:42:07Z",
    "updated": "2025-10-16T05:13:04Z",
    "link": "http://arxiv.org/pdf/2507.09383v3.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Wondmgezahu Teshome",
      "Kian Behzad",
      "Octavia Camps",
      "Michael Everett",
      "Milad Siami",
      "Mario Sznaier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.18962v2",
    "title": "Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data\n  Bootstrapping",
    "summary": "Modern foundation models often undergo iterative ``bootstrapping'' in their\npost-training phase: a model generates synthetic data, an external verifier\nfilters out low-quality samples, and the high-quality subset is used for\nfurther fine-tuning. Over multiple iterations, the model performance improves,\nraising a crucial question: How should the total budget for generation and\ntraining be allocated across iterations to maximize final performance? In this\nwork, we develop a theoretical framework for analyzing budget allocation\nstrategies. Specifically, we show that constant policies fail to converge with\nhigh probability, while increasing policies -- particularly exponential growth\npolicies -- exhibit significant theoretical advantages. Experiments on image\ndenoising with diffusion probabilistic models and math reasoning with large\nlanguage models show that both exponential and polynomial growth policies\nconsistently outperform constant policies, with exponential policies often\nproviding more stable performance.",
    "published": "2025-01-31T08:47:17Z",
    "updated": "2025-10-16T05:12:13Z",
    "link": "http://arxiv.org/pdf/2501.18962v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Pu Yang",
      "Yunzhen Feng",
      "Ziyuan Chen",
      "Yuhang Wu",
      "Zhuoyuan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.14225v2",
    "title": "Defending Diffusion Models Against Membership Inference Attacks via\n  Higher-Order Langevin Dynamics",
    "summary": "Recent advances in generative artificial intelligence applications have\nraised new data security concerns. This paper focuses on defending diffusion\nmodels against membership inference attacks. This type of attack occurs when\nthe attacker can determine if a certain data point was used to train the model.\nAlthough diffusion models are intrinsically more resistant to membership\ninference attacks than other generative models, they are still susceptible. The\ndefense proposed here utilizes critically-damped higher-order Langevin\ndynamics, which introduces several auxiliary variables and a joint diffusion\nprocess along these variables. The idea is that the presence of auxiliary\nvariables mixes external randomness that helps to corrupt sensitive input data\nearlier on in the diffusion process. This concept is theoretically investigated\nand validated on a toy dataset and a speech dataset using the Area Under the\nReceiver Operating Characteristic (AUROC) curves and the FID metric.",
    "published": "2025-09-17T17:56:20Z",
    "updated": "2025-10-16T05:00:29Z",
    "link": "http://arxiv.org/pdf/2509.14225v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Benjamin Sterling",
      "Yousef El-Laham",
      "Mnica F. Bugallo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14303v1",
    "title": "Constraint-Driven Small Language Models Based on Agent and OpenAlex\n  Knowledge Graph: Mining Conceptual Pathways and Discovering Innovation Points\n  in Academic Papers",
    "summary": "In recent years, the rapid increase in academic publications across various\nfields has posed severe challenges for academic paper analysis: scientists\nstruggle to timely and comprehensively track the latest research findings and\nmethodologies. Key concept extraction has proven to be an effective analytical\nparadigm, and its automation has been achieved with the widespread application\nof language models in industrial and scientific domains. However, existing\npaper databases are mostly limited to similarity matching and basic\nclassification of key concepts, failing to deeply explore the relational\nnetworks between concepts. This paper is based on the OpenAlex opensource\nknowledge graph. By analyzing nearly 8,000 open-source paper data from\nNovosibirsk State University, we discovered a strong correlation between the\ndistribution patterns of paper key concept paths and both innovation points and\nrare paths. We propose a prompt engineering-based key concept path analysis\nmethod. This method leverages small language models to achieve precise key\nconcept extraction and innovation point identification, and constructs an agent\nbased on a knowledge graph constraint mechanism to enhance analysis accuracy.\nThrough fine-tuning of the Qwen and DeepSeek models, we achieved significant\nimprovements in accuracy, with the models publicly available on the Hugging\nFace platform.",
    "published": "2025-10-16T04:58:28Z",
    "updated": "2025-10-16T04:58:28Z",
    "link": "http://arxiv.org/pdf/2510.14303v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG",
      "I.2.7"
    ],
    "authors": [
      "Ziye Xia",
      "Sergei S. Ospichev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.02870v2",
    "title": "Uncertainty Quantification with the Empirical Neural Tangent Kernel",
    "summary": "While neural networks have demonstrated impressive performance across various\ntasks, accurately quantifying uncertainty in their predictions is essential to\nensure their trustworthiness and enable widespread adoption in critical\nsystems. Several Bayesian uncertainty quantification (UQ) methods exist that\nare either cheap or reliable, but not both. We propose a post-hoc,\nsampling-based UQ method for over-parameterized networks at the end of\ntraining. Our approach constructs efficient and meaningful deep ensembles by\nemploying a (stochastic) gradient-descent sampling process on appropriately\nlinearized networks. We demonstrate that our method effectively approximates\nthe posterior of a Gaussian process using the empirical Neural Tangent Kernel.\nThrough a series of numerical experiments, we show that our method not only\noutperforms competing approaches in computational efficiency-often reducing\ncosts by multiple factors-but also maintains state-of-the-art performance\nacross a variety of UQ metrics for both regression and classification tasks.",
    "published": "2025-02-05T04:01:34Z",
    "updated": "2025-10-16T04:54:00Z",
    "link": "http://arxiv.org/pdf/2502.02870v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Joseph Wilson",
      "Chris van der Heide",
      "Liam Hodgkinson",
      "Fred Roosta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.02685v3",
    "title": "Symmetry-Aware GFlowNets",
    "summary": "Generative Flow Networks (GFlowNets) offer a powerful framework for sampling\ngraphs in proportion to their rewards. However, existing approaches suffer from\nsystematic biases due to inaccuracies in state transition probability\ncomputations. These biases, rooted in the inherent symmetries of graphs, impact\nboth atom-based and fragment-based generation schemes. To address this\nchallenge, we introduce Symmetry-Aware GFlowNets (SA-GFN), a method that\nincorporates symmetry corrections into the learning process through reward\nscaling. By integrating bias correction directly into the reward structure,\nSA-GFN eliminates the need for explicit state transition computations.\nEmpirical results show that SA-GFN enables unbiased sampling while enhancing\ndiversity and consistently generating high-reward graphs that closely match the\ntarget distribution.",
    "published": "2025-06-03T09:38:15Z",
    "updated": "2025-10-16T04:42:38Z",
    "link": "http://arxiv.org/pdf/2506.02685v3.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Hohyun Kim",
      "Seunggeun Lee",
      "Min-hwan Oh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03027v2",
    "title": "Lightweight Transformer for EEG Classification via Balanced Signed Graph\n  Algorithm Unrolling",
    "summary": "Samples of brain signals collected by EEG sensors have inherent\nanti-correlations that are well modeled by negative edges in a finite graph. To\ndifferentiate epilepsy patients from healthy subjects using collected EEG\nsignals, we build lightweight and interpretable transformer-like neural nets by\nunrolling a spectral denoising algorithm for signals on a balanced signed graph\n-- graph with no cycles of odd number of negative edges. A balanced signed\ngraph has well-defined frequencies that map to a corresponding positive graph\nvia similarity transform of the graph Laplacian matrices. We implement an ideal\nlow-pass filter efficiently on the mapped positive graph via Lanczos\napproximation, where the optimal cutoff frequency is learned from data. Given\nthat two balanced signed graph denoisers learn posterior probabilities of two\ndifferent signal classes during training, we evaluate their reconstruction\nerrors for binary classification of EEG signals. Experiments show that our\nmethod achieves classification performance comparable to representative deep\nlearning schemes, while employing dramatically fewer parameters.",
    "published": "2025-10-03T14:08:42Z",
    "updated": "2025-10-16T04:34:31Z",
    "link": "http://arxiv.org/pdf/2510.03027v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Junyi Yao",
      "Parham Eftekhar",
      "Gene Cheung",
      "Xujin Chris Liu",
      "Yao Wang",
      "Wei Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14287v1",
    "title": "Enhancing Time-Series Anomaly Detection by Integrating Spectral-Residual\n  Bottom-Up Attention with Reservoir Computing",
    "summary": "Reservoir computing (RC) establishes the basis for the processing of\ntime-series data by exploiting the high-dimensional spatiotemporal response of\na recurrent neural network to an input signal. In particular, RC trains only\nthe output layer weights. This simplicity has drawn attention especially in\nEdge Artificial Intelligence (AI) applications. Edge AI enables time-series\nanomaly detection in real time, which is important because detection delays can\nlead to serious incidents. However, achieving adequate anomaly-detection\nperformance with RC alone may require an unacceptably large reservoir on\nresource-constrained edge devices. Without enlarging the reservoir, attention\nmechanisms can improve accuracy, although they may require substantial\ncomputation and undermine the learning efficiency of RC. In this study, to\nimprove the anomaly detection performance of RC without sacrificing learning\nefficiency, we propose a spectral residual RC (SR-RC) that integrates the\nspectral residual (SR) method - a learning-free, bottom-up attention mechanism\n- with RC. We demonstrated that SR-RC outperformed conventional RC and\nlogistic-regression models based on values extracted by the SR method across\nbenchmark tasks and real-world time-series datasets. Moreover, because the SR\nmethod, similarly to RC, is well suited for hardware implementation, SR-RC\nsuggests a practical direction for deploying RC as Edge AI for time-series\nanomaly detection.",
    "published": "2025-10-16T04:17:30Z",
    "updated": "2025-10-16T04:17:30Z",
    "link": "http://arxiv.org/pdf/2510.14287v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hayato Nihei",
      "Sou Nobukawa",
      "Yusuke Sakemi",
      "Kazuyuki Aihara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14286v1",
    "title": "Stable Prediction of Adverse Events in Medical Time-Series Data",
    "summary": "Early event prediction (EEP) systems continuously estimate a patient's\nimminent risk to support clinical decision-making. For bedside trust, risk\ntrajectories must be accurate and temporally stable, shifting only with new,\nrelevant evidence. However, current benchmarks (a) ignore stability of risk\nscores and (b) evaluate mainly on tabular inputs, leaving trajectory behavior\nuntested. To address this gap, we introduce CAREBench, an EEP benchmark that\nevaluates deployability using multi-modal inputs-tabular EHR, ECG waveforms,\nand clinical text-and assesses temporal stability alongside predictive\naccuracy. We propose a stability metric that quantifies short-term variability\nin per-patient risk and penalizes abrupt oscillations based on local-Lipschitz\nconstants. CAREBench spans six prediction tasks such as sepsis onset and\ncompares classical learners, deep sequence models, and zero-shot LLMs. Across\ntasks, existing methods, especially LLMs, struggle to jointly optimize accuracy\nand stability, with notably poor recall at high-precision operating points.\nThese results highlight the need for models that produce evidence-aligned,\nstable trajectories to earn clinician trust in continuous monitoring settings.\n(Code: https://github.com/SeewonChoi/CAREBench.)",
    "published": "2025-10-16T04:16:54Z",
    "updated": "2025-10-16T04:16:54Z",
    "link": "http://arxiv.org/pdf/2510.14286v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Mayank Keoliya",
      "Seewon Choi",
      "Rajeev Alur",
      "Mayur Naik",
      "Eric Wong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14691v1",
    "title": "If You Hold Me Without Hurting Me: Pathways to Designing Game Audio for\n  Healthy Escapism and Player Well-being",
    "summary": "Escapism in games can support recovery or lead to harmful avoidance.\nSelf-regulation, understood as combining autonomy with positive outcomes, is\nkey to this distinction. We argue that audio, often overlooked, plays a central\nrole in regulation. It can modulate arousal, mark transitions, and provide\nclosure, yet its contribution to well-being remains underexplored. This paper\nidentifies methodological and accessibility gaps that limit recognition of\naudio's potential and outlines ways to address them. We aim to encourage\nresearchers and developers to integrate audio more deliberately into the design\nand study of healthier escapist play.",
    "published": "2025-10-16T13:57:45Z",
    "updated": "2025-10-16T13:57:45Z",
    "link": "http://arxiv.org/pdf/2510.14691v1.pdf",
    "category": [
      "cs.HC",
      "cs.MM",
      "cs.SD",
      "H.5.5; H.5.2; J.5"
    ],
    "authors": [
      "Caio Nunes",
      "Bosco Borges",
      "Georgia Cruz",
      "Ticianne Darin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14645v1",
    "title": "Block-Partitioning Strategies for Accelerated Multi-rate Encoding in\n  Adaptive VVC Streaming",
    "summary": "The demand for efficient multi-rate encoding techniques has surged with the\nincreasing prevalence of ultra-high-definition (UHD) video content,\nparticularly in adaptive streaming scenarios where a single video must be\nencoded at multiple bitrates to accommodate diverse network conditions. While\nVersatile Video Coding (VVC) significantly improves compression efficiency, it\nintroduces considerable computational complexity, making multi-rate encoding a\nresource-intensive task. This paper examines coding unit (CU) partitioning\nstrategies to minimize redundant computations in VVC while preserving high\nvideo quality. We propose single- and double-bound approaches, leveraging CU\ndepth constraints from reference encodes to guide dependent encodes across\nmultiple QPs. These methods are evaluated using VVenC with various presets,\ndemonstrating consistent improvements in encoding efficiency. Our methods\nachieve up to 11.69 % reduction in encoding time with minimal bitrate overhead\n(<0.6 %). Comparative Pareto-front (PF) analysis highlights the superior\nperformance of multi-rate approaches over existing configurations. These\nfindings validate the potential of CU-guided strategies for scalable multi-rate\nencoding in adaptive streaming.",
    "published": "2025-10-16T12:57:42Z",
    "updated": "2025-10-16T12:57:42Z",
    "link": "http://arxiv.org/pdf/2510.14645v1.pdf",
    "category": [
      "cs.MM"
    ],
    "authors": [
      "Vignesh V Menon",
      "Adam Wieckowski",
      "Yiquin Liu",
      "Benjamin Bross",
      "Detlev Marpe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23852v3",
    "title": "SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation\n  Involving How, When and Where",
    "summary": "The accompanying actions and gestures in dialogue are often closely linked to\ninteractions with the environment, such as looking toward the interlocutor or\nusing gestures to point to the described target at appropriate moments. Speech\nand semantics guide the production of gestures by determining their timing\n(WHEN) and style (HOW), while the spatial locations of interactive objects\ndictate their directional execution (WHERE). Existing approaches either rely\nsolely on descriptive language to generate motions or utilize audio to produce\nnon-interactive gestures, thereby lacking the characterization of interactive\ntiming and spatial intent. This significantly limits the applicability of\nconversational gesture generation, whether in robotics or in the fields of game\nand animation production. To address this gap, we present a full-stack\nsolution. We first established a unique data collection method to\nsimultaneously capture high-precision human motion and spatial intent. We then\ndeveloped a generation model driven by audio, language, and spatial data,\nalongside dedicated metrics for evaluating interaction timing and spatial\naccuracy. Finally, we deployed the solution on a humanoid robot, enabling rich,\ncontext-aware physical interactions.",
    "published": "2025-09-28T12:43:09Z",
    "updated": "2025-10-16T05:15:30Z",
    "link": "http://arxiv.org/pdf/2509.23852v3.pdf",
    "category": [
      "cs.GR",
      "cs.MM",
      "cs.RO"
    ],
    "authors": [
      "Yiheng Huang",
      "Junran Peng",
      "Silei Shen",
      "Jingwei Yang",
      "ZeJi Wei",
      "ChenCheng Bai",
      "Yonghao He",
      "Wei Sui",
      "Muyi Sun",
      "Yan Liu",
      "Xu-Cheng Yin",
      "Man Zhang",
      "Zhaoxiang Zhang",
      "Chuanchen Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14189v1",
    "title": "360CityGML: Realistic and Interactive Urban Visualization System\n  Integrating CityGML Model and 360 Videos",
    "summary": "We introduce a novel urban visualization system that integrates 3D urban\nmodel (CityGML) and 360{\\deg} walkthrough videos. By aligning the videos with\nthe model and dynamically projecting relevant video frames onto the geometries,\nour system creates photorealistic urban visualizations, allowing users to\nintuitively interpret geospatial data from a pedestrian view.",
    "published": "2025-10-16T00:37:46Z",
    "updated": "2025-10-16T00:37:46Z",
    "link": "http://arxiv.org/pdf/2510.14189v1.pdf",
    "category": [
      "cs.MM"
    ],
    "authors": [
      "Tatsuro Banno",
      "Mizuki Takenawa",
      "Leslie Whler",
      "Satoshi Ikehata",
      "Kiyoharu Aizawa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13794v2",
    "title": "MimicKit: A Reinforcement Learning Framework for Motion Imitation and\n  Control",
    "summary": "MimicKit is an open-source framework for training motion controllers using\nmotion imitation and reinforcement learning. The codebase provides\nimplementations of commonly-used motion-imitation techniques and RL algorithms.\nThis framework is intended to support research and applications in computer\ngraphics and robotics by providing a unified training framework, along with\nstandardized environment, agent, and data structures. The codebase is designed\nto be modular and easily configurable, enabling convenient modification and\nextension to new characters and tasks. The open-source codebase is available\nat: https://github.com/xbpeng/MimicKit.",
    "published": "2025-10-15T17:51:42Z",
    "updated": "2025-10-16T02:41:08Z",
    "link": "http://arxiv.org/pdf/2510.13794v2.pdf",
    "category": [
      "cs.GR",
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Xue Bin Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14946v1",
    "title": "EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge\n  Devices",
    "summary": "Deployment of efficient and accurate Deep Learning models has long been a\nchallenge in autonomous navigation, particularly for real-time applications on\nresource-constrained edge devices. Edge devices are limited in computing power\nand memory, making model efficiency and compression essential. In this work, we\npropose EdgeNavMamba, a reinforcement learning-based framework for\ngoal-directed navigation using an efficient Mamba object detection model. To\ntrain and evaluate the detector, we introduce a custom shape detection dataset\ncollected in diverse indoor settings, reflecting visual cues common in\nreal-world navigation. The object detector serves as a pre-processing module,\nextracting bounding boxes (BBOX) from visual input, which are then passed to an\nRL policy to control goal-oriented navigation. Experimental results show that\nthe student model achieved a reduction of 67% in size, and up to 73% in energy\nper inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,\nwhile keeping the same performance as the teacher model. EdgeNavMamba also\nmaintains high detection accuracy in MiniWorld and IsaacLab simulators while\nreducing parameters by 31% compared to the baseline. In the MiniWorld\nsimulator, the navigation policy achieves over 90% success across environments\nof varying complexity.",
    "published": "2025-10-16T17:55:56Z",
    "updated": "2025-10-16T17:55:56Z",
    "link": "http://arxiv.org/pdf/2510.14946v1.pdf",
    "category": [
      "eess.IV",
      "cs.RO"
    ],
    "authors": [
      "Romina Aalishah",
      "Mozhgan Navardi",
      "Tinoosh Mohsenin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25822v3",
    "title": "Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for\n  Adaptive Policies",
    "summary": "Existing imitation learning methods decouple perception and action, which\noverlooks the causal reciprocity between sensory representations and action\nexecution that humans naturally leverage for adaptive behaviors. To bridge this\ngap, we introduce Action-Guided Diffusion Policy (DP-AG), a unified\nrepresentation learning that explicitly models a dynamic interplay between\nperception and action through probabilistic latent dynamics. DP-AG encodes\nlatent observations into a Gaussian posterior via variational inference and\nevolves them using an action-guided SDE, where the Vector-Jacobian Product\n(VJP) of the diffusion policy's noise predictions serves as a structured\nstochastic force driving latent updates. To promote bidirectional learning\nbetween perception and action, we introduce a cycle-consistent contrastive loss\nthat organizes the gradient flow of the noise predictor into a coherent\nperception-action loop, enforcing mutually consistent transitions in both\nlatent updates and action refinements. Theoretically, we derive a variational\nlower bound for the action-guided SDE, and prove that the contrastive objective\nenhances continuity in both latent and action trajectories. Empirically, DP-AG\nsignificantly outperforms state-of-the-art methods across simulation benchmarks\nand real-world UR5 manipulation tasks. As a result, our DP-AG offers a\npromising step toward bridging biological adaptability and artificial policy\nlearning.",
    "published": "2025-09-30T05:56:00Z",
    "updated": "2025-10-16T17:37:59Z",
    "link": "http://arxiv.org/pdf/2509.25822v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jing Wang",
      "Weiting Peng",
      "Jing Tang",
      "Zeyu Gong",
      "Xihua Wang",
      "Bo Tao",
      "Li Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14914v1",
    "title": "Design of Paper Robot Building Kits",
    "summary": "Building robots is an engaging activity that provides opportunities for\nhands-on learning. However, traditional robot-building kits are usually costly\nwith limited functionality due to material and technology constraints. To\nimprove the accessibility and flexibility of such kits, we take paper as the\nbuilding material and extensively explore the versatility of paper-based\ninteractions. Based on an analysis of current robot-building kits and\npaper-based interaction research, we propose a design space for devising paper\nrobots. We also analyzed our building kit designs using this design space,\nwhere these kits demonstrate the potential of paper as a cost-effective\nmaterial for robot building. As a starting point, our design space and building\nkit examples provide a guideline that inspires and informs future research and\ndevelopment of novel paper robot-building kits.",
    "published": "2025-10-16T17:30:11Z",
    "updated": "2025-10-16T17:30:11Z",
    "link": "http://arxiv.org/pdf/2510.14914v1.pdf",
    "category": [
      "cs.HC",
      "cs.RO"
    ],
    "authors": [
      "Ruhan Yang",
      "Ellen Yi-Luen Do"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.21180v2",
    "title": "STITCHER: Real-Time Trajectory Planning with Motion Primitive Search",
    "summary": "Autonomous high-speed navigation through large, complex environments requires\nreal-time generation of agile trajectories that are dynamically feasible,\ncollision-free, and satisfy constraints. Most modern trajectory planning\ntechniques rely on numerical optimization because high-quality, expressive\ntrajectories that satisfy constraints can be systematically computed. However,\nstrict requirements on computation time and the risk of numerical instability\ncan limit the use of optimization-based planners in safety-critical situations.\nThis work presents an optimization-free planning framework called STITCHER that\nleverages graph search to generate long-range trajectories by stitching short\ntrajectory segments together in real time. STITCHER is shown to outperform\nmodern optimization-based planners through its innovative planning architecture\nand several algorithmic developments that make real-time planning possible.\nSimulation results show safe trajectories through complex environments can be\ngenerated in milliseconds that cover tens of meters.",
    "published": "2024-12-30T18:52:22Z",
    "updated": "2025-10-16T17:23:25Z",
    "link": "http://arxiv.org/pdf/2412.21180v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Helene J. Levy",
      "Brett T. Lopez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14902v1",
    "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic\n  Framework for Unseen Concept Manipulation",
    "summary": "Current vision-language-action (VLA) models, pre-trained on large-scale\nrobotic data, exhibit strong multi-task capabilities and generalize well to\nvariations in visual and language instructions for manipulation. However, their\nsuccess rate drops significantly when faced with object concepts outside the\ntraining data, such as unseen object descriptions and textures in the dataset.\nTo address this, we propose a novel agentic framework, VLA^2, which leverages\nOpenVLA as the execution backbone and effectively leverages external modules\nsuch as web retrieval and object detection to provide visual and textual\nknowledge about target objects to the VLA. This approach mitigates\ngeneralization failure when handling out-of-distribution objects. Based on the\nLIBERO simulation environment, we introduced novel objects and object\ndescriptions to construct a new evaluation benchmark with three difficulty\nlevels to test the effectiveness of our method. Our framework successfully\noutperformed the current state-of-the-art models on our designed hard-level\ngeneralization benchmark. Compared to the standalone OpenVLA baseline, VLA^2\nachieves a 44.2% improvement in the success rate in the hard-level benchmark\nand an average improvement of 20.2% in all customized environments without any\nperformance degradation on in-domain tasks. Project website:\nhttps://vla-2.github.io.",
    "published": "2025-10-16T17:18:34Z",
    "updated": "2025-10-16T17:18:34Z",
    "link": "http://arxiv.org/pdf/2510.14902v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Han Zhao",
      "Jiaxuan Zhang",
      "Wenxuan Song",
      "Pengxiang Ding",
      "Donglin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14893v1",
    "title": "STITCHER: Constrained Trajectory Planning in Known Environments with\n  Real-Time Motion Primitive Search",
    "summary": "Autonomous high-speed navigation through large, complex environments requires\nreal-time generation of agile trajectories that are dynamically feasible,\ncollision-free, and satisfy state or actuator constraints. Modern trajectory\nplanning techniques primarily use numerical optimization, as they enable the\nsystematic computation of high-quality, expressive trajectories that satisfy\nvarious constraints. However, stringent requirements on computation time and\nthe risk of numerical instability can limit the use of optimization-based\nplanners in safety-critical scenarios. This work presents an optimization-free\nplanning framework called STITCHER that stitches short trajectory segments\ntogether with graph search to compute long-range, expressive, and near-optimal\ntrajectories in real-time. STITCHER outperforms modern optimization-based\nplanners through our innovative planning architecture and several algorithmic\ndevelopments that make real-time planning possible. Extensive simulation\ntesting is performed to analyze the algorithmic components that make up\nSTITCHER, along with a thorough comparison with two state-of-the-art\noptimization planners. Simulation tests show that safe trajectories can be\ncreated within a few milliseconds for paths that span the entirety of two 50 m\nx 50 m environments. Hardware tests with a custom quadrotor verify that\nSTITCHER can produce trackable paths in real-time while respecting nonconvex\nconstraints, such as limits on tilt angle and motor forces, which are otherwise\nhard to include in optimization-based planners.",
    "published": "2025-10-16T17:12:06Z",
    "updated": "2025-10-16T17:12:06Z",
    "link": "http://arxiv.org/pdf/2510.14893v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Helene J. Levy",
      "Brett T. Lopez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14851v1",
    "title": "SADCHER: Scheduling using Attention-based Dynamic Coalitions of\n  Heterogeneous Robots in Real-Time",
    "summary": "We present Sadcher, a real-time task assignment framework for heterogeneous\nmulti-robot teams that incorporates dynamic coalition formation and task\nprecedence constraints. Sadcher is trained through Imitation Learning and\ncombines graph attention and transformers to predict assignment rewards between\nrobots and tasks. Based on the predicted rewards, a relaxed bipartite matching\nstep generates high-quality schedules with feasibility guarantees. We\nexplicitly model robot and task positions, task durations, and robots'\nremaining processing times, enabling advanced temporal and spatial reasoning\nand generalization to environments with different spatiotemporal distributions\ncompared to training. Trained on optimally solved small-scale instances, our\nmethod can scale to larger task sets and team sizes. Sadcher outperforms other\nlearning-based and heuristic baselines on randomized, unseen problems for small\nand medium-sized teams with computation times suitable for real-time operation.\nWe also explore sampling-based variants and evaluate scalability across robot\nand task counts. In addition, we release our dataset of 250,000 optimal\nschedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/",
    "published": "2025-10-16T16:23:31Z",
    "updated": "2025-10-16T16:23:31Z",
    "link": "http://arxiv.org/pdf/2510.14851v1.pdf",
    "category": [
      "cs.RO",
      "cs.MA"
    ],
    "authors": [
      "Jakob Bichler",
      "Andreu Matoses Gimenez",
      "Javier Alonso-Mora"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14849v1",
    "title": "Multi Agent Switching Mode Controller for Sound Source localization",
    "summary": "Source seeking is an important topic in robotic research, especially\nconsidering sound-based sensors since they allow the agents to locate a target\neven in critical conditions where it is not possible to establish a direct line\nof sight. In this work, we design a multi- agent switching mode control\nstrategy for acoustic-based target localization. Two scenarios are considered:\nsingle source localization, in which the agents are driven maintaining a rigid\nformation towards the target, and multi-source scenario, in which each agent\nsearches for the targets independently from the others.",
    "published": "2025-10-16T16:21:14Z",
    "updated": "2025-10-16T16:21:14Z",
    "link": "http://arxiv.org/pdf/2510.14849v1.pdf",
    "category": [
      "cs.RO",
      "cs.MA"
    ],
    "authors": [
      "Marcello Sorge",
      "Nicola Cigarini",
      "Riccardo Lorigiola",
      "Giulia Michieletto",
      "Andrea Masiero",
      "Angelo Cenedese",
      "Alberto Guarnieri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14827v1",
    "title": "Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping",
    "summary": "Safe and efficient robot operation in complex human environments can benefit\nfrom good models of site-specific motion patterns. Maps of Dynamics (MoDs)\nprovide such models by encoding statistical motion patterns in a map, but\nexisting representations use discrete spatial sampling and typically require\ncostly offline construction. We propose a continuous spatio-temporal MoD\nrepresentation based on implicit neural functions that directly map coordinates\nto the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the\nneed for discretization and imputation for unevenly sampled regions, enabling\nsmooth generalization across both space and time. Evaluated on a large public\ndataset with long-term real-world people tracking data, our method achieves\nbetter accuracy of motion representation and smoother velocity distributions in\nsparse regions while still being computationally efficient, compared to\navailable baselines. The proposed approach demonstrates a powerful and\nefficient way of modeling complex human motion patterns.",
    "published": "2025-10-16T16:04:01Z",
    "updated": "2025-10-16T16:04:01Z",
    "link": "http://arxiv.org/pdf/2510.14827v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yufei Zhu",
      "Shih-Min Yang",
      "Andrey Rudenko",
      "Tomasz P. Kucner",
      "Achim J. Lilienthal",
      "Martin Magnusson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14783v1",
    "title": "SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with\n  Model-Based Reinforcement Learning",
    "summary": "Autonomous drone racing (ADR) systems have recently achieved champion-level\nperformance, yet remain highly specific to drone racing. While end-to-end\nvision-based methods promise broader applicability, no system to date\nsimultaneously achieves full sim-to-real transfer, onboard execution, and\nchampion-level performance. In this work, we present SkyDreamer, to the best of\nour knowledge, the first end-to-end vision-based ADR policy that maps directly\nfrom pixel-level representations to motor commands. SkyDreamer builds on\ninformed Dreamer, a model-based reinforcement learning approach where the world\nmodel decodes to privileged information only available during training. By\nextending this concept to end-to-end vision-based ADR, the world model\neffectively functions as an implicit state and parameter estimator, greatly\nimproving interpretability. SkyDreamer runs fully onboard without external aid,\nresolves visual ambiguities by tracking progress using the state decoded from\nthe world model's hidden state, and requires no extrinsic camera calibration,\nenabling rapid deployment across different drones without retraining.\nReal-world experiments show that SkyDreamer achieves robust, high-speed flight,\nexecuting tight maneuvers such as an inverted loop, a split-S and a ladder,\nreaching speeds of up to 21 m/s and accelerations of up to 6 g. It further\ndemonstrates a non-trivial visual sim-to-real transfer by operating on\npoor-quality segmentation masks, and exhibits robustness to battery depletion\nby accurately estimating the maximum attainable motor RPM and adjusting its\nflight path in real-time. These results highlight SkyDreamer's adaptability to\nimportant aspects of the reality gap, bringing robustness while still achieving\nextremely high-speed, agile flight.",
    "published": "2025-10-16T15:16:26Z",
    "updated": "2025-10-16T15:16:26Z",
    "link": "http://arxiv.org/pdf/2510.14783v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Aderik Verraest",
      "Stavrow Bahnam",
      "Robin Ferede",
      "Guido de Croon",
      "Christophe De Wagter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14771v1",
    "title": "Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation\n  Learning based Dexterous Manipulation",
    "summary": "Accurate and high-fidelity demonstration data acquisition is a critical\nbottleneck for deploying robot Imitation Learning (IL) systems, particularly\nwhen dealing with heterogeneous robotic platforms. Existing teleoperation\nsystems often fail to guarantee high-precision data collection across diverse\ntypes of teleoperation devices. To address this, we developed Open TeleDex, a\nunified teleoperation framework engineered for demonstration data collection.\nOpen TeleDex specifically tackles the TripleAny challenge, seamlessly\nsupporting any robotic arm, any dexterous hand, and any external input device.\nFurthermore, we propose a novel hand pose retargeting algorithm that\nsignificantly boosts the interoperability of Open TeleDex, enabling robust and\naccurate compatibility with an even wider spectrum of heterogeneous master and\nslave equipment. Open TeleDex establishes a foundational, high-quality, and\npublicly available platform for accelerating both academic research and\nindustry development in complex robotic manipulation and IL.",
    "published": "2025-10-16T15:07:18Z",
    "updated": "2025-10-16T15:07:18Z",
    "link": "http://arxiv.org/pdf/2510.14771v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Xu Chi",
      "Chao Zhang",
      "Yang Su",
      "Lingfeng Dou",
      "Fujia Yang",
      "Jiakuo Zhao",
      "Haoyu Zhou",
      "Xiaoyou Jia",
      "Yong Zhou",
      "Shan An"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14768v1",
    "title": "Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic\n  Recovery",
    "summary": "Real-world dexterous manipulation often encounters unexpected errors and\ndisturbances, which can lead to catastrophic failures, such as dropping the\nmanipulated object. To address this challenge, we focus on the problem of\ncatching a falling object while it remains within grasping range and,\nimportantly, resetting the system to a configuration favorable for resuming the\nprimary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a\nreinforcement learning framework that incorporates a Neural Descriptor Field\n(NDF)-inspired module to extract implicit contact features. Compared to methods\nthat rely solely on object pose or point cloud input, NDFs can directly reason\nabout finger-object correspondence and adapt to different object geometries.\nOur experiments show that incorporating contact features improves training\nefficiency, enhances convergence performance for RL training, and ultimately\nleads to more successful recoveries. Additionally, we demonstrate that CADRE\ncan generalize zero-shot to unseen objects with different geometries.",
    "published": "2025-10-16T15:04:01Z",
    "updated": "2025-10-16T15:04:01Z",
    "link": "http://arxiv.org/pdf/2510.14768v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Fan Yang",
      "Zixuan Huang",
      "Abhinav Kumar",
      "Sergio Aguilera Marinovic",
      "Soshi Iba",
      "Rana Soltani Zarrin",
      "Dmitry Berenson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14653v1",
    "title": "Requirement Identification for Traffic Simulations in Driving Simulators",
    "summary": "This paper addresses the challenge of ensuring realistic traffic conditions\nby proposing a methodology that systematically identifies traffic simulation\nrequirements. Using a structured approach based on sub-goals in each study\nphase, specific technical needs are derived for microscopic levels, agent\nmodels, and visual representation. The methodology aims to maintain a high\ndegree of fidelity, enhancing both the validity of experimental outcomes and\nparticipant engagement. By providing a clear link between study objectives and\ntraffic simulation design, this approach supports robust automotive development\nand testing.",
    "published": "2025-10-16T13:10:14Z",
    "updated": "2025-10-16T13:10:14Z",
    "link": "http://arxiv.org/pdf/2510.14653v1.pdf",
    "category": [
      "cs.SE",
      "cs.RO"
    ],
    "authors": [
      "Sven Tarlowski",
      "Lutz Eckstein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14647v1",
    "title": "Spatially anchored Tactile Awareness for Robust Dexterous Manipulation",
    "summary": "Dexterous manipulation requires precise geometric reasoning, yet existing\nvisuo-tactile learning methods struggle with sub-millimeter precision tasks\nthat are routine for traditional model-based approaches. We identify a key\nlimitation: while tactile sensors provide rich contact information, current\nlearning frameworks fail to effectively leverage both the perceptual richness\nof tactile signals and their spatial relationship with hand kinematics. We\nbelieve an ideal tactile representation should explicitly ground contact\nmeasurements in a stable reference frame while preserving detailed sensory\ninformation, enabling policies to not only detect contact occurrence but also\nprecisely infer object geometry in the hand's coordinate system. We introduce\nSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an\nend-to-end policy framework that explicitly anchors tactile features to the\nhand's kinematic frame through forward kinematics, enabling accurate geometric\nreasoning without requiring object models or explicit pose estimation. Our key\ninsight is that spatially grounded tactile representations allow policies to\nnot only detect contact occurrence but also precisely infer object geometry in\nthe hand's coordinate system. We validate SaTA on challenging dexterous\nmanipulation tasks, including bimanual USB-C mating in free space, a task\ndemanding sub-millimeter alignment precision, as well as light bulb\ninstallation requiring precise thread engagement and rotational control, and\ncard sliding that demands delicate force modulation and angular precision.\nThese tasks represent significant challenges for learning-based methods due to\ntheir stringent precision requirements. Across multiple benchmarks, SaTA\nsignificantly outperforms strong visuo-tactile baselines, improving success\nrates by up to 30 percentage while reducing task completion times by 27\npercentage.",
    "published": "2025-10-16T12:59:34Z",
    "updated": "2025-10-16T12:59:34Z",
    "link": "http://arxiv.org/pdf/2510.14647v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jialei Huang",
      "Yang Ye",
      "Yuanqing Gong",
      "Xuezhou Zhu",
      "Yang Gao",
      "Kaifeng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14643v1",
    "title": "Generative Models From and For Sampling-Based MPC: A Bootstrapped\n  Approach For Adaptive Contact-Rich Manipulation",
    "summary": "We present a generative predictive control (GPC) framework that amortizes\nsampling-based Model Predictive Control (SPC) by bootstrapping it with\nconditional flow-matching models trained on SPC control sequences collected in\nsimulation. Unlike prior work relying on iterative refinement or gradient-based\nsolvers, we show that meaningful proposal distributions can be learned directly\nfrom noisy SPC data, enabling more efficient and informed sampling during\nonline planning. We further demonstrate, for the first time, the application of\nthis approach to real-world contact-rich loco-manipulation with a quadruped\nrobot. Extensive experiments in simulation and on hardware show that our method\nimproves sample efficiency, reduces planning horizon requirements, and\ngeneralizes robustly across task variations.",
    "published": "2025-10-16T12:55:28Z",
    "updated": "2025-10-16T12:55:28Z",
    "link": "http://arxiv.org/pdf/2510.14643v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Lara Brudermller",
      "Brandon Hung",
      "Xinghao Zhu",
      "Jiuguang Wang",
      "Nick Hawes",
      "Preston Culbertson",
      "Simon Le Cleac'h"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14615v1",
    "title": "Accelerated Multi-Modal Motion Planning Using Context-Conditioned\n  Diffusion Models",
    "summary": "Classical methods in robot motion planning, such as sampling-based and\noptimization-based methods, often struggle with scalability towards\nhigher-dimensional state spaces and complex environments. Diffusion models,\nknown for their capability to learn complex, high-dimensional and multi-modal\ndata distributions, provide a promising alternative when applied to motion\nplanning problems and have already shown interesting results. However, most of\nthe current approaches train their model for a single environment, limiting\ntheir generalization to environments not seen during training. The techniques\nthat do train a model for multiple environments rely on a specific camera to\nprovide the model with the necessary environmental information and therefore\nalways require that sensor. To effectively adapt to diverse scenarios without\nthe need for retraining, this research proposes Context-Aware Motion Planning\nDiffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic\ndiffusion model, conditioned on sensor-agnostic contextual information. An\nattention mechanism, integrated in the well-known U-Net architecture,\nconditions the model on an arbitrary number of contextual parameters. CAMPD is\nevaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art\napproaches on real-world tasks, showing its ability to generalize to unseen\nenvironments and generate high-quality, multi-modal trajectories, at a fraction\nof the time required by existing methods.",
    "published": "2025-10-16T12:21:56Z",
    "updated": "2025-10-16T12:21:56Z",
    "link": "http://arxiv.org/pdf/2510.14615v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Edward Sandra",
      "Lander Vanroye",
      "Dries Dirckx",
      "Ruben Cartuyvels",
      "Jan Swevers",
      "Wilm Decr"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14612v1",
    "title": "Proprioceptive Image: An Image Representation of Proprioceptive Data\n  from Quadruped Robots for Contact Estimation Learning",
    "summary": "This paper presents a novel approach for representing proprioceptive\ntime-series data from quadruped robots as structured two-dimensional images,\nenabling the use of convolutional neural networks for learning\nlocomotion-related tasks. The proposed method encodes temporal dynamics from\nmultiple proprioceptive signals, such as joint positions, IMU readings, and\nfoot velocities, while preserving the robot's morphological structure in the\nspatial arrangement of the image. This transformation captures inter-signal\ncorrelations and gait-dependent patterns, providing a richer feature space than\ndirect time-series processing. We apply this concept in the problem of contact\nestimation, a key capability for stable and adaptive locomotion on diverse\nterrains. Experimental evaluations on both real-world datasets and simulated\nenvironments show that our image-based representation consistently enhances\nprediction accuracy and generalization over conventional sequence-based models,\nunderscoring the potential of cross-modal encoding strategies for robotic state\nlearning. Our method achieves superior performance on the contact dataset,\nimproving contact state accuracy from 87.7% to 94.5% over the recently proposed\nMI-HGNN method, using a 15 times shorter window size.",
    "published": "2025-10-16T12:20:44Z",
    "updated": "2025-10-16T12:20:44Z",
    "link": "http://arxiv.org/pdf/2510.14612v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Gabriel Fischer Abati",
      "Joo Carlos Virgolino Soares",
      "Giulio Turrisi",
      "Victor Barasuol",
      "Claudio Semini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14584v1",
    "title": "A Generalized Placeability Metric for Model-Free Unified Pick-and-Place\n  Reasoning",
    "summary": "To reliably pick and place unknown objects under real-world sensing noise\nremains a challenging task, as existing methods rely on strong object priors\n(e.g., CAD models), or planar-support assumptions, limiting generalization and\nunified reasoning between grasping and placing. In this work, we introduce a\ngeneralized placeability metric that evaluates placement poses directly from\nnoisy point clouds, without any shape priors. The metric jointly scores\nstability, graspability, and clearance. From raw geometry, we extract the\nsupport surfaces of the object to generate diverse candidates for\nmulti-orientation placement and sample contacts that satisfy collision and\nstability constraints. By conditioning grasp scores on each candidate\nplacement, our proposed method enables model-free unified pick-and-place\nreasoning and selects grasp-place pairs that lead to stable, collision-free\nplacements. On unseen real objects and non-planar object supports, our metric\ndelivers CAD-comparable accuracy in predicting stability loss and generally\nproduces more physically plausible placements than learning-based predictors.",
    "published": "2025-10-16T11:43:02Z",
    "updated": "2025-10-16T11:43:02Z",
    "link": "http://arxiv.org/pdf/2510.14584v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Benno Wingender",
      "Nils Dengler",
      "Rohit Menon",
      "Sicong Pan",
      "Maren Bennewitz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14546v1",
    "title": "QuASH: Using Natural-Language Heuristics to Query Visual-Language\n  Robotic Maps",
    "summary": "Embeddings from Visual-Language Models are increasingly utilized to represent\nsemantics in robotic maps, offering an open-vocabulary scene understanding that\nsurpasses traditional, limited labels. Embeddings enable on-demand querying by\ncomparing embedded user text prompts to map embeddings via a similarity metric.\nThe key challenge in performing the task indicated in a query is that the robot\nmust determine the parts of the environment relevant to the query.\n  This paper proposes a solution to this challenge. We leverage\nnatural-language synonyms and antonyms associated with the query within the\nembedding space, applying heuristics to estimate the language space relevant to\nthe query, and use that to train a classifier to partition the environment into\nmatches and non-matches. We evaluate our method through extensive experiments,\nquerying both maps and standard image benchmarks. The results demonstrate\nincreased queryability of maps and images. Our querying technique is agnostic\nto the representation and encoder used, and requires limited training.",
    "published": "2025-10-16T10:41:31Z",
    "updated": "2025-10-16T10:41:31Z",
    "link": "http://arxiv.org/pdf/2510.14546v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Matti Pekkanen",
      "Francesco Verdoja",
      "Ville Kyrki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14511v1",
    "title": "Stability Criteria and Motor Performance in Delayed Haptic Dyadic\n  Interactions Mediated by Robots",
    "summary": "This paper establishes analytical stability criteria for robot-mediated\nhuman-human (dyadic) interaction systems, focusing on haptic communication\nunder network-induced time delays. Through frequency-domain analysis supported\nby numerical simulations, we identify both delay-independent and\ndelay-dependent stability criteria. The delay-independent criterion guarantees\nstability irrespective of the delay, whereas the delay-dependent criterion is\ncharacterised by a maximum tolerable delay before instability occurs. The\ncriteria demonstrate dependence on controller and robot dynamic parameters,\nwhere increasing stiffness reduces the maximum tolerable delay in a non-linear\nmanner, thereby heightening system vulnerability. The proposed criteria can be\ngeneralised to a wide range of robot-mediated interactions and serve as design\nguidelines for stable remote dyadic systems. Experiments with robots performing\nhuman-like movements further illustrate the correlation between stability and\nmotor performance. The findings of this paper suggest the prerequisites for\neffective delay-compensation strategies.",
    "published": "2025-10-16T09:55:33Z",
    "updated": "2025-10-16T09:55:33Z",
    "link": "http://arxiv.org/pdf/2510.14511v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Mingtian Du",
      "Suhas Raghavendra Kulkarni",
      "Simone Kager",
      "Domenico Campolo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.11109v3",
    "title": "FEWT: Improving Humanoid Robot Perception with Frequency-Enhanced\n  Wavelet-based Transformers",
    "summary": "The embodied intelligence bridges the physical world and information space.\nAs its typical physical embodiment, humanoid robots have shown great promise\nthrough robot learning algorithms in recent years. In this study, a hardware\nplatform, including humanoid robot and exoskeleton-style teleoperation cabin,\nwas developed to realize intuitive remote manipulation and efficient collection\nof anthropomorphic action data. To improve the perception representation of\nhumanoid robot, an imitation learning framework, termed Frequency-Enhanced\nWavelet-based Transformer (FEWT), was proposed, which consists of two primary\nmodules: Frequency-Enhanced Efficient Multi-Scale Attention (FE-EMA) and\nTime-Series Discrete Wavelet Transform (TS-DWT). By combining multi-scale\nwavelet decomposition with the residual network, FE-EMA can dynamically fuse\nfeatures from both cross-spatial and frequency-domain. This fusion is able to\ncapture feature information across various scales effectively, thereby\nenhancing model robustness. Experimental performance demonstrates that FEWT\nimproves the success rate of the state-of-the-art algorithm (Action Chunking\nwith Transformers, ACT baseline) by up to 30% in simulation and by 6-12% in\nreal-world.",
    "published": "2025-09-14T05:56:40Z",
    "updated": "2025-10-16T09:22:24Z",
    "link": "http://arxiv.org/pdf/2509.11109v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jiaxin Huang",
      "Hanyu Liu",
      "Yunsheng Ma",
      "Jian Shen",
      "Yilin Zheng",
      "Jiayi Wen",
      "Baishu Wan",
      "Pan Li",
      "Zhigong Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14467v1",
    "title": "Restoring Noisy Demonstration for Imitation Learning With Diffusion\n  Models",
    "summary": "Imitation learning (IL) aims to learn a policy from expert demonstrations and\nhas been applied to various applications. By learning from the expert policy,\nIL methods do not require environmental interactions or reward signals.\nHowever, most existing imitation learning algorithms assume perfect expert\ndemonstrations, but expert demonstrations often contain imperfections caused by\nerrors from human experts or sensor/control system inaccuracies. To address the\nabove problems, this work proposes a filter-and-restore framework to best\nleverage expert demonstrations with inherent noise. Our proposed method first\nfilters clean samples from the demonstrations and then learns conditional\ndiffusion models to recover the noisy ones. We evaluate our proposed framework\nand existing methods in various domains, including robot arm manipulation,\ndexterous manipulation, and locomotion. The experiment results show that our\nproposed framework consistently outperforms existing methods across all the\ntasks. Ablation studies further validate the effectiveness of each component\nand demonstrate the framework's robustness to different noise types and levels.\nThese results confirm the practical applicability of our framework to noisy\noffline demonstration data.",
    "published": "2025-10-16T09:09:22Z",
    "updated": "2025-10-16T09:09:22Z",
    "link": "http://arxiv.org/pdf/2510.14467v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Shang-Fu Chen",
      "Co Yong",
      "Shao-Hua Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.16120v2",
    "title": "FTIN: Frequency-Time Integration Network for Inertial Odometry",
    "summary": "Inertial odometry (IO) leverages inertial measurement unit (IMU) signals for\ncost-effective localization. However, high IMU sampling rates introduce\nsubstantial redundancy that impedes IO's ability to attend to salient\ncomponents, thereby creating an information bottleneck. To address this\nchallenge, we propose a cross-domain IO framework that fuses information from\nthe frequency and time domains. Specifically, we exploit the global context and\nenergy-compaction properties of frequency-domain representations to capture\nholistic motion patterns and alleviate the bottleneck. To the best of our\nknowledge, this is among the first attempts to incorporate frequency-domain\nfeature processing into IO. Experimental results on multiple public datasets\ndemonstrate the effectiveness of the proposed frequency--time-domain fusion\nstrategy.",
    "published": "2025-07-22T00:18:54Z",
    "updated": "2025-10-16T08:45:43Z",
    "link": "http://arxiv.org/pdf/2507.16120v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Shanshan Zhang",
      "Qi Zhang",
      "Siyue Wang",
      "Liqin Wu",
      "Tianshui Wen",
      "Ziheng Zhou",
      "Ao Peng",
      "Xuemin Hong",
      "Lingxiang Zheng",
      "Yu Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.16865v2",
    "title": "CKANIO: Learnable Chebyshev Polynomials for Inertial Odometry",
    "summary": "Inertial odometry (IO) relies exclusively on signals from an inertial\nmeasurement unit (IMU) for localization and offers a promising avenue for\nconsumer grade positioning. However, accurate modeling of the nonlinear motion\npatterns present in IMU signals remains the principal limitation on IO\naccuracy. To address this challenge, we propose CKANIO, an IO framework that\nintegrates Chebyshev based Kolmogorov-Arnold Networks (Chebyshev KAN).\nSpecifically, we design a novel residual architecture that leverages the\nnonlinear approximation capabilities of Chebyshev polynomials within the KAN\nframework to more effectively model the complex motion characteristics inherent\nin IMU signals. To the best of our knowledge, this work represents the first\napplication of an interpretable KAN model to IO. Experimental results on five\npublicly available datasets demonstrate the effectiveness of CKANIO.",
    "published": "2025-07-22T00:24:06Z",
    "updated": "2025-10-16T08:41:11Z",
    "link": "http://arxiv.org/pdf/2507.16865v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Shanshan Zhang",
      "Siyue Wang",
      "Tianshui Wen",
      "Liqin Wu",
      "Qi Zhang",
      "Ziheng Zhou",
      "Ao Peng",
      "Xuemin Hong",
      "Lingxiang Zheng",
      "Yu Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14414v1",
    "title": "RoboANKLE: Design, Development, and Functional Evaluation of a Robotic\n  Ankle with a Motorized Compliant Unit",
    "summary": "This study presents a powered transtibial prosthesis with complete push-off\nassistance, RoboANKLE. The design aims to fulfill specific requirements, such\nas a sufficient range of motion (RoM) while providing the necessary torque for\nachieving natural ankle motion in daily activities. Addressing the challenges\nfaced in designing active transtibial prostheses, such as maintaining energetic\nautonomy and minimizing weight, is vital for the study. With this aim, we try\nto imitate the human ankle by providing extensive push-off assistance to\nachieve a natural-like torque profile. Thus, Energy Store and Extended Release\nmechanism (ESER) is employed with a novel Extra Energy Storage (EES) mechanism.\nKinematic and kinetic analyses are carried out to determine the design\nparameters and assess the design performance. Subsequently, a Computer-Aided\nDesign (CAD) model is built and used in comprehensive dynamic and structural\nanalyses. These analyses are used for the design performance evaluation and\ndetermine the forces and torques applied to the prosthesis, which aids in\noptimizing the design for minimal weight via structural analysis and topology\noptimization. The design of the prototype is then finalized and manufactured\nfor experimental evaluation to validate the design and functionality. The\nprototype is realized with a mass of 1.92 kg and dimensions of 261x107x420 mm.\nThe Functional evaluations of the RoboANKLE revealed that it is capable of\nachieving the natural maximum dorsi-flexion angle with 95% accuracy. Also,\nThanks to the implemented mechanisms, the results show that RoboANKLE can\ngenerate 57% higher than the required torque for natural walking. The result of\nthe power generation capacity of the RoboANKLE is 10% more than the natural\npower during the gait cycle.",
    "published": "2025-10-16T08:18:51Z",
    "updated": "2025-10-16T08:18:51Z",
    "link": "http://arxiv.org/pdf/2510.14414v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Baris Baysal",
      "Omid Arfaie",
      "Ramazan Unal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.04141v2",
    "title": "NAMO-LLM: Efficient Navigation Among Movable Obstacles with Large\n  Language Model Guidance",
    "summary": "Several planners have been proposed to compute robot paths that reach desired\ngoal regions while avoiding obstacles. However, these methods fail when all\npathways to the goal are blocked. In such cases, the robot must reason about\nhow to reconfigure the environment to access task-relevant regions - a problem\nknown as Navigation Among Movable Objects (NAMO). While various solutions to\nthis problem have been developed, they often struggle to scale to highly\ncluttered environments. To address this, we propose NAMO-LLM, a sampling-based\nplanner that searches over robot and obstacle configurations to compute\nfeasible plans specifying which obstacles to move, where, and in what order.\nIts key novelty is a non-uniform sampling strategy guided by Large Language\nModels (LLMs) biasing the tree construction toward directions more likely to\nyield a solution. We show that NAMO-LLM is probabilistically complete and\ndemonstrate through experiments that it efficiently scales to cluttered\nenvironments, outperforming related works in both runtime and plan quality.",
    "published": "2025-05-07T05:45:33Z",
    "updated": "2025-10-16T08:13:38Z",
    "link": "http://arxiv.org/pdf/2505.04141v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yuqing Zhang",
      "Yiannis Kantaros"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.06500v2",
    "title": "Insect-Scale Tailless Robot with Flapping Wings: A Simple Structure and\n  Drive for Yaw Control",
    "summary": "Insect-scale micro-aerial vehicles, especially, lightweight, flapping-wing\nrobots, are becoming increasingly important for safe motion sensing in\nspatially constrained environments such as living spaces. However, yaw control\nusing flapping wings is fundamentally more difficult than using rotating wings.\nIn this study, an insect-scale, tailless robot with four paired tilted flapping\nwings (weighing 1.52 g) to enable yaw control was fabricated. It benefits from\nthe simplicity of a directly driven wing actuator with no transmission and a\nlift control signal; however, it still has an offset in the lift force.\nTherefore, an adaptive controller was designed to alleviate the offset.\nNumerical experiments confirm that the proposed controller outperforms the\nlinear quadratic integral controller. Finally, in a tethered and controlled\ndemonstration flight, the yaw drift was suppressed by the wing-tilting\narrangement and the proposed controller. The simple structure drive system\ndemonstrates the potential for future controlled flights of battery-powered,\ntailless, flapping-wing robots weighing less than 10 grams.",
    "published": "2024-07-09T02:04:51Z",
    "updated": "2025-10-16T07:06:44Z",
    "link": "http://arxiv.org/pdf/2407.06500v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Tomohiko Jimbo",
      "Takashi Ozaki",
      "Norikazu Ohta",
      "Kanae Hamaguchi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14338v1",
    "title": "Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for\n  Quadrupedal Locomotion",
    "summary": "In this work, we study risk-aware reinforcement learning for quadrupedal\nlocomotion. Our approach trains a family of risk-conditioned policies using a\nConditional Value-at-Risk (CVaR) constrained policy optimization technique that\nprovides improved stability and sample efficiency. At deployment, we adaptively\nselect the best performing policy from the family of policies using a\nmulti-armed bandit framework that uses only observed episodic returns, without\nany privileged environment information, and adapts to unknown conditions on the\nfly. Hence, we train quadrupedal locomotion policies at various levels of\nrobustness using CVaR and adaptively select the desired level of robustness\nonline to ensure performance in unknown environments. We evaluate our method in\nsimulation across eight unseen settings (by changing dynamics, contacts,\nsensing noise, and terrain) and on a Unitree Go2 robot in previously unseen\nterrains. Our risk-aware policy attains nearly twice the mean and tail\nperformance in unseen environments compared to other baselines and our\nbandit-based adaptation selects the best-performing risk-aware policy in\nunknown terrain within two minutes of operation.",
    "published": "2025-10-16T06:18:50Z",
    "updated": "2025-10-16T06:18:50Z",
    "link": "http://arxiv.org/pdf/2510.14338v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yuanhong Zeng",
      "Anushri Dixit"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13553v2",
    "title": "Hoecken-D Hand: A Novel Robotic Hand for Linear Parallel Pinching and\n  Self-Adaptive Grasping",
    "summary": "This paper presents the Hoecken-D Hand, an underactuated robotic gripper that\ncombines a modified Hoecken linkage with a differential spring mechanism to\nachieve both linear parallel pinching and a mid-stroke transition to adaptive\nenvelope. The original Hoecken linkage is reconfigured by replacing one member\nwith differential links, preserving straight-line guidance while enabling\ncontact-triggered reconfiguration without additional actuators. A\ndouble-parallelogram arrangement maintains fingertip parallelism during\nconventional pinching, whereas the differential mechanism allows one finger to\nwrap inward upon encountering an obstacle, improving stability on irregular or\nthin objects. The mechanism can be driven by a single linear actuator,\nminimizing complexity and cost; in our prototype, each finger is driven by its\nown linear actuator for simplicity. We perform kinematic modeling and force\nanalysis to characterize grasp performance, including simulated grasping forces\nand spring-opening behavior under varying geometric parameters. The design was\nprototyped using PLA-based 3D printing, achieving a linear pinching span of\napproximately 200 mm. Preliminary tests demonstrate reliable grasping in both\nmodes across a wide range of object geometries, highlighting the Hoecken-D Hand\nas a compact, adaptable, and cost-effective solution for manipulation in\nunstructured environments.",
    "published": "2025-10-15T13:49:02Z",
    "updated": "2025-10-16T05:02:44Z",
    "link": "http://arxiv.org/pdf/2510.13553v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Wentao Guo",
      "Wenzeng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.14178v2",
    "title": "TACS-Graphs: Traversability-Aware Consistent Scene Graphs for Ground\n  Robot Localization and Mapping",
    "summary": "Scene graphs have emerged as a powerful tool for robots, providing a\nstructured representation of spatial and semantic relationships for advanced\ntask planning. Despite their potential, conventional 3D indoor scene graphs\nface critical limitations, particularly under- and over-segmentation of room\nlayers in structurally complex environments. Under-segmentation misclassifies\nnon-traversable areas as part of a room, often in open spaces, while\nover-segmentation fragments a single room into overlapping segments in complex\nenvironments. These issues stem from naive voxel-based map representations that\nrely solely on geometric proximity, disregarding the structural constraints of\ntraversable spaces and resulting in inconsistent room layers within scene\ngraphs. To the best of our knowledge, this work is the first to tackle\nsegmentation inconsistency as a challenge and address it with\nTraversability-Aware Consistent Scene Graphs (TACS-Graphs), a novel framework\nthat integrates ground robot traversability with room segmentation. By\nleveraging traversability as a key factor in defining room boundaries, the\nproposed method achieves a more semantically meaningful and topologically\ncoherent segmentation, effectively mitigating the inaccuracies of voxel-based\nscene graph approaches in complex environments. Furthermore, the enhanced\nsegmentation consistency improves loop closure detection efficiency in the\nproposed Consistent Scene Graph-leveraging Loop Closure Detection (CoSG-LCD)\nleading to higher pose estimation accuracy. Experimental results confirm that\nthe proposed approach outperforms state-of-the-art methods in terms of scene\ngraph consistency and pose graph optimization performance.",
    "published": "2025-06-17T04:39:51Z",
    "updated": "2025-10-16T04:53:21Z",
    "link": "http://arxiv.org/pdf/2506.14178v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jeewon Kim",
      "Minho Oh",
      "Hyun Myung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.10832v3",
    "title": "EffiTune: Diagnosing and Mitigating Training Inefficiency for Parameter\n  Tuner in Robot Navigation System",
    "summary": "Robot navigation systems are critical for various real-world applications\nsuch as delivery services, hospital logistics, and warehouse management.\nAlthough classical navigation methods provide interpretability, they rely\nheavily on expert manual tuning, limiting their adaptability. Conversely,\npurely learning-based methods offer adaptability but often lead to instability\nand erratic robot behaviors. Recently introduced parameter tuners aim to\nbalance these approaches by integrating data-driven adaptability into classical\nnavigation frameworks. However, the parameter tuning process currently suffers\nfrom training inefficiencies and redundant sampling, with critical regions in\nenvironment often underrepresented in training data. In this paper, we propose\nEffiTune, a novel framework designed to diagnose and mitigate training\ninefficiency for parameter tuners in robot navigation systems. EffiTune first\nperforms robot-behavior-guided diagnostics to pinpoint critical bottlenecks and\nunderrepresented regions. It then employs a targeted up-sampling strategy to\nenrich the training dataset with critical samples, significantly reducing\nredundancy and enhancing training efficiency. Our comprehensive evaluation\ndemonstrates that EffiTune achieves more than a 13.5% improvement in navigation\nperformance, enhanced robustness in out-of-distribution scenarios, and a 4x\nimprovement in training efficiency within the same computational budget.",
    "published": "2024-09-17T01:49:17Z",
    "updated": "2025-10-16T03:11:28Z",
    "link": "http://arxiv.org/pdf/2409.10832v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Shiwei Feng",
      "Xuan Chen",
      "Zikang Xiong",
      "Zhiyuan Cheng",
      "Yifei Gao",
      "Siyuan Cheng",
      "Sayali Kate",
      "Xiangyu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14234v1",
    "title": "Prescribed Performance Control of Deformable Object Manipulation in\n  Spatial Latent Space",
    "summary": "Manipulating three-dimensional (3D) deformable objects presents significant\nchallenges for robotic systems due to their infinite-dimensional state space\nand complex deformable dynamics. This paper proposes a novel model-free\napproach for shape control with constraints imposed on key points. Unlike\nexisting methods that rely on feature dimensionality reduction, the proposed\ncontroller leverages the coordinates of key points as the feature vector, which\nare extracted from the deformable object's point cloud using deep learning\nmethods. This approach not only reduces the dimensionality of the feature space\nbut also retains the spatial information of the object. By extracting key\npoints, the manipulation of deformable objects is simplified into a visual\nservoing problem, where the shape dynamics are described using a deformation\nJacobian matrix. To enhance control accuracy, a prescribed performance control\nmethod is developed by integrating barrier Lyapunov functions (BLF) to enforce\nconstraints on the key points. The stability of the closed-loop system is\nrigorously analyzed and verified using the Lyapunov method. Experimental\nresults further demonstrate the effectiveness and robustness of the proposed\nmethod.",
    "published": "2025-10-16T02:26:48Z",
    "updated": "2025-10-16T02:26:48Z",
    "link": "http://arxiv.org/pdf/2510.14234v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Ning Han",
      "Gu Gong",
      "Bin Zhang",
      "Yuexuan Xu",
      "Bohan Yang",
      "Yunhui Liu",
      "David Navarro-Alarcon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.01017v3",
    "title": "Inferring Foresightedness in Dynamic Noncooperative Games",
    "summary": "Dynamic game theory is an increasingly popular tool for modeling multi-agent,\ne.g. human-robot, interactions. Game-theoretic models presume that each agent\nwishes to minimize a private cost function that depends on others' actions.\nThese games typically evolve over a fixed time horizon, specifying how far into\nthe future each agent plans. In practical settings, however, decision-makers\nmay vary in foresightedness, or how much they care about their current cost in\nrelation to their past and future costs. We conjecture that quantifying and\nestimating each agent's foresightedness from online data will enable safer and\nmore efficient interactions with other agents. To this end, we frame this\ninference problem as an inverse dynamic game. We consider a specific objective\nfunction parametrization that smoothly interpolates myopic and farsighted\nplanning. Games of this form are readily transformed into parametric mixed\ncomplementarity problems; we exploit the directional differentiability of\nsolutions to these problems with respect to their hidden parameters to solve\nfor agents' foresightedness. We conduct three experiments: one with\nsynthetically generated delivery robot motion, one with real-world data\ninvolving people walking, biking, and driving vehicles, and one using\nhigh-fidelity simulators. The results of these experiments demonstrate that\nexplicitly inferring agents' foresightedness enables game-theoretic models to\nmake 33% more accurate models for agents' behavior.",
    "published": "2024-12-02T00:31:17Z",
    "updated": "2025-10-16T01:27:08Z",
    "link": "http://arxiv.org/pdf/2412.01017v3.pdf",
    "category": [
      "cs.RO",
      "cs.GT",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Cade Armstrong",
      "Ryan Park",
      "Xinjie Liu",
      "Kushagra Gupta",
      "David Fridovich-Keil"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.09241v4",
    "title": "BlueME: Robust Underwater Robot-to-Robot Communication Using Compact\n  Magnetoelectric Antennas",
    "summary": "We present the design, development, and experimental validation of BlueME, a\ncompact magnetoelectric (ME) antenna array system for underwater robot-to-robot\ncommunication. BlueME employs ME antennas operating at their natural mechanical\nresonance frequency to efficiently transmit and receive very-low-frequency\n(VLF) electromagnetic signals underwater. We outline the design, simulation,\nfabrication, and integration of the proposed system on low-power embedded\nplatforms, focusing on portable and scalable applications. For performance\nevaluation, we deployed BlueME on an autonomous surface vehicle (ASV) and a\nremotely operated vehicle (ROV) in open-water field trials. Ocean trials\ndemonstrate that BlueME maintains reliable signal transmission at distances\nbeyond 700 meters while consuming only 10 watts of power. Field trials show\nthat the system operates effectively in challenging underwater conditions such\nas turbidity, obstacles, and multipath interference -- conditions that\ngenerally affect acoustics and optics. Our analysis also examines the impact of\ncomplete submersion on system performance and identifies key deployment\nconsiderations. This work represents the first practical underwater deployment\nof ME antennas outside the laboratory and implements the largest VLF ME array\nsystem to date. BlueME demonstrates significant potential for marine robotics\nand automation in multi-robot cooperative systems and remote sensor networks.",
    "published": "2024-11-14T07:15:24Z",
    "updated": "2025-10-15T22:33:37Z",
    "link": "http://arxiv.org/pdf/2411.09241v4.pdf",
    "category": [
      "cs.RO",
      "eess.SP"
    ],
    "authors": [
      "Mehron Talebi",
      "Sultan Mahmud",
      "Adam Khalifa",
      "Md Jahidul Islam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14117v1",
    "title": "ViTacGen: Robotic Pushing with Vision-to-Touch Generation",
    "summary": "Robotic pushing is a fundamental manipulation task that requires tactile\nfeedback to capture subtle contact forces and dynamics between the end-effector\nand the object. However, real tactile sensors often face hardware limitations\nsuch as high costs and fragility, and deployment challenges involving\ncalibration and variations between different sensors, while vision-only\npolicies struggle with satisfactory performance. Inspired by humans' ability to\ninfer tactile states from vision, we propose ViTacGen, a novel robot\nmanipulation framework designed for visual robotic pushing with vision-to-touch\ngeneration in reinforcement learning to eliminate the reliance on\nhigh-resolution real tactile sensors, enabling effective zero-shot deployment\non visual-only robotic systems. Specifically, ViTacGen consists of an\nencoder-decoder vision-to-touch generation network that generates contact depth\nimages, a standardized tactile representation, directly from visual image\nsequence, followed by a reinforcement learning policy that fuses visual-tactile\ndata with contrastive learning based on visual and generated tactile\nobservations. We validate the effectiveness of our approach in both simulation\nand real world experiments, demonstrating its superior performance and\nachieving a success rate of up to 86\\%.",
    "published": "2025-10-15T21:37:59Z",
    "updated": "2025-10-15T21:37:59Z",
    "link": "http://arxiv.org/pdf/2510.14117v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Zhiyuan Wu",
      "Yijiong Lin",
      "Yongqiang Zhao",
      "Xuyang Zhang",
      "Zhuo Chen",
      "Nathan Lepora",
      "Shan Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.00441v2",
    "title": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual\n  Navigation",
    "summary": "Visual navigation is a fundamental problem in embodied AI, yet practical\ndeployments demand long-horizon planning capabilities to address\nmulti-objective tasks. A major bottleneck is data scarcity: policies learned\nfrom limited data often overfit and fail to generalize OOD. Existing neural\nnetwork-based agents typically increase architectural complexity that\nparadoxically become counterproductive in the small-sample regime. This paper\nintroduce NeuRO, a integrated learning-to-optimize framework that tightly\ncouples perception networks with downstream task-level robust optimization.\nSpecifically, NeuRO addresses core difficulties in this integration: (i) it\ntransforms noisy visual predictions under data scarcity into convex uncertainty\nsets using Partially Input Convex Neural Networks (PICNNs) with conformal\ncalibration, which directly parameterize the optimization constraints; and (ii)\nit reformulates planning under partial observability as a robust optimization\nproblem, enabling uncertainty-aware policies that transfer across environments.\nExtensive experiments on both unordered and sequential multi-object navigation\ntasks demonstrate that NeuRO establishes SoTA performance, particularly in\ngeneralization to unseen environments. Our work thus presents a significant\nadvancement for developing robust, generalizable autonomous agents.",
    "published": "2025-10-01T02:48:28Z",
    "updated": "2025-10-15T21:05:31Z",
    "link": "http://arxiv.org/pdf/2510.00441v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yiyuan Pan",
      "Yunzhe Xu",
      "Zhe Liu",
      "Hesheng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10520v2",
    "title": "AI-Agents for Culturally Diverse Online Higher Education Environments",
    "summary": "As the global reach of online higher education continues to grow,\nuniversities are increasingly accommodating students from diverse cultural\nbackgrounds (Tereshko et al., 2024). This can present a number of challenges\nincluding linguistic barriers (Ullah et al., 2021), cultural differences in\nlearning style (Omidvar & Tan, 2012), cultural sensitivity in course design\n(Nguyen, 2022) and perceived isolation when students feel their perspectives or\nexperiences are not reflected or valued in the learning environment\n(Hansen-Brown et al., 2022). Ensuring active engagement and reasonable learning\noutcomes in such a environments requires distance educational systems that are\nnot only adaptive but also culturally resonant (Dalle et al., 2024). Both\nembodied and virtual AI-Agents have great potential in this regard as they can\nfacilitate personalized learning and adapt their interactions and content\ndelivery to align with students' cultural context. In addition, Generative AI\n(GAI), such as, Large Language Models (LLMs) can amplify the potential for\nthese culturally aware AI agents to address educational challenges due to their\nadvanced capacity for understanding and generating contextually relevant\ncontent (Wang et al., 2024). This chapter reviews existing research and\nsuggests the usage of culturally aware AI-Agents, powered by GAI, to foster\nengagement and improve learning outcomes in culturally diverse online higher\neducation environments.",
    "published": "2025-10-12T09:42:09Z",
    "updated": "2025-10-15T20:49:42Z",
    "link": "http://arxiv.org/pdf/2510.10520v2.pdf",
    "category": [
      "cs.CY",
      "cs.RO"
    ],
    "authors": [
      "Fuze Sun",
      "Paul Craig",
      "Lingyu Li",
      "Shixiangyue Meng",
      "Chuxi Nan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14072v1",
    "title": "Partial Feedback Linearization Control of a Cable-Suspended Multirotor\n  Platform for Stabilization of an Attached Load",
    "summary": "In this work, we present a novel control approach based on partial feedback\nlinearization (PFL) for the stabilization of a suspended aerial platform with\nan attached load. Such systems are envisioned for various applications in\nconstruction sites involving cranes, such as the holding and transportation of\nheavy objects. Our proposed control approach considers the underactuation of\nthe whole system while utilizing its coupled dynamics for stabilization. We\ndemonstrate using numerical stability analysis that these coupled terms are\ncrucial for the stabilization of the complete system. We also carried out\nrobustness analysis of the proposed approach in the presence of external wind\ndisturbances, sensor noise, and uncertainties in system dynamics. As our\nenvisioned target application involves cranes in outdoor construction sites,\nour control approaches rely on only onboard sensors, thus making it suitable\nfor such applications. We carried out extensive simulation studies and\nexperimental tests to validate our proposed control approach.",
    "published": "2025-10-15T20:28:06Z",
    "updated": "2025-10-15T20:28:06Z",
    "link": "http://arxiv.org/pdf/2510.14072v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Hemjyoti Das",
      "Christian Ott"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11448v2",
    "title": "A Faster and More Reliable Middleware for Autonomous Driving Systems",
    "summary": "Ensuring safety in high-speed autonomous vehicles requires rapid control\nloops and tightly bounded delays from perception to actuation. Many open-source\nautonomy systems rely on ROS 2 middleware; when multiple sensor and control\nnodes share one compute unit, ROS 2 and its DDS transports add significant\n(de)serialization, copying, and discovery overheads, shrinking the available\ntime budget. We present Sensor-in-Memory (SIM), a shared-memory transport\ndesigned for intra-host pipelines in autonomous vehicles. SIM keeps sensor data\nin native memory layouts (e.g., cv::Mat, PCL), uses lock-free bounded double\nbuffers that overwrite old data to prioritize freshness, and integrates into\nROS 2 nodes with four lines of code. Unlike traditional middleware, SIM\noperates beside ROS 2 and is optimized for applications where data freshness\nand minimal latency outweigh guaranteed completeness. SIM provides sequence\nnumbers, a writer heartbeat, and optional checksums to ensure ordering,\nliveness, and basic integrity. On an NVIDIA Jetson Orin Nano, SIM reduces\ndata-transport latency by up to 98% compared to ROS 2 zero-copy transports such\nas FastRTPS and Zenoh, lowers mean latency by about 95%, and narrows\n95th/99th-percentile tail latencies by around 96%. In tests on a\nproduction-ready Level 4 vehicle running Autoware.Universe, SIM increased\nlocalization frequency from 7.5 Hz to 9.5 Hz. Applied across all\nlatency-critical modules, SIM cut average perception-to-decision latency from\n521.91 ms to 290.26 ms, reducing emergency braking distance at 40 mph (64 km/h)\non dry concrete by 13.6 ft (4.14 m).",
    "published": "2025-10-13T14:17:14Z",
    "updated": "2025-10-15T20:12:20Z",
    "link": "http://arxiv.org/pdf/2510.11448v2.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "C.3; D.4.1; D.4.4; D.4.8; I.2.9"
    ],
    "authors": [
      "Yuankai He",
      "Weisong Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14065v1",
    "title": "Optimistic Reinforcement Learning-Based Skill Insertions for Task and\n  Motion Planning",
    "summary": "Task and motion planning (TAMP) for robotics manipulation necessitates\nlong-horizon reasoning involving versatile actions and skills. While\ndeterministic actions can be crafted by sampling or optimizing with certain\nconstraints, planning actions with uncertainty, i.e., probabilistic actions,\nremains a challenge for TAMP. On the contrary, Reinforcement Learning (RL)\nexcels in acquiring versatile, yet short-horizon, manipulation skills that are\nrobust with uncertainties. In this letter, we design a method that integrates\nRL skills into TAMP pipelines. Besides the policy, a RL skill is defined with\ndata-driven logical components that enable the skill to be deployed by symbolic\nplanning. A plan refinement sub-routine is designed to further tackle the\ninevitable effect uncertainties. In the experiments, we compare our method with\nbaseline hierarchical planning from both TAMP and RL fields and illustrate the\nstrength of the method. The results show that by embedding RL skills, we extend\nthe capability of TAMP to domains with probabilistic skills, and improve the\nplanning efficiency compared to the previous methods.",
    "published": "2025-10-15T20:11:19Z",
    "updated": "2025-10-15T20:11:19Z",
    "link": "http://arxiv.org/pdf/2510.14065v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Gaoyuan Liu",
      "Joris de Winter",
      "Yuri Durodie",
      "Denis Steckelmacher",
      "Ann Nowe",
      "Bram Vanderborght"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14063v1",
    "title": "Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous\n  Robot Teaming",
    "summary": "Multi-Agent Task Assignment and Planning (MATP) has attracted growing\nattention but remains challenging in terms of scalability, spatial reasoning,\nand adaptability in obstacle-rich environments. To address these challenges, we\npropose OATH: Adaptive Obstacle-Aware Task Assignment and Planning for\nHeterogeneous Robot Teaming, which advances MATP by introducing a novel\nobstacle-aware strategy for task assignment. First, we develop an adaptive\nHalton sequence map, the first known application of Halton sampling with\nobstacle-aware adaptation in MATP, which adjusts sampling density based on\nobstacle distribution. Second, we propose a cluster-auction-selection framework\nthat integrates obstacle-aware clustering with weighted auctions and\nintra-cluster task selection. These mechanisms jointly enable effective\ncoordination among heterogeneous robots while maintaining scalability and\nnear-optimal allocation performance. In addition, our framework leverages an\nLLM to interpret human instructions and directly guide the planner in real\ntime. We validate OATH in NVIDIA Isaac Sim, showing substantial improvements in\ntask assignment quality, scalability, adaptability to dynamic changes, and\noverall execution performance compared to state-of-the-art MATP baselines. A\nproject website is available at https://llm-oath.github.io/.",
    "published": "2025-10-15T20:04:40Z",
    "updated": "2025-10-15T20:04:40Z",
    "link": "http://arxiv.org/pdf/2510.14063v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Nan Li",
      "Jiming Ren",
      "Haris Miller",
      "Samuel Coogan",
      "Karen M. Feigh",
      "Ye Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14018v1",
    "title": "Spatially Intelligent Patrol Routes for Concealed Emitter Localization\n  by Robot Swarms",
    "summary": "This paper introduces a method for designing spatially intelligent robot\nswarm behaviors to localize concealed radio emitters. We use differential\nevolution to generate geometric patrol routes that localize unknown signals\nindependently of emitter parameters, a key challenge in electromagnetic\nsurveillance. Patrol shape and antenna type are shown to influence information\ngain, which in turn determines the effective triangulation coverage. We\nsimulate a four-robot swarm across eight configurations, assigning\npre-generated patrol routes based on a specified patrol shape and sensing\ncapability (antenna type: omnidirectional or directional). An emitter is placed\nwithin the map for each trial, with randomized position, transmission power and\nfrequency. Results show that omnidirectional localization success rates are\ndriven primarily by source location rather than signal properties, with\nfailures occurring most often when sources are placed in peripheral areas of\nthe map. Directional antennas are able to overcome this limitation due to their\nhigher gain and directivity, with an average detection success rate of 98.75%\ncompared to 80.25% for omnidirectional. Average localization errors range from\n1.01-1.30 m for directional sensing and 1.67-1.90 m for omnidirectional\nsensing; while directional sensing also benefits from shorter patrol edges.\nThese results demonstrate that a swarm's ability to predict electromagnetic\nphenomena is directly dependent on its physical interaction with the\nenvironment. Consequently, spatial intelligence, realized here through\noptimized patrol routes and antenna selection, is a critical design\nconsideration for effective robotic surveillance.",
    "published": "2025-10-15T18:55:42Z",
    "updated": "2025-10-15T18:55:42Z",
    "link": "http://arxiv.org/pdf/2510.14018v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Adam Morris",
      "Timothy Pelham",
      "Edmund R. Hunt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.17830v3",
    "title": "Effect of Performance Feedback Timing on Motor Learning for a Surgical\n  Training Task",
    "summary": "Objective: Robot-assisted minimally invasive surgery (RMIS) has become the\ngold standard for a variety of surgical procedures, but the optimal method of\ntraining surgeons for RMIS is unknown. We hypothesized that real-time, rather\nthan post-task, error feedback would better increase learning speed and reduce\nerrors. Methods: Forty-two surgical novices learned a virtual version of the\nring-on-wire task, a canonical task in RMIS training. We investigated the\nimpact of feedback timing with multi-sensory (haptic and visual) cues in three\ngroups: (1) real-time error feedback, (2) trial replay with error feedback, and\n(3) no error feedback. Results: Participant performance was evaluated based on\nthe accuracy of ring position and orientation during the task. Participants who\nreceived real-time feedback outperformed other groups in ring orientation.\nAdditionally, participants who received feedback in replay outperformed\nparticipants who did not receive any error feedback on ring orientation during\nlong, straight path sections. There were no significant differences between\ngroups for ring position overall, but participants who received real-time\nfeedback outperformed the other groups in positional accuracy on tightly curved\npath sections. Conclusion: The addition of real-time haptic and visual error\nfeedback improves learning outcomes in a virtual surgical task over error\nfeedback in replay or no error feedback at all. Significance: This work\ndemonstrates that multi-sensory error feedback delivered in real time leads to\nbetter training outcomes as compared to the same feedback delivered after task\ncompletion. This novel method of training may enable surgical trainees to\ndevelop skills with greater speed and accuracy.",
    "published": "2025-08-25T09:30:17Z",
    "updated": "2025-10-15T18:47:07Z",
    "link": "http://arxiv.org/pdf/2508.17830v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Mary Kate Gale",
      "Kailana Baker-Matsuoka",
      "Ilana Nisky",
      "Allison Okamura"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14000v1",
    "title": "A Diffusion-Refined Planner with Reinforcement Learning Priors for\n  Confined-Space Parking",
    "summary": "The growing demand for parking has increased the need for automated parking\nplanning methods that can operate reliably in confined spaces. In restricted\nand complex environments, high-precision maneuvers are required to achieve a\nhigh success rate in planning, yet existing approaches often rely on explicit\naction modeling, which faces challenges when accurately modeling the optimal\naction distribution. In this paper, we propose DRIP, a diffusion-refined\nplanner anchored in reinforcement learning (RL) prior action distribution, in\nwhich an RL-pretrained policy provides prior action distributions to regularize\nthe diffusion training process. During the inference phase the denoising\nprocess refines these coarse priors into more precise action distributions. By\nsteering the denoising trajectory through the reinforcement learning prior\ndistribution during training, the diffusion model inherits a well-informed\ninitialization, resulting in more accurate action modeling, a higher planning\nsuccess rate, and reduced inference steps. We evaluate our approach across\nparking scenarios with varying degrees of spatial constraints. Experimental\nresults demonstrate that our method significantly improves planning performance\nin confined-space parking environments while maintaining strong generalization\nin common scenarios.",
    "published": "2025-10-15T18:30:14Z",
    "updated": "2025-10-15T18:30:14Z",
    "link": "http://arxiv.org/pdf/2510.14000v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Mingyang Jiang",
      "Yueyuan Li",
      "Jiaru Zhang",
      "Songan Zhang",
      "Ming Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.03408v3",
    "title": "Opti-Acoustic Scene Reconstruction in Highly Turbid Underwater\n  Environments",
    "summary": "Scene reconstruction is an essential capability for underwater robots\nnavigating in close proximity to structures. Monocular vision-based\nreconstruction methods are unreliable in turbid waters and lack depth scale\ninformation. Sonars are robust to turbid water and non-uniform lighting\nconditions, however, they have low resolution and elevation ambiguity. This\nwork proposes a real-time opti-acoustic scene reconstruction method that is\nspecially optimized to work in turbid water. Our strategy avoids having to\nidentify point features in visual data and instead identifies regions of\ninterest in the data. We then match relevant regions in the image to\ncorresponding sonar data. A reconstruction is obtained by leveraging range data\nfrom the sonar and elevation data from the camera image. Experimental\ncomparisons against other vision-based and sonar-based approaches at varying\nturbidity levels, and field tests conducted in marina environments, validate\nthe effectiveness of the proposed approach. We have made our code open-source\nto facilitate reproducibility and encourage community engagement.",
    "published": "2025-08-05T12:53:45Z",
    "updated": "2025-10-15T18:03:11Z",
    "link": "http://arxiv.org/pdf/2508.03408v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Ivana Collado-Gonzalez",
      "John McConnell",
      "Paul Szenher",
      "Brendan Englot"
    ]
  }
]