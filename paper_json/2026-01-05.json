[
  {
    "id": "http://arxiv.org/abs/2506.09827v3",
    "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection",
    "summary": "Speech emotion recognition (SER) systems are constrained by existing datasets that typically cover only 6-10 basic emotions, lack scale and diversity, and face ethical challenges when collecting sensitive emotional states. We introduce EMONET-VOICE, a comprehensive resource addressing these limitations through two components: (1) EmoNet-Voice Big, a 5,000-hour multilingual pre-training dataset spanning 40 fine-grained emotion categories across 11 voices and 4 languages, and (2) EmoNet-Voice Bench, a rigorously validated benchmark of 4,7k samples with unanimous expert consensus on emotion presence and intensity levels. Using state-of-the-art synthetic voice generation, our privacy-preserving approach enables ethical inclusion of sensitive emotions (e.g., pain, shame) while maintaining controlled experimental conditions. Each sample underwent validation by three psychology experts. We demonstrate that our Empathic Insight models trained on our synthetic data achieve strong real-world dataset generalization, as tested on EmoDB and RAVDESS. Furthermore, our comprehensive evaluation reveals that while high-arousal emotions (e.g., anger: 95% accuracy) are readily detected, the benchmark successfully exposes the difficulty of distinguishing perceptually similar emotions (e.g., sadness vs. distress: 63% discrimination), providing quantifiable metrics for advancing nuanced emotion AI. EMONET-VOICE establishes a new paradigm for large-scale, ethically-sourced, fine-grained SER research.",
    "published": "2025-06-11T15:06:59Z",
    "updated": "2026-01-05T18:59:29Z",
    "link": "http://arxiv.org/pdf/2506.09827v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Christoph Schuhmann",
      "Robert Kaczmarczyk",
      "Gollam Rabby",
      "Felix Friedrich",
      "Maurice Kraus",
      "Kourosh Nadi",
      "Huu Nguyen",
      "Kristian Kersting",
      "Sören Auer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.21907v2",
    "title": "SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?",
    "summary": "Spatial transcriptomics assays are rapidly increasing in scale and complexity, making computational analysis a major bottleneck in biological discovery. Although frontier AI agents have improved dramatically at software engineering and general data analysis, it remains unclear whether they can extract biological insight from messy, real-world spatial datasets. We introduce SpatialBench, a benchmark of 146 verifiable problems derived from practical spatial analysis workflows spanning five spatial technologies and seven task categories. Each problem provides a snapshot of experimental data immediately prior to an analysis step and a deterministic grader that evaluates recovery of a key biological result. Benchmark data on frontier models shows that base model accuracy remains low (20-38% across model families), with strong model-task and model-platform interactions. Harness design has a large empirical effect on performance, indicating that tools, prompts, control flow, and execution environment should be evaluated and improved as first-class objects. SpatialBench serves both as a measurement tool and a diagnostic lens for developing agents that can interact with real spatial datasets faithfully, transparently, and reproducibly.",
    "published": "2025-12-26T07:40:11Z",
    "updated": "2026-01-05T18:55:51Z",
    "link": "http://arxiv.org/pdf/2512.21907v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Kenny Workman",
      "Zhen Yang",
      "Harihara Muralidharan",
      "Hannah Le"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02357v1",
    "title": "DARC: Drum accompaniment generation with fine-grained rhythm control",
    "summary": "In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.",
    "published": "2026-01-05T18:55:43Z",
    "updated": "2026-01-05T18:55:43Z",
    "link": "http://arxiv.org/pdf/2601.02357v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "authors": [
      "Trey Brosnan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02346v1",
    "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
    "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.",
    "published": "2026-01-05T18:44:27Z",
    "updated": "2026-01-05T18:44:27Z",
    "link": "http://arxiv.org/pdf/2601.02346v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      " Falcon LLM Team",
      "Iheb Chaabane",
      "Puneesh Khanna",
      "Suhail Mohmad",
      "Slim Frikha",
      "Shi Hu",
      "Abdalgader Abubaker",
      "Reda Alami",
      "Mikhail Lubinets",
      "Mohamed El Amine Seddik",
      "Hakim Hacid"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.04346v6",
    "title": "Improving Action Smoothness for a Cascaded Online Learning Flight Control System",
    "summary": "This paper aims to improve the action smoothness of a cascaded online learning flight control system. Although the cascaded structure is widely used in flight control design, its stability can be compromised by oscillatory control actions, which poses challenges for practical engineering applications. To address this issue, we introduce an online temporal smoothness technique and a low-pass filter to reduce the amplitude and frequency of the control actions. Fast Fourier Transform (FFT) is used to analyze policy performance in the frequency domain. Simulation results demonstrate the improvements achieved by the two proposed techniques.",
    "published": "2025-07-06T11:19:34Z",
    "updated": "2026-01-05T18:39:22Z",
    "link": "http://arxiv.org/pdf/2507.04346v6.pdf",
    "category": [
      "eess.SY",
      "cs.AI"
    ],
    "authors": [
      "Yifei Li",
      "Erik-jan van Kampen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.18773v3",
    "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit KV Cache",
    "summary": "The growth of long-context Large Language Models (LLMs) significantly increases memory and bandwidth pressure during autoregressive decoding due to the expanding Key-Value (KV) cache. While accuracy-preserving KV-cache quantization (e.g., 4-bit or 2-bit) reduces memory footprint, existing systems decode inefficiently by relying solely on CUDA cores, underutilizing Tensor Cores-the dominant compute resource on GPUs.\n  We present BitDecoding, the first inference system to efficiently decode low-bit KV caches by cooperatively leveraging CUDA cores and Tensor Cores. BitDecoding smartly induces Tensor-Core-friendly layouts, introduces warp-level dequantization parallelism, and provides unified system support through query transformation, high-performance tensor- and channel-wise quantization, and a software-pipelined dequantization kernel enabling mixed-precision execution. Architecture-aware optimizations further leverage Hopper's warpgroup tensor instructions and Blackwell's NVFP4 (MXFP4) tensor formats.\n  Evaluated on Blackwell, Hopper, and Ampere GPUs, BitDecoding achieves an average 7.5x decoding speedup over FP16 FlashDecoding-v2, up to 8.6x on Blackwell with NVFP4, and up to 4.3x over state-of-the-art approaches. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x. BitDecoding is open-sourced at https://github.com/OpenBitSys/BitDecoding.",
    "published": "2025-03-24T15:22:41Z",
    "updated": "2026-01-05T18:08:27Z",
    "link": "http://arxiv.org/pdf/2503.18773v3.pdf",
    "category": [
      "cs.AR",
      "cs.AI",
      "cs.CL",
      "cs.PF"
    ],
    "authors": [
      "Dayou Du",
      "Shijie Cao",
      "Jianyi Cheng",
      "Luo Mai",
      "Ting Cao",
      "Mao Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02316v1",
    "title": "DatBench: Discriminative, Faithful, and Efficient VLM Evaluations",
    "summary": "Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.",
    "published": "2026-01-05T18:07:51Z",
    "updated": "2026-01-05T18:07:51Z",
    "link": "http://arxiv.org/pdf/2601.02316v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Siddharth Joshi",
      "Haoli Yin",
      "Rishabh Adiga",
      "Ricardo Monti",
      "Aldo Carranza",
      "Alex Fang",
      "Alvin Deng",
      "Amro Abbas",
      "Brett Larsen",
      "Cody Blakeney",
      "Darren Teh",
      "David Schwab",
      "Fan Pan",
      "Haakon Mongstad",
      "Jack Urbanek",
      "Jason Lee",
      "Jason Telanoff",
      "Josh Wills",
      "Kaleigh Mentzer",
      "Luke Merrick",
      "Parth Doshi",
      "Paul Burstein",
      "Pratyush Maini",
      "Scott Loftin",
      "Spandan Das",
      "Tony Jiang",
      "Vineeth Dorna",
      "Zhengping Wang",
      "Bogdan Gaza",
      "Ari Morcos",
      "Matthew Leavitt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02314v1",
    "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
    "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.",
    "published": "2026-01-05T18:05:29Z",
    "updated": "2026-01-05T18:05:29Z",
    "link": "http://arxiv.org/pdf/2601.02314v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Sourena Khanzadeh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02311v1",
    "title": "Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies",
    "summary": "Training large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activations) across devices using five modes (replicated, sharded, sharded-with-gather, materialized, offloaded). From placement alone, without implementation details, we derive memory consumption and communication volume. Our predictions match published results exactly: ZeRO-3 uses 8x less memory than data parallelism at 1.5x communication cost, as reported in the original paper. We prove two conditions (gradient integrity, state consistency) are necessary and sufficient for distributed training to match single-device results, and provide composition rules for combining strategies safely. The framework unifies ZeRO Stages 1-3, Fully Sharded Data Parallel (FSDP), tensor parallelism, and pipeline parallelism as instances with different placement choices.",
    "published": "2026-01-05T18:01:38Z",
    "updated": "2026-01-05T18:01:38Z",
    "link": "http://arxiv.org/pdf/2601.02311v1.pdf",
    "category": [
      "cs.DC",
      "cs.AI"
    ],
    "authors": [
      "Deep Pankajbhai Mehta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.06478v2",
    "title": "Anytime-Valid Answer Sufficiency Certificates for LLM Generation via Sequential Information Lift",
    "summary": "We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), which applies anytime-valid sequential testing to language model generation stopping. Our approach tracks information lift, defined as the log-likelihood ratio between the full model and deliberately weakened \"skeleton\" baselines, using self-normalized empirical-Bernstein e-processes that provide formal delta-level error control regardless of stopping time. This delta guarantee controls premature stopping when information lift is insufficient relative to the skeleton, and it does not imply delta control of factual incorrectness or hallucinations. We handle unknown centering through online mean estimation, combine multiple parameters via mixture e-processes, and support adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL reduces generation length by 22 to 28 percent relative to sequential baselines while maintaining delta-level control with 12 percent computational overhead. We introduce automated skeletons (distilled submodels and randomized logits) and show robustness across skeleton families. Composing EDFL with a lightweight correctness gate (sentence boundaries plus a verifier) improves end-task correctness while preserving anytime-valid guarantees by only delaying stopping. Our certificates control information sufficiency, not factual correctness. Specifically, 10.9 percent of stopped sequences remain incorrect even with the gate (13.2 to 22.7 percent without it). EDFL serves as a first-stage filter that can reduce verification burden: when applied to stopped sequences, the gate validates 83 percent of stops, requiring full verification only for the remaining 17 percent, plus all non-stopped sequences. EDFL is not a standalone solution for safety-critical domains.",
    "published": "2025-10-07T21:28:53Z",
    "updated": "2026-01-05T17:33:34Z",
    "link": "http://arxiv.org/pdf/2510.06478v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Sanjeda Akter",
      "Ibne Farabi Shihab",
      "Anuj Sharma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.01208v2",
    "title": "Language as a Wave Phenomenon: Iso-Energetic Phase-Locking and Semantic Interference in Neural Networks",
    "summary": "Conventional deep learning paradigms rely on metabolically expensive magnitude-based representations, rendering them fundamentally incompatible with passive photonic hardware. We introduce PRISM, a sequence modeling architecture that bridges high-level reasoning and physical constraints by enforcing an Iso-Energetic (Unity Gain) principle, compelling the network to encode semantic information exclusively in the phase angle. Validated on the WMT14 translation benchmark, PRISM achieves a 0.799 COMET score, demonstrating that phase-based reasoning competes with standard Transformers (0.821) and functionally matches unconstrained spectral baselines like FNet (0.805), despite enforcing strict energy constraints and requiring 11.5% fewer parameters. Furthermore, to verify hardware feasibility, we simulate a Holographic Backpropagation mechanism on a noisy, 4-bit optical correlator. Ablation studies reveal a substantial performance gain (48.4% vs. 62.4%) over a frozen baseline, proving that the proposed phase-steering mechanism actively optimizes physical parameters under strict energy constraints. These results establish an existence proof that ultra-low-power, passive optical hardware can support high-level linguistic intelligence without sacrificing representational capacity.",
    "published": "2025-12-01T02:46:15Z",
    "updated": "2026-01-05T17:26:51Z",
    "link": "http://arxiv.org/pdf/2512.01208v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Alper Yıldırım",
      "İbrahim Yücedağ"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.01544v3",
    "title": "Causal Consistency Regularization: Training Verifiably Sensitive Reasoning in Large Language Models",
    "summary": "Large language models can produce correct answers while relying on flawed reasoning traces, partly because common training objectives reward final-answer correctness rather than faithful intermediate reasoning. This undermines trustworthiness in high-stakes settings. We propose Counterfactual Sensitivity Regularization (CSR), a training paradigm that improves reasoning faithfulness by enforcing causal consistency between reasoning steps and outcomes. CSR automatically applies operator-level interventions to reasoning traces, such as swapping \"+\" with \"-\", to generate minimally perturbed counterfactual rationales, and penalizes the model when these logically invalid traces still lead to the original answer. Our implementation is efficient, adding about 9 percent training overhead via a warm-start curriculum and token-subset optimization.\n  We evaluate faithfulness using Counterfactual Outcome Sensitivity (COS), which measures how appropriately answers change under logical perturbations. Across arithmetic (GSM8K), logical deduction (ProofWriter), multi-hop question answering (HotpotQA), and code generation (MBPP), CSR yields improved accuracy versus faithfulness trade-offs, establishing a new Pareto frontier. CSR improves faithfulness over standard fine-tuning and process supervision by up to 70 percentage points, and transfers across model families with 94.2 to 96.7 percent success in structured domains. CSR also complements inference-time methods such as self-consistency. Overall, CSR offers a practical route to more reliable reasoning in structured domains, including mathematics, formal logic, and code, where operators are well-defined and verifiable, covering an estimated 40 to 60 percent of high-stakes reasoning deployments.",
    "published": "2025-09-01T15:18:46Z",
    "updated": "2026-01-05T17:24:02Z",
    "link": "http://arxiv.org/pdf/2509.01544v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Sanjeda Akter",
      "Ibne Farabi Shihab",
      "Anuj Sharma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02285v1",
    "title": "pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs",
    "summary": "PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).",
    "published": "2026-01-05T17:15:26Z",
    "updated": "2026-01-05T17:15:26Z",
    "link": "http://arxiv.org/pdf/2601.02285v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Tobias Schimanski",
      "Imene Kolli",
      "Jingwei Ni",
      "Yu Fan",
      "Ario Saeid Vaghefi",
      "Elliott Ash",
      "Markus Leippold"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02273v1",
    "title": "TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation",
    "summary": "Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \\textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \\textbf{5.2\\%} of model parameters ($\\sim$4.9M). On the challenging CHASE\\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git",
    "published": "2026-01-05T17:03:45Z",
    "updated": "2026-01-05T17:03:45Z",
    "link": "http://arxiv.org/pdf/2601.02273v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Salim Khazem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.24880v2",
    "title": "mHC: Manifold-Constrained Hyper-Connections",
    "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.",
    "published": "2025-12-31T14:16:26Z",
    "updated": "2026-01-05T16:51:18Z",
    "link": "http://arxiv.org/pdf/2512.24880v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Zhenda Xie",
      "Yixuan Wei",
      "Huanqi Cao",
      "Chenggang Zhao",
      "Chengqi Deng",
      "Jiashi Li",
      "Damai Dai",
      "Huazuo Gao",
      "Jiang Chang",
      "Kuai Yu",
      "Liang Zhao",
      "Shangyan Zhou",
      "Zhean Xu",
      "Zhengyan Zhang",
      "Wangding Zeng",
      "Shengding Hu",
      "Yuqing Wang",
      "Jingyang Yuan",
      "Lean Wang",
      "Wenfeng Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02246v1",
    "title": "A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets",
    "summary": "Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.",
    "published": "2026-01-05T16:26:32Z",
    "updated": "2026-01-05T16:26:32Z",
    "link": "http://arxiv.org/pdf/2601.02246v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Annoor Sharara Akhand"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02242v1",
    "title": "VIBE: Visual Instruction Based Editor",
    "summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.",
    "published": "2026-01-05T16:17:20Z",
    "updated": "2026-01-05T16:17:20Z",
    "link": "http://arxiv.org/pdf/2601.02242v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Grigorii Alekseenko",
      "Aleksandr Gordeev",
      "Irina Tolstykh",
      "Bulat Suleimanov",
      "Vladimir Dokholyan",
      "Georgii Fedorov",
      "Sergey Yakubson",
      "Aleksandra Tsybina",
      "Mikhail Chernyshov",
      "Maksim Kuprashevich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.20629v3",
    "title": "Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning",
    "summary": "This study proposes a multi-agent language framework that enables continual strategy evolution without fine-tuning the language model's parameters. The core idea is to liberate the latent vectors of abstract concepts from traditional static semantic representations, allowing them to be continuously updated through environmental interaction and reinforcement feedback. We construct a dual-loop architecture: the behavior loop adjusts action preferences based on environmental rewards, while the language loop updates the external latent vectors by reflecting on the semantic embeddings of generated text.\n  Together, these mechanisms allow agents to develop stable and disentangled strategic styles over long-horizon multi-round interactions. Experiments show that agents' latent spaces exhibit clear convergence trajectories under reflection-driven updates, along with structured shifts at critical moments. Moreover, the system demonstrates an emergent ability to implicitly infer and continually adapt to emotional agents, even without shared rewards. These results indicate that, without modifying model parameters, an external latent space can provide language agents with a low-cost, scalable, and interpretable form of abstract strategic representation.",
    "published": "2025-11-28T23:36:45Z",
    "updated": "2026-01-05T16:14:39Z",
    "link": "http://arxiv.org/pdf/2512.20629v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Wenlong Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.01752v3",
    "title": "Tuning without Peeking: Provable Generalization Bounds and Robust LLM Post-Training",
    "summary": "Gradient-based optimization is the workhorse of deep learning, offering efficient and scalable training via backpropagation. However, exposing gradients during training can leak sensitive information about the underlying data, raising privacy and security concerns such as susceptibility to data poisoning attacks. In contrast, black box optimization methods, which treat the model as an opaque function, relying solely on function evaluations to guide optimization, offer a promising alternative in scenarios where data access is restricted, adversarial risks are high, or overfitting is a concern. This paper introduces BBoxER, an evolutionary black-box method for LLM post-training that induces an information bottleneck via implicit compression of the training data. Leveraging the tractability of information flow, we provide non-vacuous generalization bounds and strong theoretical guarantees for privacy, robustness to data poisoning attacks, and extraction attacks. In experiments with LLMs, we demonstrate empirically that black-box optimization methods, despite the scalability and computational challenges inherent to black-box approaches, are able to learn, showing how a few iterations of BBoxER improve performance, generalize well on a benchmark of reasoning datasets, and are robust to membership inference attacks. This positions BBoxER as an attractive add-on on top of gradient-based optimization, offering suitability for deployment in restricted or privacy-sensitive environments while also providing non-vacuous generalization guarantees.",
    "published": "2025-07-02T14:29:30Z",
    "updated": "2026-01-05T16:10:09Z",
    "link": "http://arxiv.org/pdf/2507.01752v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "authors": [
      "Ismail Labiad",
      "Mathurin Videau",
      "Matthieu Kowalski",
      "Marc Schoenauer",
      "Alessandro Leite",
      "Julia Kempe",
      "Olivier Teytaud"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.15231v2",
    "title": "CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications",
    "summary": "The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).",
    "published": "2025-12-17T09:31:57Z",
    "updated": "2026-01-05T15:48:10Z",
    "link": "http://arxiv.org/pdf/2512.15231v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhengchao Chen",
      "Haoran Wang",
      "Jing Yao",
      "Pedram Ghamisi",
      "Jun Zhou",
      "Peter M. Atkinson",
      "Bing Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.20182v2",
    "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
    "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
    "published": "2025-12-23T09:20:32Z",
    "updated": "2026-01-05T15:43:00Z",
    "link": "http://arxiv.org/pdf/2512.20182v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Shuzheng Si",
      "Qingyi Wang",
      "Haozhe Zhao",
      "Yuzhuo Bai",
      "Guanqiao Chen",
      "Kangyang Luo",
      "Gang Chen",
      "Fanchao Qi",
      "Minjia Zhang",
      "Baobao Chang",
      "Maosong Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02215v1",
    "title": "LLM-Empowered Functional Safety and Security by Design in Automotive Systems",
    "summary": "This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.",
    "published": "2026-01-05T15:37:08Z",
    "updated": "2026-01-05T15:37:08Z",
    "link": "http://arxiv.org/pdf/2601.02215v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Nenad Petrovic",
      "Vahid Zolfaghari",
      "Fengjunjie Pan",
      "Alois Knoll"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02206v1",
    "title": "Seeing the Unseen: Zooming in the Dark with Event Cameras",
    "summary": "This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.",
    "published": "2026-01-05T15:31:07Z",
    "updated": "2026-01-05T15:31:07Z",
    "link": "http://arxiv.org/pdf/2601.02206v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Dachun Kai",
      "Zeyu Xiao",
      "Huyue Zhu",
      "Jiaxiao Wang",
      "Yueyi Zhang",
      "Xiaoyan Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02204v1",
    "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
    "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.",
    "published": "2026-01-05T15:27:04Z",
    "updated": "2026-01-05T15:27:04Z",
    "link": "http://arxiv.org/pdf/2601.02204v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Huichao Zhang",
      "Liao Qu",
      "Yiheng Liu",
      "Hang Chen",
      "Yangyang Song",
      "Yongsheng Dong",
      "Shikun Sun",
      "Xian Li",
      "Xu Wang",
      "Yi Jiang",
      "Hu Ye",
      "Bo Chen",
      "Yiming Gao",
      "Peng Liu",
      "Akide Liu",
      "Zhipeng Yang",
      "Qili Deng",
      "Linjie Xing",
      "Jiyang Liu",
      "Zhao Wang",
      "Yang Zhou",
      "Mingcong Liu",
      "Yi Zhang",
      "Qian He",
      "Xiwei Hu",
      "Zhongqi Qi",
      "Jie Shao",
      "Zhiye Fu",
      "Shuai Wang",
      "Fangmin Chen",
      "Xuezhi Chai",
      "Zhihua Wu",
      "Yitong Wang",
      "Zehuan Yuan",
      "Daniel K. Du",
      "Xinglong Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.08873v2",
    "title": "UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models",
    "summary": "Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.",
    "published": "2025-11-12T01:27:02Z",
    "updated": "2026-01-05T15:26:03Z",
    "link": "http://arxiv.org/pdf/2511.08873v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Shouang Wei",
      "Min Zhang",
      "Xin Lin",
      "Bo Jiang",
      "Kun Kuang",
      "Zhongxiang Dai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02200v1",
    "title": "Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics",
    "summary": "We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.",
    "published": "2026-01-05T15:23:55Z",
    "updated": "2026-01-05T15:23:55Z",
    "link": "http://arxiv.org/pdf/2601.02200v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Markus Borg",
      "Nadim Hagatulah",
      "Adam Tornhill",
      "Emma Söderberg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.22458v2",
    "title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey",
    "summary": "This survey examines evaluation methods for large language model (LLM)-based agents in multi-turn conversational settings. Using a PRISMA-inspired framework, we systematically reviewed nearly 250 scholarly sources, capturing the state of the art from various venues of publication, and establishing a solid foundation for our analysis. Our study offers a structured approach by developing two interrelated taxonomy systems: one that defines \\emph{what to evaluate} and another that explains \\emph{how to evaluate}. The first taxonomy identifies key components of LLM-based agents for multi-turn conversations and their evaluation dimensions, including task completion, response quality, user experience, memory and context retention, as well as planning and tool integration. These components ensure that the performance of conversational agents is assessed in a holistic and meaningful manner. The second taxonomy system focuses on the evaluation methodologies. It categorizes approaches into annotation-based evaluations, automated metrics, hybrid strategies that combine human assessments with quantitative measures, and self-judging methods utilizing LLMs. This framework not only captures traditional metrics derived from language understanding, such as BLEU and ROUGE scores, but also incorporates advanced techniques that reflect the dynamic, interactive nature of multi-turn dialogues.",
    "published": "2025-03-28T14:08:40Z",
    "updated": "2026-01-05T15:14:04Z",
    "link": "http://arxiv.org/pdf/2503.22458v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Shengyue Guan",
      "Jindong Wang",
      "Jiang Bian",
      "Bin Zhu",
      "Jian-guang Lou",
      "Haoyi Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20113v3",
    "title": "Discovering Association Rules in High-Dimensional Small Tabular Data",
    "summary": "Association Rule Mining (ARM) aims to discover patterns between features in datasets in the form of propositional rules, supporting both knowledge discovery and interpretable machine learning in high-stakes decision-making. However, in high-dimensional settings, rule explosion and computational overhead render popular algorithmic approaches impractical without effective search space reduction, challenges that propagate to downstream tasks. Neurosymbolic methods, such as Aerial+, have recently been proposed to address the rule explosion in ARM. While they tackle the high dimensionality of the data, they also inherit limitations of neural networks, particularly reduced performance in low-data regimes.\n  This paper makes three key contributions to association rule discovery in high-dimensional tabular data. First, we empirically show that Aerial+ scales one to two orders of magnitude better than state-of-the-art algorithmic and neurosymbolic baselines across five real-world datasets. Second, we introduce the novel problem of ARM in high-dimensional, low-data settings, such as gene expression data from the biomedicine domain with around 18k features and 50 samples. Third, we propose two fine-tuning approaches to Aerial+ using tabular foundation models. Our proposed approaches are shown to significantly improve rule quality on five real-world datasets, demonstrating their effectiveness in low-data, high-dimensional scenarios.",
    "published": "2025-09-24T13:37:53Z",
    "updated": "2026-01-05T14:53:38Z",
    "link": "http://arxiv.org/pdf/2509.20113v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Erkan Karabulut",
      "Daniel Daza",
      "Paul Groth",
      "Victoria Degeler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19406v5",
    "title": "TimeMosaic: Temporal Heterogeneity Guided Time Series Forecasting via Adaptive Granularity Patch and Segment-wise Decoding",
    "summary": "Multivariate time series forecasting is essential in domains such as finance, transportation, climate, and energy. However, existing patch-based methods typically adopt fixed-length segmentation, overlooking the heterogeneity of local temporal dynamics and the decoding heterogeneity of forecasting. Such designs lose details in information-dense regions, introduce redundancy in stable segments, and fail to capture the distinct complexities of short-term and long-term horizons. We propose TimeMosaic, a forecasting framework that aims to address temporal heterogeneity. TimeMosaic employs adaptive patch embedding to dynamically adjust granularity according to local information density, balancing motif reuse with structural clarity while preserving temporal continuity. In addition, it introduces segment-wise decoding that treats each prediction horizon as a related subtask and adapts to horizon-specific difficulty and information requirements, rather than applying a single uniform decoder. Extensive evaluations on benchmark datasets demonstrate that TimeMosaic delivers consistent improvements over existing methods, and our model trained on the large-scale corpus with 321 billion observations achieves performance competitive with state-of-the-art TSFMs.",
    "published": "2025-09-23T09:20:00Z",
    "updated": "2026-01-05T14:53:28Z",
    "link": "http://arxiv.org/pdf/2509.19406v5.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Kuiye Ding",
      "Fanda Fan",
      "Chunyi Hou",
      "Zheya Wang",
      "Lei Wang",
      "Zhengxin Yang",
      "Jianfeng Zhan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02170v1",
    "title": "Streaming Hallucination Detection in Long Chain-of-Thought Reasoning",
    "summary": "Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.",
    "published": "2026-01-05T14:47:41Z",
    "updated": "2026-01-05T14:47:41Z",
    "link": "http://arxiv.org/pdf/2601.02170v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Haolang Lu",
      "Minghui Pan",
      "Ripeng Li",
      "Guoshun Nan",
      "Jialin Zhuang",
      "Zijie Zhao",
      "Zhongxiang Sun",
      "Kun Wang",
      "Yang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02163v1",
    "title": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning",
    "summary": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.",
    "published": "2026-01-05T14:39:43Z",
    "updated": "2026-01-05T14:39:43Z",
    "link": "http://arxiv.org/pdf/2601.02163v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Chuanrui Hu",
      "Xingze Gao",
      "Zuyi Zhou",
      "Dannong Xu",
      "Yi Bai",
      "Xintong Li",
      "Hui Zhang",
      "Tong Li",
      "Chong Zhang",
      "Lidong Bing",
      "Yafeng Deng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02158v1",
    "title": "FormationEval, an open multiple-choice benchmark for petroleum geoscience",
    "summary": "This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\\% accuracy, with Gemini 3 Pro Preview reaching 99.8\\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.",
    "published": "2026-01-05T14:36:02Z",
    "updated": "2026-01-05T14:36:02Z",
    "link": "http://arxiv.org/pdf/2601.02158v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "physics.geo-ph"
    ],
    "authors": [
      "Almaz Ermilov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.24470v2",
    "title": "Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models",
    "summary": "The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained VLM fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert->fallback maneuver->operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird's-eye-view perception and short-horizon replanning. Website: kimachristensen.github.io/bridge_policy",
    "published": "2025-12-30T21:20:41Z",
    "updated": "2026-01-05T14:30:28Z",
    "link": "http://arxiv.org/pdf/2512.24470v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Kim Alexander Christensen",
      "Andreas Gudahl Tufte",
      "Alexey Gusev",
      "Rohan Sinha",
      "Milan Ganai",
      "Ole Andreas Alsos",
      "Marco Pavone",
      "Martin Steinert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02151v1",
    "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting",
    "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.",
    "published": "2026-01-05T14:28:17Z",
    "updated": "2026-01-05T14:28:17Z",
    "link": "http://arxiv.org/pdf/2601.02151v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Muxi Diao",
      "Lele Yang",
      "Wuxuan Gong",
      "Yutong Zhang",
      "Zhonghao Yan",
      "Yufei Han",
      "Kongming Liang",
      "Weiran Xu",
      "Zhanyu Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02149v1",
    "title": "AI-enhanced tuning of quantum dot Hamiltonians toward Majorana modes",
    "summary": "We propose a neural network-based model capable of learning the broad landscape of working regimes in quantum dot simulators, and using this knowledge to autotune these devices - based on transport measurements - toward obtaining Majorana modes in the structure. The model is trained in an unsupervised manner on synthetic data in the form of conductance maps, using a physics-informed loss that incorporates key properties of Majorana zero modes. We show that, with appropriate training, a deep vision-transformer network can efficiently memorize relation between Hamiltonian parameters and structures on conductance maps and use it to propose parameters update for a quantum dot chain that drive the system toward topological phase. Starting from a broad range of initial detunings in parameter space, a single update step is sufficient to generate nontrivial zero modes. Moreover, by enabling an iterative tuning procedure - where the system acquires updated conductance maps at each step - we demonstrate that the method can address a much larger region of the parameter space.",
    "published": "2026-01-05T14:25:49Z",
    "updated": "2026-01-05T14:25:49Z",
    "link": "http://arxiv.org/pdf/2601.02149v1.pdf",
    "category": [
      "cond-mat.mes-hall",
      "cond-mat.dis-nn",
      "cs.AI"
    ],
    "authors": [
      "Mateusz Krawczyk",
      "Jarosław Pawłowski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02147v1",
    "title": "BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models",
    "summary": "Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.",
    "published": "2026-01-05T14:22:20Z",
    "updated": "2026-01-05T14:22:20Z",
    "link": "http://arxiv.org/pdf/2601.02147v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Sunny Gupta",
      "Shounak Das",
      "Amit Sethi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02144v1",
    "title": "Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts",
    "summary": "Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric \"router\" to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.",
    "published": "2026-01-05T14:16:11Z",
    "updated": "2026-01-05T14:16:11Z",
    "link": "http://arxiv.org/pdf/2601.02144v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Boxuan Lyu",
      "Soichiro Murakami",
      "Hidetaka Kamigaito",
      "Peinan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.11320v2",
    "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints",
    "summary": "Large Language Models (LLMs) power many modern applications, but their inference procedure poses unique scheduling challenges: the Key-Value (KV) cache grows dynamically during response generation, and memory overflow triggers eviction that can cascade into system-wide failures. Even when memory capacity exceeds the theoretical requirement, conventional scheduling algorithms fail because they do not account for this dynamic memory growth -- a system that should be stable can become unstable under poor scheduling.\n  This paper formulates LLM inference optimization as a multi-stage online scheduling problem. We develop a fluid dynamics approximation to establish a tractable benchmark and derive the Waiting for Accumulated Inference Threshold (WAIT) algorithm. WAIT uses threshold-based batching to prevent eviction by keeping the system near load balance, achieving near-optimal throughput when output lengths are known.\n  For practical settings where output lengths are unknown at arrival, we introduce Nested WAIT. Rather than predicting output lengths, Nested WAIT classifies prompts on-the-fly: short prompts complete early and exit, while longer prompts naturally advance to later segments. A safety buffer provides high-probability protection against memory overflow with only logarithmic overhead.\n  Theoretical analysis establishes near-optimal performance in the asymptotic regime. Experiments on Llama-7B with an A100 GPU demonstrate that our approach achieves superior throughput and reduced latency compared to vLLM and Sarathi. This work applies operations research principles to establish a theoretical framework for LLM deployment under memory constraints.",
    "published": "2025-04-15T16:00:21Z",
    "updated": "2026-01-05T14:10:45Z",
    "link": "http://arxiv.org/pdf/2504.11320v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "math.OC",
      "stat.ML"
    ],
    "authors": [
      "Ruicheng Ao",
      "Gan Luo",
      "David Simchi-Levi",
      "Xinshang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.24793v2",
    "title": "SwiftEmbed: Ultra-Fast Text Embeddings via Static Token Lookup for Real-Time Applications",
    "summary": "We present a static token lookup methodology for text embedding generation that achieves 1.12 ms p50 latency for single text embeddings while maintaining 60.6 MTEB average score across 8 representative tasks, corresponding to 89% of contextual model quality. The Rust implementation delivers 50,000 requests per second throughput through static embedding lookup, optimized mean pooling, and zero-copy IEEE754 binary serialization. Evaluation demonstrates exceptional duplicate detection performance (90.1% AP), strong semantic similarity (76.1% Spearman correlation), and domain-specific performance ranging from 75% to 131% of baseline across specialized domains. The system enables real-time embedding applications where sub-5ms latency is critica",
    "published": "2025-10-27T13:40:26Z",
    "updated": "2026-01-05T14:08:35Z",
    "link": "http://arxiv.org/pdf/2510.24793v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Edouard Lansiaux",
      "Antoine Simonet",
      "Eric Wiel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.14704v2",
    "title": "Japanese Children's Riddles as a Benchmark for Machine Insight and Metacognition",
    "summary": "Benchmark saturation and contamination have obscured genuine advances in reasoning for large language models (LLMs). We introduce NazoNazo Benchmark, a low-cost, renewable test built from Japanese children's riddles that demand insight-based reasoning, or representational shifts rather than knowledge recall. We evaluate 38 frontier LLMs (2023-2025) on 201 riddles and a 120-item human-comparison subset, finding that non-reasoning models average 7.6%, reasoning models 17.6%, and humans ~53% accuracy. Importantly, thought-log analysis reveals that reasoning in Japanese did not necessarily improve accuracy, indicating that language understanding alone is insufficient for insight reasoning. Notably, models sometimes generated correct candidates but failed to endorse them, suggesting weak metacognitive control rather than a lack of knowledge. This \"verification failure\" indicates that CoT outputs can reflect genuine intermediate reasoning states rather than post-hoc rationalizations. By exposing this metacognitive bottleneck - models' inability to recognize when they are right - the benchmark provides a scalable, cross-linguistic testbed for studying machine insight, confidence calibration, and self-evaluation. NazoNazo Benchmark thus offers not only a fresh challenge to current LLMs but also a concrete target for developing AI metacognitive psychology and enhancing machine Aha! capability.",
    "published": "2025-09-18T07:50:04Z",
    "updated": "2026-01-05T13:57:38Z",
    "link": "http://arxiv.org/pdf/2509.14704v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Masaharu Mizumoto",
      "Dat Nguyen",
      "Zhiheng Han",
      "Jiyuan Fang",
      "Heyuan Guan",
      "Xingfu Li",
      "Naoya Shiraishi",
      "Yo Nakawake",
      "Le Minh Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02126v1",
    "title": "Remote Sensing Change Detection via Weak Temporal Supervision",
    "summary": "Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.",
    "published": "2026-01-05T13:57:02Z",
    "updated": "2026-01-05T13:57:02Z",
    "link": "http://arxiv.org/pdf/2601.02126v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xavier Bou",
      "Elliot Vincent",
      "Gabriele Facciolo",
      "Rafael Grompone von Gioi",
      "Jean-Michel Morel",
      "Thibaud Ehret"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02125v1",
    "title": "SingingBot: An Avatar-Driven System for Robotic Face Singing Performance",
    "summary": "Equipping robotic faces with singing capabilities is crucial for empathetic Human-Robot Interaction. However, existing robotic face driving research primarily focuses on conversations or mimicking static expressions, struggling to meet the high demands for continuous emotional expression and coherence in singing. To address this, we propose a novel avatar-driven framework for appealing robotic singing. We first leverage portrait video generation models embedded with extensive human priors to synthesize vivid singing avatars, providing reliable expression and emotion guidance. Subsequently, these facial features are transferred to the robot via semantic-oriented mapping functions that span a wide expression space. Furthermore, to quantitatively evaluate the emotional richness of robotic singing, we propose the Emotion Dynamic Range metric to measure the emotional breadth within the Valence-Arousal space, revealing that a broad emotional spectrum is crucial for appealing performances. Comprehensive experiments prove that our method achieves rich emotional expressions while maintaining lip-audio synchronization, significantly outperforming existing approaches.",
    "published": "2026-01-05T13:56:36Z",
    "updated": "2026-01-05T13:56:36Z",
    "link": "http://arxiv.org/pdf/2601.02125v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Zhuoxiong Xu",
      "Xuanchen Li",
      "Yuhao Cheng",
      "Fei Xu",
      "Yichao Yan",
      "Xiaokang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02123v1",
    "title": "DeCode: Decoupling Content and Delivery for Medical QA",
    "summary": "Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\\%$ to $49.8\\%$, corresponding to a $75\\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.",
    "published": "2026-01-05T13:54:38Z",
    "updated": "2026-01-05T13:54:38Z",
    "link": "http://arxiv.org/pdf/2601.02123v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Po-Jen Ko",
      "Chen-Han Tsai",
      "Yu-Shao Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02121v1",
    "title": "Inferring Network Evolutionary History via Structure-State Coupled Learning",
    "summary": "Inferring a network's evolutionary history from a single final snapshot with limited temporal annotations is fundamental yet challenging. Existing approaches predominantly rely on topology alone, which often provides insufficient and noisy cues. This paper leverages network steady-state dynamics -- converged node states under a given dynamical process -- as an additional and widely accessible observation for network evolution history inference. We propose CS$^2$, which explicitly models structure-state coupling to capture how topology modulates steady states and how the two signals jointly improve edge discrimination for formation-order recovery. Experiments on six real temporal networks, evaluated under multiple dynamical processes, show that CS$^2$ consistently outperforms strong baselines, improving pairwise edge precedence accuracy by 4.0% on average and global ordering consistency (Spearman-$ρ$) by 7.7% on average. CS$^2$ also more faithfully recovers macroscopic evolution trajectories such as clustering formation, degree heterogeneity, and hub growth. Moreover, a steady-state-only variant remains competitive when reliable topology is limited, highlighting steady states as an independent signal for evolution inference.",
    "published": "2026-01-05T13:53:44Z",
    "updated": "2026-01-05T13:53:44Z",
    "link": "http://arxiv.org/pdf/2601.02121v1.pdf",
    "category": [
      "cs.SI",
      "cs.AI"
    ],
    "authors": [
      "En Xu",
      "Shihe Zhou",
      "Huandong Wang",
      "Jingtao Ding",
      "Yong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.22792v2",
    "title": "A Universal and Robust Framework for Multiple Gas Recognition Based-on Spherical Normalization-Coupled Mahalanobis Algorithm",
    "summary": "Electronic nose (E-nose) systems face two interconnected challenges in open-set gas recognition: feature distribution shift caused by signal drift and decision boundary failure induced by unknown gas interference. Existing methods predominantly rely on Euclidean distance or conventional classifiers, failing to account for anisotropic feature distributions and dynamic signal intensity variations. To address these issues, this study proposes the Spherical Normalization coupled Mahalanobis (SNM) module, a universal post-processing module for open-set gas recognition. First, it achieves geometric decoupling through cascaded batch and L2 normalization, projecting features onto a unit hypersphere to eliminate signal intensity fluctuations. Second, it utilizes Mahalanobis distance to construct adaptive ellipsoidal decision boundaries that conform to the anisotropic feature geometry. The architecture-agnostic SNM-Module seamlessly integrates with mainstream backbones including Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and Transformer. Experiments on the public Vergara dataset demonstrate that the Transformer+SNM configuration achieves near-theoretical-limit performance in discriminating among multiple target gases, with an AUROC of 0.9977 and an unknown gas detection rate of 99.57% at 5% false positive rate, significantly outperforming state-of-the-art methods with a 3.0% AUROC improvement and 91.0% standard deviation reduction compared to Class Anchor Clustering (CAC). The module maintains exceptional robustness across five sensor positions, with standard deviations below 0.0028. This work effectively addresses the critical challenge of simultaneously achieving high accuracy and high stability in open-set gas recognition, providing solid support for industrial E-nose deployment.",
    "published": "2025-12-28T05:33:05Z",
    "updated": "2026-01-05T13:46:47Z",
    "link": "http://arxiv.org/pdf/2512.22792v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "authors": [
      "Shuai Chen",
      "Yang Song",
      "Chen Wang",
      "Ziran Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.02115v3",
    "title": "Coward: Collision-based Watermark for Proactive Federated Backdoor Detection",
    "summary": "Backdoor detection is currently the mainstream defense against backdoor attacks in federated learning (FL), where a small number of malicious clients can upload poisoned updates to compromise the federated global model. Existing backdoor detection techniques fall into two categories, passive and proactive, depending on whether the server proactively intervenes in the training process. However, both of them have inherent limitations in practice: passive detection methods are disrupted by common non-i.i.d. data distributions and random participation of FL clients, whereas current proactive detection methods are misled by an inevitable out-of-distribution (OOD) bias because they rely on backdoor coexistence effects. To address these issues, we introduce a novel proactive detection method dubbed Coward, inspired by our discovery of multi-backdoor collision effects, in which consecutively planted, distinct backdoors significantly suppress earlier ones. Correspondingly, we modify the federated global model by injecting a carefully designed backdoor-collided watermark, implemented via regulated dual-mapping learning on OOD data. This design not only enables an inverted detection paradigm compared to existing proactive methods, thereby naturally counteracting the adverse impact of OOD prediction bias, but also introduces a low-disruptive training intervention that inherently limits the strength of OOD bias, leading to significantly fewer misjudgments. Extensive experiments on benchmark datasets show that Coward achieves state-of-the-art detection performance, effectively alleviates OOD prediction bias, and remains robust against potential adaptive attacks. The code for our method is available at https://github.com/still2009/cowardFL.",
    "published": "2025-08-04T06:51:33Z",
    "updated": "2026-01-05T13:45:47Z",
    "link": "http://arxiv.org/pdf/2508.02115v3.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Wenjie Li",
      "Siying Gu",
      "Yiming Li",
      "Kangjie Chen",
      "Zhili Chen",
      "Tianwei Zhang",
      "Shu-Tao Xia",
      "Dacheng Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.23260v2",
    "title": "Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation",
    "summary": "Safety alignment -- training large language models (LLMs) to refuse harmful requests while remaining helpful -- is critical for responsible deployment. Prior work established that safety behaviors are governed by low-rank structures, suggesting parameter-efficient fine-tuning (PEFT) should be well-suited for alignment. However, Low-Rank Adaptation (LoRA) consistently underperforms full fine-tuning and reinforcement learning on safety benchmarks. We attribute this gap to semantic entanglement: safety-relevant directions are intertwined with unrelated concepts due to polysemanticity, impeding implicit subspace identification. To address this, we propose SAILS (Safety Alignment via Interpretable Low-rank Subspace), which leverages Sparse Autoencoders (SAEs) to disentangle representations into monosemantic features, constructs an interpretable safety subspace from SAE decoder directions, and uses it to initialize LoRA adapters. Theoretically, we prove that SAE-based identification achieves arbitrarily small recovery error under monosemanticity assumptions, while direct identification suffers an irreducible error floor. Empirically, SAILS achieves up to 99.6% safety rate on Gemma-2-9B -- exceeding full fine-tuning by 7.4 points and matching RLHF-based models -- while updating only 0.19% of parameters and providing interpretability.",
    "published": "2025-12-29T07:39:49Z",
    "updated": "2026-01-05T13:39:39Z",
    "link": "http://arxiv.org/pdf/2512.23260v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Dianyun Wang",
      "Qingsen Ma",
      "Yuhu Shang",
      "Zhifeng Lu",
      "Zhenbo Xu",
      "Lechen Ning",
      "Huijia Wu",
      "Zhaofeng He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.05623v2",
    "title": "Deployability-Centric Infrastructure-as-Code Generation: Fail, Learn, Refine, and Succeed through LLM-Empowered DevOps Simulation",
    "summary": "Infrastructure-as-Code (IaC) generation holds significant promise for automating cloud infrastructure provisioning. Recent advances in Large Language Models (LLMs) present a promising opportunity to democratize IaC development by generating deployable infrastructure templates from natural language descriptions. However, current evaluation focuses on syntactic correctness while ignoring deployability, the critical measure of the utility of IaC configuration files. Six state-of-the-art LLMs performed poorly on deployability, achieving only 20.8$\\sim$30.2% deployment success rate on the first attempt. In this paper, we construct DPIaC-Eval, the first deployability-centric IaC template benchmark consisting of 153 real-world scenarios cross 58 unique services. Also, we propose an LLM-based deployability-centric framework, dubbed IaCGen, that uses iterative feedback mechanism encompassing format verification, syntax checking, and live deployment stages, thereby closely mirroring the real DevOps workflows. Results show that IaCGen can make 54.6$\\sim$91.6% generated IaC templates from all evaluated models deployable in the first 10 iterations. Additionally, human-in-the-loop feedback that provide direct guidance for the deployability errors, can further boost the performance to over 90% passItr@25 on all evaluated LLMs. Furthermore, we explore the trustworthiness of the generated IaC templates on user intent alignment and security compliance. The poor performance (25.2% user requirement coverage and 8.4% security compliance rate) indicates a critical need for continued research in this domain.",
    "published": "2025-06-05T22:53:12Z",
    "updated": "2026-01-05T13:38:13Z",
    "link": "http://arxiv.org/pdf/2506.05623v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Tianyi Zhang",
      "Shidong Pan",
      "Zejun Zhang",
      "Zhenchang Xing",
      "Xiaoyu Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.18770v6",
    "title": "Multimodal Adversarial Defense for Vision-Language Models by Leveraging One-To-Many Relationships",
    "summary": "Pre-trained vision-language (VL) models are highly vulnerable to adversarial attacks. However, existing defense methods primarily focus on image classification, overlooking two key aspects of VL tasks: multimodal attacks, where both image and text can be perturbed, and the one-to-many relationship of images and texts, where a single image can correspond to multiple textual descriptions and vice versa (1:N and N:1). This work is the first to explore defense strategies against multimodal attacks in VL tasks, whereas prior VL defense methods focus on vision robustness. We propose multimodal adversarial training (MAT), which incorporates adversarial perturbations in both image and text modalities during training, significantly outperforming existing unimodal defenses. Furthermore, we discover that MAT is limited by deterministic one-to-one (1:1) image-text pairs in VL training data. To address this, we conduct a comprehensive study on leveraging one-to-many relationships to enhance robustness, investigating diverse augmentation techniques. Our analysis shows that, for a more effective defense, augmented image-text pairs should be well-aligned, diverse, yet avoid distribution shift -- conditions overlooked by prior research. This work pioneers defense strategies against multimodal attacks, providing insights for building robust VLMs from both optimization and data perspectives. Our code is publicly available at https://github.com/CyberAgentAILab/multimodal-adversarial-training.",
    "published": "2024-05-29T05:20:02Z",
    "updated": "2026-01-05T13:34:30Z",
    "link": "http://arxiv.org/pdf/2405.18770v6.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Futa Waseda",
      "Antonio Tejero-de-Pablos",
      "Isao Echizen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02105v1",
    "title": "LION-DG: Layer-Informed Initialization with Deep Gradient Protocols for Accelerated Neural Network Training",
    "summary": "Weight initialization remains decisive for neural network optimization, yet existing methods are largely layer-agnostic. We study initialization for deeply-supervised architectures with auxiliary classifiers, where untrained auxiliary heads can destabilize early training through gradient interference.\n  We propose LION-DG, a layer-informed initialization that zero-initializes auxiliary classifier heads while applying standard He-initialization to the backbone. We prove that this implements Gradient Awakening: auxiliary gradients are exactly zero at initialization, then phase in naturally as weights grow -- providing an implicit warmup without hyperparameters.\n  Experiments on CIFAR-10 and CIFAR-100 with DenseNet-DS and ResNet-DS architectures demonstrate: (1) DenseNet-DS: +8.3% faster convergence on CIFAR-10 with comparable accuracy, (2) Hybrid approach: Combining LSUV with LION-DG achieves best accuracy (81.92% on CIFAR-10), (3) ResNet-DS: Positive speedup on CIFAR-100 (+11.3%) with side-tap auxiliary design.\n  We identify architecture-specific trade-offs and provide clear guidelines for practitioners. LION-DG is simple, requires zero hyperparameters, and adds no computational overhead.",
    "published": "2026-01-05T13:33:09Z",
    "updated": "2026-01-05T13:33:09Z",
    "link": "http://arxiv.org/pdf/2601.02105v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hyunjun Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.06779v2",
    "title": "Pedagogical Reflections on the Holistic Cognitive Development (HCD) Framework and AI-Augmented Learning in Creative Computing",
    "summary": "This paper presents an expanded account of the Holistic Cognitive Development (HCD) framework for reflective and creative learning in computing education. The HCD framework integrates design thinking, experiential learning, and reflective practice into a unified constructivist pedagogy emphasizing autonomy, ownership, and scaffolding. It is applied across courses in game design (CS3247, CS4350), virtual reality (CS4240), and extended reality systems, where students engage in iterative cycles of thinking, creating, criticizing, and reflecting. The paper also examines how AI-augmented systems such as iReflect, ReflexAI, and Knowledge Graph-enhanced LLM feedback tools operationalize the HCD framework through scalable, personalized feedback. Empirical findings demonstrate improved reflective depth, feedback quality, and learner autonomy. The work advocates a balance of supportive autonomy in supervision, where students practice self-directed inquiry while guided through structured reflection and feedback.",
    "published": "2025-11-10T07:07:37Z",
    "updated": "2026-01-05T13:31:19Z",
    "link": "http://arxiv.org/pdf/2511.06779v2.pdf",
    "category": [
      "cs.MM",
      "cs.AI"
    ],
    "authors": [
      "Anand Bhojan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.23506v4",
    "title": "Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier",
    "summary": "The recent advancement of Multimodal Large Language Models (MLLMs) is transforming human-computer interaction (HCI) from surface-level exchanges into more nuanced and emotionally intelligent communication. To realize this shift, emotion understanding becomes essential allowing systems to capture subtle cues underlying user intent. Furthermore, providing faithful explanations for predicted emotions is crucial to ensure interpretability and build user trust. However, current MLLM-based methods often generate emotion explanations that diverge from the target labels and sometimes even contradict their own predicted emotions. This inconsistency poses a critical risk for misunderstanding and erodes reliability in interactive settings. To address this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and an Explanation Reward. Our method guides the model to produce reasoning that is explicitly consistent with the target emotion during multimodal emotion recognition without modifying the model architecture or requiring additional paired video-description annotations. Our method significantly improves faithful explanation-prediction consistency and explanation emotion accuracy on the MAFW and DFEW datasets. Through extensive experiments and human evaluations, we show that our approach not only enhances alignment between explanation and prediction but also empowers MLLMs to deliver emotionally coherent, trustworthy interactions, marking a key step toward truly human-like HCI systems.",
    "published": "2025-10-27T16:40:17Z",
    "updated": "2026-01-05T13:24:37Z",
    "link": "http://arxiv.org/pdf/2510.23506v4.pdf",
    "category": [
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Hyeongseop Rha",
      "Jeong Hun Yeo",
      "Yeonju Kim",
      "Yong Man Ro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.20957v3",
    "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents",
    "summary": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.",
    "published": "2025-12-24T05:27:53Z",
    "updated": "2026-01-05T13:23:35Z",
    "link": "http://arxiv.org/pdf/2512.20957v3.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Zhaoxi Zhang",
      "Yitong Duan",
      "Yanzhi Zhang",
      "Yiming Xu",
      "Jiyan He",
      "Yunfang Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.22673v2",
    "title": "TravelBench: A Broader Real-World Benchmark for Multi-Turn and Tool-Using Travel Planning",
    "summary": "Travel planning is a natural real-world task to test large language models (LLMs) planning and tool-use abilities. Although prior work has studied LLM performance on travel planning, existing settings still differ from real-world needs, mainly due to limited domain coverage, insufficient modeling of users' implicit preferences in multi-turn conversations, and a lack of clear evaluation of agents' capability boundaries. To mitigate these gaps, we propose \\textbf{TravelBench}, a benchmark for fully real-world travel planning. We collect user queries, user profile and tools from real scenarios, and construct three subtasks-Single-Turn, Multi-Turn, and Unsolvable-to evaluate agent's three core capabilities in real settings: (1) solving problems autonomously, (2) interacting with users over multiple turns to refine requirements, and (3) recognizing the limits of own abilities. To enable stable tool invocation and reproducible evaluation, we cache real tool-call results and build a sandbox environment that integrates ten travel-related tools. Agents can combine these tools to solve most practical travel planning problems, and our systematic verification demonstrates the stability of the proposed benchmark. We further evaluate multiple LLMs on TravelBench and conduct an in-depth analysis of their behaviors and performance. TravelBench provides a practical and reproducible evaluation benchmark to advance research on LLM agents for travel planning.\\footnote{Our code and data will be available after internal review.",
    "published": "2025-12-27T18:25:14Z",
    "updated": "2026-01-05T13:19:13Z",
    "link": "http://arxiv.org/pdf/2512.22673v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xiang Cheng",
      "Yulan Hu",
      "Xiangwen Zhang",
      "Lu Xu",
      "Zheng Pan",
      "Xin Li",
      "Yong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02085v1",
    "title": "Vision-Based Early Fault Diagnosis and Self-Recovery for Strawberry Harvesting Robots",
    "summary": "Strawberry harvesting robots faced persistent challenges such as low integration of visual perception, fruit-gripper misalignment, empty grasping, and strawberry slippage from the gripper due to insufficient gripping force, all of which compromised harvesting stability and efficiency in orchard environments. To overcome these issues, this paper proposed a visual fault diagnosis and self-recovery framework that integrated multi-task perception with corrective control strategies. At the core of this framework was SRR-Net, an end-to-end multi-task perception model that simultaneously performed strawberry detection, segmentation, and ripeness estimation, thereby unifying visual perception with fault diagnosis. Based on this integrated perception, a relative error compensation method based on the simultaneous target-gripper detection was designed to address positional misalignment, correcting deviations when error exceeded the tolerance threshold. To mitigate empty grasping and fruit-slippage faults, an early abort strategy was implemented. A micro-optical camera embedded in the end-effector provided real-time visual feedback, enabling grasp detection during the deflating stage and strawberry slip prediction during snap-off through MobileNet V3-Small classifier and a time-series LSTM classifier. Experiments demonstrated that SRR-Net maintained high perception accuracy. For detection, it achieved a precision of 0.895 and recall of 0.813 on strawberries, and 0.972/0.958 on hands. In segmentation, it yielded a precision of 0.887 and recall of 0.747 for strawberries, and 0.974/0.947 for hands. For ripeness estimation, SRR-Net attained a mean absolute error of 0.035, while simultaneously supporting multi-task perception and sustaining a competitive inference speed of 163.35 FPS.",
    "published": "2026-01-05T13:12:42Z",
    "updated": "2026-01-05T13:12:42Z",
    "link": "http://arxiv.org/pdf/2601.02085v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Meili Sun",
      "Chunjiang Zhao",
      "Lichao Yang",
      "Hao Liu",
      "Shimin Hu",
      "Ya Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22461v2",
    "title": "CMDAR: A Chinese Multi-scene Dynamic Audio Reasoning Benchmark with Diverse Challenges",
    "summary": "The ability to reason from audio, including speech, environmental sounds, and music, is essential for AI agents to interact effectively in real-world scenarios. Existing benchmarks mainly focus on static or single-scene settings and English audio data and do not fully capture scenarios where multiple speakers, unfolding events, and heterogeneous audio sources interact. To address these challenges, we introduce CMDAR, a Chinese benchmark for evaluating models on complex, multi-scene, and dynamically evolving audio reasoning tasks. CMDAR comprises 3,000 carefully curated question-answer pairs linked to diverse audio clips, covering five categories of complex reasoning and spanning three question types. We benchmark 26 state-of-the-art audio language models on CMDAR and observe that they exhibit limitations in complex reasoning tasks. In CMDAR-main, Qwen2.5-Omni achieves 76.67% accuracy, whereas GPT-4o Audio reaches 68.47%. However, GPT-4o Audio substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice with multiple audios and open-ended tasks. And we provide detail analysis corresponding suggestions for the future development of large audio language models.",
    "published": "2025-09-26T15:12:46Z",
    "updated": "2026-01-05T13:09:54Z",
    "link": "http://arxiv.org/pdf/2509.22461v2.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "authors": [
      "Hui Li",
      "Changhao Jiang",
      "Hongyu Wang",
      "Ming Zhang",
      "Jiajun Sun",
      "Zhixiong Yang",
      "Yifei Cao",
      "Shihan Dou",
      "Xiaoran Fan",
      "Baoyu Fan",
      "Tao Ji",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02080v1",
    "title": "The Homogeneity Trap: Spectral Collapse in Doubly-Stochastic Deep Networks",
    "summary": "Doubly-stochastic matrices (DSM) are increasingly utilized in structure-preserving deep architectures -- such as Optimal Transport layers and Sinkhorn-based attention -- to enforce numerical stability and probabilistic interpretability. In this work, we identify a critical spectral degradation phenomenon inherent to these constraints, termed the Homogeneity Trap. We demonstrate that the maximum-entropy bias, typical of Sinkhorn-based projections, drives the mixing operator towards the uniform barycenter, thereby suppressing the subdominant singular value σ_2 and filtering out high-frequency feature components. We derive a spectral bound linking σ_2 to the network's effective depth, showing that high-entropy constraints restrict feature transformation to a shallow effective receptive field. Furthermore, we formally demonstrate that Layer Normalization fails to mitigate this collapse in noise-dominated regimes; specifically, when spectral filtering degrades the Signal-to-Noise Ratio (SNR) below a critical threshold, geometric structure is irreversibly lost to noise-induced orthogonal collapse. Our findings highlight a fundamental trade-off between entropic stability and spectral expressivity in DSM-constrained networks.",
    "published": "2026-01-05T13:09:42Z",
    "updated": "2026-01-05T13:09:42Z",
    "link": "http://arxiv.org/pdf/2601.02080v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yizhi Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02076v1",
    "title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows",
    "summary": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.",
    "published": "2026-01-05T12:57:33Z",
    "updated": "2026-01-05T12:57:33Z",
    "link": "http://arxiv.org/pdf/2601.02076v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yingte Shu",
      "Yuchuan Tian",
      "Chao Xu",
      "Yunhe Wang",
      "Hanting Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02071v1",
    "title": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations",
    "summary": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.",
    "published": "2026-01-05T12:50:50Z",
    "updated": "2026-01-05T12:50:50Z",
    "link": "http://arxiv.org/pdf/2601.02071v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Adeshola Okubena",
      "Yusuf Ali Mohammed",
      "Moe Elbadawi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.18901v2",
    "title": "Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models",
    "summary": "We present Gabliteration, a novel neural weight modification technique that advances beyond traditional abliteration methods by implementing adaptive multi-directional projections with regularized layer selection. Our approach addresses the fundamental limitation of existing methods that compromise model quality while attempting to modify specific behavioral patterns. Through dynamic layer optimization, regularized projection matrices, and adaptive scaling mechanisms, we achieve theoretically superior weight modification while minimizing quality degradation in unrelated domains. We validate our method through the gabliterated-v1 model series (0.6B to 4B parameters) available on Hugging Face, demonstrating practical applicability across multiple model scales.",
    "published": "2025-12-21T22:12:54Z",
    "updated": "2026-01-05T12:45:28Z",
    "link": "http://arxiv.org/pdf/2512.18901v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Gökdeniz Gülmez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02065v1",
    "title": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory",
    "summary": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings",
    "published": "2026-01-05T12:41:44Z",
    "updated": "2026-01-05T12:41:44Z",
    "link": "http://arxiv.org/pdf/2601.02065v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Md. Asif Hossain",
      "Nabil Subhan",
      "Mantasha Rahman Mahi",
      "Jannatul Ferdous Nabila"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.18384v2",
    "title": "AI Prior Art Search: Semantic Clusters and Evaluation Infrastructure",
    "summary": "The key to success in automating prior art search in patent research using artificial intelligence (AI) lies in developing large datasets for machine learning (ML) and ensuring their availability. This work is dedicated to providing a comprehensive solution to the problem of creating infrastructure for research in this field, including datasets and tools for calculating search quality criteria. The paper discusses the concept of semantic clusters of patent documents that determine the state of the art in a given subject, as proposed by the authors. A definition of such semantic clusters is also provided. Prior art search is presented as the task of identifying elements within a semantic cluster of patent documents in the subject area specified by the document under consideration. A generator of user-configurable datasets for ML, based on collections of U.S. and Russian patent documents, is described. The dataset generator creates a database of links to documents in semantic clusters. Then, based on user-defined parameters, it forms a dataset of semantic clusters in JSON format for ML. A collection of publicly available patent documents was created. The collection contains 14 million semantic clusters of US patent documents and 1 million clusters of Russian patent documents. To evaluate ML outcomes, it is proposed to calculate search quality scores that account for semantic clusters of the documents being searched. To automate the evaluation process, the paper describes a utility developed by the authors for assessing the quality of prior art document search.",
    "published": "2025-12-20T14:51:57Z",
    "updated": "2026-01-05T12:39:45Z",
    "link": "http://arxiv.org/pdf/2512.18384v2.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Boris Genin",
      "Alexander Gorbunov",
      "Dmitry Zolkin",
      "Igor Nekrasov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02061v1",
    "title": "Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management",
    "summary": "Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.",
    "published": "2026-01-05T12:35:33Z",
    "updated": "2026-01-05T12:35:33Z",
    "link": "http://arxiv.org/pdf/2601.02061v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Faizan Ahmed",
      "Aniket Dixit",
      "James Brusey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02060v1",
    "title": "Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming",
    "summary": "Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.",
    "published": "2026-01-05T12:33:37Z",
    "updated": "2026-01-05T12:33:37Z",
    "link": "http://arxiv.org/pdf/2601.02060v1.pdf",
    "category": [
      "cs.PL",
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Nguyet-Anh H. Lang",
      "Eric Lang",
      "Thanh Le-Cong",
      "Bach Le",
      "Quyet-Thang Huynh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02046v1",
    "title": "Agentic Retoucher for Text-To-Image Generation",
    "summary": "Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.",
    "published": "2026-01-05T12:06:43Z",
    "updated": "2026-01-05T12:06:43Z",
    "link": "http://arxiv.org/pdf/2601.02046v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Shaocheng Shen",
      "Jianfeng Liang. Chunlei Cai",
      "Cong Geng",
      "Huiyu Duan",
      "Xiaoyun Zhang",
      "Qiang Hu",
      "Guangtao Zhai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.01010v2",
    "title": "v-PuNNs: van der Put Neural Networks for Transparent Ultrametric Representation Learning",
    "summary": "Conventional deep learning models embed data in Euclidean space $\\mathbb{R}^d$, a poor fit for strictly hierarchical objects such as taxa, word senses, or file systems. We introduce van der Put Neural Networks (v-PuNNs), the first architecture whose neurons are characteristic functions of p-adic balls in $\\mathbb{Z}_p$. Under our Transparent Ultrametric Representation Learning (TURL) principle every weight is itself a p-adic number, giving exact subtree semantics. A new Finite Hierarchical Approximation Theorem shows that a depth-K v-PuNN with $\\sum_{j=0}^{K-1}p^{\\,j}$ neurons universally represents any K-level tree. Because gradients vanish in this discrete space, we propose Valuation-Adaptive Perturbation Optimization (VAPO), with a fast deterministic variant (HiPaN-DS) and a moment-based one (HiPaN / Adam-VAPO). On three canonical benchmarks our CPU-only implementation sets new state-of-the-art: WordNet nouns (52,427 leaves) 99.96% leaf accuracy in 16 min; GO molecular-function 96.9% leaf / 100% root in 50 s; NCBI Mammalia Spearman $ρ= -0.96$ with true taxonomic distance. The learned metric is perfectly ultrametric (zero triangle violations), and its fractal and information-theoretic properties are analyzed. Beyond classification we derive structural invariants for quantum systems (HiPaQ) and controllable generative codes for tabular data (Tab-HiPaN). v-PuNNs therefore bridge number theory and deep learning, offering exact, interpretable, and efficient models for hierarchical data.",
    "published": "2025-08-01T18:23:38Z",
    "updated": "2026-01-05T12:06:41Z",
    "link": "http://arxiv.org/pdf/2508.01010v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Gnankan Landry Regis N'guessan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02045v1",
    "title": "The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers",
    "summary": "This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools.",
    "published": "2026-01-05T12:02:57Z",
    "updated": "2026-01-05T12:02:57Z",
    "link": "http://arxiv.org/pdf/2601.02045v1.pdf",
    "category": [
      "cs.PL",
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Shuoming Zhang",
      "Jiacheng Zhao",
      "Qiuchu Yu",
      "Chunwei Xia",
      "Zheng Wang",
      "Xiaobing Feng",
      "Huimin Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.24639v2",
    "title": "Causal Ordering for Structure Learning from Time Series",
    "summary": "Predicting causal structure from time series data is crucial for understanding complex phenomena in physiology, brain connectivity, climate dynamics, and socio-economic behaviour. Causal discovery in time series is hindered by the combinatorial complexity of identifying true causal relationships, especially as the number of variables and time points grow. A common approach to simplify the task is the so-called ordering-based methods. Traditional ordering methods inherently limit the representational capacity of the resulting model. In this work, we fix this issue by leveraging multiple valid causal orderings, instead of a single one as standard practice. We propose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based causal discovery for temporal data. By integrating multiple orderings, DOTS effectively recovers the transitive closure of the underlying directed acyclic graph, mitigating spurious artifacts inherent in single-ordering approaches. We formalise the problem under standard assumptions such as stationarity and the additive noise model, and leverage score matching with diffusion processes to enable efficient Hessian estimation. Extensive experiments validate the approach. Empirical evaluations on synthetic and real-world datasets demonstrate that DOTS outperforms state-of-the-art baselines, offering a scalable and robust approach to temporal causal discovery. On synthetic benchmarks ($d{=}\\!3-\\!6$ variables, $T{=}200\\!-\\!5{,}000$ samples), DOTS improves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the CausalTime real-world benchmark ($d{=}20\\!-\\!36$), while baselines remain the best on individual datasets, DOTS attains the highest average summary-graph $F1$ while halving runtime relative to graph-optimisation methods. These results establish DOTS as a scalable and accurate solution for temporal causal discovery.",
    "published": "2025-10-28T17:06:15Z",
    "updated": "2026-01-05T12:02:02Z",
    "link": "http://arxiv.org/pdf/2510.24639v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Pedro P. Sanchez",
      "Damian Machlanski",
      "Steven McDonagh",
      "Sotirios A. Tsaftaris"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02043v1",
    "title": "Simulated Reasoning is Reasoning",
    "summary": "Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., \"symbolic reasoning\". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can \"reason\" by way of imitating the process of \"thinking out loud\", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the \"stochastic parrot\" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.",
    "published": "2026-01-05T12:00:04Z",
    "updated": "2026-01-05T12:00:04Z",
    "link": "http://arxiv.org/pdf/2601.02043v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Hendrik Kempt",
      "Alon Lavie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.05434v3",
    "title": "LTLBench: Towards Benchmarks for Evaluating Temporal Reasoning in Large Language Models",
    "summary": "Temporal Reasoning (TR) is a critical ability for LLMs to understand and reason over temporal information and relationships between events. To study the TR ability in LLMs, prior works provide different ways for evaluating various aspects of TR ability. In this work, we propose an alternative perspective for evaluating TR ability by leveraging Linear Temporal Logic (LTL), and develop a pipeline to automatically synthesize challenges for assessing the TR ability of LLMs. Based on this pipeline, we construct a dataset, namely LTLBench, consisting of $2000$ TR challenges, and benchmark 12 LLMs across 5 different methods. Furthermore, we conduct additional experiments to investigate the impact of increasing the number of formula operators and events on both LLM performance and the complexity of TR problems. We also perform qualitative analyses of their reasoning processes and the effects of varying the number of events and formula operators, which reveal 3 main issues in their temporal reasoning processes and the unexpected performance changes observed as problem complexity increases. We expect this work to provide valuable insights into the TR ability of LLMs.",
    "published": "2024-07-07T16:37:06Z",
    "updated": "2026-01-05T11:55:15Z",
    "link": "http://arxiv.org/pdf/2407.05434v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Weizhi Tang",
      "Kwabena Nuamah",
      "Vaishak Belle"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.23732v2",
    "title": "When in Doubt, Consult: Expert Debate for Sexism Detection via Confidence-Based Routin",
    "summary": "Sexist content online increasingly appears in subtle, context-dependent forms that evade traditional detection methods. Its interpretation often depends on overlapping linguistic, psychological, legal, and cultural dimensions, which produce mixed and sometimes contradictory signals, even in annotated datasets. These inconsistencies, combined with label scarcity and class imbalance, result in unstable decision boundaries and cause fine-tuned models to overlook subtler, underrepresented forms of harm. Together, these limitations point to the need for a design that explicitly addresses the combined effects of (i) underrepresentation, (ii) noise, and (iii) conceptual ambiguity in both data and model predictions. To address these challenges, we propose a two-stage framework that unifies (i) targeted training procedures to adapt supervision to scarce and noisy data with (ii) selective, reasoning-based inference to handle ambiguous or borderline cases. Our training setup applies class-balanced focal loss, class-aware batching, and post-hoc threshold calibration to mitigate label imbalance and noisy supervision. At inference time, a dynamic routing mechanism classifies high-confidence cases directly and escalates uncertain instances to a novel \\textit{Collaborative Expert Judgment} (CEJ) module, which prompts multiple personas and consolidates their reasoning through a judge model. Our approach achieves state-of-the-art results across several benchmarks, with F1 gains of +4.48% and +1.30% on EDOS Tasks A and B, respectively, and a +2.79% improvement in ICM on EXIST 2025 Task 1.1.",
    "published": "2025-12-21T05:48:57Z",
    "updated": "2026-01-05T11:54:54Z",
    "link": "http://arxiv.org/pdf/2512.23732v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Anwar Alajmi",
      "Gabriele Pergola"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.07404v2",
    "title": "On LLMs' Internal Representation of Code Correctness",
    "summary": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.",
    "published": "2025-12-08T10:38:03Z",
    "updated": "2026-01-05T11:52:55Z",
    "link": "http://arxiv.org/pdf/2512.07404v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Francisco Ribeiro",
      "Claudio Spiess",
      "Prem Devanbu",
      "Sarah Nadi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.24191v2",
    "title": "Beyond Prompts: Space-Time Decoupling Control-Plane Jailbreaks in LLM Structured Output",
    "summary": "Content Warning: This paper may contain unsafe or harmful content generated by LLMs that may be offensive to readers. Large Language Models (LLMs) are extensively used as tooling platforms through structured output APIs to ensure syntax compliance so that robust integration with existing software, like agent systems, can be achieved. However, the feature enabling the functionality of grammar-guided structured output presents significant security vulnerabilities. In this work, we reveal a critical control-plane attack surface orthogonal to traditional data-plane vulnerabilities. We introduce Constrained Decoding Attack (CDA), a novel jailbreak class that weaponizes structured output constraints to bypass both external auditing and internal safety alignment. Unlike prior attacks focused on input prompt designs, CDA operates by embedding malicious intent in schema-level grammar rules (control-plane) while maintaining benign surface prompts (data-plane). We instantiate this with two proof-of-concept attacks: EnumAttack, which embeds malicious content in enum fields; and the more evasive DictAttack, which decouples the malicious payload across a benign prompt and a dictionary-based grammar. Our evaluation spans a broad spectrum of 13 proprietary/open-weight models. In particular, DictAttack achieves 94.3--99.5% ASR across five benchmarks on gpt-5, gemini-2.5-pro, deepseek-r1, and gpt-oss-120b. Furthermore, we demonstrate the significant challenge in defending against these threats: while basic grammar auditing mitigates EnumAttack, the more sophisticated DictAttack maintains a 75.8% ASR even against multiple state-of-the-art jailbreak guardrails. This exposes a critical \"semantic gap\" in current safety architectures and underscores the urgent need for cross-plane defenses that can bridge the data and control planes to secure the LLM generation pipeline.",
    "published": "2025-03-31T15:08:06Z",
    "updated": "2026-01-05T11:49:07Z",
    "link": "http://arxiv.org/pdf/2503.24191v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Shuoming Zhang",
      "Jiacheng Zhao",
      "Hanyuan Dong",
      "Ruiyuan Xu",
      "Zhicheng Li",
      "Yangyu Zhang",
      "Shuaijiang Li",
      "Yuan Wen",
      "Chunwei Xia",
      "Zheng Wang",
      "Xiaobing Feng",
      "Huimin Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.09881v3",
    "title": "Uncertainty Quantification of Surrogate Models using Conformal Prediction",
    "summary": "Data-driven surrogate models offer quick approximations to complex numerical and experimental systems but typically lack uncertainty quantification, limiting their reliability in safety-critical applications. While Bayesian methods provide uncertainty estimates, they offer no statistical guarantees and struggle with high-dimensional spatio-temporal problems due to computational costs. We present a conformal prediction (CP) framework that provides statistically guaranteed marginal coverage for surrogate models in a model-agnostic manner with near-zero computational cost. Our approach handles high-dimensional spatio-temporal outputs by performing cell-wise calibration while preserving the tensorial structure of predictions. Through extensive empirical evaluation across diverse applications including fluid dynamics, magnetohydrodynamics, weather forecasting, and fusion diagnostics, we demonstrate that CP achieves empirical coverage with valid error bars regardless of model architecture, training regime, or output dimensionality. We evaluate three nonconformity scores (conformalised quantile regression, absolute error residual, and standard deviation) for both deterministic and probabilistic models, showing that guaranteed coverage holds even for out-of-distribution predictions where models are deployed on physics regimes different from training data. Calibration requires only seconds to minutes on standard hardware. The framework enables rigorous validation of pre-trained surrogate models for downstream applications without retraining. While CP provides marginal rather than conditional coverage and assumes exchangeability between calibration and test data, our method circumvents the curse of dimensionality inherent in traditional uncertainty quantification approaches, offering a practical tool for trustworthy deployment of machine learning in physical sciences.",
    "published": "2024-08-19T10:46:19Z",
    "updated": "2026-01-05T11:48:30Z",
    "link": "http://arxiv.org/pdf/2408.09881v3.pdf",
    "category": [
      "cs.AI",
      "physics.ao-ph",
      "physics.plasm-ph"
    ],
    "authors": [
      "Vignesh Gopakumar",
      "Ander Gray",
      "Joel Oskarsson",
      "Lorenzo Zanisi",
      "Daniel Giles",
      "Matt J. Kusner",
      "Stanislas Pamela",
      "Marc Peter Deisenroth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02031v1",
    "title": "Output Embedding Centering for Stable LLM Pretraining",
    "summary": "Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.",
    "published": "2026-01-05T11:44:05Z",
    "updated": "2026-01-05T11:44:05Z",
    "link": "http://arxiv.org/pdf/2601.02031v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Felix Stollenwerk",
      "Anna Lokrantz",
      "Niclas Hertzberg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02023v1",
    "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs",
    "summary": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.",
    "published": "2026-01-05T11:30:56Z",
    "updated": "2026-01-05T11:30:56Z",
    "link": "http://arxiv.org/pdf/2601.02023v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Amirali Ebrahimzadeh",
      "Seyyed M. Salili"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.20075v5",
    "title": "I Large Language Models possono nascondere un testo in un altro testo della stessa lunghezza",
    "summary": "A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present Calgacus, a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.\n  --\nUn testo di senso compiuto può essere nascosto all'interno di un altro testo completamente diverso, eppure coerente e plausibile, della stessa lunghezza. Ad esempio, un tweet che celebra un leader politico potrebbe celare un tweet che lo critica duramente, o un'anonima recensione di un prodotto potrebbe in realtà codificare un manoscritto segreto. Questa sconcertante possibilità è oggi alla nostra portata grazie ai Large Language Models (LLM); in questo articolo presentiamo Calgacus, un protocollo semplice ed efficiente per realizzarla. Mostriamo che anche modesti LLM open-source da 8 miliardi di parametri sono sufficienti per ottenere risultati di alta qualità, e che un messaggio lungo quanto questo abstract può essere codificato e decodificato su un comune portatile in pochi secondi. L'esistenza di tale protocollo dimostra un radicale disaccoppiamento del testo dall'intento del suo autore, erodendo ulteriormente la fiducia nella comunicazione scritta, già scossa dall'ascesa dei chatbot basati su LLMs. Illustriamo ciò con uno scenario concreto: un'azienda potrebbe offrire pubblicamente i servizi di un LLM senza filtri nascondendo le sue risposte all'interno di risposte apparentemente innocue generate da un LLM considerato sicuro. Questa possibilità solleva questioni urgenti per la sicurezza dell'Intelligenza Artificiale e sfida la nostra comprensione di cosa significhi, per un Large Language Model, sapere qualcosa.",
    "published": "2025-10-22T23:16:50Z",
    "updated": "2026-01-05T11:25:29Z",
    "link": "http://arxiv.org/pdf/2510.20075v5.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Antonio Norelli",
      "Michael Bronstein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02016v1",
    "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
    "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
    "published": "2026-01-05T11:24:34Z",
    "updated": "2026-01-05T11:24:34Z",
    "link": "http://arxiv.org/pdf/2601.02016v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "authors": [
      "Matthias Bartolo",
      "Dylan Seychell",
      "Gabriel Hili",
      "Matthew Montebello",
      "Carl James Debono",
      "Saviour Formosa",
      "Konstantinos Makantasis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02015v1",
    "title": "Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects",
    "summary": "Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.",
    "published": "2026-01-05T11:24:33Z",
    "updated": "2026-01-05T11:24:33Z",
    "link": "http://arxiv.org/pdf/2601.02015v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IT"
    ],
    "authors": [
      "Omar Momen",
      "Emilie Sitter",
      "Berenike Herrmann",
      "Sina Zarrieß"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02010v1",
    "title": "A neural network for modeling human concept formation, understanding and communication",
    "summary": "A remarkable capability of the human brain is to form more abstract conceptual representations from sensorimotor experiences and flexibly apply them independent of direct sensory inputs. However, the computational mechanism underlying this ability remains poorly understood. Here, we present a dual-module neural network framework, the CATS Net, to bridge this gap. Our model consists of a concept-abstraction module that extracts low-dimensional conceptual representations, and a task-solving module that performs visual judgement tasks under the hierarchical gating control of the formed concepts. The system develops transferable semantic structure based on concept representations that enable cross-network knowledge transfer through conceptual communication. Model-brain fitting analyses reveal that these emergent concept spaces align with both neurocognitive semantic model and brain response structures in the human ventral occipitotemporal cortex, while the gating mechanisms mirror that in the semantic control brain network. This work establishes a unified computational framework that can offer mechanistic insights for understanding human conceptual cognition and engineering artificial systems with human-like conceptual intelligence.",
    "published": "2026-01-05T11:19:07Z",
    "updated": "2026-01-05T11:19:07Z",
    "link": "http://arxiv.org/pdf/2601.02010v1.pdf",
    "category": [
      "q-bio.NC",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Liangxuan Guo",
      "Haoyang Chen",
      "Yang Chen",
      "Yanchao Bi",
      "Shan Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02008v1",
    "title": "XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging",
    "summary": "Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.",
    "published": "2026-01-05T11:17:33Z",
    "updated": "2026-01-05T11:17:33Z",
    "link": "http://arxiv.org/pdf/2601.02008v1.pdf",
    "category": [
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Midhat Urooj",
      "Ayan Banerjee",
      "Sandeep Gupta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02002v1",
    "title": "Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models",
    "summary": "Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.",
    "published": "2026-01-05T11:03:56Z",
    "updated": "2026-01-05T11:03:56Z",
    "link": "http://arxiv.org/pdf/2601.02002v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Antonio Colacicco",
      "Vito Guida",
      "Dario Di Palma",
      "Fedelucio Narducci",
      "Tommaso Di Noia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.12589v3",
    "title": "ULTra: Unveiling Latent Token Interpretability in Transformer-Based Understanding and Segmentation",
    "summary": "Transformers have revolutionized Computer Vision (CV) through self-attention mechanisms. However, their complexity makes latent token representations difficult to interpret. We introduce ULTra, a framework for interpreting Transformer embeddings and uncovering meaningful semantic patterns within them. ULTra enables unsupervised semantic segmentation using pre-trained models without requiring fine-tuning. Additionally, we propose a self-supervised training approach that refines segmentation performance by learning an external transformation matrix without modifying the underlying model. Our method achieves state-of-the-art performance in unsupervised semantic segmentation, outperforming existing segmentation methods. Furthermore, we validate ULTra for model interpretation on both synthetic and real-world scenarios, including Object Selection and interpretable text summarization using LLMs, demonstrating its broad applicability in explaining the semantic structure of latent token representations.",
    "published": "2024-11-15T19:36:50Z",
    "updated": "2026-01-05T11:02:56Z",
    "link": "http://arxiv.org/pdf/2411.12589v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Hesam Hosseini",
      "Ghazal Hosseini Mighan",
      "Amirabbas Afzali",
      "Sajjad Amini",
      "Amir Houmansadr"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01997v1",
    "title": "Exploring Diversity, Novelty, and Popularity Bias in ChatGPT's Recommendations",
    "summary": "ChatGPT has emerged as a versatile tool, demonstrating capabilities across diverse domains. Given these successes, the Recommender Systems (RSs) community has begun investigating its applications within recommendation scenarios primarily focusing on accuracy. While the integration of ChatGPT into RSs has garnered significant attention, a comprehensive analysis of its performance across various dimensions remains largely unexplored. Specifically, the capabilities of providing diverse and novel recommendations or exploring potential biases such as popularity bias have not been thoroughly examined. As the use of these models continues to expand, understanding these aspects is crucial for enhancing user satisfaction and achieving long-term personalization.\n  This study investigates the recommendations provided by ChatGPT-3.5 and ChatGPT-4 by assessing ChatGPT's capabilities in terms of diversity, novelty, and popularity bias. We evaluate these models on three distinct datasets and assess their performance in Top-N recommendation and cold-start scenarios. The findings reveal that ChatGPT-4 matches or surpasses traditional recommenders, demonstrating the ability to balance novelty and diversity in recommendations. Furthermore, in the cold-start scenario, ChatGPT models exhibit superior performance in both accuracy and novelty, suggesting they can be particularly beneficial for new users. This research highlights the strengths and limitations of ChatGPT's recommendations, offering new perspectives on the capacity of these models to provide recommendations beyond accuracy-focused metrics.",
    "published": "2026-01-05T10:56:01Z",
    "updated": "2026-01-05T10:56:01Z",
    "link": "http://arxiv.org/pdf/2601.01997v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Dario Di Palma",
      "Giovanni Maria Biancofiore",
      "Vito Walter Anelli",
      "Fedelucio Narducci",
      "Tommaso Di Noia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01993v1",
    "title": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support",
    "summary": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.",
    "published": "2026-01-05T10:54:18Z",
    "updated": "2026-01-05T10:54:18Z",
    "link": "http://arxiv.org/pdf/2601.01993v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Dong Xue",
      "Jicheng Tu",
      "Ming Wang",
      "Xin Yan",
      "Fangzhou Liu",
      "Jie Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.14244v4",
    "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition",
    "summary": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.",
    "published": "2025-12-16T09:52:58Z",
    "updated": "2026-01-05T10:52:55Z",
    "link": "http://arxiv.org/pdf/2512.14244v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yiqing Zhou",
      "Yu Lei",
      "Shuzheng Si",
      "Qingyan Sun",
      "Wei Wang",
      "Yifei Wu",
      "Hao Wen",
      "Gang Chen",
      "Fanchao Qi",
      "Maosong Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01989v1",
    "title": "VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis",
    "summary": "Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies.",
    "published": "2026-01-05T10:48:12Z",
    "updated": "2026-01-05T10:48:12Z",
    "link": "http://arxiv.org/pdf/2601.01989v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Aly R. Elkammar",
      "Karim M. Gamaleldin",
      "Catherine M. Elias"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01982v1",
    "title": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems",
    "summary": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.",
    "published": "2026-01-05T10:36:40Z",
    "updated": "2026-01-05T10:36:40Z",
    "link": "http://arxiv.org/pdf/2601.01982v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Noel Thomas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01976v1",
    "title": "CNC-TP: Classifier Nominal Concept Based on Top-Pertinent Attributes",
    "summary": "Knowledge Discovery in Databases (KDD) aims to exploit the vast amounts of data generated daily across various domains of computer applications. Its objective is to extract hidden and meaningful knowledge from datasets through a structured process comprising several key steps: data selection, preprocessing, transformation, data mining, and visualization. Among the core data mining techniques are classification and clustering. Classification involves predicting the class of new instances using a classifier trained on labeled data. Several approaches have been proposed in the literature, including Decision Tree Induction, Bayesian classifiers, Nearest Neighbor search, Neural Networks, Support Vector Machines, and Formal Concept Analysis (FCA). The last one is recognized as an effective approach for interpretable and explainable learning. It is grounded in the mathematical structure of the concept lattice, which enables the generation of formal concepts and the discovery of hidden relationships among them. In this paper, we present a state-of-theart review of FCA-based classifiers. We explore various methods for computing closure operators from nominal data and introduce a novel approach for constructing a partial concept lattice that focuses on the most relevant concepts. Experimental results are provided to demonstrate the efficiency of the proposed method.",
    "published": "2026-01-05T10:32:10Z",
    "updated": "2026-01-05T10:32:10Z",
    "link": "http://arxiv.org/pdf/2601.01976v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yasmine Souissi",
      "Fabrice Boissier",
      "Nida Meddouri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.21214v3",
    "title": "Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines",
    "summary": "Reasoning models have demonstrated remarkable capabilities in complex reasoning tasks. However, ensuring their safety against adversarial jailbreak prompts remains a critical challenge. Due to the covert and deceptive nature of such prompts, they can often evade built-in safety mechanisms and lead to the generation of harmful content. This underscores the need for an adaptive safety alignment approach that enables models to autonomously reinforce their defenses in response to adversarial inputs. This paper introduces the Synthesized Guideline-based Adaptive Safety Alignment (SGASA) framework, which internalizes model-generated safety guidelines to strengthen models' ability to enhance robustness against harmful adversarial prompts while minimizing unnecessary refusals of benign requests. SGASA consists of two key stages: Data Pre-synthesis, which generates safety guidelines and augmented prompts; and Alignment Fine-tuning, which leverages Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO) to embed these guidelines into the model. Extensive experiments across multiple datasets demonstrate that SGASA significantly improves model safety, validating its adaptive and scalable effectiveness.",
    "published": "2025-11-26T09:44:32Z",
    "updated": "2026-01-05T10:26:08Z",
    "link": "http://arxiv.org/pdf/2511.21214v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yuhang Wang",
      "Yanxu Zhu",
      "Dongyuan Lu",
      "Jitao Sang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01966v1",
    "title": "Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior",
    "summary": "Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit problem: for a fine-tuned model and a training prompt-response pair, can we infer whether the model was trained on the original prompt or its LLM-refined version within a mixed corpus? This matters for dataset governance and dispute resolution when training data are contested. However, it is non-trivial in practice: refined and raw instances are interleaved in the training corpus with unknown, source-dependent mixture ratios, making it harder to develop provenance methods that generalize across models and training setups. In this paper, we formalize this audit task as Refinement Provenance Inference (RPI) and show that prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious. Building on this phenomenon, we propose RePro, a logit-based provenance framework that fuses teacher-forced likelihood features with logit-ranking signals. During training, RePro learns a transferable representation via shadow fine-tuning, and uses a lightweight linear head to infer provenance on unseen victims without training-data access. Empirically, RePro consistently attains strong performance and transfers well across refiners, suggesting that it exploits refiner-agnostic distribution shifts rather than rewrite-style artifacts.",
    "published": "2026-01-05T10:16:41Z",
    "updated": "2026-01-05T10:16:41Z",
    "link": "http://arxiv.org/pdf/2601.01966v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Bo Yin",
      "Qi Li",
      "Runpeng Yu",
      "Xinchao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.09040v2",
    "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better",
    "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.",
    "published": "2025-06-10T17:57:50Z",
    "updated": "2026-01-05T10:14:19Z",
    "link": "http://arxiv.org/pdf/2506.09040v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Dianyi Wang",
      "Wei Song",
      "Yikun Wang",
      "Siyuan Wang",
      "Kaicheng Yu",
      "Zhongyu Wei",
      "Jiaqi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.06065v2",
    "title": "Posets and Bounded Probabilities for Discovering Order-inducing Features in Event Knowledge Graphs",
    "summary": "Event knowledge graphs (EKG) extend the classical notion of a trace to capture multiple, interacting views of a process execution. In this paper, we tackle the open problem of automating EKG discovery from uncurated data through a principled probabilistic framing based on the outcome space resulting from featured-derived partial orders on events. From this we derive an EKG discovery algorithm based on statistical inference rather than an ad hoc or heuristic-based strategy, or relying on manual analysis from domain experts.\n  This approach comes at the computational cost of exploring a large, non-convex hypothesis space. In particular, solving the maximum likelihood term in our objective function involves counting the number of linear extensions of posets, which in general is #P-complete. Fortunately, bound estimates suffice for model comparison, and admit incorporation into a bespoke branch-and-bound algorithm. We establish an upper bound on our objective function which we show to be antitonic w.r.t. search depth for branching rules that are monotonic w.r.t. model inclusion. This allows pruning of large portions of the search space, which we show experimentally leads to rapid convergence toward optimal solutions that are consistent with manually built EKGs.",
    "published": "2024-10-08T14:12:51Z",
    "updated": "2026-01-05T10:12:52Z",
    "link": "http://arxiv.org/pdf/2410.06065v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Christoffer Olling Back",
      "Jakob Grue Simonsen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.08269v2",
    "title": "Geometry-induced Regularization in Deep ReLU Neural Networks",
    "summary": "Neural networks with a large number of parameters often do not overfit, owing to implicit regularization that favors \\lq good\\rq{} networks. Other related and puzzling phenomena include properties of flat minima, saddle-to-saddle dynamics, and neuron alignment. To investigate these phenomena, we study the local geometry of deep ReLU neural networks. We show that, for a fixed architecture, as the weights vary, the image of a sample $X$ forms a set whose local dimension changes. The parameter space is partitioned into regions where this local dimension remains constant. The local dimension is invariant under the natural symmetries of ReLU networks (i.e., positive rescalings and neuron permutations). We establish then that the network's geometry induces a regularization, with the local dimension serving as a key measure of regularity. Moreover, we relate the local dimension to a new notion of flatness of minima and to saddle-to-saddle dynamics. For shallow networks, we also show that the local dimension is connected to the number of linear regions perceived by $X$, offering insight into the effects of regularization. This is further supported by experiments and linked to neuron alignment. Our analysis offers, for the first time, a simple and unified geometric explanation that applies to all learning contexts for these phenomena, which are usually studied in isolation. Finally, we explore the practical computation of the local dimension and present experiments on the MNIST dataset, which highlight geometry-induced regularization in this setting.",
    "published": "2024-02-13T07:49:57Z",
    "updated": "2026-01-05T09:58:43Z",
    "link": "http://arxiv.org/pdf/2402.08269v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "math.OC",
      "math.ST"
    ],
    "authors": [
      "Joachim Bona-Pellissier",
      "François Malgouyres",
      "François Bachoc"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01944v1",
    "title": "The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities",
    "summary": "In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored.\n  We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.",
    "published": "2026-01-05T09:50:37Z",
    "updated": "2026-01-05T09:50:37Z",
    "link": "http://arxiv.org/pdf/2601.01944v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.PL"
    ],
    "authors": [
      "Matteo Esposito",
      "Andrea Janes",
      "Valentina Lenarduzzi",
      "Davide Taibi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01939v1",
    "title": "OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation",
    "summary": "In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.",
    "published": "2026-01-05T09:48:18Z",
    "updated": "2026-01-05T09:48:18Z",
    "link": "http://arxiv.org/pdf/2601.01939v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Victor Sanchez",
      "Chris Reinke",
      "Ahamed Mohamed",
      "Xavier Alameda-Pineda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26217v2",
    "title": "Benchmarking Deep Learning Convolutions on Energy-constrained CPUs",
    "summary": "This work evaluates State-of-the-Art convolution algorithms for CPU-based CNN inference. Although most prior studies focus on GPUs or NPUs, CPU implementations remain comparatively under-optimized. Our first contribution is to provide fair benchmarking for embedded CPU inference. We evaluate direct, GEMM-based, and Winograd convolutions across modern CPUs from ARM, Intel, AMD, and NVIDIA vendors, considering both latency and energy efficiency. To the best of our knowledge, this is the first study to present a fair, cross-vendor comparison of CPU energy consumption using a high-resolution socket-level measurement platform. To validate our methodology, we further compare socket-level power measurements with estimates derived from model-specific registers (MSRs), finding that MSRs underestimate the power consumption of convolution inference by 10--30%. Our results show that the ARM\\R Cortex-A78AE CPU combined with an implicit GEMM convolution implementation offers the best trade-off between latency and power consumption, achieving ResNet50v1.5 inference in 102 ms with an average power of 25.3 W, corresponding to 2.58 J.",
    "published": "2025-09-30T13:19:00Z",
    "updated": "2026-01-05T09:47:47Z",
    "link": "http://arxiv.org/pdf/2509.26217v2.pdf",
    "category": [
      "cs.AI",
      "cs.AR"
    ],
    "authors": [
      "Enrique Galvez",
      "Adrien Cassagne",
      "Alix Munier",
      "Manuel Bouyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01932v1",
    "title": "Visualizing the Structure of Lenia Parameter Space",
    "summary": "Continuous cellular automata are rocketing in popularity, yet developing a theoretical understanding of their behaviour remains a challenge. In the case of Lenia, a few fundamental open problems include determining what exactly constitutes a soliton, what is the overall structure of the parameter space, and where do the solitons occur in it. In this abstract, we present a new method to automatically classify Lenia systems into four qualitatively different dynamical classes. This allows us to detect moving solitons, and to provide an interactive visualization of Lenia's parameter space structure on our website https://lenia-explorer.vercel.app/. The results shed new light on the above-mentioned questions and lead to several observations: the existence of new soliton families for parameters where they were not believed to exist, or the universality of the phase space structure across various kernels.",
    "published": "2026-01-05T09:35:06Z",
    "updated": "2026-01-05T09:35:06Z",
    "link": "http://arxiv.org/pdf/2601.01932v1.pdf",
    "category": [
      "nlin.CG",
      "cs.AI"
    ],
    "authors": [
      "Barbora Hudcová",
      "František Dušek",
      "Marco Tuccio",
      "Clément Hongler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01931v1",
    "title": "DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems",
    "summary": "Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce DéjàQ, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of DéjàQ, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.",
    "published": "2026-01-05T09:27:49Z",
    "updated": "2026-01-05T09:27:49Z",
    "link": "http://arxiv.org/pdf/2601.01931v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Willem Röpke",
      "Samuel Coward",
      "Andrei Lupu",
      "Thomas Foster",
      "Tim Rocktäschel",
      "Jakob Foerster"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.21027v6",
    "title": "Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles",
    "summary": "For solving zero-sum games involving non-transitivity, a useful approach is to maintain a policy population to approximate the Nash Equilibrium (NE). Previous studies have shown that the Policy Space Response Oracles (PSRO) algorithm is an effective framework for solving such games. However, current methods initialize a new policy from scratch or inherit a single historical policy in Best Response (BR), missing the opportunity to leverage past policies to generate a better BR. In this paper, we propose Fusion-PSRO, which employs Nash Policy Fusion to initialize a new policy for BR training. Nash Policy Fusion serves as an implicit guiding policy that starts exploration on the current Meta-NE, thus providing a closer approximation to BR. Moreover, it insightfully captures a weighted moving average of past policies, dynamically adjusting these weights based on the Meta-NE in each iteration. This cumulative process further enhances the policy population. Empirical results on classic benchmarks show that Fusion-PSRO achieves lower exploitability, thereby mitigating the shortcomings of previous research on policy initialization in BR.",
    "published": "2024-05-31T17:16:29Z",
    "updated": "2026-01-05T09:24:45Z",
    "link": "http://arxiv.org/pdf/2405.21027v6.pdf",
    "category": [
      "cs.GT",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Jiesong Lian",
      "Yucong Huang",
      "Chengdong Ma",
      "Mingzhi Wang",
      "Ying Wen",
      "Long Hu",
      "Yixue Hao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01930v1",
    "title": "MCGI: Manifold-Consistent Graph Indexing for Billion-Scale Disk-Resident Vector Search",
    "summary": "Graph-based Approximate Nearest Neighbor (ANN) search often suffers from performance degradation in high-dimensional spaces due to the ``Euclidean-Geodesic mismatch,'' where greedy routing diverges from the underlying data manifold. To address this, we propose Manifold-Consistent Graph Indexing (MCGI), a geometry-aware and disk-resident indexing method that leverages Local Intrinsic Dimensionality (LID) to dynamically adapt search strategies to the data's intrinsic geometry. Unlike standard algorithms that treat dimensions uniformly, MCGI modulates its beam search budget based on in situ geometric analysis, eliminating dependency on static hyperparameters. Theoretical analysis confirms that MCGI enables improved approximation guarantees by preserving manifold-consistent topological connectivity. Empirically, MCGI achieves 5.8$\\times$ higher throughput at 95\\% recall on high-dimensional GIST1M compared to state-of-the-art DiskANN. On the billion-scale SIFT1B dataset, MCGI further validates its scalability by reducing high-recall query latency by 3$\\times$, while maintaining performance parity on standard lower-dimensional datasets.",
    "published": "2026-01-05T09:23:48Z",
    "updated": "2026-01-05T09:23:48Z",
    "link": "http://arxiv.org/pdf/2601.01930v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Dongfang Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01927v1",
    "title": "Theoretical Convergence of SMOTE-Generated Samples",
    "summary": "Imbalanced data affects a wide range of machine learning applications, from healthcare to network security. As SMOTE is one of the most popular approaches to addressing this issue, it is imperative to validate it not only empirically but also theoretically. In this paper, we provide a rigorous theoretical analysis of SMOTE's convergence properties. Concretely, we prove that the synthetic random variable Z converges in probability to the underlying random variable X. We further prove a stronger convergence in mean when X is compact. Finally, we show that lower values of the nearest neighbor rank lead to faster convergence offering actionable guidance to practitioners. The theoretical results are supported by numerical experiments using both real-life and synthetic data. Our work provides a foundational understanding that enhances data augmentation techniques beyond imbalanced data scenarios.",
    "published": "2026-01-05T09:19:45Z",
    "updated": "2026-01-05T09:19:45Z",
    "link": "http://arxiv.org/pdf/2601.01927v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Firuz Kamalov",
      "Hana Sulieman",
      "Witold Pedrycz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.17999v2",
    "title": "GNN-XAR: A Graph Neural Network for Explainable Activity Recognition in Smart Homes",
    "summary": "Sensor-based Human Activity Recognition (HAR) in smart home environments is crucial for several applications, especially in the healthcare domain. The majority of the existing approaches leverage deep learning models. While these approaches are effective, the rationale behind their outputs is opaque. Recently, eXplainable Artificial Intelligence (XAI) approaches emerged to provide intuitive explanations to the output of HAR models. To the best of our knowledge, these approaches leverage classic deep models like CNNs or RNNs. Recently, Graph Neural Networks (GNNs) proved to be effective for sensor-based HAR. However, existing approaches are not designed with explainability in mind. In this work, we propose the first explainable Graph Neural Network explicitly designed for smart home HAR. Our results on two public datasets show that this approach provides better explanations than state-of-the-art methods while also slightly improving the recognition rate.",
    "published": "2025-02-25T09:05:13Z",
    "updated": "2026-01-05T09:14:27Z",
    "link": "http://arxiv.org/pdf/2502.17999v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Michele Fiori",
      "Davide Mor",
      "Gabriele Civitarese",
      "Claudio Bettini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01921v1",
    "title": "A Defect is Being Born: How Close Are We? A Time Sensitive Forecasting Approach",
    "summary": "Background. Defect prediction has been a highly active topic among researchers in the Empirical Software Engineering field. Previous literature has successfully achieved the most accurate prediction of an incoming fault and identified the features and anomalies that precede it through just-in-time prediction. As software systems evolve continuously, there is a growing need for time-sensitive methods capable of forecasting defects before they manifest.\n  Aim. Our study seeks to explore the effectiveness of time-sensitive techniques for defect forecasting. Moreover, we aim to investigate the early indicators that precede the occurrence of a defect.\n  Method. We will train multiple time-sensitive forecasting techniques to forecast the future bug density of a software project, as well as identify the early symptoms preceding the occurrence of a defect.\n  Expected results. Our expected results are translated into empirical evidence on the effectiveness of our approach for early estimation of bug proneness.",
    "published": "2026-01-05T09:11:29Z",
    "updated": "2026-01-05T09:11:29Z",
    "link": "http://arxiv.org/pdf/2601.01921v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Mikel Robredo",
      "Matteo Esposito",
      "Fabio Palomba",
      "Rafael Peñaloza",
      "Valentina Lenarduzzi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.23163v2",
    "title": "Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs",
    "summary": "The screenplay serves as the foundation for television production, defining narrative structure, character development, and dialogue. While Large Language Models (LLMs) show great potential in creative writing, direct end-to-end generation approaches often fail to produce well-crafted screenplays. We argue this failure stems from forcing a single model to simultaneously master two disparate capabilities: creative narrative construction and rigid format adherence. The resulting outputs may mimic superficial style but lack the deep structural integrity and storytelling substance required for professional use. To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage Refinement (DSR), a decomposed framework that decouples creative narrative generation from format conversion. The first stage transforms a brief outline into rich, novel-style prose. The second stage refines this narrative into a professionally formatted screenplay. This separation enables the model to specialize in one distinct capability at each stage. A key challenge in implementing DSR is the scarcity of paired outline-to-novel training data. We address this through hybrid data synthesis: reverse synthesis deconstructs existing screenplays into structured inputs, while forward synthesis leverages these inputs to generate high-quality narrative texts as training targets. Blind evaluations by professional screenwriters show that DSR achieves a 75% win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of human-level performance. Our work demonstrates that decomposed generation architecture with tailored data synthesis effectively specializes LLMs in complex creative domains.",
    "published": "2025-10-27T09:41:29Z",
    "updated": "2026-01-05T09:07:45Z",
    "link": "http://arxiv.org/pdf/2510.23163v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Hang Lei",
      "Shengyi Zong",
      "Zhaoyan Li",
      "Ziren Zhou",
      "Hao Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.20704v2",
    "title": "Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models",
    "summary": "The increasing integration of Visual Language Models (VLMs) into AI systems necessitates robust model alignment, especially when handling multimodal content that combines text and images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual vulnerabilities under evaluated. To address this gap, we propose \\textbf{Text2VLM}, a novel multi-stage pipeline that adapts text-only datasets into multimodal formats, specifically designed to evaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline identifies harmful content in the original text and converts it into a typographic image, creating a multimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased susceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in the current models' alignment. This is in addition to a significant performance gap compared to closed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment of extracted salient concepts; text summarization and output classification align with human expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to the development of more robust safety mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities, Text2VLM plays a role in advancing the safe deployment of VLMs in diverse, real-world applications.",
    "published": "2025-07-28T10:57:44Z",
    "updated": "2026-01-05T09:07:09Z",
    "link": "http://arxiv.org/pdf/2507.20704v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Gabriel Downer",
      "Sean Craven",
      "Damian Ruck",
      "Jake Thomas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.07842v2",
    "title": "Alignment-Aware Quantization for LLM Safety",
    "summary": "Safety and efficiency are paramount yet often conflicting requirements for deploying Large Language Models (LLMs). While LLMs are trained to follow human alignment for safety, Post-Training Quantization (PTQ) is applied afterward to ensure efficiency. Here we identify a fundamental flaw in the conventional PTQ paradigm: quantization can turn into a safety vulnerability if it only aims to achieve low perplexity. To address this, we propose \\textbf{Alignment-Aware Quantization (AAQ)}, a novel approach that integrates an \\textbf{Alignment-Preserving Contrastive (APC)} loss into the PTQ pipeline. Our method explicitly preserves alignment by encouraging the quantized model to mimic its safe, instruction-tuned model while diverging from the unaligned, pre-trained counterpart. AAQ achieves robust safety alignment without specialized safety-focused datasets, using only standard calibration data. We show that AAQ is compatible with standard PTQ techniques and enables robust 4-bit (W4A4) quantization across diverse model families. Our work resolves the critical trade-off between efficiency and safety, paving the way toward LLMs that are both efficient and trustworthy. Anonymized code is available in the supplementary material.",
    "published": "2025-11-11T05:24:30Z",
    "updated": "2026-01-05T08:58:48Z",
    "link": "http://arxiv.org/pdf/2511.07842v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Sunghyun Wee",
      "Suyoung Kim",
      "Hyeonjin Kim",
      "Kyomin Hwang",
      "Nojun Kwak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01910v1",
    "title": "MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning",
    "summary": "Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.\n  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.",
    "published": "2026-01-05T08:55:27Z",
    "updated": "2026-01-05T08:55:27Z",
    "link": "http://arxiv.org/pdf/2601.01910v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Minh Hieu Ha",
      "Khanh Ly Ta",
      "Hung Phan",
      "Tung Doan",
      "Tung Dao",
      "Dao Tran",
      "Huynh Thi Thanh Binh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01908v1",
    "title": "Nodule-DETR: A Novel DETR Architecture with Frequency-Channel Attention for Ultrasound Thyroid Nodule Detection",
    "summary": "Thyroid cancer is the most common endocrine malignancy, and its incidence is rising globally. While ultrasound is the preferred imaging modality for detecting thyroid nodules, its diagnostic accuracy is often limited by challenges such as low image contrast and blurred nodule boundaries. To address these issues, we propose Nodule-DETR, a novel detection transformer (DETR) architecture designed for robust thyroid nodule detection in ultrasound images. Nodule-DETR introduces three key innovations: a Multi-Spectral Frequency-domain Channel Attention (MSFCA) module that leverages frequency analysis to enhance features of low-contrast nodules; a Hierarchical Feature Fusion (HFF) module for efficient multi-scale integration; and Multi-Scale Deformable Attention (MSDA) to flexibly capture small and irregularly shaped nodules. We conducted extensive experiments on a clinical dataset of real-world thyroid ultrasound images. The results demonstrate that Nodule-DETR achieves state-of-the-art performance, outperforming the baseline model by a significant margin of 0.149 in mAP@0.5:0.95. The superior accuracy of Nodule-DETR highlights its significant potential for clinical application as an effective tool in computer-aided thyroid diagnosis. The code of work is available at https://github.com/wjj1wjj/Nodule-DETR.",
    "published": "2026-01-05T08:53:04Z",
    "updated": "2026-01-05T08:53:04Z",
    "link": "http://arxiv.org/pdf/2601.01908v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Jingjing Wang",
      "Qianglin Liu",
      "Zhuo Xiao",
      "Xinning Yao",
      "Bo Liu",
      "Lu Li",
      "Lijuan Niu",
      "Fugen Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01904v1",
    "title": "Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning",
    "summary": "Learning from Preferences in Reinforcement Learning (PbRL) has gained attention recently, as it serves as a natural fit for complicated tasks where the reward function is not easily available. However, preferences often come with uncertainty and noise if they are not from perfect teachers. Much prior literature aimed to detect noise, but with limited types of noise and most being uniformly distributed with no connection to observations. In this work, we formalize the notion of targeted feature-dependent noise and propose several variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise.\n  We evaluate feature-dependent noise, where noise is correlated with certain features in complex continuous control tasks from DMControl and Meta-world. Our experiments show that in some feature-dependent noise settings, the state-of-the-art noise-robust PbRL method's learning performance is significantly deteriorated, while PbRL method with no explicit denoising can surprisingly outperform noise-robust PbRL in majority settings.\n  We also find language model's noise exhibits similar characteristics to feature-dependent noise, thereby simulating realistic humans and call for further study in learning with feature-dependent noise robustly.",
    "published": "2026-01-05T08:49:30Z",
    "updated": "2026-01-05T08:49:30Z",
    "link": "http://arxiv.org/pdf/2601.01904v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yuxuan Li",
      "Harshith Reddy Kethireddy",
      "Srijita Das"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.10954v3",
    "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks",
    "summary": "Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the existing GitHub issue resolution data construction pipeline is challenging and labor-intensive. We identify three key limitations in existing pipelines: (1) test patches collected often omit binary file changes; (2) the manual construction of evaluation environments is labor-intensive; and (3) the fail2pass validation phase requires manual inspection of test logs and writing custom parsing code to extract test status from logs. In this paper, we propose SWE-Factory, a fully automated issue resolution data construction pipeline, to resolve these limitations. First, our pipeline automatically recovers missing binary test files and ensures the correctness of test patches. Second, we introduce SWE-Builder, a LLM-based multi-agent system that automates evaluation environment construction. Third, we introduce a standardized, exit-code-based log parsing method to automatically extract test status, enabling a fully automated fail2pass validation. Experiments on 671 real-world GitHub issues across four programming languages show that our method can effectively construct valid evaluation environments for GitHub issues at a reasonable cost. For example, with GPT-4.1 mini, our SWE-Builder constructs 337 valid task instances out of 671 issues, at $0.047 per instance. Our ablation study further shows the effectiveness of different components of SWE-Builder. We also demonstrate through manual inspection that our exit-code-based fail2pass validation method is highly accurate, achieving an F1 score of 0.99. Additionally, we conduct an exploratory experiment to investigate whether we can use SWE-Factory to enhance models' software engineering ability.",
    "published": "2025-06-12T17:54:17Z",
    "updated": "2026-01-05T08:48:37Z",
    "link": "http://arxiv.org/pdf/2506.10954v3.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Lianghong Guo",
      "Yanlin Wang",
      "Caihua Li",
      "Wei Tao",
      "Pengyu Yang",
      "Jiachi Chen",
      "Haoyu Song",
      "Duyu Tang",
      "Zibin Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.06388v4",
    "title": "Convergence of a L2 regularized Policy Gradient Algorithm for the Multi Armed Bandit",
    "summary": "Although Multi Armed Bandit (MAB) on one hand and the policy gradient approach on the other hand are among the most used frameworks of Reinforcement Learning, the theoretical properties of the policy gradient algorithm used for MAB have not been given enough attention. We investigate in this work the convergence of such a procedure for the situation when a $L2$ regularization term is present jointly with the 'softmax' parametrization. We prove convergence under appropriate technical hypotheses and test numerically the procedure including situations beyond the theoretical setting. The tests show that a time dependent regularized procedure can improve over the canonical approach especially when the initial guess is far from the solution.",
    "published": "2024-02-09T13:10:04Z",
    "updated": "2026-01-05T08:44:46Z",
    "link": "http://arxiv.org/pdf/2402.06388v4.pdf",
    "category": [
      "stat.ML",
      "cs.AI",
      "cs.DS",
      "cs.LG",
      "math.NA"
    ],
    "authors": [
      "Stefana Anita",
      "Gabriel Turinici"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01896v1",
    "title": "Tackling the Inherent Difficulty of Noise Filtering in RAG",
    "summary": "Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.",
    "published": "2026-01-05T08:40:37Z",
    "updated": "2026-01-05T08:40:37Z",
    "link": "http://arxiv.org/pdf/2601.01896v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jingyu Liu",
      "Jiaen Lin",
      "Yong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05753v2",
    "title": "A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning",
    "summary": "The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.",
    "published": "2025-12-05T14:39:50Z",
    "updated": "2026-01-05T08:40:11Z",
    "link": "http://arxiv.org/pdf/2512.05753v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Wencheng Cai",
      "Xuchao Gao",
      "Congying Han",
      "Mingqiang Li",
      "Tiande Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.03802v4",
    "title": "Balancing Fidelity and Plasticity: Aligning Mixed-Precision Fine-Tuning with Linguistic Hierarchies",
    "summary": "Deploying and fine-tuning Large Language Models (LLMs) on resource-constrained edge devices requires navigating a strict trade-off between memory footprint and task performance. While Quantization-Aware Fine-tuning has emerged as a viable solution, existing paradigms typically decouple quantization and adapter optimization. This separation overlooks a fundamental theoretical constraint we identify as the \\textit{Fidelity-Plasticity Trade-off}: a layer's capacity to adapt to new tasks (Plasticity) is inherently constrained by the information capacity of its frozen weights (Fidelity). Aggressively quantizing semantically critical layers creates an information bottleneck that no amount of adapter rank can recover, while high precision in robust syntactic layers wastes valuable memory. To address this, we introduce \\textbf{QR-Adaptor}, a unified framework that jointly optimizes per-layer quantization bit-width and LoRA rank. By formulating resource allocation as a multi-objective search aligned with the model's linguistic hierarchy, our method systematically liberates memory from redundancy-heavy layers to reinvest in capacity-critical ones. Extensive experiments demonstrate that QR-Adaptor establishes a new Pareto frontier: notably, a model fine-tuned under a strict 4-bit memory budget achieves performance rivaling 16-bit baselines, demonstrating that precise resource alignment is as critical as model size.",
    "published": "2025-05-02T08:46:01Z",
    "updated": "2026-01-05T08:32:05Z",
    "link": "http://arxiv.org/pdf/2505.03802v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Changhai Zhou",
      "Shiyang Zhang",
      "Yuhua Zhou",
      "Qian Qiao",
      "Jun Gao",
      "Shichao Weng",
      "Weizhong Zhang",
      "Cheng Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01887v1",
    "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
    "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
    "published": "2026-01-05T08:26:34Z",
    "updated": "2026-01-05T08:26:34Z",
    "link": "http://arxiv.org/pdf/2601.01887v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jiawen Zhang",
      "Lipeng He",
      "Kejia Chen",
      "Jian Lou",
      "Jian Liu",
      "Xiaohu Yang",
      "Ruoxi Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.08344v3",
    "title": "What Breaks Knowledge Graph based RAG? Empirical Insights into Reasoning under Incomplete Knowledge",
    "summary": "Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) is an increasingly explored approach for combining the reasoning capabilities of large language models with the structured evidence of knowledge graphs. However, current evaluation practices fall short: existing benchmarks often include questions that can be directly answered using existing triples in KG, making it unclear whether models perform reasoning or simply retrieve answers directly. Moreover, inconsistent evaluation metrics and lenient answer matching criteria further obscure meaningful comparisons. In this work, we introduce a general method for constructing benchmarks, together with an evaluation protocol, to systematically assess KG-RAG methods under knowledge incompleteness. Our empirical results show that current KG-RAG methods have limited reasoning ability under missing knowledge, often rely on internal memorization, and exhibit varying degrees of generalization depending on their design.",
    "published": "2025-08-11T10:55:06Z",
    "updated": "2026-01-05T08:14:16Z",
    "link": "http://arxiv.org/pdf/2508.08344v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Dongzhuoran Zhou",
      "Yuqicheng Zhu",
      "Xiaxia Wang",
      "Hongkuan Zhou",
      "Yuan He",
      "Jiaoyan Chen",
      "Steffen Staab",
      "Evgeny Kharlamov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01878v1",
    "title": "Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs",
    "summary": "Socio-cognitive benchmarks for large language models (LLMs) often fail to predict real-world behavior, even when models achieve high benchmark scores. Prior work has attributed this evaluation-deployment gap to problems of measurement and validity. While these critiques are insightful, we argue that they overlook a more fundamental issue: many socio-cognitive evaluations proceed without an explicit theoretical specification of the target capability, leaving the assumptions linking task performance to competence implicit. Without this theoretical grounding, benchmarks that exercise only narrow subsets of a capability are routinely misinterpreted as evidence of broad competence: a gap that creates a systemic validity illusion by masking the failure to evaluate the capability's other essential dimensions. To address this gap, we make two contributions. First, we diagnose and formalize this theory gap as a foundational failure that undermines measurement and enables systematic overgeneralization of benchmark results. Second, we introduce the Theory Trace Card (TTC), a lightweight documentation artifact designed to accompany socio-cognitive evaluations, which explicitly outlines the theoretical basis of an evaluation, the components of the target capability it exercises, its operationalization, and its limitations. We argue that TTCs enhance the interpretability and reuse of socio-cognitive evaluations by making explicit the full validity chain, which links theory, task operationalization, scoring, and limitations, without modifying benchmarks or requiring agreement on a single theory.",
    "published": "2026-01-05T08:06:50Z",
    "updated": "2026-01-05T08:06:50Z",
    "link": "http://arxiv.org/pdf/2601.01878v1.pdf",
    "category": [
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Farzan Karimi-Malekabadi",
      "Suhaib Abdurahman",
      "Zhivar Sourati",
      "Jackson Trager",
      "Morteza Dehghani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01875v1",
    "title": "Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence",
    "summary": "Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.",
    "published": "2026-01-05T08:02:49Z",
    "updated": "2026-01-05T08:02:49Z",
    "link": "http://arxiv.org/pdf/2601.01875v1.pdf",
    "category": [
      "cs.AI",
      "q-bio.QM"
    ],
    "authors": [
      "Kewen Cao",
      "Jianxu Chen",
      "Yongbing Zhang",
      "Ye Zhang",
      "Hongxiao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01874v1",
    "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
    "summary": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\\Rightarrow$internalization$\\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.",
    "published": "2026-01-05T08:02:18Z",
    "updated": "2026-01-05T08:02:18Z",
    "link": "http://arxiv.org/pdf/2601.01874v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Shuhang Chen",
      "Yunqiu Xu",
      "Junjie Xie",
      "Aojun Lu",
      "Tao Feng",
      "Zeying Huang",
      "Ning Zhang",
      "Yi Sun",
      "Yi Yang",
      "Hangjie Yuan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.14150v2",
    "title": "PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario",
    "summary": "Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are publicly available at: https://emorzz1g.github.io/PathFinder/.",
    "published": "2025-12-16T07:15:15Z",
    "updated": "2026-01-05T07:45:27Z",
    "link": "http://arxiv.org/pdf/2512.14150v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhijie Zhong",
      "Zhiwen Yu",
      "Pengyu Li",
      "Jianming Lv",
      "C. L. Philip Chen",
      "Min Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01857v1",
    "title": "Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios",
    "summary": "As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.",
    "published": "2026-01-05T07:35:12Z",
    "updated": "2026-01-05T07:35:12Z",
    "link": "http://arxiv.org/pdf/2601.01857v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Defei Xia",
      "Bingfeng Pi",
      "Shenbin Zhang",
      "Song Hua",
      "Yunfei Wei",
      "Lei Zuo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01852v1",
    "title": "MORE: Multi-Objective Adversarial Attacks on Speech Recognition",
    "summary": "The emergence of large-scale automatic speech recognition (ASR) models such as Whisper has greatly expanded their adoption across diverse real-world applications. Ensuring robustness against even minor input perturbations is therefore critical for maintaining reliable performance in real-time environments. While prior work has mainly examined accuracy degradation under adversarial attacks, robustness with respect to efficiency remains largely unexplored. This narrow focus provides only a partial understanding of ASR model vulnerabilities. To address this gap, we conduct a comprehensive study of ASR robustness under multiple attack scenarios. We introduce MORE, a multi-objective repetitive doubling encouragement attack, which jointly degrades recognition accuracy and inference efficiency through a hierarchical staged repulsion-anchoring mechanism. Specifically, we reformulate multi-objective adversarial optimization into a hierarchical framework that sequentially achieves the dual objectives. To further amplify effectiveness, we propose a novel repetitive encouragement doubling objective (REDO) that induces duplicative text generation by maintaining accuracy degradation and periodically doubling the predicted sequence length. Overall, MORE compels ASR models to produce incorrect transcriptions at a substantially higher computational cost, triggered by a single adversarial input. Experiments show that MORE consistently yields significantly longer transcriptions while maintaining high word error rates compared to existing baselines, underscoring its effectiveness in multi-objective adversarial attack.",
    "published": "2026-01-05T07:27:57Z",
    "updated": "2026-01-05T07:27:57Z",
    "link": "http://arxiv.org/pdf/2601.01852v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Xiaoxue Gao",
      "Zexin Li",
      "Yiming Chen",
      "Nancy F. Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01844v1",
    "title": "Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation",
    "summary": "Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.",
    "published": "2026-01-05T07:16:29Z",
    "updated": "2026-01-05T07:16:29Z",
    "link": "http://arxiv.org/pdf/2601.01844v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Udiptaman Das",
      "Krishnasai B. Atmakuri",
      "Duy Ho",
      "Chi Lee",
      "Yugyung Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01839v1",
    "title": "The Machine Learning Canvas: Empirical Findings on Why Strategy Matters More Than AI Code Generation",
    "summary": "Despite the growing popularity of AI coding assistants, over 80% of machine learning (ML) projects fail to deliver real business value. This study creates and tests a Machine Learning Canvas, a practical framework that combines business strategy, software engineering, and data science in order to determine the factors that lead to the success of ML projects. We surveyed 150 data scientists and analyzed their responses using statistical modeling. We identified four key success factors: Strategy (clear goals and planning), Process (how work gets done), Ecosystem (tools and infrastructure), and Support (organizational backing and resources). Our results show that these factors are interconnected - each one affects the next. For instance, strong organizational support results in a clearer strategy (β= 0.432, p < 0.001), which improves work processes (β= 0.428, p < 0.001) and builds better infrastructure (β= 0.547, p < 0.001). Together, these elements determine whether a project succeeds. The surprising finding? Although AI assistants make coding faster, they don't guarantee project success. AI assists with the \"how\" of coding but cannot replace the \"why\" and \"what\" of strategic thinking.",
    "published": "2026-01-05T07:02:58Z",
    "updated": "2026-01-05T07:02:58Z",
    "link": "http://arxiv.org/pdf/2601.01839v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Martin Prause"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01836v1",
    "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
    "summary": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.",
    "published": "2026-01-05T06:57:45Z",
    "updated": "2026-01-05T06:57:45Z",
    "link": "http://arxiv.org/pdf/2601.01836v1.pdf",
    "category": [
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Dasol Choi",
      "DongGeon Lee",
      "Brigitta Jesica Kartono",
      "Helena Berndt",
      "Taeyoun Kwon",
      "Joonwon Jang",
      "Haon Park",
      "Hwanjo Yu",
      "Minsuk Kahng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01835v1",
    "title": "RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection from Skin Images",
    "summary": "In this paper, a deep learning approach for Mpox diagnosis named Customized Residual SwinTransformerV2 (RSwinV2) has been proposed, trying to enhance the capability of lesion classification by employing the RSwinV2 tool-assisted vision approach. In the RSwinV2 method, a hierarchical structure of the transformer has been customized based on the input dimensionality, embedding structure, and output targeted by the method. In this RSwinV2 approach, the input image has been split into non-overlapping patches and processed using shifted windows and attention in these patches. This process has helped the method link all the windows efficiently by avoiding the locality issues of non-overlapping regions in attention, while being computationally efficient. RSwinV2 has further developed based on SwinTransformer and has included patch and position embeddings to take advantage of the transformer global-linking capability by employing multi-head attention in these embeddings. Furthermore, RSwinV2 has developed and incorporated the Inverse Residual Block (IRB) into this method, which utilizes convolutional skip connections with these inclusive designs to address the vanishing gradient issues during processing. RSwinV2 inclusion of IRB has therefore facilitated this method to link global patterns as well as local patterns; hence, its integrity has helped improve lesion classification capability by minimizing variability of Mpox and increasing differences of Mpox, chickenpox, measles, and cowpox. In testing SwinV2, its accuracy of 96.21 and an F1score of 95.62 have been achieved on the Kaggle public dataset, which has outperformed standard CNN models and SwinTransformers; RSwinV2 vector has thus proved its valiance as a computer-assisted tool for Mpox lesion observation interpretation.",
    "published": "2026-01-05T06:57:26Z",
    "updated": "2026-01-05T06:57:26Z",
    "link": "http://arxiv.org/pdf/2601.01835v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Rashid Iqbal",
      "Saddam Hussain Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01832v1",
    "title": "Yukthi Opus: A Multi-Chain Hybrid Metaheuristic for Large-Scale NP-Hard Optimization",
    "summary": "We present Yukthi Opus (YO), a multi-chain hybrid metaheuristic designed for NP-hard optimization under explicit evaluation budget constraints. YO integrates three complementary mechanisms in a structured two-phase architecture: Markov Chain Monte Carlo (MCMC) for global exploration, greedy local search for exploitation, and simulated annealing with adaptive reheating to enable controlled escape from local minima. A dedicated burn-in phase allocates evaluations to probabilistic exploration, after which a hybrid optimization loop refines promising candidates. YO further incorporates a spatial blacklist mechanism to avoid repeated evaluation of poor regions and a multi-chain execution strategy to improve robustness and reduce sensitivity to initialization.\n  We evaluate YO on three benchmarks: the Rastrigin function (5D) with ablation studies, the Traveling Salesman Problem with 50 to 200 cities, and the Rosenbrock function (5D) with comparisons against established optimizers including CMA-ES, Bayesian optimization, and accelerated particle swarm optimization. Results show that MCMC exploration and greedy refinement are critical for solution quality, while simulated annealing and multi-chain execution primarily improve stability and variance reduction. Overall, YO achieves competitive performance on large and multimodal problems while maintaining predictable evaluation budgets, making it suitable for expensive black-box optimization settings.",
    "published": "2026-01-05T06:51:08Z",
    "updated": "2026-01-05T06:51:08Z",
    "link": "http://arxiv.org/pdf/2601.01832v1.pdf",
    "category": [
      "cs.NE",
      "cs.AI"
    ],
    "authors": [
      "SB Danush Vikraman",
      "Hannah Abagail",
      "Prasanna Kesavraj",
      "Gajanan V Honnavar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01831v1",
    "title": "ARIES: A Scalable Multi-Agent Orchestration Framework for Real-Time Epidemiological Surveillance and Outbreak Monitoring",
    "summary": "Global health surveillance is currently facing a challenge of Knowledge Gaps. While general-purpose AI has proliferated, it remains fundamentally unsuited for the high-stakes epidemiological domain due to chronic hallucinations and an inability to navigate specialized data silos. This paper introduces ARIES (Agentic Retrieval Intelligence for Epidemiological Surveillance), a specialized, autonomous multi-agent framework designed to move beyond static, disease-specific dashboards toward a dynamic intelligence ecosystem. Built on a hierarchical command structure, ARIES utilizes GPTs to orchestrate a scalable swarm of sub-agents capable of autonomously querying World Health Organization (WHO), Center for Disease Control and Prevention (CDC), and peer-reviewed research papers. By automating the extraction and logical synthesis of surveillance data, ARIES provides a specialized reasoning that identifies emergent threats and signal divergence in near real-time. This modular architecture proves that a task-specific agentic swarm can outperform generic models, offering a robust, extensible for next-generation outbreak response and global health intelligence.",
    "published": "2026-01-05T06:50:40Z",
    "updated": "2026-01-05T06:50:40Z",
    "link": "http://arxiv.org/pdf/2601.01831v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "cs.IR",
      "cs.SE"
    ],
    "authors": [
      "Aniket Wattamwar",
      "Sampson Akwafuo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01828v1",
    "title": "Emergent Introspective Awareness in Large Language Models",
    "summary": "We investigate whether large language models can introspect on their internal states. It is difficult to answer this question through conversation alone, as genuine introspection cannot be distinguished from confabulations. Here, we address this challenge by injecting representations of known concepts into a model's activations, and measuring the influence of these manipulations on the model's self-reported states. We find that models can, in certain scenarios, notice the presence of injected concepts and accurately identify them. Models demonstrate some ability to recall prior internal representations and distinguish them from raw text inputs. Strikingly, we find that some models can use their ability to recall prior intentions in order to distinguish their own outputs from artificial prefills. In all these experiments, Claude Opus 4 and 4.1, the most capable models we tested, generally demonstrate the greatest introspective awareness; however, trends across models are complex and sensitive to post-training strategies. Finally, we explore whether models can explicitly control their internal representations, finding that models can modulate their activations when instructed or incentivized to \"think about\" a concept. Overall, our results indicate that current language models possess some functional introspective awareness of their own internal states. We stress that in today's models, this capacity is highly unreliable and context-dependent; however, it may continue to develop with further improvements to model capabilities.",
    "published": "2026-01-05T06:47:41Z",
    "updated": "2026-01-05T06:47:41Z",
    "link": "http://arxiv.org/pdf/2601.01828v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jack Lindsey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.00020v2",
    "title": "Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing",
    "summary": "Electroencephalography (EEG)-based brain-computer interfaces (BCIs) are strongly affected by non-stationary neural signals that vary across sessions and individuals, limiting the generalization of subject-agnostic models and motivating adaptive and personalized learning on resource-constrained platforms. Programmable memristive hardware offers a promising substrate for such post-deployment adaptation; however, practical realization is challenged by limited weight resolution, device variability, nonlinear programming dynamics, and finite device endurance. In this work, we show that spiking neural networks (SNNs) can be deployed on ferroelectric memristive synaptic devices for adaptive EEG-based motor imagery decoding under realistic device constraints. We fabricate, characterize, and model ferroelectric synapses. We evaluate a convolutional-recurrent SNN architecture under two complementary deployment strategies: (i) device-aware training using a ferroelectric synapse model, and (ii) transfer of software-trained weights followed by low-overhead on-device re-tuning. To enable efficient adaptation, we introduce a device-aware weight-update strategy in which gradient-based updates are accumulated digitally and converted into discrete programming events only when a threshold is exceeded, emulating nonlinear, state-dependent programming dynamics while reducing programming frequency. Both deployment strategies achieve classification performance comparable to state-of-the-art software-based SNNs. Furthermore, subject-specific transfer learning achieved by retraining only the final network layers improves classification accuracy. These results demonstrate that programmable ferroelectric hardware can support robust, low-overhead adaptation in spiking neural networks, opening a practical path toward personalized neuromorphic processing of neural signals.",
    "published": "2025-12-22T01:09:24Z",
    "updated": "2026-01-05T06:41:55Z",
    "link": "http://arxiv.org/pdf/2601.00020v2.pdf",
    "category": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Nikhil Garg",
      "Anxiong Song",
      "Niklas Plessnig",
      "Nathan Savoia",
      "Laura Bégon-Lours"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.04497v4",
    "title": "Opportunities and Challenges of Large Language Models for Low-Resource Languages in Humanities Research",
    "summary": "Low-resource languages serve as invaluable repositories of human history, embodying cultural evolution and intellectual diversity. Despite their significance, these languages face critical challenges, including data scarcity and technological limitations, which hinder their comprehensive study and preservation. Recent advancements in large language models (LLMs) offer transformative opportunities for addressing these challenges, enabling innovative methodologies in linguistic, historical, and cultural research. This study systematically evaluates the applications of LLMs in low-resource language research, encompassing linguistic variation, historical documentation, cultural expressions, and literary analysis. By analyzing technical frameworks, current methodologies, and ethical considerations, this paper identifies key challenges such as data accessibility, model adaptability, and cultural sensitivity. Given the cultural, historical, and linguistic richness inherent in low-resource languages, this work emphasizes interdisciplinary collaboration and the development of customized models as promising avenues for advancing research in this domain. By underscoring the potential of integrating artificial intelligence with the humanities to preserve and study humanity's linguistic and cultural heritage, this study fosters global efforts towards safeguarding intellectual diversity.",
    "published": "2024-11-30T00:10:56Z",
    "updated": "2026-01-05T05:58:43Z",
    "link": "http://arxiv.org/pdf/2412.04497v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Tianyang Zhong",
      "Zhenyuan Yang",
      "Zhengliang Liu",
      "Ruidong Zhang",
      "Weihang You",
      "Yiheng Liu",
      "Haiyang Sun",
      "Yi Pan",
      "Yiwei Li",
      "Yifan Zhou",
      "Hanqi Jiang",
      "Junhao Chen",
      "Tianming Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01816v1",
    "title": "Admissibility Alignment",
    "summary": "This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.\n  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.",
    "published": "2026-01-05T05:58:19Z",
    "updated": "2026-01-05T05:58:19Z",
    "link": "http://arxiv.org/pdf/2601.01816v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Chris Duffey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.24617v2",
    "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
    "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\\textbf{decoupled $μ$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\\textbf{+2.69$\\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.",
    "published": "2025-12-31T04:19:33Z",
    "updated": "2026-01-05T05:44:29Z",
    "link": "http://arxiv.org/pdf/2512.24617v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xingwei Qu",
      "Shaowen Wang",
      "Zihao Huang",
      "Kai Hua",
      "Fan Yin",
      "Rui-Jie Zhu",
      "Jundong Zhou",
      "Qiyang Min",
      "Zihao Wang",
      "Yizhi Li",
      "Tianyu Zhang",
      "He Xing",
      "Zheng Zhang",
      "Yuxuan Song",
      "Tianyu Zheng",
      "Zhiyuan Zeng",
      "Chenghua Lin",
      "Ge Zhang",
      "Wenhao Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.12885v3",
    "title": "VAR-MATH: Probing True Mathematical Reasoning in LLMS via Symbolic Multi-Instance Benchmarks",
    "summary": "Recent advances in reinforcement learning (RL) have led to substantial improvements in the mathematical reasoning abilities of LLMs, as measured by standard benchmarks. Yet these gains often persist even when models are trained with flawed signals, such as random or inverted rewards. This raises a fundamental question: do such improvements reflect genuine reasoning, or are they merely artifacts of overfitting to benchmark-specific patterns? To answer this question, we adopt an evaluation-centric perspective and highlight two critical shortcomings in existing protocols. First, benchmark contamination arises because test problems are publicly available, thereby increasing the risk of data leakage. Second, evaluation fragility results from reliance on single-instance assessments, which are sensitive to stochastic outputs and fail to capture reasoning consistency. These limitations suggest the need for a new evaluation paradigm that can probe reasoning ability beyond memorization and one-off success. As response, we propose VAR-MATH, a symbolic evaluation framework that converts fixed numerical problems into parameterized templates and requires models to solve multiple instantiations of each. This design enforces consistency across structurally equivalent variants, mitigates contamination, and enhances robustness through bootstrapped metrics. We apply VAR-MATH to transform three popular benchmarks, AMC23, AIME24, and AIME25, into their symbolic counterparts, VAR-AMC23, VAR-AIME24, and VAR-AIME25. Experimental results show substantial performance drops for RL-trained models on these variabilized benchmarks, especially for smaller models, with average declines of 47.9\\% on AMC23, 58.8\\% on AIME24, and 72.9\\% on AIME25. These findings indicate that some existing RL methods rely on superficial heuristics and fail to generalize beyond specific numerical forms.",
    "published": "2025-07-17T08:10:55Z",
    "updated": "2026-01-05T05:39:03Z",
    "link": "http://arxiv.org/pdf/2507.12885v3.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jian Yao",
      "Ran Cheng",
      "Kay Chen Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01807v1",
    "title": "Adaptive Hybrid Optimizer based Framework for Lumpy Skin Disease Identification",
    "summary": "Lumpy Skin Disease (LSD) is a contagious viral infection that significantly deteriorates livestock health, thereby posing a serious threat to the global economy and food security. Owing to its rapid spread characteristics, early and precise identification is crucial to prevent outbreaks and ensure timely intervention. In this paper, we propose a hybrid deep learning-based approach called LUMPNet for the early detection of LSD. LUMPNet utilizes image data to detect and classify skin nodules -- the primary indicator of LSD. To this end, LUMPNet uses YOLOv11, EfficientNet-based CNN classifier with compound scaling, and a novel adaptive hybrid optimizer. More precisely, LUMPNet detects and localizes LSD skin nodules and lesions on cattle images. It exploits EfficientNet to classify the localized cattle images into LSD-affected or healthy categories. To stabilize and accelerate the training of YOLOv11 and EfficientNet hybrid model, a novel adaptive hybrid optimizer is proposed and utilized. We evaluate LUMPNet at various stages of LSD using a publicly available dataset. Results indicate that the proposed scheme achieves 99% LSD detection training accuracy, and outperforms existing schemes. The model also achieves validation accuracy of 98%. Moreover, for further evaluation, we conduct a case study using an optimized EfficientNet-B0 model trained with the AdamW optimizer, and compare its performance with LUMPNet. The results show that LUMPNet achieves superior performance.",
    "published": "2026-01-05T05:35:45Z",
    "updated": "2026-01-05T05:35:45Z",
    "link": "http://arxiv.org/pdf/2601.01807v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      " Ubaidullah",
      "Muhammad Abid Hussain",
      "Mohsin Raza Jafri",
      "Rozi Khan",
      "Moid Sandhu",
      "Abd Ullah Khan",
      "Hyundong Shin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01803v1",
    "title": "Moments Matter:Stabilizing Policy Optimization using Return Distributions",
    "summary": "Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.",
    "published": "2026-01-05T05:27:11Z",
    "updated": "2026-01-05T05:27:11Z",
    "link": "http://arxiv.org/pdf/2601.01803v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Dennis Jabs",
      "Aditya Mohan",
      "Marius Lindauer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01802v1",
    "title": "PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism and Comprehensive AI Psychological Counselor",
    "summary": "To develop a reliable AI for psychological assessment, we introduce \\texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \\textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \\textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \\textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \\texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.",
    "published": "2026-01-05T05:26:57Z",
    "updated": "2026-01-05T05:26:57Z",
    "link": "http://arxiv.org/pdf/2601.01802v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Qianjun Pan",
      "Junyi Wang",
      "Jie Zhou",
      "Yutao Yang",
      "Junsong Li",
      "Kaiyin Xu",
      "Yougen Zhou",
      "Yihan Li",
      "Jingyuan Zhao",
      "Qin Chen",
      "Ningning Zhou",
      "Kai Chen",
      "Liang He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01800v1",
    "title": "Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving",
    "summary": "Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\\% across all cases compared to state-of-the-art baseline methods.",
    "published": "2026-01-05T05:20:16Z",
    "updated": "2026-01-05T05:20:16Z",
    "link": "http://arxiv.org/pdf/2601.01800v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Qi Wei",
      "Junchao Fan",
      "Zhao Yang",
      "Jianhua Wang",
      "Jingkai Mao",
      "Xiaolin Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.06065v3",
    "title": "ScRPO: From Errors to Insights",
    "summary": "We introduce Self-correction Relative Policy Optimization (ScRPO), a novel reinforcement learning framework designed to empower large language models with advanced mathematical reasoning capabilities through iterative self-reflection and error correction. The ScRPO framework operates in two distinct phases: (1) Trial-and-error learning stage, where the model is trained via GRPO, and incorrect responses are collected to form an \"error pool\"; and (2) Self-correction learning stage, which guides the model to introspectively analyze and rectify the reasoning flaws behind its previous errors. Extensive evaluations across challenging mathematical benchmarks, including AIME, AMC, Olympiad, MATH-500, and GSM8k, validate the efficacy of our approach. Using DeepSeek-R1-Distill-Qwen-1.5B and 7B as backbones, ScRPO achieves average accuracies of 64.8% and 77.8%, respectively. This represents a significant improvement of 6.0% and 3.2% over vanilla baselines, consistently outperforming strong post-training methods such as DAPO and GRPO. These findings establish ScRPO as a robust paradigm for enabling autonomous self-improvement in AI systems, particularly in tasks with limited external feedback.",
    "published": "2025-11-08T16:30:44Z",
    "updated": "2026-01-05T05:19:37Z",
    "link": "http://arxiv.org/pdf/2511.06065v3.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Lianrui Li",
      "Dakuan Lu",
      "Jiawei Shao",
      "Xuelong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01798v1",
    "title": "VerLM: Explaining Face Verification Using Natural Language",
    "summary": "Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.",
    "published": "2026-01-05T05:16:07Z",
    "updated": "2026-01-05T05:16:07Z",
    "link": "http://arxiv.org/pdf/2601.01798v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Syed Abdul Hannan",
      "Hazim Bukhari",
      "Thomas Cantalapiedra",
      "Eman Ansar",
      "Massa Baali",
      "Rita Singh",
      "Bhiksha Raj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01792v1",
    "title": "HyperCLOVA X 8B Omni",
    "summary": "In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.",
    "published": "2026-01-05T05:06:11Z",
    "updated": "2026-01-05T05:06:11Z",
    "link": "http://arxiv.org/pdf/2601.01792v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "authors": [
      " NAVER Cloud HyperCLOVA X Team"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.25070v2",
    "title": "Scaling Open-Ended Reasoning to Predict the Future",
    "summary": "High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.",
    "published": "2025-12-31T18:59:51Z",
    "updated": "2026-01-05T18:45:47Z",
    "link": "http://arxiv.org/pdf/2512.25070v2.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Nikhil Chandak",
      "Shashwat Goel",
      "Ameya Prabhu",
      "Moritz Hardt",
      "Jonas Geiping"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.14301v3",
    "title": "SteganoBackdoor: Stealthy and Data-Efficient Backdoor Attacks on Language Models",
    "summary": "Modern language models remain vulnerable to backdoor attacks via poisoned data, where training inputs containing a trigger are paired with a target output, causing the model to reproduce that behavior whenever the trigger appears at inference time. Recent work has emphasized stealthy attacks that stress-test data-curation defenses using stylized artifacts or token-level perturbations as triggers, but this focus leaves a more practically relevant threat model underexplored: backdoors tied to naturally occurring semantic concepts. We introduce SteganoBackdoor, an optimization-based framework that constructs SteganoPoisons, steganographic poisoned training examples in which a backdoor payload is distributed across a fluent sentence while exhibiting no representational overlap with the inference-time semantic trigger. Across diverse model architectures, SteganoBackdoor achieves high attack success under constrained poisoning budgets and remains effective under conservative data-level filtering, highlighting a blind spot in existing data-curation defenses.",
    "published": "2025-11-18T09:56:16Z",
    "updated": "2026-01-05T18:33:56Z",
    "link": "http://arxiv.org/pdf/2511.14301v3.pdf",
    "category": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Eric Xue",
      "Ruiyi Zhang",
      "Pengtao Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02337v1",
    "title": "Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling",
    "summary": "Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.",
    "published": "2026-01-05T18:32:45Z",
    "updated": "2026-01-05T18:32:45Z",
    "link": "http://arxiv.org/pdf/2601.02337v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Berk Atil",
      "Rebecca J. Passonneau",
      "Ninareh Mehrabi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.00388v2",
    "title": "Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach",
    "summary": "Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.",
    "published": "2026-01-01T16:51:41Z",
    "updated": "2026-01-05T18:27:19Z",
    "link": "http://arxiv.org/pdf/2601.00388v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Biao Wu",
      "Meng Fang",
      "Ling Chen",
      "Ke Xu",
      "Tao Cheng",
      "Jun Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02320v1",
    "title": "Estimating Text Temperature",
    "summary": "Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora.",
    "published": "2026-01-05T18:09:41Z",
    "updated": "2026-01-05T18:09:41Z",
    "link": "http://arxiv.org/pdf/2601.02320v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Nikolay Mikhaylovskiy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.09665v3",
    "title": "Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling",
    "summary": "Wildfires have become increasingly frequent, irregular, and severe in recent years. Understanding how affected populations perceive and respond during wildfire crises is critical for timely and empathetic disaster response. Social media platforms offer a crowd-sourced channel to capture evolving public discourse, providing hyperlocal information and insight into public sentiment. This study analyzes Reddit discourse during the 2025 Los Angeles wildfires, spanning from the onset of the disaster to full containment. We collect 385 posts and 114,879 comments related to the Palisades and Eaton fires. We adopt topic modeling methods to identify the latent topics, enhanced by large language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we develop a hierarchical framework to categorize latent topics, consisting of two main categories, Situational Awareness (SA) and Crisis Narratives (CN). The volume of SA category closely aligns with real-world fire progressions, peaking within the first 2-5 days as the fires reach the maximum extent. The most frequent co-occurring category set of public health and safety, loss and damage, and emergency resources expands on a wide range of health-related latent topics, including environmental health, occupational health, and one health. Grief signals and mental health risks consistently accounted for 60 percentage and 40 percentage of CN instances, respectively, with the highest total volume occurring at night. This study contributes the first annotated social media dataset on the 2025 LA fires, and introduces a scalable multi-layer framework that leverages topic modeling for crisis discourse analysis. By identifying persistent public health concerns, our results can inform more empathetic and adaptive strategies for disaster response, public health communication, and future research in comparable climate-related disaster events.",
    "published": "2025-05-14T16:31:08Z",
    "updated": "2026-01-05T18:01:24Z",
    "link": "http://arxiv.org/pdf/2505.09665v3.pdf",
    "category": [
      "cs.SI",
      "cs.CL"
    ],
    "authors": [
      "Sulong Zhou",
      "Qunying Huang",
      "Shaoheng Zhou",
      "Yun Hang",
      "Xinyue Ye",
      "Aodong Mei",
      "Kathryn Phung",
      "Yuning Ye",
      "Uma Govindswamy",
      "Zehan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02303v1",
    "title": "Classifying several dialectal Nawatl varieties",
    "summary": "Mexico is a country with a large number of indigenous languages, among which the most widely spoken is Nawatl, with more than two million people currently speaking it (mainly in North and Central America). Despite its rich cultural heritage, which dates back to the 15th century, Nawatl is a language with few computer resources. The problem is compounded when it comes to its dialectal varieties, with approximately 30 varieties recognised, not counting the different spellings in the written forms of the language. In this research work, we addressed the problem of classifying Nawatl varieties using Machine Learning and Neural Networks.",
    "published": "2026-01-05T17:38:55Z",
    "updated": "2026-01-05T17:38:55Z",
    "link": "http://arxiv.org/pdf/2601.02303v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Juan-José Guzmán-Landa",
      "Juan-Manuel Torres-Moreno",
      "Miguel Figueroa-Saavedra",
      "Carlos-Emiliano González-Gallardo",
      "Graham Ranger",
      "Martha Lorena-Avendaño-Garrido"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.06057v2",
    "title": "MIND Your Reasoning: A Meta-Cognitive Intuitive-Reflective Network for Dual-Reasoning in Multimodal Stance Detection",
    "summary": "Multimodal Stance Detection (MSD) is a crucial task for understanding public opinion on social media. Existing methods predominantly operate by learning to fuse modalities. They lack an explicit reasoning process to discern how inter-modal dynamics, such as irony or conflict, collectively shape the user's final stance, leading to frequent misjudgments. To address this, we advocate for a paradigm shift from *learning to fuse* to *learning to reason*. We introduce **MIND**, a **M**eta-cognitive **I**ntuitive-reflective **N**etwork for **D**ual-reasoning. Inspired by the dual-process theory of human cognition, MIND operationalizes a self-improving loop. It first generates a rapid, intuitive hypothesis by querying evolving Modality and Semantic Experience Pools. Subsequently, a meta-cognitive reflective stage uses Modality-CoT and Semantic-CoT to scrutinize this initial judgment, distill superior adaptive strategies, and evolve the experience pools themselves. These dual experience structures are continuously refined during training and recalled at inference to guide robust and context-aware stance decisions. Extensive experiments on the MMSD benchmark demonstrate that our MIND significantly outperforms most baseline models and exhibits strong generalization.",
    "published": "2025-11-08T15:56:24Z",
    "updated": "2026-01-05T17:33:44Z",
    "link": "http://arxiv.org/pdf/2511.06057v2.pdf",
    "category": [
      "cs.CL",
      "cs.MM"
    ],
    "authors": [
      "Bingbing Wang",
      "Zhengda Jin",
      "Bin Liang",
      "Wenjie Li",
      "Jing Li",
      "Ruifeng Xu",
      "Min Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02298v1",
    "title": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)",
    "summary": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.",
    "published": "2026-01-05T17:33:16Z",
    "updated": "2026-01-05T17:33:16Z",
    "link": "http://arxiv.org/pdf/2601.02298v1.pdf",
    "category": [
      "cs.CL",
      "eess.SP"
    ],
    "authors": [
      "Mahmoud Elgenedy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02236v1",
    "title": "CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models",
    "summary": "Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive \"long-jump\" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM",
    "published": "2026-01-05T16:09:22Z",
    "updated": "2026-01-05T16:09:22Z",
    "link": "http://arxiv.org/pdf/2601.02236v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yihao Liang",
      "Ze Wang",
      "Hao Chen",
      "Ximeng Sun",
      "Jialian Wu",
      "Xiaodong Yu",
      "Jiang Liu",
      "Emad Barsoum",
      "Zicheng Liu",
      "Niraj K. Jha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02224v1",
    "title": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality",
    "summary": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.",
    "published": "2026-01-05T15:52:20Z",
    "updated": "2026-01-05T15:52:20Z",
    "link": "http://arxiv.org/pdf/2601.02224v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Fabian Lukassen",
      "Jan Herrmann",
      "Christoph Weisser",
      "Benjamin Saefken",
      "Thomas Kneib"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25664v2",
    "title": "QFrBLiMP: a Quebec-French Benchmark of Linguistic Minimal Pairs",
    "summary": "In this paper, we introduce the Quebec-French Benchmark of Linguistic Minimal Pairs (QFrBLiMP), a corpus designed to evaluate LLMs' linguistic knowledge of prominent grammatical phenomena in Quebec-French. QFrBLiMP comprises 1,761 minimal pairs annotated with 20 LPs. Specifically, these minimal pairs have been created by manually modifying sentences extracted from an official online resource maintained by a Québec government institution. Each pair is annotated by 12 Quebec-French native speakers, who select the sentence they consider grammatical from the two. These annotations are used to compare the competency of LLMs with that of humans. We evaluate different LLMs on QFrBLiMP and MultiBLiMP-Fr by observing the rate of higher probabilities assigned to the sentences of each minimal pair for each category. We find that while grammatical competence scales with model size, a clear hierarchy of difficulty emerges. All benchmarked models consistently fail on phenomena requiring deep semantic understanding, revealing a critical limitation. Finally, our statistical analysis comparing QFrBLiMP and MultiBLiMP reveals a significant performance degradation for most models on Quebec-French; however, the most capable models remain within the statistical significance interval, demonstrating cross-dialectal robustness.",
    "published": "2025-09-30T02:00:26Z",
    "updated": "2026-01-05T15:50:05Z",
    "link": "http://arxiv.org/pdf/2509.25664v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "David Beauchemin",
      "Pier-Luc Veilleux",
      "Johanna-Pascale Roy",
      "Richard Khoury"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02209v1",
    "title": "ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging",
    "summary": "The Arabic language is characterized by a rich tapestry of regional dialects that differ substantially in phonetics and lexicon, reflecting the geographic and cultural diversity of its speakers. Despite the availability of many multi-dialect datasets, mapping speech to fine-grained dialect sources, such as cities, remains underexplored. We present ARCADE (Arabic Radio Corpus for Audio Dialect Evaluation), the first Arabic speech dataset designed explicitly with city-level dialect granularity. The corpus comprises Arabic radio speech collected from streaming services across the Arab world. Our data pipeline captures 30-second segments from verified radio streams, encompassing both Modern Standard Arabic (MSA) and diverse dialectal speech. To ensure reliability, each clip was annotated by one to three native Arabic reviewers who assigned rich metadata, including emotion, speech type, dialect category, and a validity flag for dialect identification tasks. The resulting corpus comprises 6,907 annotations and 3,790 unique audio segments spanning 58 cities across 19 countries. These fine-grained annotations enable robust multi-task learning, serving as a benchmark for city-level dialect tagging. We detail the data collection methodology, assess audio quality, and provide a comprehensive analysis of label distributions. The dataset is available on: https://huggingface.co/datasets/riotu-lab/ARCADE-full",
    "published": "2026-01-05T15:32:17Z",
    "updated": "2026-01-05T15:32:17Z",
    "link": "http://arxiv.org/pdf/2601.02209v1.pdf",
    "category": [
      "cs.CL",
      "cs.CY",
      "cs.SD"
    ],
    "authors": [
      "Omer Nacar",
      "Serry Sibaee",
      "Adel Ammar",
      "Yasser Alhabashi",
      "Nadia Samer Sibai",
      "Yara Farouk Ahmed",
      "Ahmed Saud Alqusaiyer",
      "Sulieman Mahmoud AlMahmoud",
      "Abdulrhman Mamdoh Mukhaniq",
      "Lubaba Raed",
      "Sulaiman Mohammed Alatwah",
      "Waad Nasser Alqahtani",
      "Yousif Abdulmajeed Alnasser",
      "Mohamed Aziz Khadraoui",
      "Wadii Boulila"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02186v1",
    "title": "Toward Global Large Language Models in Medicine",
    "summary": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.",
    "published": "2026-01-05T15:05:49Z",
    "updated": "2026-01-05T15:05:49Z",
    "link": "http://arxiv.org/pdf/2601.02186v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Rui Yang",
      "Huitao Li",
      "Weihao Xuan",
      "Heli Qi",
      "Xin Li",
      "Kunyu Yu",
      "Yingjian Chen",
      "Rongrong Wang",
      "Jacques Behmoaras",
      "Tianxi Cai",
      "Bibhas Chakraborty",
      "Qingyu Chen",
      "Lionel Tim-Ee Cheng",
      "Marie-Louise Damwanza",
      "Chido Dzinotyiwei",
      "Aosong Feng",
      "Chuan Hong",
      "Yusuke Iwasawa",
      "Yuhe Ke",
      "Linah Kitala",
      "Taehoon Ko",
      "Jisan Lee",
      "Irene Li",
      "Jonathan Chong Kai Liew",
      "Hongfang Liu",
      "Lian Leng Low",
      "Edison Marrese-Taylor",
      "Yutaka Matsuo",
      "Isheanesu Misi",
      "Yilin Ning",
      "Jasmine Chiat Ling Ong",
      "Marcus Eng Hock Ong",
      "Enrico Petretto",
      "Hossein Rouhizadeh",
      "Abiram Sandralegar",
      "Oren Schreier",
      "Iain Bee Huat Tan",
      "Patrick Tan",
      "Daniel Shu Wei Ting",
      "Junjue Wang",
      "Chunhua Weng",
      "Matthew Yu Heng Wong",
      "Fang Wu",
      "Yunze Xiao",
      "Xuhai Xu",
      "Qingcheng Zeng",
      "Zhuo Zheng",
      "Yifan Peng",
      "Douglas Teodoro",
      "Nan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02179v1",
    "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
    "summary": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.",
    "published": "2026-01-05T14:58:04Z",
    "updated": "2026-01-05T14:58:04Z",
    "link": "http://arxiv.org/pdf/2601.02179v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Caiqi Zhang",
      "Ruihan Yang",
      "Xiaochen Zhu",
      "Chengzu Li",
      "Tiancheng Hu",
      "Yijiang River Dong",
      "Deqing Yang",
      "Nigel Collier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02128v1",
    "title": "Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation",
    "summary": "Segmenting speech transcripts into thematic sections benefits both downstream processing and users who depend on written text for accessibility. We introduce a novel approach to hierarchical topic segmentation in transcripts, generating multi-level tables of contents that capture both topic and subtopic boundaries. We compare zero-shot prompting and LoRA fine-tuning on large language models, while also exploring the integration of high-level speech pause features. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show significant improvements over established topic segmentation baselines. Additionally, we adapt a common evaluation measure for multi-level segmentation, taking into account all hierarchical levels within one metric.",
    "published": "2026-01-05T14:00:48Z",
    "updated": "2026-01-05T14:00:48Z",
    "link": "http://arxiv.org/pdf/2601.02128v1.pdf",
    "category": [
      "cs.CL",
      "eess.AS"
    ],
    "authors": [
      "Steffen Freisinger",
      "Philipp Seeberger",
      "Thomas Ranzenberger",
      "Tobias Bocklet",
      "Korbinian Riedhammer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.27052v3",
    "title": "VISTA Score: Verification In Sequential Turn-based Assessment",
    "summary": "Hallucination--defined here as generating statements unsupported or contradicted by available evidence or conversational context--remains a major obstacle to deploying conversational AI systems in settings that demand factual reliability. Existing metrics either evaluate isolated responses or treat unverifiable content as errors, limiting their use for multi-turn dialogue. We introduce VISTA (Verification In Sequential Turn-based Assessment), a framework for evaluating conversational factuality through claim-level verification and sequential consistency tracking. VISTA decomposes each assistant turn into atomic factual claims, verifies them against trusted sources and dialogue history, and categorizes unverifiable statements (subjective, contradicted, lacking evidence, or abstaining). Across eight large language models and four dialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA substantially improves hallucination detection over FACTSCORE and LLM-as-Judge baselines. Human evaluation confirms that VISTA's decomposition improves annotator agreement and reveals inconsistencies in existing benchmarks. By modeling factuality as a dynamic property of conversation, VISTA offers a more transparent, human-aligned measure of truthfulness in dialogue systems.",
    "published": "2025-10-30T23:45:13Z",
    "updated": "2026-01-05T13:07:39Z",
    "link": "http://arxiv.org/pdf/2510.27052v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ashley Lewis",
      "Andrew Perrault",
      "Eric Fosler-Lussier",
      "Michael White"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.09972v3",
    "title": "SIP-BMM: Constructing the Capability--Efficiency Pareto Set for LLMs via Structural Importance Prior Bayesian Model Merging",
    "summary": "Constructing a Pareto set is pivotal for navigating the capability--efficiency trade-offs in Large Language Models (LLMs). However, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the curse of dimensionality, rendering the search space computationally intractable. To resolve this dichotomy, we propose Structural Importance Prior Bayesian Model Merging (SIP-BMM), a framework that automatically constructs the LLM Pareto set. SIP-BMM renders high-dimensional layer-wise search tractable by introducing an importance-aware Sparse Axis-Aligned Subspace Bayesian Optimization (SAASBO) strategy. By leveraging a structural importance prior derived from task-vector differences, our method guides SAASBO to automatically identify critical layers, thereby dramatically reducing the effective dimensionality without sacrificing the granularity of full-model control. The entire process is automated within an evolutionary loop driven by the Log-Noisy Expected Hypervolume Improvement ($q$NEHVI) acquisition function. Experiments demonstrate that SIP-BMM discovers a stronger and denser Pareto front than competitive baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/MiLab-HITSZ/2026-SIPBMM.",
    "published": "2025-12-10T15:32:56Z",
    "updated": "2026-01-05T12:45:09Z",
    "link": "http://arxiv.org/pdf/2512.09972v3.pdf",
    "category": [
      "cs.LG",
      "cs.CL",
      "cs.NE"
    ],
    "authors": [
      "Kesheng Chen",
      "Yamin Hu",
      "Zhenqian Zhu",
      "Wenjian Luo",
      "Yiya Diao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.00454v3",
    "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple LLM Judges",
    "summary": "Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the \"LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast, flexible, and fine-grained dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.",
    "published": "2025-08-01T09:26:01Z",
    "updated": "2026-01-05T11:45:54Z",
    "link": "http://arxiv.org/pdf/2508.00454v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yuqi Tang",
      "Kehua Feng",
      "Yunfeng Wang",
      "Zhiwen Chen",
      "Chengfei Lv",
      "Gang Yu",
      "Keyan Ding",
      "Huajun Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.20773v2",
    "title": "Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization",
    "summary": "Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.",
    "published": "2025-12-23T21:21:08Z",
    "updated": "2026-01-05T11:37:37Z",
    "link": "http://arxiv.org/pdf/2512.20773v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ziyi Zhu",
      "Olivier Tieleman",
      "Caitlin A. Stamatis",
      "Luka Smyth",
      "Thomas D. Hull",
      "Daniel R. Cahn",
      "Matteo Malgaroli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01972v1",
    "title": "Hidden State Poisoning Attacks against Mamba-based Language Models",
    "summary": "State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.",
    "published": "2026-01-05T10:27:19Z",
    "updated": "2026-01-05T10:27:19Z",
    "link": "http://arxiv.org/pdf/2601.01972v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Alexandre Le Mercier",
      "Chris Develder",
      "Thomas Demeester"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01964v1",
    "title": "CSF: Contrastive Semantic Features for Direct Multilingual Sign Language Generation",
    "summary": "Sign language translation systems typically require English as an intermediary language, creating barriers for non-English speakers in the global deaf community. We present Canonical Semantic Form (CSF), a language-agnostic semantic representation framework that enables direct translation from any source language to sign language without English mediation. CSF decomposes utterances into nine universal semantic slots: event, intent, time, condition, agent, object, location, purpose, and modifier. A key contribution is our comprehensive condition taxonomy comprising 35 condition types across eight semantic categories, enabling nuanced representation of conditional expressions common in everyday communication. We train a lightweight transformer-based extractor (0.74 MB) that achieves 99.03% average slot extraction accuracy across four typologically diverse languages: English, Vietnamese, Japanese, and French. The model demonstrates particularly strong performance on condition classification (99.4% accuracy) despite the 35-class complexity. With inference latency of 3.02ms on CPU, our approach enables real-time sign language generation in browser-based applications. We release our code, trained models, and multilingual dataset to support further research in accessible sign language technology.",
    "published": "2026-01-05T10:15:35Z",
    "updated": "2026-01-05T10:15:35Z",
    "link": "http://arxiv.org/pdf/2601.01964v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Tran Sy Bao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.23065v3",
    "title": "TabiBERT: A Large-Scale ModernBERT Foundation Model and A Unified Benchmark for Turkish",
    "summary": "Since the inception of BERT, encoder-only Transformers have evolved significantly in computational efficiency, training stability, and long-context modeling. ModernBERT consolidates these advances by integrating Rotary Positional Embeddings (RoPE), FlashAttention, and refined normalization. Despite these developments, Turkish NLP lacks a monolingual encoder trained from scratch, incorporating such modern architectural paradigms. This work introduces TabiBERT, a monolingual Turkish encoder based on ModernBERT architecture trained from scratch on a large, curated corpus. TabiBERT is pre-trained on one trillion tokens sampled from an 84.88B token multi-domain corpus: web text (73%), scientific publications (20%), source code (6%), and mathematical content (0.3%). It supports 8,192-token context length (16x original BERT), achieves up to 2.65x inference speedup, and reduces GPU memory consumption, enabling larger batch sizes. We introduce TabiBench with 28 datasets across eight task categories with standardized splits and protocols, evaluated using GLUE-style macro-averaging. TabiBERT attains 77.58 on TabiBench, outperforming BERTurk by 1.62 points and establishing state-of-the-art on five of eight categories, with particularly strong gains on question answering (+9.55 points), code retrieval (+2.41 points), and academic understanding (+0.66 points). Compared with task-specific prior best results, including specialized models like TurkishBERTweet, TabiBERT achieves +1.47 average improvement, indicating robust cross-domain generalization. We release model weights, training configurations, and evaluation code for transparent, reproducible Turkish encoder research.",
    "published": "2025-12-28T20:18:22Z",
    "updated": "2026-01-05T10:15:27Z",
    "link": "http://arxiv.org/pdf/2512.23065v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Melikşah Türker",
      "A. Ebrar Kızıloğlu",
      "Onur Güngör",
      "Susan Üsküdarlı"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.03627v4",
    "title": "Sorting the Babble in Babel: Assessing the Performance of Language Identification Algorithms on the OpenAlex Database",
    "summary": "This project aims to optimize the linguistic indexing of the OpenAlex database by comparing the performance of various Python-based language identification procedures on different metadata corpora extracted from a manually-annotated article sample \\footnote{OpenAlex used the results presented in this article to inform the language metadata overhaul carried out as part of its recent Walden system launch. The precision and recall performance of each algorithm, corpus, and language is first analyzed, followed by an assessment of processing speeds recorded for each algorithm and corpus type. These different performance measures are then simulated at the database level using probabilistic confusion matrices for each algorithm, corpus, and language, as well as a probabilistic modeling of relative article language frequencies for the whole OpenAlex database. Results show that procedure performance strongly depends on the importance given to each of the measures implemented: for contexts where precision is preferred, using the LangID algorithm on the greedy corpus gives the best results; however, for all cases where recall is considered at least slightly more important than precision or as soon as processing times are given any kind of consideration, the procedure that consists in the application of the FastText algorithm on the Titles corpus outperforms all other alternatives. Given the lack of truly multilingual large-scale bibliographic databases, it is hoped that these results help confirm and foster the unparalleled potential of the OpenAlex database for cross-linguistic and comprehensive measurement and evaluation.",
    "published": "2025-02-05T21:29:09Z",
    "updated": "2026-01-05T10:00:12Z",
    "link": "http://arxiv.org/pdf/2502.03627v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Maxime Holmberg Sainte-Marie",
      "Diego Kozlowski",
      "Lucía Céspedes",
      "Vincent Larivière"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.21170v3",
    "title": "Cosmos: Compressed and Smooth Latent Space for Text Diffusion Modeling",
    "summary": "Autoregressive language models dominate modern text generation, yet their sequential nature introduces fundamental limitations: decoding is slow, and maintaining global coherence remains challenging. Diffusion models offer a promising alternative by enabling parallel generation and flexible control; however, their application to text generation is hindered by the high dimensionality of token-level representations. We introduce Cosmos, a novel approach to text generation that operates entirely in a compressed, smooth latent space tailored specifically for diffusion. This space is learned using an autoencoder trained simultaneously for token-level reconstruction and alignment with frozen activations from a pretrained language encoder, providing robust semantic grounding and enabling effective perturbation-based augmentations. Empirically, we demonstrate that text representations can be compressed by $8\\times$ while maintaining generation quality comparable to token-level diffusion models. Furthermore, increasing the latent sequence length allows Cosmos to surpass both diffusion-based and autoregressive baselines. We evaluate Cosmos on four diverse generative tasks including story generation, question generation, summarization, and detoxification and compare it with various generative paradigms. Cosmos achieves comparable or superior generation quality while offering more than $2\\times$ faster inference. Code is released at \\href{https://github.com/MeshchaninovViacheslav/cosmos}{GitHub}",
    "published": "2025-06-26T12:05:13Z",
    "updated": "2026-01-05T09:34:15Z",
    "link": "http://arxiv.org/pdf/2506.21170v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Viacheslav Meshchaninov",
      "Egor Chimbulatov",
      "Alexander Shabalin",
      "Aleksandr Abramov",
      "Dmitry Vetrov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.20293v2",
    "title": "AprielGuard",
    "summary": "Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.",
    "published": "2025-12-23T12:01:32Z",
    "updated": "2026-01-05T09:05:32Z",
    "link": "http://arxiv.org/pdf/2512.20293v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jaykumar Kasundra",
      "Anjaneya Praharaj",
      "Sourabh Surana",
      "Lakshmi Sirisha Chodisetty",
      "Sourav Sharma",
      "Abhigya Verma",
      "Abhishek Bhardwaj",
      "Debasish Kanhar",
      "Aakash Bhagat",
      "Khalil Slimi",
      "Seganrasan Subramanian",
      "Sathwik Tejaswi Madhusudhan",
      "Ranga Prasad Chenna",
      "Srinivas Sunkara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01885v1",
    "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents",
    "summary": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.",
    "published": "2026-01-05T08:24:16Z",
    "updated": "2026-01-05T08:24:16Z",
    "link": "http://arxiv.org/pdf/2601.01885v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yi Yu",
      "Liuyi Yao",
      "Yuexiang Xie",
      "Qingquan Tan",
      "Jiaqi Feng",
      "Yaliang Li",
      "Libing Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01868v1",
    "title": "DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs",
    "summary": "Multimodal Large Language Models (MLLMs) show promise for medical applications, yet progress in dermatology lags due to limited training data, narrow task coverage, and lack of clinically-grounded supervision that mirrors expert diagnostic workflows. We present a comprehensive framework to address these gaps. First, we introduce DermoInstruct, a large-scale morphology-anchored instruction corpus comprising 211,243 images and 772,675 trajectories across five task formats, capturing the complete diagnostic pipeline from morphological observation and clinical reasoning to final diagnosis. Second, we establish DermoBench, a rigorous benchmark evaluating 11 tasks across four clinical axes: Morphology, Diagnosis, Reasoning, and Fairness, including a challenging subset of 3,600 expert-verified open-ended instances and human performance baselines. Third, we develop DermoGPT, a dermatology reasoning MLLM trained via supervised fine-tuning followed by our Morphologically-Anchored Visual-Inference-Consistent (MAVIC) reinforcement learning objective, which enforces consistency between visual observations and diagnostic conclusions. At inference, we deploy Confidence-Consistency Test-time adaptation (CCT) for robust predictions. Experiments show DermoGPT significantly outperforms 16 representative baselines across all axes, achieving state-of-the-art performance while substantially narrowing the human-AI gap. DermoInstruct, DermoBench and DermoGPT will be made publicly available at https://github.com/mendicant04/DermoGPT upon acceptance.",
    "published": "2026-01-05T07:55:36Z",
    "updated": "2026-01-05T07:55:36Z",
    "link": "http://arxiv.org/pdf/2601.01868v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jinghan Ru",
      "Siyuan Yan",
      "Yuguo Yin",
      "Yuexian Zou",
      "Zongyuan Ge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.06933v2",
    "title": "MATEX: A Multi-Agent Framework for Explaining Ethereum Transactions",
    "summary": "Understanding the economic intent of Ethereum transactions is critical for user safety, yet current tools expose only raw on-chain data, leading to widespread \"blind signing\" (approving transactions without understanding them). Through interviews with 16 Web3 users, we find that effective explanations should be structured, risk-aware, and grounded at the token-flow level.\n  Based on interviews, we propose TxSum, a new task and dataset of 100 complex Ethereum transactions annotated with natural-language summaries and step-wise semantic labels (intent, mechanism, etc.). We then introduce MATEX, a multi-agent system that emulates human experts' dual-process reasoning. MATEX achieves the highest faithfulness and intent clarity among strong baselines. It boosts user comprehension by 23.6% on complex transactions and doubles users' ability to find real attacks, significantly reducing blind signing.",
    "published": "2025-12-07T17:23:55Z",
    "updated": "2026-01-05T07:55:21Z",
    "link": "http://arxiv.org/pdf/2512.06933v2.pdf",
    "category": [
      "cs.CE",
      "cs.CL",
      "cs.HC"
    ],
    "authors": [
      "Zifan Peng",
      "Jingyi Zheng",
      "Yule Liu",
      "Huaiyu Jia",
      "Qiming Ye",
      "Jingyu Liu",
      "Xufeng Yang",
      "Mingchen Li",
      "Qingyuan Gong",
      "Xuechao Wang",
      "Xinlei He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.14335v3",
    "title": "Context-aware Decoding Reduces Hallucination in Query-focused Summarization",
    "summary": "Query-focused summarization (QFS) aims to provide a summary of a single document/multi documents that can satisfy the information needs of a given query. It is useful for various real-world applications, such as abstractive snippet generation or more recent retrieval augmented generation (RAG). A prototypical QFS pipeline consists of a retriever (sparse or dense retrieval) and a generator (usually a large language model). However, applying large language models (LLM) potentially leads to hallucinations, especially when the evidence contradicts the prior belief of LLMs. There has been growing interest in developing new decoding methods to improve generation quality and reduce hallucination. In this work, we conduct a large-scale reproducibility study on one recently proposed decoding method\\, -- \\,Context-aware Decoding (CAD). In addition to replicating CAD's experiments on news summarization datasets, we include experiments on QFS datasets, and conduct more rigorous analysis on computational complexity and hyperparameter sensitivity. Experiments with eight different language models show that performance-wise, CAD improves QFS quality by (1) reducing factuality errors/hallucinations while (2) mostly retaining the match of lexical patterns, measured by ROUGE scores, while also at a cost of increased inference-time FLOPs and reduced decoding speed. The \\href{https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs}{code implementation} based on Huggingface Library is made available",
    "published": "2023-12-21T23:42:13Z",
    "updated": "2026-01-05T07:54:39Z",
    "link": "http://arxiv.org/pdf/2312.14335v3.pdf",
    "category": [
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "Zhichao Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01862v1",
    "title": "Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment",
    "summary": "Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score.\n  The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.",
    "published": "2026-01-05T07:46:29Z",
    "updated": "2026-01-05T07:46:29Z",
    "link": "http://arxiv.org/pdf/2601.01862v1.pdf",
    "category": [
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "Nuo Chen",
      "Hanpei Fang",
      "Piaohong Wang",
      "Jiqun Liu",
      "Tetsuya Sakai",
      "Xiao-Ming Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.18168v2",
    "title": "Improving End-to-End Training of Retrieval-Augmented Generation Models via Joint Stochastic Approximation",
    "summary": "Retrieval-augmented generation (RAG) has become a widely recognized paradigm to combine parametric memory with non-parametric memories. An RAG model consists of two serial connecting components (retriever and generator). A major challenge in end-to-end optimization of the RAG model is that marginalization over relevant passages (modeled as discrete latent variables) from a knowledge base is required. Traditional top-K marginalization and variational RAG (VRAG) suffer from biased or high-variance gradient estimates. In this paper, we propose and develop joint stochastic approximation (JSA) based end-to-end training of RAG, which is referred to as JSA-RAG. The JSA algorithm is a stochastic extension of the EM (expectation-maximization) algorithm and is particularly powerful in estimating discrete latent variable models. Extensive experiments are conducted on five datasets for two tasks (open-domain question answering, knowledge-grounded dialogs) and show that JSA-RAG significantly outperforms both vanilla RAG and VRAG. Further analysis shows the efficacy of JSA-RAG from the perspectives of generation, retrieval, and low-variance gradient estimate.",
    "published": "2025-08-25T16:17:16Z",
    "updated": "2026-01-05T07:31:37Z",
    "link": "http://arxiv.org/pdf/2508.18168v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hongyu Cao",
      "Yuxuan Wu",
      "Yucheng Cai",
      "Xianyu Zhao",
      "Zhijian Ou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.18276v3",
    "title": "RankMamba: Benchmarking Mamba's Document Ranking Performance in the Era of Transformers",
    "summary": "Transformer structure has achieved great success in multiple applied machine learning communities, such as natural language processing (NLP), computer vision (CV) and information retrieval (IR). Transformer architecture's core mechanism\\, -- \\,attention requires $O(n^2)$ time complexity in training and $O(n)$ time complexity in inference. Many works have been proposed to improve the attention mechanism's scalability, such as Flash Attention and Multi-query Attention. A different line of work aims to design new mechanisms to replace attention. Recently, a notable model structure Mamba, which is based on state space models, has achieved transformer-equivalent performance in multiple sequence modeling tasks. In this work, we examine Mamba's efficacy through the lens of a classical IR task\\, -- \\,document ranking. A reranker model takes a query and a document as input, and predicts a scalar relevance score. This task demands the language model's ability to comprehend lengthy contextual inputs and to capture the interaction between query and document tokens. We find that \\textbf{(1) Mamba models achieve competitive performance compared to transformer-based models with the same training recipe; (2) but also have a lower training throughput in comparison to efficient transformer implementations such as flash attention.} We hope this study can serve as a starting point to explore \\mamba models in other classical IR tasks. Our \\href{https://github.com/zhichaoxu-shufe/RankMamba}{code implementation} is made public to facilitate reproducibility. Refer to~\\cite{xu-etal-2025-state} for more comprehensive experiments and results, including passage ranking.",
    "published": "2024-03-27T06:07:05Z",
    "updated": "2026-01-05T07:14:50Z",
    "link": "http://arxiv.org/pdf/2403.18276v3.pdf",
    "category": [
      "cs.IR",
      "cs.CL"
    ],
    "authors": [
      "Zhichao Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01842v1",
    "title": "Towards Automated Lexicography: Generating and Evaluating Definitions for Learner's Dictionaries",
    "summary": "We study dictionary definition generation (DDG), i.e., the generation of non-contextualized definitions for given headwords. Dictionary definitions are an essential resource for learning word senses, but manually creating them is costly, which motivates us to automate the process. Specifically, we address learner's dictionary definition generation (LDDG), where definitions should consist of simple words. First, we introduce a reliable evaluation approach for DDG, based on our new evaluation criteria and powered by an LLM-as-a-judge. To provide reference definitions for the evaluation, we also construct a Japanese dataset in collaboration with a professional lexicographer. Validation results demonstrate that our evaluation approach agrees reasonably well with human annotators. Second, we propose an LDDG approach via iterative simplification with an LLM. Experimental results indicate that definitions generated by our approach achieve high scores on our criteria while maintaining lexical simplicity.",
    "published": "2026-01-05T07:11:24Z",
    "updated": "2026-01-05T07:11:24Z",
    "link": "http://arxiv.org/pdf/2601.01842v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yusuke Ide",
      "Adam Nohejl",
      "Joshua Tanner",
      "Hitomi Yanaka",
      "Christopher Lindsay",
      "Taro Watanabe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01827v1",
    "title": "Aspect Extraction from E-Commerce Product and Service Reviews",
    "summary": "Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.",
    "published": "2026-01-05T06:45:51Z",
    "updated": "2026-01-05T06:45:51Z",
    "link": "http://arxiv.org/pdf/2601.01827v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Valiant Lance D. Dionela",
      "Fatima Kriselle S. Dy",
      "Robin James M. Hombrebueno",
      "Aaron Rae M. Nicolas",
      "Charibeth K. Cheng",
      "Raphael W. Gonda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01825v1",
    "title": "CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning",
    "summary": "Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.",
    "published": "2026-01-05T06:44:29Z",
    "updated": "2026-01-05T06:44:29Z",
    "link": "http://arxiv.org/pdf/2601.01825v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yaxin Cui",
      "Yuanqiang Zeng",
      "Jiapeng Yan",
      "Keling Lin",
      "Kai Ji",
      "Jianhui Zeng",
      "Sheng Zhang",
      "Xin Luo",
      "Binzhu Su",
      "Chaolai Shen",
      "Jiahao Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.12869v2",
    "title": "ERA-IT: Aligning Semantic Models with Revealed Economic Preference for Real-Time and Explainable Patent Valuation",
    "summary": "Valuing intangible assets under uncertainty remains a critical challenge in the strategic management of technological innovation due to the information asymmetry inherent in high-dimensional technical specifications. Traditional bibliometric indicators, such as citation counts, fail to address this friction in a timely manner due to the systemic latency inherent in data accumulation. To bridge this gap, this study proposes the Economic Reasoning Alignment via Instruction Tuning (ERA-IT) framework. We theoretically conceptualize patent renewal history as a revealed economic preference and leverage it as an objective supervisory signal to align the generative reasoning of Large Language Models (LLMs) with market realities, a process we term Eco-Semantic Alignment. Using a randomly sampled dataset of 10,000 European Patent Office patents across diverse technological domains, we trained the model not only to predict value tiers but also to reverse-engineer the Economic Chain-of-Thought from unstructured text. Empirical results demonstrate that ERA-IT significantly outperforms both conventional econometric models and zero-shot LLMs in predictive accuracy. More importantly, by generating explicit, logically grounded rationales for valuation, the framework serves as a transparent cognitive scaffold for decision-makers, reducing the opacity of black-box AI in high-stakes intellectual property management.",
    "published": "2025-12-14T23:04:07Z",
    "updated": "2026-01-05T05:04:17Z",
    "link": "http://arxiv.org/pdf/2512.12869v2.pdf",
    "category": [
      "cs.CE",
      "cs.CL"
    ],
    "authors": [
      "Yongmin Yoo",
      "Seungwoo Kim",
      "Jingjiang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02359v1",
    "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
    "summary": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.",
    "published": "2026-01-05T18:59:54Z",
    "updated": "2026-01-05T18:59:54Z",
    "link": "http://arxiv.org/pdf/2601.02359v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kaede Shiohara",
      "Toshihiko Yamasaki",
      "Vladislav Golyanik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02358v1",
    "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
    "summary": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.",
    "published": "2026-01-05T18:56:34Z",
    "updated": "2026-01-05T18:56:34Z",
    "link": "http://arxiv.org/pdf/2601.02358v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junyi Chen",
      "Tong He",
      "Zhoujie Fu",
      "Pengfei Wan",
      "Kun Gai",
      "Weicai Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02356v1",
    "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
    "summary": "We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.",
    "published": "2026-01-05T18:55:32Z",
    "updated": "2026-01-05T18:55:32Z",
    "link": "http://arxiv.org/pdf/2601.02356v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jing Tan",
      "Zhaoyang Zhang",
      "Yantao Shen",
      "Jiarui Cai",
      "Shuo Yang",
      "Jiajun Wu",
      "Wei Xia",
      "Zhuowen Tu",
      "Stefano Soatto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02353v1",
    "title": "Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices",
    "summary": "Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\\% while maintaining 92.3\\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.",
    "published": "2026-01-05T18:55:05Z",
    "updated": "2026-01-05T18:55:05Z",
    "link": "http://arxiv.org/pdf/2601.02353v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Shahnawaz Alam",
      "Mohammed Mudassir Uddin",
      "Mohammed Kaif Pasha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10196v2",
    "title": "Explainable AI Technique in Lung Cancer Detection Using Convolutional Neural Networks",
    "summary": "Early detection of lung cancer is critical to improving survival outcomes. We present a deep learning framework for automated lung cancer screening from chest computed tomography (CT) images with integrated explainability. Using the IQ-OTH/NCCD dataset (1,197 scans across Normal, Benign, and Malignant classes), we evaluate a custom convolutional neural network (CNN) and three fine-tuned transfer learning backbones: DenseNet121, ResNet152, and VGG19. Models are trained with cost-sensitive learning to mitigate class imbalance and evaluated via accuracy, precision, recall, F1-score, and ROC-AUC. While ResNet152 achieved the highest accuracy (97.3%), DenseNet121 provided the best overall balance in precision, recall, and F1 (up to 92%, 90%, 91%, respectively). We further apply Shapley Additive Explanations (SHAP) to visualize evidence contributing to predictions, improving clinical transparency. Results indicate that CNN-based approaches augmented with explainability can provide fast, accurate, and interpretable support for lung cancer screening, particularly in resource-limited settings.",
    "published": "2025-08-13T21:02:38Z",
    "updated": "2026-01-05T18:51:53Z",
    "link": "http://arxiv.org/pdf/2508.10196v2.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Nishan Rai",
      "Sujan Khatri",
      "Devendra Risal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02339v1",
    "title": "Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding",
    "summary": "Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.",
    "published": "2026-01-05T18:33:50Z",
    "updated": "2026-01-05T18:33:50Z",
    "link": "http://arxiv.org/pdf/2601.02339v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jingming He",
      "Chongyi Li",
      "Shiqi Wang",
      "Sam Kwong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02329v1",
    "title": "BEDS: Bayesian Emergent Dissipative Structures",
    "summary": "We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.\n  We derive fundamental mathematical constants (e, π, φ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking Gödel's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.\n  As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.",
    "published": "2026-01-05T18:21:02Z",
    "updated": "2026-01-05T18:21:02Z",
    "link": "http://arxiv.org/pdf/2601.02329v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Laurent Caraffa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.03862v2",
    "title": "Diminishing Returns in Self-Supervised Learning",
    "summary": "Transformer-based architectures have become a dominant paradigm in vision and language, but their success is often attributed to large model capacity and massive training data. In this work, we examine how self-supervised pre-training, intermediate fine-tuning, and downstream fine-tuning interact in a low-capacity regime, using a 5M-parameter Vision Transformer for semantic segmentation. Across multiple data scales, we find that masked image modeling pre-training and downstream fine-tuning reliably improve performance, but with clear diminishing returns as supervision increases. In contrast, inserting an intermediate classification fine-tuning stage consistently degrades downstream performance, with the largest drops occurring precisely where pre-training is most effective. Through an analysis of patch-level representation geometry, we show that classification-based intermediate supervision actively interferes with representations learned during pre-training by collapsing spatial structure critical for dense prediction. These results indicate that, in small models, the geometry of supervision matters more than the number of training stages: misaligned intermediate objectives can negate the benefits of pre-training rather than amplify them.",
    "published": "2025-12-03T15:11:44Z",
    "updated": "2026-01-05T18:17:53Z",
    "link": "http://arxiv.org/pdf/2512.03862v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Oli Bridge",
      "Huey Sun",
      "Botond Branyicskai-Nagy",
      "Charles D'Ornano",
      "Shomit Basu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02318v1",
    "title": "Fusion2Print: Deep Flash-Non-Flash Fusion for Contactless Fingerprint Matching",
    "summary": "Contactless fingerprint recognition offers a hygienic and convenient alternative to contact-based systems, enabling rapid acquisition without latent prints, pressure artifacts, or hygiene risks. However, contactless images often show degraded ridge clarity due to illumination variation, subcutaneous skin discoloration, and specular reflections. Flash captures preserve ridge detail but introduce noise, whereas non-flash captures reduce noise but lower ridge contrast. We propose Fusion2Print (F2P), the first framework to systematically capture and fuse paired flash-non-flash contactless fingerprints. We construct a custom paired dataset, FNF Database, and perform manual flash-non-flash subtraction to isolate ridge-preserving signals. A lightweight attention-based fusion network also integrates both modalities, emphasizing informative channels and suppressing noise, and then a U-Net enhancement module produces an optimally weighted grayscale image. Finally, a deep embedding model with cross-domain compatibility, generates discriminative and robust representations in a unified embedding space compatible with both contactless and contact-based fingerprints for verification. F2P enhances ridge clarity and achieves superior recognition performance (AUC=0.999, EER=1.12%) over single-capture baselines (Verifinger, DeepPrint).",
    "published": "2026-01-05T18:09:27Z",
    "updated": "2026-01-05T18:09:27Z",
    "link": "http://arxiv.org/pdf/2601.02318v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Roja Sahoo",
      "Anoop Namboodiri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02315v1",
    "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
    "summary": "Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \\href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}",
    "published": "2026-01-05T18:07:21Z",
    "updated": "2026-01-05T18:07:21Z",
    "link": "http://arxiv.org/pdf/2601.02315v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Saurabh Kaushik",
      "Lalit Maurya",
      "Beth Tellman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02309v1",
    "title": "360DVO: Deep Visual Odometry for Monocular 360-Degree Camera",
    "summary": "Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage",
    "published": "2026-01-05T17:52:50Z",
    "updated": "2026-01-05T17:52:50Z",
    "link": "http://arxiv.org/pdf/2601.02309v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiaopeng Guo",
      "Yinzhe Xu",
      "Huajian Huang",
      "Sai-Kit Yeung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02299v1",
    "title": "SortWaste: A Densely Annotated Dataset for Object Detection in Industrial Waste Sorting",
    "summary": "The increasing production of waste, driven by population growth, has created challenges in managing and recycling materials effectively. Manual waste sorting is a common practice; however, it remains inefficient for handling large-scale waste streams and presents health risks for workers. On the other hand, existing automated sorting approaches still struggle with the high variability, clutter, and visual complexity of real-world waste streams. The lack of real-world datasets for waste sorting is a major reason automated systems for this problem are underdeveloped. Accordingly, we introduce SortWaste, a densely annotated object detection dataset collected from a Material Recovery Facility. Additionally, we contribute to standardizing waste detection in sorting lines by proposing ClutterScore, an objective metric that gauges the scene's hardness level using a set of proxies that affect visual complexity (e.g., object count, class and size entropy, and spatial overlap). In addition to these contributions, we provide an extensive benchmark of state-of-the-art object detection models, detailing their results with respect to the hardness level assessed by the proposed metric. Despite achieving promising results (mAP of 59.7% in the plastic-only detection task), performance significantly decreases in highly cluttered scenes. This highlights the need for novel and more challenging datasets on the topic.",
    "published": "2026-01-05T17:34:50Z",
    "updated": "2026-01-05T17:34:50Z",
    "link": "http://arxiv.org/pdf/2601.02299v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sara Inácio",
      "Hugo Proença",
      "João C. Neves"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02289v1",
    "title": "Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery",
    "summary": "Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.",
    "published": "2026-01-05T17:24:50Z",
    "updated": "2026-01-05T17:24:50Z",
    "link": "http://arxiv.org/pdf/2601.02289v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tom Burgert",
      "Leonard Hackel",
      "Paolo Rota",
      "Begüm Demir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02281v1",
    "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
    "summary": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
    "published": "2026-01-05T17:11:00Z",
    "updated": "2026-01-05T17:11:00Z",
    "link": "http://arxiv.org/pdf/2601.02281v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shuai Yuan",
      "Yantai Yang",
      "Xiaotian Yang",
      "Xupeng Zhang",
      "Zhonghao Zhao",
      "Lingming Zhang",
      "Zhipeng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02267v1",
    "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
    "summary": "Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html",
    "published": "2026-01-05T16:51:45Z",
    "updated": "2026-01-05T16:51:45Z",
    "link": "http://arxiv.org/pdf/2601.02267v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Renke Wang",
      "Zhenyu Zhang",
      "Ying Tai",
      "Jian Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02256v1",
    "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
    "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.",
    "published": "2026-01-05T16:36:40Z",
    "updated": "2026-01-05T16:36:40Z",
    "link": "http://arxiv.org/pdf/2601.02256v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Shikun Sun",
      "Liao Qu",
      "Huichao Zhang",
      "Yiheng Liu",
      "Yangyang Song",
      "Xian Li",
      "Xu Wang",
      "Yi Jiang",
      "Daniel K. Du",
      "Xinglong Wu",
      "Jia Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02253v1",
    "title": "Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission",
    "summary": "The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters.",
    "published": "2026-01-05T16:33:13Z",
    "updated": "2026-01-05T16:33:13Z",
    "link": "http://arxiv.org/pdf/2601.02253v1.pdf",
    "category": [
      "cs.LG",
      "cs.AR",
      "cs.CV"
    ],
    "authors": [
      "Emrah Mete",
      "Emin Erkan Korkmaz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02249v1",
    "title": "SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection",
    "summary": "Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.",
    "published": "2026-01-05T16:31:41Z",
    "updated": "2026-01-05T16:31:41Z",
    "link": "http://arxiv.org/pdf/2601.02249v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiantai Xiang",
      "Guangyao Zhou",
      "Zixiao Wen",
      "Wenshuai Li",
      "Ben Niu",
      "Feng Wang",
      "Lijia Huang",
      "Qiantong Wang",
      "Yuhan Liu",
      "Zongxu Pan",
      "Yuxin Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.02570v3",
    "title": "TI-PREGO: Chain of Thought and In-Context Learning for Online Mistake Detection in PRocedural EGOcentric Videos",
    "summary": "Identifying procedural errors online from egocentric videos is a critical yet challenging task across various domains, including manufacturing, healthcare, and skill-based training. The nature of such mistakes is inherently open-set, as unforeseen or novel errors may occur, necessitating robust detection systems that do not rely on prior examples of failure. Currently, however, no technique effectively detects open-set procedural mistakes online.\n  We propose a dual branch architecture to address this problem in an online fashion: one branch continuously performs step recognition from the input egocentric video, while the other anticipates future steps based on the recognition module's output. Mistakes are detected as mismatches between the currently recognized action and the action predicted by the anticipation module. The recognition branch takes input frames, predicts the current action, and aggregates frame-level results into action tokens. The anticipation branch, specifically, leverages the solid pattern-matching capabilities of Large Language Models (LLMs) to predict action tokens based on previously predicted ones.\n  Given the online nature of the task, we also thoroughly benchmark the difficulties associated with per-frame evaluations, particularly the need for accurate and timely predictions in dynamic online scenarios.\n  Extensive experiments on two procedural datasets demonstrate the challenges and opportunities of leveraging a dual-branch architecture for mistake detection, showcasing the effectiveness of our proposed approach. In a thorough evaluation including recognition and anticipation variants and state-of-the-art models, our method reveals its robustness and effectiveness in online applications.",
    "published": "2024-11-04T20:03:06Z",
    "updated": "2026-01-05T16:20:54Z",
    "link": "http://arxiv.org/pdf/2411.02570v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Leonardo Plini",
      "Luca Scofano",
      "Edoardo De Matteis",
      "Guido Maria D'Amely di Melendugno",
      "Alessandro Flaborea",
      "Andrea Sanchietti",
      "Giovanni Maria Farinella",
      "Fabio Galasso",
      "Antonino Furnari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02228v1",
    "title": "FMVP: Masked Flow Matching for Adversarial Video Purification",
    "summary": "Video recognition models remain vulnerable to adversarial attacks, while existing diffusion-based purification methods suffer from inefficient sampling and curved trajectories. Directly regressing clean videos from adversarial inputs often fails to recover faithful content due to the subtle nature of perturbations; this necessitates physically shattering the adversarial structure. Therefore, we propose Flow Matching for Adversarial Video Purification FMVP. FMVP physically shatters global adversarial structures via a masking strategy and reconstructs clean video dynamics using Conditional Flow Matching (CFM) with an inpainting objective. To further decouple semantic content from adversarial noise, we design a Frequency-Gated Loss (FGL) that explicitly suppresses high-frequency adversarial residuals while preserving low-frequency fidelity. We design Attack-Aware and Generalist training paradigms to handle known and unknown threats, respectively. Extensive experiments on UCF-101 and HMDB-51 demonstrate that FMVP outperforms state-of-the-art methods (DiffPure, Defense Patterns (DP), Temporal Shuffling (TS) and FlowPure), achieving robust accuracy exceeding 87% against PGD and 89% against CW attacks. Furthermore, FMVP demonstrates superior robustness against adaptive attacks (DiffHammer) and functions as a zero-shot adversarial detector, attaining detection accuracies of 98% for PGD and 79% for highly imperceptible CW attacks.",
    "published": "2026-01-05T15:55:46Z",
    "updated": "2026-01-05T15:55:46Z",
    "link": "http://arxiv.org/pdf/2601.02228v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Duoxun Tang",
      "Xueyi Zhang",
      "Chak Hin Wang",
      "Xi Xiao",
      "Dasen Dai",
      "Xinhang Jiang",
      "Wentao Shi",
      "Rui Li",
      "Qing Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.20610v2",
    "title": "PrevMatch: Revisiting and Maximizing Temporal Knowledge in Semi-Supervised Semantic Segmentation",
    "summary": "In semi-supervised semantic segmentation, the Mean Teacher- and co-training-based approaches are employed to mitigate confirmation bias and coupling problems. However, despite their high performance, these approaches frequently involve complex training pipelines and a substantial computational burden, limiting the scalability and compatibility of these methods. In this paper, we propose a PrevMatch framework that effectively mitigates the aforementioned limitations by maximizing the utilization of the temporal knowledge obtained during the training process. The PrevMatch framework relies on two core strategies: (1) we reconsider the use of temporal knowledge and thus directly utilize previous models obtained during training to generate additional pseudo-label guidance, referred to as previous guidance. (2) we design a highly randomized ensemble strategy to maximize the effectiveness of the previous guidance. PrevMatch, a simple yet effective plug-in method, can be seamlessly integrated into existing semi-supervised learning frameworks with minimal computational overhead. Experimental results on three benchmark semantic segmentation datasets show that incorporating PrevMatch into existing methods significantly improves their performance. Furthermore, our analysis indicates that PrevMatch facilitates stable optimization during training, resulting in improved generalization performance.",
    "published": "2024-05-31T03:54:59Z",
    "updated": "2026-01-05T15:45:20Z",
    "link": "http://arxiv.org/pdf/2405.20610v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wooseok Shin",
      "Hyun Joon Park",
      "Jin Sob Kim",
      "Juan Yun",
      "Se Hong Park",
      "Sung Won Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02212v1",
    "title": "Prior-Guided DETR for Ultrasound Nodule Detection",
    "summary": "Accurate detection of ultrasound nodules is essential for the early diagnosis and treatment of thyroid and breast cancers. However, this task remains challenging due to irregular nodule shapes, indistinct boundaries, substantial scale variations, and the presence of speckle noise that degrades structural visibility. To address these challenges, we propose a prior-guided DETR framework specifically designed for ultrasound nodule detection. Instead of relying on purely data-driven feature learning, the proposed framework progressively incorporates different prior knowledge at multiple stages of the network. First, a Spatially-adaptive Deformable FFN with Prior Regularization (SDFPR) is embedded into the CNN backbone to inject geometric priors into deformable sampling, stabilizing feature extraction for irregular and blurred nodules. Second, a Multi-scale Spatial-Frequency Feature Mixer (MSFFM) is designed to extract multi-scale structural priors, where spatial-domain processing emphasizes contour continuity and boundary cues, while frequency-domain modeling captures global morphology and suppresses speckle noise. Furthermore, a Dense Feature Interaction (DFI) mechanism propagates and exploits these prior-modulated features across all encoder layers, enabling the decoder to enhance query refinement under consistent geometric and structural guidance. Experiments conducted on two clinically collected thyroid ultrasound datasets (Thyroid I and Thyroid II) and two public benchmarks (TN3K and BUSI) for thyroid and breast nodules demonstrate that the proposed method achieves superior accuracy compared with 18 detection methods, particularly in detecting morphologically complex nodules.The source code is publicly available at https://github.com/wjj1wjj/Ultrasound-DETR.",
    "published": "2026-01-05T15:32:58Z",
    "updated": "2026-01-05T15:32:58Z",
    "link": "http://arxiv.org/pdf/2601.02212v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jingjing Wang",
      "Zhuo Xiao",
      "Xinning Yao",
      "Bo Liu",
      "Lijuan Niu",
      "Xiangzhi Bai",
      "Fugen Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02211v1",
    "title": "Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion",
    "summary": "Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.",
    "published": "2026-01-05T15:32:53Z",
    "updated": "2026-01-05T15:32:53Z",
    "link": "http://arxiv.org/pdf/2601.02211v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Binglei Li",
      "Mengping Yang",
      "Zhiyu Tan",
      "Junping Zhang",
      "Hao Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02203v1",
    "title": "Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules",
    "summary": "Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\\% of a full fine-tune (98.84\\% vs. 99.67\\%) while training 97.2\\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.",
    "published": "2026-01-05T15:27:04Z",
    "updated": "2026-01-05T15:27:04Z",
    "link": "http://arxiv.org/pdf/2601.02203v1.pdf",
    "category": [
      "cs.CV",
      "cs.CR"
    ],
    "authors": [
      "Oliver Custance",
      "Saad Khan",
      "Simon Parkinson",
      "Quan Z. Sheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02201v1",
    "title": "CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents",
    "summary": "The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.",
    "published": "2026-01-05T15:24:05Z",
    "updated": "2026-01-05T15:24:05Z",
    "link": "http://arxiv.org/pdf/2601.02201v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Keyu Wang",
      "Bingchen Miao",
      "Wendong Bu",
      "Yu Wu",
      "Juncheng Li",
      "Shengyu Zhang",
      "Wenqiao Zhang",
      "Siliang Tang",
      "Jun Xiao",
      "Yueting Zhuang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02198v1",
    "title": "Mind the Gap: Continuous Magnification Sampling for Pathology Foundation Models",
    "summary": "In histopathology, pathologists examine both tissue architecture at low magnification and fine-grained morphology at high magnification. Yet, the performance of pathology foundation models across magnifications and the effect of magnification sampling during training remain poorly understood. We model magnification sampling as a multi-source domain adaptation problem and develop a simple theoretical framework that reveals systematic trade-offs between sampling strategies. We show that the widely used discrete uniform sampling of magnifications (0.25, 0.5, 1.0, 2.0 mpp) leads to degradation at intermediate magnifications. We introduce continuous magnification sampling, which removes gaps in magnification coverage while preserving performance at standard scales. Further, we derive sampling distributions that optimize representation quality across magnification scales. To evaluate these strategies, we introduce two new benchmarks (TCGA-MS, BRACS-MS) with appropriate metrics. Our experiments show that continuous sampling substantially improves over discrete sampling at intermediate magnifications, with gains of up to 4 percentage points in balanced classification accuracy, and that optimized distributions can further improve performance. Finally, we evaluate current histopathology foundation models, finding that magnification is a primary driver of performance variation across models. Our work paves the way towards future pathology foundation models that perform reliably across magnifications.",
    "published": "2026-01-05T15:19:59Z",
    "updated": "2026-01-05T15:19:59Z",
    "link": "http://arxiv.org/pdf/2601.02198v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Alexander Möllers",
      "Julius Hense",
      "Florian Schulz",
      "Timo Milbich",
      "Maximilian Alber",
      "Lukas Ruff"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02189v1",
    "title": "QuIC: A Quantum-Inspired Interaction Classifier for Revitalizing Shallow CNNs in Fine-Grained Recognition",
    "summary": "Deploying deep learning models for Fine-Grained Visual Classification (FGVC) on resource-constrained edge devices remains a significant challenge. While deep architectures achieve high accuracy on benchmarks like CUB-200-2011, their computational cost is often prohibitive. Conversely, shallow networks (e.g., AlexNet, VGG) offer efficiency but fail to distinguish visually similar sub-categories. This is because standard Global Average Pooling (GAP) heads capture only first-order statistics, missing the subtle high-order feature interactions required for FGVC. While Bilinear CNNs address this, they suffer from high feature dimensionality and instability during training. To bridge this gap, we propose the Quantum-inspired Interaction Classifier (QuIC). Drawing inspiration from quantum mechanics, QuIC models feature channels as interacting quantum states and captures second-order feature covariance via a learnable observable operator. Designed as a lightweight, plug-and-play module, QuIC supports stable, single-stage end-to-end training without exploding feature dimensions. Experimental results demonstrate that QuIC significantly revitalizes shallow backbones: it boosts the Top-1 accuracy of VGG16 by nearly 20% and outperforms state-of-the-art attention mechanisms (SE-Block) on ResNet18. Qualitative analysis, including t-SNE visualization, further confirms that QuIC resolves ambiguous cases by explicitly attending to fine-grained discriminative features and enforcing compact intra-class clustering.",
    "published": "2026-01-05T15:09:18Z",
    "updated": "2026-01-05T15:09:18Z",
    "link": "http://arxiv.org/pdf/2601.02189v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Cheng Ying Wu",
      "Yen Jui Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02177v1",
    "title": "Why Commodity WiFi Sensors Fail at Multi-Person Gait Identification: A Systematic Analysis Using ESP32",
    "summary": "WiFi Channel State Information (CSI) has shown promise for single-person gait identification, with numerous studies reporting high accuracy. However, multi-person identification remains largely unexplored, with the limited existing work relying on complex, expensive setups requiring modified firmware. A critical question remains unanswered: is poor multi-person performance an algorithmic limitation or a fundamental hardware constraint? We systematically evaluate six diverse signal separation methods (FastICA, SOBI, PCA, NMF, Wavelet, Tensor Decomposition) across seven scenarios with 1-10 people using commodity ESP32 WiFi sensors--a simple, low-cost, off-the-shelf solution. Through novel diagnostic metrics (intra-subject variability, inter-subject distinguishability, performance degradation rate), we reveal that all methods achieve similarly low accuracy (45-56\\%, $σ$=3.74\\%) with statistically insignificant differences (p $>$ 0.05). Even the best-performing method, NMF, achieves only 56\\% accuracy. Our analysis reveals high intra-subject variability, low inter-subject distinguishability, and severe performance degradation as person count increases, indicating that commodity ESP32 sensors cannot provide sufficient signal quality for reliable multi-person separation.",
    "published": "2026-01-05T14:55:38Z",
    "updated": "2026-01-05T14:55:38Z",
    "link": "http://arxiv.org/pdf/2601.02177v1.pdf",
    "category": [
      "cs.CV",
      "cs.CR"
    ],
    "authors": [
      "Oliver Custance",
      "Saad Khan",
      "Simon Parkinson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.01510v2",
    "title": "Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering",
    "summary": "While significant advancements have been made in video question answering (VideoQA), the potential benefits of enhancing model generalization through tailored difficulty scheduling have been largely overlooked in existing research. This paper seeks to bridge that gap by incorporating VideoQA into a curriculum learning (CL) framework that progressively trains models from simpler to more complex data. Recognizing that conventional self-paced CL methods rely on training loss for difficulty measurement, which might not accurately reflect the intricacies of video-question pairs, we introduce the concept of uncertainty-aware CL. Here, uncertainty serves as the guiding principle for dynamically adjusting the difficulty. Furthermore, we address the challenge posed by uncertainty by presenting a probabilistic modeling approach for VideoQA. Specifically, we conceptualize VideoQA as a stochastic computation graph, where the hidden representations are treated as stochastic variables. This yields two distinct types of uncertainty: one related to the inherent uncertainty in the data and another pertaining to the model's confidence. In practice, we seamlessly integrate the VideoQA model into our framework and conduct comprehensive experiments. The findings affirm that our approach not only achieves enhanced performance but also effectively quantifies uncertainty in the context of VideoQA.",
    "published": "2024-01-03T02:29:34Z",
    "updated": "2026-01-05T14:50:13Z",
    "link": "http://arxiv.org/pdf/2401.01510v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haopeng Li",
      "Mohammed Bennamoun",
      "Jun Liu",
      "Hossein Rahmani",
      "Qiuhong Ke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02141v1",
    "title": "Efficient Unrolled Networks for Large-Scale 3D Inverse Problems",
    "summary": "Deep learning-based methods have revolutionized the field of imaging inverse problems, yielding state-of-the-art performance across various imaging domains. The best performing networks incorporate the imaging operator within the network architecture, typically in the form of deep unrolling. However, in large-scale problems, such as 3D imaging, most existing methods fail to incorporate the operator in the architecture due to the prohibitive amount of memory required by global forward operators, which hinder typical patching strategies. In this work, we present a domain partitioning strategy and normal operator approximations that enable the training of end-to-end reconstruction models incorporating forward operators of arbitrarily large problems into their architecture. The proposed method achieves state-of-the-art performance on 3D X-ray cone-beam tomography and 3D multi-coil accelerated MRI, while requiring only a single GPU for both training and inference.",
    "published": "2026-01-05T14:12:43Z",
    "updated": "2026-01-05T14:12:43Z",
    "link": "http://arxiv.org/pdf/2601.02141v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Romain Vo",
      "Julián Tachella"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02139v1",
    "title": "Beyond Segmentation: An Oil Spill Change Detection Framework Using Synthetic SAR Imagery",
    "summary": "Marine oil spills are urgent environmental hazards that demand rapid and reliable detection to minimise ecological and economic damage. While Synthetic Aperture Radar (SAR) imagery has become a key tool for large-scale oil spill monitoring, most existing detection methods rely on deep learning-based segmentation applied to single SAR images. These static approaches struggle to distinguish true oil spills from visually similar oceanic features (e.g., biogenic slicks or low-wind zones), leading to high false positive rates and limited generalizability, especially under data-scarce conditions. To overcome these limitations, we introduce Oil Spill Change Detection (OSCD), a new bi-temporal task that focuses on identifying changes between pre- and post-spill SAR images. As real co-registered pre-spill imagery is not always available, we propose the Temporal-Aware Hybrid Inpainting (TAHI) framework, which generates synthetic pre-spill images from post-spill SAR data. TAHI integrates two key components: High-Fidelity Hybrid Inpainting for oil-free reconstruction, and Temporal Realism Enhancement for radiometric and sea-state consistency. Using TAHI, we construct the first OSCD dataset and benchmark several state-of-the-art change detection models. Results show that OSCD significantly reduces false positives and improves detection accuracy compared to conventional segmentation, demonstrating the value of temporally-aware methods for reliable, scalable oil spill monitoring in real-world scenarios.",
    "published": "2026-01-05T14:10:13Z",
    "updated": "2026-01-05T14:10:13Z",
    "link": "http://arxiv.org/pdf/2601.02139v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chenyang Lai",
      "Shuaiyu Chen",
      "Tianjin Huang",
      "Siyang Song",
      "Guangliang Cheng",
      "Chunbo Luo",
      "Zeyu Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.20776v2",
    "title": "RingMo-Agent: A Unified Remote Sensing Foundation Model for Multi-Platform and Multi-Modal Reasoning",
    "summary": "Remote sensing (RS) images from multiple modalities and platforms exhibit diverse details due to differences in sensor characteristics and imaging perspectives. Existing vision-language research in RS largely relies on relatively homogeneous data sources. Moreover, they still remain limited to conventional visual perception tasks such as classification or captioning. As a result, these methods fail to serve as a unified and standalone framework capable of effectively handling RS imagery from diverse sources in real-world applications. To address these issues, we propose RingMo-Agent, a model designed to handle multi-modal and multi-platform data that performs perception and reasoning tasks based on user textual instructions. Compared with existing models, RingMo-Agent 1) is supported by a large-scale vision-language dataset named RS-VL3M, comprising over 3 million image-text pairs, spanning optical, SAR, and infrared (IR) modalities collected from both satellite and UAV platforms, covering perception and challenging reasoning tasks; 2) learns modality adaptive representations by incorporating separated embedding layers to construct isolated features for heterogeneous modalities and reduce cross-modal interference; 3) unifies task modeling by introducing task-specific tokens and employing a token-based high-dimensional hidden state decoding mechanism designed for long-horizon spatial tasks. Extensive experiments on various RS vision-language tasks demonstrate that RingMo-Agent not only proves effective in both visual understanding and sophisticated analytical tasks, but also exhibits strong generalizability across different platforms and sensing modalities.",
    "published": "2025-07-28T12:39:33Z",
    "updated": "2026-01-05T14:04:30Z",
    "link": "http://arxiv.org/pdf/2507.20776v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Huiyang Hu",
      "Peijin Wang",
      "Yingchao Feng",
      "Kaiwen Wei",
      "Wenxin Yin",
      "Wenhui Diao",
      "Mengyu Wang",
      "Hanbo Bi",
      "Kaiyue Kang",
      "Tong Ling",
      "Kun Fu",
      "Xian Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02112v1",
    "title": "Car Drag Coefficient Prediction from 3D Point Clouds Using a Slice-Based Surrogate Model",
    "summary": "The automotive industry's pursuit of enhanced fuel economy and performance necessitates efficient aerodynamic design. However, traditional evaluation methods such as computational fluid dynamics (CFD) and wind tunnel testing are resource intensive, hindering rapid iteration in the early design stages. Machine learning-based surrogate models offer a promising alternative, yet many existing approaches suffer from high computational complexity, limited interpretability, or insufficient accuracy for detailed geometric inputs. This paper introduces a novel lightweight surrogate model for the prediction of the aerodynamic drag coefficient (Cd) based on a sequential slice-wise processing of the geometry of the 3D vehicle. Inspired by medical imaging, 3D point clouds of vehicles are decomposed into an ordered sequence of 2D cross-sectional slices along the stream-wise axis. Each slice is encoded by a lightweight PointNet2D module, and the sequence of slice embeddings is processed by a bidirectional LSTM to capture longitudinal geometric evolution. The model, trained and evaluated on the DrivAerNet++ dataset, achieves a high coefficient of determination (R^2 > 0.9528) and a low mean absolute error (MAE approx 6.046 x 10^{-3}) in Cd prediction. With an inference time of approximately 0.025 seconds per sample on a consumer-grade GPU, our approach provides fast, accurate, and interpretable aerodynamic feedback, facilitating more agile and informed automotive design exploration.",
    "published": "2026-01-05T13:41:20Z",
    "updated": "2026-01-05T13:41:20Z",
    "link": "http://arxiv.org/pdf/2601.02112v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Utkarsh Singh",
      "Absaar Ali",
      "Adarsh Roy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02107v1",
    "title": "MagicFight: Personalized Martial Arts Combat Video Generation",
    "summary": "Amid the surge in generic text-to-video generation, the field of personalized human video generation has witnessed notable advancements, primarily concentrated on single-person scenarios. However, to our knowledge, the domain of two-person interactions, particularly in the context of martial arts combat, remains uncharted. We identify a significant gap: existing models for single-person dancing generation prove insufficient for capturing the subtleties and complexities of two engaged fighters, resulting in challenges such as identity confusion, anomalous limbs, and action mismatches. To address this, we introduce a pioneering new task, Personalized Martial Arts Combat Video Generation. Our approach, MagicFight, is specifically crafted to overcome these hurdles. Given this pioneering task, we face a lack of appropriate datasets. Thus, we generate a bespoke dataset using the game physics engine Unity, meticulously crafting a multitude of 3D characters, martial arts moves, and scenes designed to represent the diversity of combat. MagicFight refines and adapts existing models and strategies to generate high-fidelity two-person combat videos that maintain individual identities and ensure seamless, coherent action sequences, thereby laying the groundwork for future innovations in the realm of interactive video content creation.\n  Website: https://MingfuYAN.github.io/MagicFight/\n  Dataset: https://huggingface.co/datasets/MingfuYAN/KungFu-Fiesta",
    "published": "2026-01-05T13:34:17Z",
    "updated": "2026-01-05T13:34:17Z",
    "link": "http://arxiv.org/pdf/2601.02107v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiancheng Huang",
      "Mingfu Yan",
      "Songyan Chen",
      "Yi Huang",
      "Shifeng Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02103v1",
    "title": "HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures",
    "summary": "Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.",
    "published": "2026-01-05T13:32:37Z",
    "updated": "2026-01-05T13:32:37Z",
    "link": "http://arxiv.org/pdf/2601.02103v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yating Wang",
      "Yuan Sun",
      "Xuan Wang",
      "Ran Yi",
      "Boyao Zhou",
      "Yipengjing Sun",
      "Hongyu Liu",
      "Yinuo Wang",
      "Lizhuang Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02102v1",
    "title": "360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images",
    "summary": "3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.",
    "published": "2026-01-05T13:28:28Z",
    "updated": "2026-01-05T13:28:28Z",
    "link": "http://arxiv.org/pdf/2601.02102v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiaqi Yao",
      "Zhongmiao Yan",
      "Jingyi Xu",
      "Songpengcheng Xia",
      "Yan Xiang",
      "Ling Pei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.16073v4",
    "title": "TD3Net: A temporal densely connected multi-dilated convolutional network for lipreading",
    "summary": "The word-level lipreading approach typically employs a two-stage framework with separate frontend and backend architectures to model dynamic lip movements. Each component has been extensively studied, and in the backend architecture, temporal convolutional networks (TCNs) have been widely adopted in state-of-the-art methods. Recently, dense skip connections have been introduced in TCNs to mitigate the limited density of the receptive field, thereby improving the modeling of complex temporal representations. However, their performance remains constrained owing to potential information loss regarding the continuous nature of lip movements, caused by blind spots in the receptive field. To address this limitation, we propose TD3Net, a temporal densely connected multi-dilated convolutional network that combines dense skip connections and multi-dilated temporal convolutions as the backend architecture. TD3Net covers a wide and dense receptive field without blind spots by applying different dilation factors to skip-connected features. Experimental results on a word-level lipreading task using two large publicly available datasets, Lip Reading in the Wild (LRW) and LRW-1000, indicate that the proposed method achieves performance comparable to state-of-the-art methods. It achieved higher accuracy with fewer parameters and lower floating-point operations compared to existing TCN-based backend architectures. Moreover, visualization results suggest that our approach effectively utilizes diverse temporal features while preserving temporal continuity, presenting notable advantages in lipreading systems. The code is available at our GitHub repository (https://github.com/Leebh-kor/TD3Net).",
    "published": "2025-06-19T06:55:03Z",
    "updated": "2026-01-05T13:27:16Z",
    "link": "http://arxiv.org/pdf/2506.16073v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Byung Hoon Lee",
      "Wooseok Shin",
      "Sung Won Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02098v1",
    "title": "InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting",
    "summary": "Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.",
    "published": "2026-01-05T13:26:02Z",
    "updated": "2026-01-05T13:26:02Z",
    "link": "http://arxiv.org/pdf/2601.02098v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jinlong Fan",
      "Shanshan Zhao",
      "Liang Zheng",
      "Jing Zhang",
      "Yuxiang Yang",
      "Mingming Gong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02096v1",
    "title": "Dancing Points: Synthesizing Ballroom Dancing with Three-Point Inputs",
    "summary": "Ballroom dancing is a structured yet expressive motion category. Its highly diverse movement and complex interactions between leader and follower dancers make the understanding and synthesis challenging. We demonstrate that the three-point trajectory available from a virtual reality (VR) device can effectively serve as a dancer's motion descriptor, simplifying the modeling and synthesis of interplay between dancers' full-body motions down to sparse trajectories. Thanks to the low dimensionality, we can employ an efficient MLP network to predict the follower's three-point trajectory directly from the leader's three-point input for certain types of ballroom dancing, addressing the challenge of modeling high-dimensional full-body interaction. It also prevents our method from overfitting thanks to its compact yet explicit representation. By leveraging the inherent structure of the movements and carefully planning the autoregressive procedure, we show a deterministic neural network is able to translate three-point trajectories into a virtual embodied avatar, which is typically considered under-constrained and requires generative models for common motions. In addition, we demonstrate this deterministic approach generalizes beyond small, structured datasets like ballroom dancing, and performs robustly on larger, more diverse datasets such as LaFAN. Our method provides a computationally- and data-efficient solution, opening new possibilities for immersive paired dancing applications. Code and pre-trained models for this paper are available at https://peizhuoli.github.io/dancing-points.",
    "published": "2026-01-05T13:24:12Z",
    "updated": "2026-01-05T13:24:12Z",
    "link": "http://arxiv.org/pdf/2601.02096v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Peizhuo Li",
      "Sebastian Starke",
      "Yuting Ye",
      "Olga Sorkine-Hornung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.21582v3",
    "title": "Data-Augmented Multimodal Feature Fusion for Multiclass Visual Recognition of Oral Cancer Lesions",
    "summary": "Oral cancer is frequently diagnosed at later stages due to its similarity to other lesions. Existing research on computer aided diagnosis has made progress using deep learning; however, most approaches remain limited by small, imbalanced datasets and a dependence on single-modality features, which restricts model generalization in real-world clinical settings. To address these limitations, this study proposes a novel data-augmentation driven multimodal feature-fusion framework integrated within a (Vision Recognition)VR assisted oral cancer recognition system. Our method combines extensive data centric augmentation with fused clinical and image-based representations to enhance model robustness and reduce diagnostic ambiguity. Using a stratified training pipeline and an EfficientNetV2 B1 backbone, the system improves feature diversity, mitigates imbalance, and strengthens the learned multimodal embeddings. Experimental evaluation demonstrates that the proposed framework achieves an overall accuracy of 82.57 percent on 2 classes, 65.13 percent on 3 classes, and 54.97 percent on 4 classes, outperforming traditional single stream CNN models. These results highlight the effectiveness of multimodal feature fusion combined with strategic augmentation for reliable early oral cancer lesion recognition and serve as a foundation for immersive VR based clinical decision support tools.",
    "published": "2025-11-26T16:56:42Z",
    "updated": "2026-01-05T13:20:10Z",
    "link": "http://arxiv.org/pdf/2511.21582v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Joy Naoum",
      "Revana Salama",
      "Ali Hamdi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02091v1",
    "title": "MCD-Net: A Lightweight Deep Learning Baseline for Optical-Only Moraine Segmentation",
    "summary": "Glacial segmentation is essential for reconstructing past glacier dynamics and evaluating climate-driven landscape change. However, weak optical contrast and the limited availability of high-resolution DEMs hinder automated mapping. This study introduces the first large-scale optical-only moraine segmentation dataset, comprising 3,340 manually annotated high-resolution images from Google Earth covering glaciated regions of Sichuan and Yunnan, China. We develop MCD-Net, a lightweight baseline that integrates a MobileNetV2 encoder, a Convolutional Block Attention Module (CBAM), and a DeepLabV3+ decoder. Benchmarking against deeper backbones (ResNet152, Xception) shows that MCD-Net achieves 62.3\\% mean Intersection over Union (mIoU) and 72.8\\% Dice coefficient while reducing computational cost by more than 60\\%. Although ridge delineation remains constrained by sub-pixel width and spectral ambiguity, the results demonstrate that optical imagery alone can provide reliable moraine-body segmentation. The dataset and code are publicly available at https://github.com/Lyra-alpha/MCD-Net, establishing a reproducible benchmark for moraine-specific segmentation and offering a deployable baseline for high-altitude glacial monitoring.",
    "published": "2026-01-05T13:18:11Z",
    "updated": "2026-01-05T13:18:11Z",
    "link": "http://arxiv.org/pdf/2601.02091v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhehuan Cao",
      "Fiseha Berhanu Tesema",
      "Ping Fu",
      "Jianfeng Ren",
      "Ahmed Nasr"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02088v1",
    "title": "PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction",
    "summary": "Orthognathic surgery repositions jaw bones to restore occlusion and enhance facial aesthetics. Accurate simulation of postoperative facial morphology is essential for preoperative planning. However, traditional biomechanical models are computationally expensive, while geometric deep learning approaches often lack interpretability. In this study, we develop and validate a physics-informed geometric deep learning framework named PhysSFI-Net for precise prediction of soft tissue deformation following orthognathic surgery. PhysSFI-Net consists of three components: a hierarchical graph module with craniofacial and surgical plan encoders combined with attention mechanisms to extract skeletal-facial interaction features; a Long Short-Term Memory (LSTM)-based sequential predictor for incremental soft tissue deformation; and a biomechanics-inspired module for high-resolution facial surface reconstruction. Model performance was assessed using point cloud shape error (Hausdorff distance), surface deviation error, and landmark localization error (Euclidean distances of craniomaxillofacial landmarks) between predicted facial shapes and corresponding ground truths. A total of 135 patients who underwent combined orthodontic and orthognathic treatment were included for model training and validation. Quantitative analysis demonstrated that PhysSFI-Net achieved a point cloud shape error of 1.070 +/- 0.088 mm, a surface deviation error of 1.296 +/- 0.349 mm, and a landmark localization error of 2.445 +/- 1.326 mm. Comparative experiments indicated that PhysSFI-Net outperformed the state-of-the-art method ACMT-Net in prediction accuracy. In conclusion, PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation.",
    "published": "2026-01-05T13:14:19Z",
    "updated": "2026-01-05T13:14:19Z",
    "link": "http://arxiv.org/pdf/2601.02088v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiahao Bao",
      "Huazhen Liu",
      "Yu Zhuang",
      "Leran Tao",
      "Xinyu Xu",
      "Yongtao Shi",
      "Mengjia Cheng",
      "Yiming Wang",
      "Congshuang Ku",
      "Ting Zeng",
      "Yilang Du",
      "Siyi Chen",
      "Shunyao Shen",
      "Suncheng Xiang",
      "Hongbo Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.21408v2",
    "title": "VALLR: Visual ASR Language Model for Lip Reading",
    "summary": "Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex task requiring the interpretation of spoken language exclusively from visual cues, primarily lip movements and facial expressions. This task is especially challenging due to the absence of auditory information and the inherent ambiguity when visually distinguishing phonemes that have overlapping visemes where different phonemes appear identical on the lips. Current methods typically attempt to predict words or characters directly from these visual cues, but this approach frequently encounters high error rates due to coarticulation effects and viseme ambiguity. We propose a novel two-stage, phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that addresses these longstanding challenges. First, our model predicts a compact sequence of phonemes from visual inputs using a Video Transformer with a CTC head, thereby reducing the task complexity and achieving robust speaker invariance. This phoneme output then serves as the input to a fine-tuned Large Language Model (LLM), which reconstructs coherent words and sentences by leveraging broader linguistic context. Unlike existing methods that either predict words directly-often faltering on visually similar phonemes-or rely on large-scale multimodal pre-training, our approach explicitly encodes intermediate linguistic structure while remaining highly data efficient. We demonstrate state-of-the-art performance on two challenging datasets, LRS2 and LRS3, where our method achieves significant reductions in Word Error Rate (WER) achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data than the next best approach.",
    "published": "2025-03-27T11:52:08Z",
    "updated": "2026-01-05T12:55:42Z",
    "link": "http://arxiv.org/pdf/2503.21408v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Marshall Thomas",
      "Edward Fish",
      "Richard Bowden"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10838v2",
    "title": "Unsupervised Stereo via Multi-Baseline Geometry-Consistent Self-Training",
    "summary": "Photometric loss and pseudo-label-based self-training are two widely used methods for training stereo networks on unlabeled data. However, they both struggle to provide accurate supervision in occluded regions. The former lacks valid correspondences, while the latter's pseudo labels are often unreliable. To overcome these limitations, we present S$^3$, a simple yet effective framework based on multi-baseline geometry consistency. Unlike conventional self-training where teacher and student share identical stereo pairs, S$^3$ assigns them different target images, introducing natural visibility asymmetry. Regions occluded in the student's view often remain visible and matchable to the teacher, enabling reliable pseudo labels even in regions where photometric supervision fails. The teacher's disparities are rescaled to align with the student's baseline and used to guide student learning. An occlusion-aware weighting strategy is further proposed to mitigate unreliable supervision in teacher-occluded regions and to encourage the student to learn robust occlusion completion. To support training, we construct MBS20K, a multi-baseline stereo dataset synthesized using the CARLA simulator. Extensive experiments demonstrate that S$^3$ provides effective supervision in both occluded and non-occluded regions, achieves strong generalization performance, and surpasses previous state-of-the-art methods on the KITTI 2015 and 2012 benchmarks.",
    "published": "2025-08-14T17:03:47Z",
    "updated": "2026-01-05T12:52:56Z",
    "link": "http://arxiv.org/pdf/2508.10838v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Peng Xu",
      "Zhiyu Xiang",
      "Tingming Bai",
      "Tianyu Pu",
      "Kai Wang",
      "Chaojie Ji",
      "Zhihao Yang",
      "Eryun Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02072v1",
    "title": "SketchRodGS: Sketch-based Extraction of Slender Geometries for Animating Gaussian Splatting Scenes",
    "summary": "Physics simulation of slender elastic objects often requires discretization as a polyline. However, constructing a polyline from Gaussian splatting is challenging as Gaussian splatting lacks connectivity information and the configuration of Gaussian primitives contains much noise. This paper presents a method to extract a polyline representation of the slender part of the objects in a Gaussian splatting scene from the user's sketching input. Our method robustly constructs a polyline mesh that represents the slender parts using the screen-space shortest path analysis that can be efficiently solved using dynamic programming. We demonstrate the effectiveness of our approach in several in-the-wild examples.",
    "published": "2026-01-05T12:51:12Z",
    "updated": "2026-01-05T12:51:12Z",
    "link": "http://arxiv.org/pdf/2601.02072v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Haato Watanabe",
      "Nobuyuki Umetani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.05221v2",
    "title": "SAM-aware Test-time Adaptation for Universal Medical Image Segmentation",
    "summary": "Leveraging the Segment Anything Model (SAM) for medical image segmentation remains challenging due to its limited adaptability across diverse medical domains. Although fine-tuned variants, such as MedSAM, improve performance in scenarios similar to the training modalities or organs, they may lack generalizability to unseen data. To overcome this limitation, we propose SAM-aware Test-time Adaptation (SAM-TTA), a lightweight and flexible framework that preserves SAM's inherent generalization ability while enhancing segmentation accuracy for medical images. SAM-TTA tackles two major challenges: (1) input-level discrepancy caused by channel mismatches between natural and medical images, and (2) semantic-level discrepancy due to different object characteristics in natural versus medical images (e.g., with clear boundaries vs. ambiguous structures). To this end, we introduce two complementary components: a self-adaptive Bezier Curve-based Transformation (SBCT), which maps single-channel medical images into SAM-compatible three-channel images via a few learnable parameters to be optimized at test time; and IoU-guided Multi-scale Adaptation (IMA), which leverages SAM's intrinsic IoU scores to enforce high output confidence, dual-scale prediction consistency, and intermediate feature consistency, to improve semantic-level alignments. Extensive experiments on eight public medical image segmentation tasks, covering six grayscale and two color (endoscopic) tasks, demonstrate that SAM-TTA consistently outperforms state-of-the-art test-time adaptation methods. Notably, on six grayscale datasets, SAM-TTA even surpasses fully fine-tuned models, achieving significant Dice improvements (i.e., average 4.8% and 7.4% gains over MedSAM and SAM-Med2D) and establishing a new paradigm for universal medical image segmentation. Code is available at https://github.com/JianghaoWu/SAM-TTA.",
    "published": "2025-06-05T16:38:16Z",
    "updated": "2026-01-05T12:49:21Z",
    "link": "http://arxiv.org/pdf/2506.05221v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jianghao Wu",
      "Yicheng Wu",
      "Yutong Xie",
      "Wenjia Bai",
      "You Zhang",
      "Feilong Tang",
      "Yulong Li",
      "Imran Razzak",
      "Daniel F Schmidt",
      "Yasmeen George"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13454v2",
    "title": "Test-Time Modification: Inverse Domain Transformation for Robust Perception",
    "summary": "Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.",
    "published": "2025-12-15T15:51:30Z",
    "updated": "2026-01-05T12:06:46Z",
    "link": "http://arxiv.org/pdf/2512.13454v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Arpit Jadon",
      "Joshua Niemeijer",
      "Yuki M. Asano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.24015v2",
    "title": "On Exact Editing of Flow-Based Diffusion Models",
    "summary": "Recent methods in flow-based diffusion editing have enabled direct transformations between source and target image distribution without explicit inversion. However, the latent trajectories in these methods often exhibit accumulated velocity errors, leading to semantic inconsistency and loss of structural fidelity. We propose Conditioned Velocity Correction (CVC), a principled framework that reformulates flow-based editing as a distribution transformation problem driven by a known source prior. CVC rethinks the role of velocity in inter-distribution transformation by introducing a dual-perspective velocity conversion mechanism. This mechanism explicitly decomposes the latent evolution into two components: a structure-preserving branch that remains consistent with the source trajectory, and a semantically-guided branch that drives a controlled deviation toward the target distribution. The conditional velocity field exhibits an absolute velocity error relative to the true underlying distribution trajectory, which inherently introduces potential instability and trajectory drift in the latent space. To address this quantifiable deviation and maintain fidelity to the true flow, we apply a posterior-consistent update to the resulting conditional velocity field. This update is derived from Empirical Bayes Inference and Tweedie correction, which ensures a mathematically grounded error compensation over time. Our method yields stable and interpretable latent dynamics, achieving faithful reconstruction alongside smooth local semantic conversion. Comprehensive experiments demonstrate that CVC consistently achieves superior fidelity, better semantic alignment, and more reliable editing behavior across diverse tasks.",
    "published": "2025-12-30T06:29:20Z",
    "updated": "2026-01-05T12:05:00Z",
    "link": "http://arxiv.org/pdf/2512.24015v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zixiang Li",
      "Yue Song",
      "Jianing Peng",
      "Ting Liu",
      "Jun Huang",
      "Xiaochao Qu",
      "Luoqi Liu",
      "Wei Wang",
      "Yao Zhao",
      "Yunchao Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02038v1",
    "title": "AlignVTOFF: Texture-Spatial Feature Alignment for High-Fidelity Virtual Try-Off",
    "summary": "Virtual Try-Off (VTOFF) is a challenging multimodal image generation task that aims to synthesize high-fidelity flat-lay garments under complex geometric deformation and rich high-frequency textures. Existing methods often rely on lightweight modules for fast feature extraction, which struggles to preserve structured patterns and fine-grained details, leading to texture attenuation during generation.To address these issues, we propose AlignVTOFF, a novel parallel U-Net framework built upon a Reference U-Net and Texture-Spatial Feature Alignment (TSFA). The Reference U-Net performs multi-scale feature extraction and enhances geometric fidelity, enabling robust modeling of deformation while retaining complex structured patterns. TSFA then injects the reference garment features into a frozen denoising U-Net via a hybrid attention design, consisting of a trainable cross-attention module and a frozen self-attention module. This design explicitly aligns texture and spatial cues and alleviates the loss of high-frequency information during the denoising process.Extensive experiments across multiple settings demonstrate that AlignVTOFF consistently outperforms state-of-the-art methods, producing flat-lay garment results with improved structural realism and high-frequency detail fidelity.",
    "published": "2026-01-05T11:50:02Z",
    "updated": "2026-01-05T11:50:02Z",
    "link": "http://arxiv.org/pdf/2601.02038v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yihan Zhu",
      "Mengying Ge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02036v1",
    "title": "GDRO: Group-level Reward Post-training Suitable for Diffusion Models",
    "summary": "Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.",
    "published": "2026-01-05T11:47:18Z",
    "updated": "2026-01-05T11:47:18Z",
    "link": "http://arxiv.org/pdf/2601.02036v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Yiyang Wang",
      "Xi Chen",
      "Xiaogang Xu",
      "Yu Liu",
      "Hengshuang Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02029v1",
    "title": "Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding",
    "summary": "This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.",
    "published": "2026-01-05T11:42:49Z",
    "updated": "2026-01-05T11:42:49Z",
    "link": "http://arxiv.org/pdf/2601.02029v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Toshihiko Nishimura",
      "Hirofumi Abe",
      "Kazuhiko Murasaki",
      "Taiga Yoshida",
      "Ryuichi Tanida"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.01505v5",
    "title": "Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports",
    "summary": "Reasoning over sports videos for question answering is an important task with numerous applications, such as player training and information retrieval. However, this task has not been explored due to the lack of relevant datasets and the challenging nature it presents. Most datasets for video question answering (VideoQA) focus mainly on general and coarse-grained understanding of daily-life videos, which is not applicable to sports scenarios requiring professional action understanding and fine-grained motion analysis. In this paper, we introduce the first dataset, named Sports-QA, specifically designed for the sports VideoQA task. The Sports-QA dataset includes various types of questions, such as descriptions, chronologies, causalities, and counterfactual conditions, covering multiple sports. Furthermore, to address the characteristics of the sports VideoQA task, we propose a new Auto-Focus Transformer (AFT) capable of automatically focusing on particular scales of temporal information for question answering. We conduct extensive experiments on Sports-QA, including baseline studies and the evaluation of different methods. The results demonstrate that our AFT achieves state-of-the-art performance.",
    "published": "2024-01-03T02:22:34Z",
    "updated": "2026-01-05T11:29:59Z",
    "link": "http://arxiv.org/pdf/2401.01505v5.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haopeng Li",
      "Andong Deng",
      "Jun Liu",
      "Hossein Rahmani",
      "Yulan Guo",
      "Bernt Schiele",
      "Mohammed Bennamoun",
      "Qiuhong Ke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02020v1",
    "title": "Adapting Depth Anything to Adverse Imaging Conditions with Events",
    "summary": "Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems. Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur. These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions. Typically, existing approaches incorporate event cameras to leverage their high dynamic range and temporal resolution, aiming to compensate for corrupted frame features. However, such specialized fusion models are predominantly trained from scratch on domain-specific datasets, thereby failing to inherit the open-world knowledge and robust generalization inherent to foundation models. In this work, we propose ADAE, an event-guided spatiotemporal fusion framework for Depth Anything in degraded scenes. Our design is guided by two key insights: 1) Entropy-Aware Spatial Fusion. We adaptively merge frame-based and event-based features using an information entropy strategy to indicate illumination-induced degradation. 2) Motion-Guided Temporal Correction. We resort to the event-based motion cue to recalibrate ambiguous features in blurred regions. Under our unified framework, the two components are complementary to each other and jointly enhance Depth Anything under adverse imaging conditions. Extensive experiments have been performed to verify the superiority of the proposed method. Our code will be released upon acceptance.",
    "published": "2026-01-05T11:29:49Z",
    "updated": "2026-01-05T11:29:49Z",
    "link": "http://arxiv.org/pdf/2601.02020v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shihan Peng",
      "Yuyang Xiong",
      "Hanyu Zhou",
      "Zhiwei Shi",
      "Haoyue Liu",
      "Gang Chen",
      "Luxin Yan",
      "Yi Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02018v1",
    "title": "Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement",
    "summary": "Segment Anything Models (SAMs), known for their exceptional zero-shot segmentation performance, have garnered significant attention in the research community. Nevertheless, their performance drops significantly on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM++, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Additionally, to improve compatibility between the pre-trained diffusion model and the segmentation framework, we introduce two techniques, i.e., Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). However, the above components lack explicit guidance regarding the degree of degradation. The model is forced to implicitly fit a complex noise distribution that spans conditions from mild noise to severe artifacts, which substantially increases the learning burden and leads to suboptimal reconstructions. To address this issue, we further introduce a Degradation-aware Adaptive Enhancement (DAE) mechanism. The key principle of DAE is to decouple the reconstruction process for arbitrary-quality features into two stages: degradation-level prediction and degradation-aware reconstruction. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. Extensive experiments demonstrate that GleSAM++ significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM++ also performs well on unseen degradations, underscoring the versatility of our approach and dataset.",
    "published": "2026-01-05T11:28:58Z",
    "updated": "2026-01-05T11:28:58Z",
    "link": "http://arxiv.org/pdf/2601.02018v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Guangqian Guo",
      "Aixi Ren",
      "Yong Guo",
      "Xuehui Yu",
      "Jiacheng Tian",
      "Wenli Li",
      "Yaoxing Wang",
      "Shan Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.02660v3",
    "title": "PMGS: Reconstruction of Projectile Motion Across Large Spatiotemporal Spans via 3D Gaussian Splatting",
    "summary": "Modeling complex rigid motion across large spatiotemporal spans remains an unresolved challenge in dynamic reconstruction. Existing paradigms are mainly confined to short-term, small-scale deformation and offer limited consideration for physical consistency. This study proposes PMGS, focusing on reconstructing Projectile Motion via 3D Gaussian Splatting. The workflow comprises two stages: 1) Target Modeling: achieving object-centralized reconstruction through dynamic scene decomposition and an improved point density control; 2) Motion Recovery: restoring full motion sequences by learning per-frame SE(3) poses. We introduce an acceleration consistency constraint to bridge Newtonian mechanics and pose estimation, and design a dynamic simulated annealing strategy that adaptively schedules learning rates based on motion states. Furthermore, we devise a Kalman fusion scheme to optimize error accumulation from multi-source observations to mitigate disturbances. Experiments show PMGS's superior performance in reconstructing high-speed nonlinear rigid motion compared to mainstream dynamic methods.",
    "published": "2025-08-04T17:49:37Z",
    "updated": "2026-01-05T11:22:32Z",
    "link": "http://arxiv.org/pdf/2508.02660v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yijun Xu",
      "Jingrui Zhang",
      "Yuhan Chen",
      "Dingwen Wang",
      "Lei Yu",
      "Chu He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10710v2",
    "title": "CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation",
    "summary": "Diffusion-based text-to-image generation models have demonstrated strong performance in terms of image quality and diversity. However, they still struggle to generate images that accurately reflect the number of objects specified in the input prompt. Several approaches have been proposed that rely on either external counting modules for iterative refinement or quantity representations derived from learned tokens or latent features. However, they still have limitations in accurately reflecting the specified number of objects and overlook an important structural characteristic--The number of object instances in the generated image is largely determined in the early timesteps of the denoising process. To correctly reflect the object quantity for image generation, the highly activated regions in the object cross-attention map at the early timesteps should match the input object quantity, while each region should be clearly separated. To address this issue, we propose \\textit{CountCluster}, a method that guides the object cross-attention map to be clustered according to the specified object count in the input, without relying on any external tools or additional training. The proposed method partitions the object cross-attention map into $k$ clusters at inference time based on attention scores, defines an ideal distribution in which each cluster is spatially well-separated, and optimizes the latent to align with this target distribution. Our method achieves an average improvement of 18.5\\%p in object count accuracy compared to existing methods, and demonstrates superior quantity control performance across a variety of prompts. Code will be released at: https://github.com/JoohyeonL22/CountCluster",
    "published": "2025-08-14T14:53:53Z",
    "updated": "2026-01-05T11:17:43Z",
    "link": "http://arxiv.org/pdf/2508.10710v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Joohyeon Lee",
      "Jin-Seop Lee",
      "Jee-Hyong Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01998v1",
    "title": "Nighttime Hazy Image Enhancement via Progressively and Mutually Reinforcing Night-Haze Priors",
    "summary": "Enhancing the visibility of nighttime hazy images is challenging due to the complex degradation distributions. Existing methods mainly address a single type of degradation (e.g., haze or low-light) at a time, ignoring the interplay of different degradation types and resulting in limited visibility improvement. We observe that the domain knowledge shared between low-light and haze priors can be reinforced mutually for better visibility. Based on this key insight, in this paper, we propose a novel framework that enhances visibility in nighttime hazy images by reinforcing the intrinsic consistency between haze and low-light priors mutually and progressively. In particular, our model utilizes image-, patch-, and pixel-level experts that operate across visual and frequency domains to recover global scene structure, regional patterns, and fine-grained details progressively. A frequency-aware router is further introduced to adaptively guide the contribution of each expert, ensuring robust image restoration. Extensive experiments demonstrate the superior performance of our model on nighttime dehazing benchmarks both quantitatively and qualitatively. Moreover, we showcase the generalizability of our model in daytime dehazing and low-light enhancement tasks.",
    "published": "2026-01-05T10:58:02Z",
    "updated": "2026-01-05T10:58:02Z",
    "link": "http://arxiv.org/pdf/2601.01998v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chen Zhu",
      "Huiwen Zhang",
      "Mu He",
      "Yujie Li",
      "Xiaotian Qiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01992v1",
    "title": "API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning",
    "summary": "Real-world image dehazing is a fundamental yet challenging task in low-level vision. Existing learning-based methods often suffer from significant performance degradation when applied to complex real-world hazy scenes, primarily due to limited training data and the intrinsic complexity of haze density distributions.To address these challenges, we introduce a novel Adaptive Patch Importance-aware (API) framework for generalizable real-world image dehazing. Specifically, our framework consists of an Automatic Haze Generation (AHG) module and a Density-aware Haze Removal (DHR) module. AHG provides a hybrid data augmentation strategy by generating realistic and diverse hazy images as additional high-quality training data. DHR considers hazy regions with varying haze density distributions for generalizable real-world image dehazing in an adaptive patch importance-aware manner. To alleviate the ambiguity of the dehazed image details, we further introduce a new Multi-Negative Contrastive Dehazing (MNCD) loss, which fully utilizes information from multiple negative samples across both spatial and frequency domains. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple real-world benchmarks, delivering strong results in both quantitative metrics and qualitative visual quality, and exhibiting robust generalization across diverse haze distributions.",
    "published": "2026-01-05T10:53:41Z",
    "updated": "2026-01-05T10:53:41Z",
    "link": "http://arxiv.org/pdf/2601.01992v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chen Zhu",
      "Huiwen Zhang",
      "Yujie Li",
      "Mu He",
      "Xiaotian Qiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01984v1",
    "title": "Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation",
    "summary": "Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.",
    "published": "2026-01-05T10:38:26Z",
    "updated": "2026-01-05T10:38:26Z",
    "link": "http://arxiv.org/pdf/2601.01984v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Weijian Ma",
      "Shizhao Sun",
      "Tianyu Yu",
      "Ruiyu Wang",
      "Tat-Seng Chua",
      "Jiang Bian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05539v2",
    "title": "Ideal Observer for Segmentation of Dead Leaves Images",
    "summary": "The human visual environment is comprised of different surfaces that are distributed in space. The parts of a scene that are visible at any one time are governed by the occlusion of overlapping objects. In this work we consider \"dead leaves\" models, which replicate these occlusions when generating images by layering objects on top of each other. A dead leaves model is a generative model comprised of distributions for object position, shape, color and texture. An image is generated from a dead leaves model by sampling objects (\"leaves\") from these distributions until a stopping criterion is reached, usually when the image is fully covered or until a given number of leaves was sampled. Here, we describe a theoretical approach, based on previous work, to derive a Bayesian ideal observer for the partition of a given set of pixels based on independent dead leaves model distributions. Extending previous work, we provide step-by-step explanations for the computation of the posterior probability as well as describe factors that determine the feasibility of practically applying this computation. The dead leaves image model and the associated ideal observer can be applied to study segmentation decisions in a limited number of pixels, providing a principled upper-bound on performance, to which humans and vision algorithms could be compared.",
    "published": "2025-12-05T08:53:11Z",
    "updated": "2026-01-05T10:20:11Z",
    "link": "http://arxiv.org/pdf/2512.05539v2.pdf",
    "category": [
      "cs.CV",
      "math.ST",
      "stat.ME"
    ],
    "authors": [
      "Swantje Mahncke",
      "Malte Ott"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.11480v2",
    "title": "CADMorph: Geometry-Driven Parametric CAD Editing via a Plan-Generate-Verify Loop",
    "summary": "A Computer-Aided Design (CAD) model encodes an object in two coupled forms: a parametric construction sequence and its resulting visible geometric shape. During iterative design, adjustments to the geometric shape inevitably require synchronized edits to the underlying parametric sequence, called geometry-driven parametric CAD editing. The task calls for 1) preserving the original sequence's structure, 2) ensuring each edit's semantic validity, and 3) maintaining high shape fidelity to the target shape, all under scarce editing data triplets. We present CADMorph, an iterative plan-generate-verify framework that orchestrates pretrained domain-specific foundation models during inference: a parameter-to-shape (P2S) latent diffusion model and a masked-parameter-prediction (MPP) model. In the planning stage, cross-attention maps from the P2S model pinpoint the segments that need modification and offer editing masks. The MPP model then infills these masks with semantically valid edits in the generation stage. During verification, the P2S model embeds each candidate sequence in shape-latent space, measures its distance to the target shape, and selects the closest one. The three stages leverage the inherent geometric consciousness and design knowledge in pretrained priors, and thus tackle structure preservation, semantic validity, and shape fidelity respectively. Besides, both P2S and MPP models are trained without triplet data, bypassing the data-scarcity bottleneck. CADMorph surpasses GPT-4o and specialized CAD baselines, and supports downstream applications such as iterative editing and reverse-engineering enhancement.",
    "published": "2025-12-12T11:29:59Z",
    "updated": "2026-01-05T10:15:00Z",
    "link": "http://arxiv.org/pdf/2512.11480v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Weijian Ma",
      "Shizhao Sun",
      "Ruiyu Wang",
      "Jiang Bian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01963v1",
    "title": "Forget Less by Learning Together through Concept Consolidation",
    "summary": "Custom Diffusion Models (CDMs) have gained significant attention due to their remarkable ability to personalize generative processes. However, existing CDMs suffer from catastrophic forgetting when continuously learning new concepts. Most prior works attempt to mitigate this issue under the sequential learning setting with a fixed order of concept inflow and neglect inter-concept interactions. In this paper, we propose a novel framework - Forget Less by Learning Together (FL2T) - that enables concurrent and order-agnostic concept learning while addressing catastrophic forgetting. Specifically, we introduce a set-invariant inter-concept learning module where proxies guide feature selection across concepts, facilitating improved knowledge retention and transfer. By leveraging inter-concept guidance, our approach preserves old concepts while efficiently incorporating new ones. Extensive experiments, across three datasets, demonstrates that our method significantly improves concept retention and mitigates catastrophic forgetting, highlighting the effectiveness of inter-concept catalytic behavior in incremental concept learning of ten tasks with at least 2% gain on average CLIP Image Alignment scores.",
    "published": "2026-01-05T10:14:16Z",
    "updated": "2026-01-05T10:14:16Z",
    "link": "http://arxiv.org/pdf/2601.01963v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Arjun Ramesh Kaushik",
      "Naresh Kumar Devulapally",
      "Vishnu Suresh Lokhande",
      "Nalini Ratha",
      "Venu Govindaraju"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01957v1",
    "title": "AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing",
    "summary": "Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.",
    "published": "2026-01-05T10:02:22Z",
    "updated": "2026-01-05T10:02:22Z",
    "link": "http://arxiv.org/pdf/2601.01957v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tianbo Wang",
      "Yuqing Ma",
      "Kewei Liao",
      "Zhange Zhang",
      "Simin Li",
      "Jinyang Guo",
      "Xianglong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01955v1",
    "title": "MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization",
    "summary": "Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and semantically aligned motion transfer within DiT-based T2V models. Our key insight is that effective motion transfer requires \\romannumeral1) explicit disentanglement of motion from appearance and \\romannumeral 2) adaptive customization of motion to target content. MotionAdapter first isolates motion by analyzing cross-frame attention within 3D full-attention modules to extract attention-derived motion fields. To bridge the semantic gap between reference and target videos, we further introduce a DINO-guided motion customization module that rearranges and refines motion fields based on content correspondences. The customized motion field is then used to guide the DiT denoising process, ensuring that the synthesized video inherits the reference motion while preserving target appearance and semantics. Extensive experiments demonstrate that MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, MotionAdapter naturally supports complex motion transfer and motion editing tasks such as zooming.",
    "published": "2026-01-05T10:01:27Z",
    "updated": "2026-01-05T10:01:27Z",
    "link": "http://arxiv.org/pdf/2601.01955v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhexin Zhang",
      "Yifeng Zhu",
      "Yangyang Xu",
      "Long Chen",
      "Yong Du",
      "Shengfeng He",
      "Jun Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01950v1",
    "title": "Face Normal Estimation from Rags to Riches",
    "summary": "Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.",
    "published": "2026-01-05T09:57:24Z",
    "updated": "2026-01-05T09:57:24Z",
    "link": "http://arxiv.org/pdf/2601.01950v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Meng Wang",
      "Wenjing Dai",
      "Jiawan Zhang",
      "Xiaojie Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.10977v2",
    "title": "Point Cloud to Mesh Reconstruction: Methods, Trade-offs, and Implementation Guide",
    "summary": "Reconstructing meshes from point clouds is a fundamental task in computer vision with applications spanning robotics, autonomous systems, and medical imaging. Selecting an appropriate learning-based method requires understanding trade-offs between computational efficiency, geometric accuracy, and output constraints. This paper categorizes over fifteen methods into five paradigms -- PointNet family, autoencoder architectures, deformation-based methods, point-move techniques, and primitive-based approaches -- and provides practical guidance for method selection. We contribute: (1) a decision framework mapping input/output requirements to suitable paradigms, (2) a failure mode analysis to assist practitioners in debugging implementations, (3) standardized comparisons on ShapeNet benchmarks, and (4) a curated list of maintained codebases with implementation resources. By synthesizing both theoretical foundations and practical considerations, this work serves as an entry point for practitioners and researchers new to learning-based 3D mesh reconstruction.",
    "published": "2024-12-14T21:39:43Z",
    "updated": "2026-01-05T09:49:06Z",
    "link": "http://arxiv.org/pdf/2412.10977v2.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Fatima Zahra Iguenfer",
      "Achraf Hsain",
      "Hiba Amissa",
      "Yousra Chtouki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01926v1",
    "title": "MacVQA: Adaptive Memory Allocation and Global Noise Filtering for Continual Visual Question Answering",
    "summary": "Visual Question Answering (VQA) requires models to reason over multimodal information, combining visual and textual data. With the development of continual learning, significant progress has been made in retaining knowledge and adapting to new information in the VQA domain. However, current methods often struggle with balancing knowledge retention, adaptation, and robust feature representation. To address these challenges, we propose a novel framework with adaptive memory allocation and global noise filtering called MacVQA for visual question answering. MacVQA fuses visual and question information while filtering noise to ensure robust representations, and employs prototype-based memory allocation to optimize feature quality and memory usage. These designs enable MacVQA to balance knowledge acquisition, retention, and compositional generalization in continual VQA learning. Experiments on ten continual VQA tasks show that MacVQA outperforms existing baselines, achieving 43.38% average accuracy and 2.32% average forgetting on standard tasks, and 42.53% average accuracy and 3.60% average forgetting on novel composition tasks.",
    "published": "2026-01-05T09:18:09Z",
    "updated": "2026-01-05T09:18:09Z",
    "link": "http://arxiv.org/pdf/2601.01926v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhifei Li",
      "Yiran Wang",
      "Chenyi Xiong",
      "Yujing Xia",
      "Xiaoju Hou",
      "Yue Zhao",
      "Miao Zhang",
      "Kui Xiao",
      "Bing Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01925v1",
    "title": "AR-MOT: Autoregressive Multi-object Tracking",
    "summary": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.",
    "published": "2026-01-05T09:17:28Z",
    "updated": "2026-01-05T09:17:28Z",
    "link": "http://arxiv.org/pdf/2601.01925v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Lianjie Jia",
      "Yuhan Wu",
      "Binghao Ran",
      "Yifan Wang",
      "Lijun Wang",
      "Huchuan Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.18539v2",
    "title": "AdaVLN: Towards Visual Language Navigation in Continuous Indoor Environments with Moving Humans",
    "summary": "Visual Language Navigation is a task that challenges robots to navigate in realistic environments based on natural language instructions. While previous research has largely focused on static settings, real-world navigation must often contend with dynamic human obstacles. Hence, we propose an extension to the task, termed Adaptive Visual Language Navigation (AdaVLN), which seeks to narrow this gap. AdaVLN requires robots to navigate complex 3D indoor environments populated with dynamically moving human obstacles, adding a layer of complexity to navigation tasks that mimic the real-world. To support exploration of this task, we also present AdaVLN simulator and AdaR2R datasets. The AdaVLN simulator enables easy inclusion of fully animated human models directly into common datasets like Matterport3D. We also introduce a \"freeze-time\" mechanism for both the navigation task and simulator, which pauses world state updates during agent inference, enabling fair comparisons and experimental reproducibility across different hardware. We evaluate several baseline models on this task, analyze the unique challenges introduced by AdaVLN, and demonstrate its potential to bridge the sim-to-real gap in VLN research.",
    "published": "2024-11-27T17:36:08Z",
    "updated": "2026-01-05T09:17:12Z",
    "link": "http://arxiv.org/pdf/2411.18539v2.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Dillon Loh",
      "Tomasz Bednarz",
      "Xinxing Xia",
      "Frank Guan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.23239v2",
    "title": "RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models",
    "summary": "Diffusion-based remote sensing (RS) generative foundation models are cruial for downstream tasks. However, these models rely on large amounts of globally representative data, which often contain redundancy, noise, and class imbalance, reducing training efficiency and preventing convergence. Existing RS diffusion foundation models typically aggregate multiple classification datasets or apply simplistic deduplication, overlooking the distributional requirements of generation modeling and the heterogeneity of RS imagery. To address these limitations, we propose a training-free, two-stage data pruning approach that quickly select a high-quality subset under high pruning ratios, enabling a preliminary foundation model to converge rapidly and serve as a versatile backbone for generation, downstream fine-tuning, and other applications. Our method jointly considers local information content with global scene-level diversity and representativeness. First, an entropy-based criterion efficiently removes low-information samples. Next, leveraging RS scene classification datasets as reference benchmarks, we perform scene-aware clustering with stratified sampling to improve clustering effectiveness while reducing computational costs on large-scale unlabeled data. Finally, by balancing cluster-level uniformity and sample representativeness, the method enables fine-grained selection under high pruning ratios while preserving overall diversity and representativeness. Experiments show that, even after pruning 85\\% of the training data, our method significantly improves convergence and generation quality. Furthermore, diffusion foundation models trained with our method consistently achieve state-of-the-art performance across downstream tasks, including super-resolution and semantic image synthesis. This data pruning paradigm offers practical guidance for developing RS generative foundation models.",
    "published": "2025-12-29T06:44:06Z",
    "updated": "2026-01-05T09:01:02Z",
    "link": "http://arxiv.org/pdf/2512.23239v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Fan Wei",
      "Runmin Dong",
      "Yushan Lai",
      "Yixiang Yang",
      "Zhaoyang Luo",
      "Jinxiao Zhang",
      "Miao Yang",
      "Shuai Yuan",
      "Jiyao Zhao",
      "Bin Luo",
      "Haohuan Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01915v1",
    "title": "TalkPhoto: A Versatile Training-Free Conversational Assistant for Intelligent Image Editing",
    "summary": "Thanks to the powerful language comprehension capabilities of Large Language Models (LLMs), existing instruction-based image editing methods have introduced Multimodal Large Language Models (MLLMs) to promote information exchange between instructions and images, ensuring the controllability and flexibility of image editing. However, these frameworks often build a multi-instruction dataset to train the model to handle multiple editing tasks, which is not only time-consuming and labor-intensive but also fails to achieve satisfactory results. In this paper, we present TalkPhoto, a versatile training-free image editing framework that facilitates precise image manipulation through conversational interaction. We instruct the open-source LLM with a specially designed prompt template to analyze user needs after receiving instructions and hierarchically invoke existing advanced editing methods, all without additional training. Moreover, we implement a plug-and-play and efficient invocation of image editing methods, allowing complex and unseen editing tasks to be integrated into the current framework, achieving stable and high-quality editing results. Extensive experiments demonstrate that our method not only provides more accurate invocation with fewer token consumption but also achieves higher editing quality across various image editing tasks.",
    "published": "2026-01-05T09:00:32Z",
    "updated": "2026-01-05T09:00:32Z",
    "link": "http://arxiv.org/pdf/2601.01915v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yujie Hu",
      "Zecheng Tang",
      "Xu Jiang",
      "Weiqi Li",
      "Jian Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01914v1",
    "title": "Learning Action Hierarchies via Hybrid Geometric Diffusion",
    "summary": "Temporal action segmentation is a critical task in video understanding, where the goal is to assign action labels to each frame in a video. While recent advances leverage iterative refinement-based strategies, they fail to explicitly utilize the hierarchical nature of human actions. In this work, we propose HybridTAS - a novel framework that incorporates a hybrid of Euclidean and hyperbolic geometries into the denoising process of diffusion models to exploit the hierarchical structure of actions. Hyperbolic geometry naturally provides tree-like relationships between embeddings, enabling us to guide the action label denoising process in a coarse-to-fine manner: higher diffusion timesteps are influenced by abstract, high-level action categories (root nodes), while lower timesteps are refined using fine-grained action classes (leaf nodes). Extensive experiments on three benchmark datasets, GTEA, 50Salads, and Breakfast, demonstrate that our method achieves state-of-the-art performance, validating the effectiveness of hyperbolic-guided denoising for the temporal action segmentation task.",
    "published": "2026-01-05T08:59:07Z",
    "updated": "2026-01-05T08:59:07Z",
    "link": "http://arxiv.org/pdf/2601.01914v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Arjun Ramesh Kaushik",
      "Nalini K. Ratha",
      "Venu Govindaraju"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01892v1",
    "title": "Forget Less by Learning from Parents Through Hierarchical Relationships",
    "summary": "Custom Diffusion Models (CDMs) offer impressive capabilities for personalization in generative modeling, yet they remain vulnerable to catastrophic forgetting when learning new concepts sequentially. Existing approaches primarily focus on minimizing interference between concepts, often neglecting the potential for positive inter-concept interactions. In this work, we present Forget Less by Learning from Parents (FLLP), a novel framework that introduces a parent-child inter-concept learning mechanism in hyperbolic space to mitigate forgetting. By embedding concept representations within a Lorentzian manifold, naturally suited to modeling tree-like hierarchies, we define parent-child relationships in which previously learned concepts serve as guidance for adapting to new ones. Our method not only preserves prior knowledge but also supports continual integration of new concepts. We validate FLLP on three public datasets and one synthetic benchmark, showing consistent improvements in both robustness and generalization.",
    "published": "2026-01-05T08:35:36Z",
    "updated": "2026-01-05T08:35:36Z",
    "link": "http://arxiv.org/pdf/2601.01892v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Arjun Ramesh Kaushik",
      "Naresh Kumar Devulapally",
      "Vishnu Suresh Lokhande",
      "Nalini K. Ratha",
      "Venu Govindaraju"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01891v1",
    "title": "Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems",
    "summary": "The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.",
    "published": "2026-01-05T08:34:17Z",
    "updated": "2026-01-05T08:34:17Z",
    "link": "http://arxiv.org/pdf/2601.01891v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Niloufar Alipour Talemi",
      "Julia Boone",
      "Fatemeh Afghah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.22425v4",
    "title": "Wukong's 72 Transformations: High-fidelity Textured 3D Morphing via Flow Models",
    "summary": "We present WUKONG, a novel training-free framework for high-fidelity textured 3D morphing that takes a pair of source and target prompts (image or text) as input. Unlike conventional methods -- which rely on manual correspondence matching and deformation trajectory estimation (limiting generalization and requiring costly preprocessing) -- WUKONG leverages the generative prior of flow-based transformers to produce high-fidelity 3D transitions with rich texture details. To ensure smooth shape transitions, we exploit the inherent continuity of flow-based generative processes and formulate morphing as an optimal transport barycenter problem. We further introduce a sequential initialization strategy to prevent abrupt geometric distortions and preserve identity coherence. For faithful texture preservation, we propose a similarity-guided semantic consistency mechanism that selectively retains high-frequency details and enables precise control over blending dynamics. This empowers WUKONG to support both global texture transitions and identity-preserving texture morphing, catering to diverse generation needs. Extensive quantitative and qualitative evaluations demonstrate that WUKONG significantly outperforms state-of-the-art methods, achieving superior results across diverse geometry and texture variations.",
    "published": "2025-11-27T13:03:57Z",
    "updated": "2026-01-05T08:01:23Z",
    "link": "http://arxiv.org/pdf/2511.22425v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Minghao Yin",
      "Yukang Cao",
      "Kai Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01870v1",
    "title": "Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion",
    "summary": "Existing text-driven infrared and visible image fusion approaches often rely on textual information at the sentence level, which can lead to semantic noise from redundant text and fail to fully exploit the deeper semantic value of textual information. To address these issues, we propose a novel fusion approach named Entity-Guided Multi-Task learning for infrared and visible image fusion (EGMT). Our approach includes three key innovative components: (i) A principled method is proposed to extract entity-level textual information from image captions generated by large vision-language models, eliminating semantic noise from raw text while preserving critical semantic information; (ii) A parallel multi-task learning architecture is constructed, which integrates image fusion with a multi-label classification task. By using entities as pseudo-labels, the multi-label classification task provides semantic supervision, enabling the model to achieve a deeper understanding of image content and significantly improving the quality and semantic density of the fused image; (iii) An entity-guided cross-modal interactive module is also developed to facilitate the fine-grained interaction between visual and entity-level textual features, which enhances feature representation by capturing cross-modal dependencies at both inter-visual and visual-entity levels. To promote the wide application of the entity-guided image fusion framework, we release the entity-annotated version of four public datasets (i.e., TNO, RoadScene, M3FD, and MSRS). Extensive experiments demonstrate that EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency, compared to the state-of-the-art methods. The code and dataset will be publicly available at https://github.com/wyshao-01/EGMT.",
    "published": "2026-01-05T08:00:03Z",
    "updated": "2026-01-05T08:00:03Z",
    "link": "http://arxiv.org/pdf/2601.01870v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wenyu Shao",
      "Hongbo Liu",
      "Yunchuan Ma",
      "Ruili Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01865v1",
    "title": "RRNet: Configurable Real-Time Video Enhancement with Arbitrary Local Lighting Variations",
    "summary": "With the growing demand for real-time video enhancement in live applications, existing methods often struggle to balance speed and effective exposure control, particularly under uneven lighting. We introduce RRNet (Rendering Relighting Network), a lightweight and configurable framework that achieves a state-of-the-art tradeoff between visual quality and efficiency. By estimating parameters for a minimal set of virtual light sources, RRNet enables localized relighting through a depth-aware rendering module without requiring pixel-aligned training data. This object-aware formulation preserves facial identity and supports real-time, high-resolution performance using a streamlined encoder and lightweight prediction head. To facilitate training, we propose a generative AI-based dataset creation pipeline that synthesizes diverse lighting conditions at low cost. With its interpretable lighting control and efficient architecture, RRNet is well suited for practical applications such as video conferencing, AR-based portrait enhancement, and mobile photography. Experiments show that RRNet consistently outperforms prior methods in low-light enhancement, localized illumination adjustment, and glare removal.",
    "published": "2026-01-05T07:50:59Z",
    "updated": "2026-01-05T07:50:59Z",
    "link": "http://arxiv.org/pdf/2601.01865v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wenlong Yang",
      "Canran Jin",
      "Weihang Yuan",
      "Chao Wang",
      "Lifeng Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.02565v3",
    "title": "SJTU:Spatial judgments in multimodal models towards unified segmentation through coordinate detection",
    "summary": "Despite significant advances in vision-language understanding, implementing image segmentation within multimodal architectures remains a fundamental challenge in modern artificial intelligence systems. Existing vision-language models, which primarily rely on backbone architectures or CLIP-based embedding learning, demonstrate inherent limitations in fine-grained spatial localization and operational capabilities. This paper introduces SJTU: Spatial Judgments in Multimodal Models - Towards Unified Segmentation through Coordinate Detection, a framework that leverages spatial coordinate understanding to bridge vision-language interaction and precise segmentation, enabling accurate target identification through natural language instructions. The framework presents an approach for integrating segmentation techniques with vision-language models through spatial inference in multimodal space. By utilizing normalized coordinate detection for bounding boxes and transforming them into actionable segmentation outputs, we establish a connection between spatial and language representations in multimodal architectures. Experimental results demonstrate superior performance across benchmark datasets, achieving IoU scores of 0.5958 on COCO 2017 and 0.6758 on Pascal VOC. Testing on a single NVIDIA RTX 3090 GPU with 512x512 resolution images yields an average inference time of 7 seconds per image, demonstrating the framework's effectiveness in both accuracy and practical deployability. The project code is available at https://github.com/jw-chae/SJTU",
    "published": "2024-12-03T16:53:58Z",
    "updated": "2026-01-05T07:34:56Z",
    "link": "http://arxiv.org/pdf/2412.02565v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Joongwon Chae",
      "Zhenyu Wang",
      "Peiwu Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01856v1",
    "title": "GCR: Geometry-Consistent Routing for Task-Agnostic Continual Anomaly Detection",
    "summary": "Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior.\n  We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning.\n  Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR",
    "published": "2026-01-05T07:33:50Z",
    "updated": "2026-01-05T07:33:50Z",
    "link": "http://arxiv.org/pdf/2601.01856v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Joongwon Chae",
      "Lihui Luo",
      "Yang Liu",
      "Runming Wang",
      "Dongmei Yu",
      "Zeming Liang",
      "Xi Yuan",
      "Dayan Zhang",
      "Zhenglin Chen",
      "Peiwu Qin",
      "Ilmoon Chae"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01847v1",
    "title": "ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting",
    "summary": "Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate both emotional expressions and style features remains a significant challenge. In this paper, we propose ESGaussianFace, an innovative framework for emotional and stylized audio-driven facial animation. Our approach leverages 3D Gaussian Splatting to reconstruct 3D scenes and render videos, ensuring efficient generation of 3D consistent results. We propose an emotion-audio-guided spatial attention method that effectively integrates emotion features with audio content features. Through emotion-guided attention, the model is able to reconstruct facial details across different emotional states more accurately. To achieve emotional and stylized deformations of the 3D Gaussian points through emotion and style features, we introduce two 3D Gaussian deformation predictors. Futhermore, we propose a multi-stage training strategy, enabling the step-by-step learning of the character's lip movements, emotional variations, and style features. Our generated results exhibit high efficiency, high quality, and 3D consistency. Extensive experimental results demonstrate that our method outperforms existing state-of-the-art techniques in terms of lip movement accuracy, expression variation, and style feature expressiveness.",
    "published": "2026-01-05T07:19:38Z",
    "updated": "2026-01-05T07:19:38Z",
    "link": "http://arxiv.org/pdf/2601.01847v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chuhang Ma",
      "Shuai Tan",
      "Ye Pan",
      "Jiaolong Yang",
      "Xin Tong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01822v1",
    "title": "DisCo-FLoc: Using Dual-Level Visual-Geometric Contrasts to Disambiguate Depth-Aware Visual Floorplan Localization",
    "summary": "Since floorplan data is readily available, long-term persistent, and robust to changes in visual appearance, visual Floorplan Localization (FLoc) has garnered significant attention. Existing methods either ingeniously match geometric priors or utilize sparse semantics to reduce FLoc uncertainty. However, they still suffer from ambiguous FLoc caused by repetitive structures within minimalist floorplans. Moreover, expensive but limited semantic annotations restrict their applicability. To address these issues, we propose DisCo-FLoc, which utilizes dual-level visual-geometric Contrasts to Disambiguate depth-aware visual Floc, without requiring additional semantic labels. Our solution begins with a ray regression predictor tailored for ray-casting-based FLoc, predicting a series of FLoc candidates using depth estimation expertise. In addition, a novel contrastive learning method with position-level and orientation-level constraints is proposed to strictly match depth-aware visual features with the corresponding geometric structures in the floorplan. Such matches can effectively eliminate FLoc ambiguity and select the optimal imaging pose from FLoc candidates. Exhaustive comparative studies on two standard visual Floc benchmarks demonstrate that our method outperforms the state-of-the-art semantic-based method, achieving significant improvements in both robustness and accuracy.",
    "published": "2026-01-05T06:31:24Z",
    "updated": "2026-01-05T06:31:24Z",
    "link": "http://arxiv.org/pdf/2601.01822v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Shiyong Meng",
      "Tao Zou",
      "Bolei Chen",
      "Chaoxu Mu",
      "Jianxin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01818v1",
    "title": "Robust Egocentric Visual Attention Prediction Through Language-guided Scene Context-aware Learning",
    "summary": "As the demand for analyzing egocentric videos grows, egocentric visual attention prediction, anticipating where a camera wearer will attend, has garnered increasing attention. However, it remains challenging due to the inherent complexity and ambiguity of dynamic egocentric scenes. Motivated by evidence that scene contextual information plays a crucial role in modulating human attention, in this paper, we present a language-guided scene context-aware learning framework for robust egocentric visual attention prediction. We first design a context perceiver which is guided to summarize the egocentric video based on a language-based scene description, generating context-aware video representations. We then introduce two training objectives that: 1) encourage the framework to focus on the target point-of-interest regions and 2) suppress distractions from irrelevant regions which are less likely to attract first-person attention. Extensive experiments on Ego4D and Aria Everyday Activities (AEA) datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance and enhanced robustness across diverse, dynamic egocentric scenarios.",
    "published": "2026-01-05T06:14:41Z",
    "updated": "2026-01-05T06:14:41Z",
    "link": "http://arxiv.org/pdf/2601.01818v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sungjune Park",
      "Hongda Mao",
      "Qingshuang Chen",
      "Yong Man Ro",
      "Yelin Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01804v1",
    "title": "Causality-Aware Temporal Projection for Video Understanding in Video-LLMs",
    "summary": "Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.",
    "published": "2026-01-05T05:30:13Z",
    "updated": "2026-01-05T05:30:13Z",
    "link": "http://arxiv.org/pdf/2601.01804v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhengjian Kang",
      "Qi Chen",
      "Rui Liu",
      "Kangtong Mo",
      "Xingyu Zhang",
      "Xiaoyu Deng",
      "Ye Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02360v1",
    "title": "Heterogeneous Low-Bandwidth Pre-Training of LLMs",
    "summary": "Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.",
    "published": "2026-01-05T18:59:57Z",
    "updated": "2026-01-05T18:59:57Z",
    "link": "http://arxiv.org/pdf/2601.02360v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yazan Obeidi",
      "Amir Sarfi",
      "Joel Lidin",
      "Paul Janson",
      "Eugene Belilovsky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.05510v2",
    "title": "Causal Multi-fidelity Surrogate Forward and Inverse Models for ICF Implosions",
    "summary": "Continued progress in inertial confinement fusion (ICF) requires solving inverse problems relating experimental observations to simulation input parameters, followed by design optimization. However, such high-dimensional dynamic PDE-constrained optimization problems are extremely challenging or even intractable. It has been recently shown that inverse problems can be solved by only considering certain robust features. Here we consider the ICF capsule's deuterium-tritium (DT) interface, and construct a causal, dynamic, multifidelity reduced-order surrogate that maps from a time-dependent radiation temperature drive to the interface's radius and velocity dynamics. The surrogate targets an ODE embedding of DT interface dynamics, and is constructed by learning a controller for a base analytical model using low- and high-fidelity simulation training data with respect to radiation energy group structure. After demonstrating excellent accuracy of the surrogate interface model, we use machine learning (ML) models with surrogate-generated data to solve inverse problems optimizing radiation temperature drive to reproduce observed interface dynamics. For sparse snapshots in time, the ML model further characterizes the most informative times at which to sample dynamics. Altogether we demonstrate how operator learning, causal architectures, and physical inductive bias can be integrated to accelerate discovery, design, and diagnostics in high-energy-density systems.",
    "published": "2025-09-05T21:39:53Z",
    "updated": "2026-01-05T18:34:15Z",
    "link": "http://arxiv.org/pdf/2509.05510v2.pdf",
    "category": [
      "physics.comp-ph",
      "cs.LG"
    ],
    "authors": [
      "Tyler E. Maltba",
      "Ben S. Southworth",
      "Jeffrey R. Haack",
      "Marc L. Klasky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02324v1",
    "title": "Hunting for \"Oddballs\" with Machine Learning: Detecting Anomalous Exoplanets Using a Deep-Learned Low-Dimensional Representation of Transit Spectra with Autoencoders",
    "summary": "This study explores the application of autoencoder-based machine learning techniques for anomaly detection to identify exoplanet atmospheres with unconventional chemical signatures using a low-dimensional data representation. We use the Atmospheric Big Challenge (ABC) database, a publicly available dataset with over 100,000 simulated exoplanet spectra, to construct an anomaly detection scenario by defining CO2-rich atmospheres as anomalies and CO2-poor atmospheres as the normal class. We benchmarked four different anomaly detection strategies: Autoencoder Reconstruction Loss, One-Class Support Vector Machine (1 class-SVM), K-means Clustering, and Local Outlier Factor (LOF). Each method was evaluated in both the original spectral space and the autoencoder's latent space using Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC) metrics. To test the performance of the different methods under realistic conditions, we introduced Gaussian noise levels ranging from 10 to 50 ppm. Our results indicate that anomaly detection is consistently more effective when performed within the latent space across all noise levels. Specifically, K-means clustering in the latent space emerged as a stable and high-performing method. We demonstrate that this anomaly detection approach is robust to noise levels up to 30 ppm (consistent with realistic space-based observations) and remains viable even at 50 ppm when leveraging latent space representations. On the other hand, the performance of the anomaly detection methods applied directly in the raw spectral space degrades significantly with increasing the level of noise. This suggests that autoencoder-driven dimensionality reduction offers a robust methodology for flagging chemically anomalous targets in large-scale surveys where exhaustive retrievals are computationally prohibitive.",
    "published": "2026-01-05T18:15:53Z",
    "updated": "2026-01-05T18:15:53Z",
    "link": "http://arxiv.org/pdf/2601.02324v1.pdf",
    "category": [
      "astro-ph.EP",
      "astro-ph.IM",
      "cs.LG"
    ],
    "authors": [
      "Alexander Roman",
      "Emilie Panek",
      "Roy T. Forestano",
      "Eyup B. Unlu",
      "Katia Matcheva",
      "Konstantin T. Matchev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02322v1",
    "title": "Environment-Adaptive Covariate Selection: Learning When to Use Spurious Correlations for Out-of-Distribution Prediction",
    "summary": "Out-of-distribution (OOD) prediction is often approached by restricting models to causal or invariant covariates, avoiding non-causal spurious associations that may be unstable across environments. Despite its theoretical appeal, this strategy frequently underperforms empirical risk minimization (ERM) in practice. We investigate the source of this gap and show that such failures naturally arise when only a subset of the true causes of the outcome is observed. In these settings, non-causal spurious covariates can serve as informative proxies for unobserved causes and substantially improve prediction, except under distribution shifts that break these proxy relationships. Consequently, the optimal set of predictive covariates is neither universal nor necessarily exhibits invariant relationships with the outcome across all environments, but instead depends on the specific type of shift encountered. Crucially, we observe that different covariate shifts induce distinct, observable signatures in the covariate distribution itself. Moreover, these signatures can be extracted from unlabeled data in the target OOD environment and used to assess when proxy covariates remain reliable and when they fail. Building on this observation, we propose an environment-adaptive covariate selection (EACS) algorithm that maps environment-level covariate summaries to environment-specific covariate sets, while allowing the incorporation of prior causal knowledge as constraints. Across simulations and applied datasets, EACS consistently outperforms static causal, invariant, and ERM-based predictors under diverse distribution shifts.",
    "published": "2026-01-05T18:13:02Z",
    "updated": "2026-01-05T18:13:02Z",
    "link": "http://arxiv.org/pdf/2601.02322v1.pdf",
    "category": [
      "stat.ME",
      "cs.LG"
    ],
    "authors": [
      "Shuozhi Zuo",
      "Yixin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.05600v2",
    "title": "Non-omniscient backdoor injection with one poison sample: Proving the one-poison hypothesis for linear regression, linear classification, and 2-layer ReLU neural networks",
    "summary": "Backdoor poisoning attacks are a threat to machine learning models trained on large data collected from untrusted sources; these attacks enable attackers to inject malicious behavior into the model that can be triggered by specially crafted inputs. Prior work has established bounds on the success of backdoor attacks and their impact on the benign learning task, however, an open question is what amount of poison data is needed for a successful backdoor attack. Typical attacks either use few samples but need much information about the data points, or need to poison many data points.\n  In this paper, we formulate the one-poison hypothesis: An adversary with one poison sample and limited background knowledge can inject a backdoor with zero backdooring-error and without significantly impacting the benign learning task performance. Moreover, we prove the one-poison hypothesis for linear regression, linear classification, and 2-layer ReLU neural networks. For adversaries that utilize a direction unused by the clean data distribution for the poison sample, we prove for linear classification and linear regression that the resulting model is functionally equivalent to a model where the poison was excluded from training. We build on prior work on statistical backdoor learning to show that in all other cases, the impact on the benign learning task is still limited. We validate our theoretical results experimentally with realistic benchmark data sets.",
    "published": "2025-08-07T17:41:33Z",
    "updated": "2026-01-05T18:07:06Z",
    "link": "http://arxiv.org/pdf/2508.05600v2.pdf",
    "category": [
      "cs.LG",
      "cs.CR"
    ],
    "authors": [
      "Thorsten Peinemann",
      "Paula Arnold",
      "Sebastian Berndt",
      "Thomas Eisenbarth",
      "Esfandiar Mohammadi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02313v1",
    "title": "Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning",
    "summary": "Coding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML), participating nodes are rewarded for accepted contributions. This incentive structure naturally gives rise to rational adversaries who act strategically rather than behaving in purely malicious ways.\n  In this paper, we first motivate the need for coding in the presence of rational adversaries, particularly in the context of outsourced computation in decentralized systems. We contrast this need with existing approaches and highlight their limitations. We then introduce the game of coding, a novel game-theoretic framework that extends coding theory to trust-minimized settings where honest nodes are not in the majority. Focusing on repetition coding, we highlight two key features of this framework: (1) the ability to achieve a non-zero probability of data recovery even when adversarial nodes are in the majority, and (2) Sybil resistance, i.e., the equilibrium remains unchanged even as the number of adversarial nodes increases. Finally, we explore scenarios in which the adversary's strategy is unknown and outline several open problems for future research.",
    "published": "2026-01-05T18:04:32Z",
    "updated": "2026-01-05T18:04:32Z",
    "link": "http://arxiv.org/pdf/2601.02313v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hanzaleh Akbari Nodehi",
      "Viveck R. Cadambe",
      "Mohammad Ali Maddah-Ali"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02310v1",
    "title": "Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay",
    "summary": "High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the 'shape' of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the 'dead-zones' being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting.",
    "published": "2026-01-05T17:59:42Z",
    "updated": "2026-01-05T17:59:42Z",
    "link": "http://arxiv.org/pdf/2601.02310v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ahmad Makinde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02307v1",
    "title": "Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck",
    "summary": "We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with Rényi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.",
    "published": "2026-01-05T17:49:39Z",
    "updated": "2026-01-05T17:49:39Z",
    "link": "http://arxiv.org/pdf/2601.02307v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Dina El Zein",
      "James Henderson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.04847v3",
    "title": "Grounded Test-Time Adaptation for LLM Agents",
    "summary": "Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.",
    "published": "2025-11-06T22:24:35Z",
    "updated": "2026-01-05T17:43:48Z",
    "link": "http://arxiv.org/pdf/2511.04847v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Arthur Chen",
      "Zuxin Liu",
      "Jianguo Zhang",
      "Akshara Prabhakar",
      "Zhiwei Liu",
      "Shelby Heinecke",
      "Silvio Savarese",
      "Victor Zhong",
      "Caiming Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.15991v2",
    "title": "Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning",
    "summary": "The emerging field of Quantum Machine Learning (QML) has shown promising advantages in accelerating processing speed and effectively handling the high dimensionality associated with complex datasets. Quantum Computing (QC) enables more efficient data manipulation through the quantum properties of superposition and entanglement. In this paper, we present a novel approach combining quantum and classical machine learning techniques to explore the impact of quantum properties for anomaly detection in Automatic Dependent Surveillance-Broadcast (ADS-B) data. We compare the performance of a Hybrid-Fully Connected Quantum Neural Network (H-FQNN) with different loss functions and use a publicly available ADS-B dataset to evaluate the performance. The results demonstrate competitive performance in detecting anomalies, with accuracies ranging from 90.17% to 94.05%, comparable to the performance of a traditional Fully Connected Neural Network (FNN) model, which achieved accuracies between 91.50% and 93.37%.",
    "published": "2025-09-19T13:59:36Z",
    "updated": "2026-01-05T16:58:16Z",
    "link": "http://arxiv.org/pdf/2509.15991v2.pdf",
    "category": [
      "quant-ph",
      "cs.LG"
    ],
    "authors": [
      "Rani Naaman",
      "Felipe Gohring de Magalhaes",
      "Jean-Yves Ouattara",
      "Gabriela Nicolescu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02265v1",
    "title": "Predicting Early and Complete Drug Release from Long-Acting Injectables Using Explainable Machine Learning",
    "summary": "Polymer-based long-acting injectables (LAIs) have transformed the treatment of chronic diseases by enabling controlled drug delivery, thus reducing dosing frequency and extending therapeutic duration. Achieving controlled drug release from LAIs requires extensive optimization of the complex underlying physicochemical properties. Machine learning (ML) can accelerate LAI development by modeling the complex relationships between LAI properties and drug release. However, recent ML studies have provided limited information on key properties that modulate drug release, due to the lack of custom modeling and analysis tailored to LAI data. This paper presents a novel data transformation and explainable ML approach to synthesize actionable information from 321 LAI formulations by predicting early drug release at 24, 48, and 72 hours, classification of release profile types, and prediction of complete release profiles. These three experiments investigate the contribution and control of LAI material characteristics in early and complete drug release profiles. A strong correlation (>0.65) is observed between the true and predicted drug release in 72 hours, while a 0.87 F1-score is obtained in classifying release profile types. A time-independent ML framework predicts delayed biphasic and triphasic curves with better performance than current time-dependent approaches. Shapley additive explanations reveal the relative influence of material characteristics during early and for complete release which fill several gaps in previous in-vitro and ML-based studies. The novel approach and findings can provide a quantitative strategy and recommendations for scientists to optimize the drug-release dynamics of LAI. The source code for the model implementation is publicly available.",
    "published": "2026-01-05T16:49:17Z",
    "updated": "2026-01-05T16:49:17Z",
    "link": "http://arxiv.org/pdf/2601.02265v1.pdf",
    "category": [
      "q-bio.BM",
      "cs.LG"
    ],
    "authors": [
      "Karla N. Robles",
      "Manar D. Samad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.11143v4",
    "title": "Development of a high-resolution indoor radon map using a new machine learning-based probabilistic model and German radon survey data",
    "summary": "Accurate knowledge of indoor radon concentration is crucial for assessing radon-related health effects or identifying radon-prone areas. Indoor radon concentration at the national scale is usually estimated on the basis of extensive measurement campaigns. However, characteristics of the sampled households often differ from the characteristics of the target population owing to the large number of relevant factors that control the indoor radon concentration, such as the availability of geogenic radon or floor level. We propose a model-based approach that allows a more realistic estimation of indoor radon distribution with a higher spatial resolution than a purely data-based approach. A modeling approach was used by applying a quantile regression forest to estimate the probability distribution function of indoor radon for each floor level of each residential building in Germany. Based on the estimated probability distribution function,a probabilistic Monte Carlo sampling technique was applied, enabling the combination and population weighting of floor-level predictions. In this way,the uncertainty of the individual predictions is effectively propagated into the estimate of variability at the aggregated level. The results show an approximate lognormal distribution of indoor radon in dwellings in Germany with an arithmetic mean of 63 Bq/m3, a geometric mean of 41 Bq/m3, and a 95th percentile of 180 Bq/m3. The exceedance probabilities for 100 and 300 Bq/m3 are 12.5% (10.5 million people affected) and 2.2 % (1.9 million people affected), respectively. The advantages of our approach are that it yields a) an accurate estimation of indoor radon concentration even if the survey is not fully representative with respect to floor level and radon concentration in soil, and b) an estimate of the indoor radon distribution with a much higher spatial resolution than basic descriptive statistics.",
    "published": "2023-10-17T10:51:05Z",
    "updated": "2026-01-05T16:47:03Z",
    "link": "http://arxiv.org/pdf/2310.11143v4.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "physics.data-an"
    ],
    "authors": [
      "Eric Petermann",
      "Peter Bossew",
      "Joachim Kemski",
      "Valeria Gruber",
      "Nils Suhr",
      "Bernd Hoffmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02264v1",
    "title": "POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network",
    "summary": "Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at https://huggingface.co/datasets/BorisKriuk/Poseidon, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research.",
    "published": "2026-01-05T16:46:34Z",
    "updated": "2026-01-05T16:46:34Z",
    "link": "http://arxiv.org/pdf/2601.02264v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Boris Kriuk",
      "Fedor Kriuk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.09503v4",
    "title": "Towards Fair In-Context Learning with Tabular Foundation Models",
    "summary": "Transformer-based tabular foundation models have recently demonstrated promising in-context learning (ICL) performance on structured data, emerging as competitive alternatives to gradient-boosted trees. However, the fairness implications of this new paradigm remain largely unexplored. We present the first investigation of fairness in tabular ICL, evaluating three recently proposed foundation models--TabPFNv2, TabICL, and TabDPT--on multiple benchmark datasets. To mitigate biases, we explore three pre-processing fairness-enhancing methods: correlation removal (decorrelating input features from the sensitive attribute), group-balanced sample selection (ensuring equal representation of protected groups in context examples), and uncertainty-based sample selection (prioritizing context examples with high sensitive-attribute prediction uncertainty). Our experiments show that the uncertainty-based strategy consistently improves group fairness metrics (e.g., demographic parity, equalized odds, and equal opportunity) with minimal impact on predictive accuracy. We release our code to facilitate reproducibility https://github.com/patrikken/Fair-TabICL.",
    "published": "2025-05-14T15:53:14Z",
    "updated": "2026-01-05T16:39:29Z",
    "link": "http://arxiv.org/pdf/2505.09503v4.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Patrik Kenfack",
      "Samira Ebrahimi Kahou",
      "Ulrich Aïvodji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.18499v4",
    "title": "Training More Robust Classification Model via Discriminative Loss and Gaussian Noise Injection",
    "summary": "Robustness of deep neural networks to input noise remains a critical challenge, as naive noise injection often degrades accuracy on clean (uncorrupted) data. We propose a novel training framework that addresses this trade-off through two complementary objectives. First, we introduce a loss function applied at the penultimate layer that explicitly enforces intra-class compactness and increases the margin to analytically defined decision boundaries. This enhances feature discriminativeness and class separability for clean data. Second, we propose a class-wise feature alignment mechanism that brings noisy data clusters closer to their clean counterparts. Furthermore, we provide a theoretical analysis demonstrating that improving feature stability under additive Gaussian noise implicitly reduces the curvature of the softmax loss landscape in input space, as measured by Hessian eigenvalues.This thus naturally enhances robustness without explicit curvature penalties. Conversely, we also theoretically show that lower curvatures lead to more robust models. We validate the effectiveness of our method on standard benchmarks and our custom dataset. Our approach significantly reinforces model robustness to various perturbations while maintaining high accuracy on clean data, advancing the understanding and practice of noise-robust deep learning.",
    "published": "2024-05-28T18:10:45Z",
    "updated": "2026-01-05T16:38:03Z",
    "link": "http://arxiv.org/pdf/2405.18499v4.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Hai-Vy Nguyen",
      "Fabrice Gamboa",
      "Sixin Zhang",
      "Reda Chhaibi",
      "Serge Gratton",
      "Thierry Giaccone"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02257v1",
    "title": "Improved Accuracy for Private Continual Cardinality Estimation in Fully Dynamic Streams via Matrix Factorization",
    "summary": "We study differentially-private statistics in the fully dynamic continual observation model, where many updates can arrive at each time step and updates to a stream can involve both insertions and deletions of an item. Earlier work (e.g., Jain et al., NeurIPS 2023 for counting distinct elements; Raskhodnikova & Steiner, PODS 2025 for triangle counting with edge updates) reduced the respective cardinality estimation problem to continual counting on the difference stream associated with the true function values on the input stream. In such reductions, a change in the original stream can cause many changes in the difference stream, this poses a challenge for applying private continual counting algorithms to obtain optimal error bounds. We improve the accuracy of several such reductions by studying the associated $\\ell_p$-sensitivity vectors of the resulting difference streams and isolating their properties.\n  We demonstrate that our framework gives improved bounds for counting distinct elements, estimating degree histograms, and estimating triangle counts (under a slightly relaxed privacy model), thus offering a general approach to private continual cardinality estimation in streaming settings. Our improved accuracy stems from tight analysis of known factorization mechanisms for the counting matrix in this setting; the key technical challenge is arguing that one can use state-of-the-art factorizations for sensitivity vector sets with the properties we isolate. Empirically and analytically, we demonstrate that our improved error bounds offer a substantial improvement in accuracy for cardinality estimation problems over a large range of parameters.",
    "published": "2026-01-05T16:36:59Z",
    "updated": "2026-01-05T16:36:59Z",
    "link": "http://arxiv.org/pdf/2601.02257v1.pdf",
    "category": [
      "cs.CR",
      "cs.DS",
      "cs.LG"
    ],
    "authors": [
      "Joel Daniel Andersson",
      "Palak Jain",
      "Satchit Sivakumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.20762v2",
    "title": "Subgroup Discovery with the Cox Model",
    "summary": "We study the problem of subgroup discovery for survival analysis, where the goal is to find an interpretable subset of the data on which a Cox model is highly accurate. Our work is the first to study this particular subgroup problem, for which we make several contributions.\n  Subgroup discovery methods generally require a \"quality function\" in order to sift through and select the most advantageous subgroups. We first examine why existing natural choices for quality functions are insufficient to solve the subgroup discovery problem for the Cox model. To address the shortcomings of existing metrics, we introduce two technical innovations: the *expected prediction entropy (EPE)*, a novel metric for evaluating survival models which predict a hazard function; and the *conditional rank statistics (CRS)*, a statistical object which quantifies the deviation of an individual point to the distribution of survival times in an existing subgroup. We study the EPE and CRS theoretically and show that they can solve many of the problems with existing metrics.\n  We introduce a total of eight algorithms for the Cox subgroup discovery problem. The main algorithm is able to take advantage of both the EPE and the CRS, allowing us to give theoretical correctness results for this algorithm in a well-specified setting. We evaluate all of the proposed methods empirically on both synthetic and real data. The experiments confirm our theory, showing that our contributions allow for the recovery of a ground-truth subgroup in well-specified cases, as well as leading to better model fit compared to naively fitting the Cox model to the whole dataset in practical settings. Lastly, we conduct a case study on jet engine simulation data from NASA. The discovered subgroups uncover known nonlinearities/homogeneity in the data, and which suggest design choices which have been mirrored in practice.",
    "published": "2025-12-23T20:49:05Z",
    "updated": "2026-01-05T16:29:26Z",
    "link": "http://arxiv.org/pdf/2512.20762v2.pdf",
    "category": [
      "cs.LG",
      "math.ST",
      "stat.ML"
    ],
    "authors": [
      "Zachary Izzo",
      "Iain Melvin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02241v1",
    "title": "From Mice to Trains: Amortized Bayesian Inference on Graph Data",
    "summary": "Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration.",
    "published": "2026-01-05T16:16:28Z",
    "updated": "2026-01-05T16:16:28Z",
    "link": "http://arxiv.org/pdf/2601.02241v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Svenja Jedhoff",
      "Elizaveta Semenova",
      "Aura Raulo",
      "Anne Meyer",
      "Paul-Christian Bürkner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02232v1",
    "title": "ELLA: Efficient Lifelong Learning for Adapters in Large Language Models",
    "summary": "Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\\%$ and a $35\\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.",
    "published": "2026-01-05T15:58:08Z",
    "updated": "2026-01-05T15:58:08Z",
    "link": "http://arxiv.org/pdf/2601.02232v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shristi Das Biswas",
      "Yue Zhang",
      "Anwesan Pal",
      "Radhika Bhargava",
      "Kaushik Roy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.04665v2",
    "title": "Perch 2.0: The Bittern Lesson for Bioacoustics",
    "summary": "Perch is a performant pre-trained model for bioacoustics. It was trained in supervised fashion, providing both off-the-shelf classification scores for thousands of vocalizing species as well as strong embeddings for transfer learning. In this new release, Perch 2.0, we expand from training exclusively on avian species to a large multi-taxa dataset. The model is trained with self-distillation using a prototype-learning classifier as well as a new source-prediction training criterion. Perch 2.0 obtains state-of-the-art performance on the BirdSet and BEANS benchmarks. It also outperforms specialized marine models on marine transfer learning tasks, despite having almost no marine training data. We present hypotheses as to why fine-grained species classification is a particularly robust pre-training task for bioacoustics.",
    "published": "2025-08-06T17:34:43Z",
    "updated": "2026-01-05T15:38:05Z",
    "link": "http://arxiv.org/pdf/2508.04665v2.pdf",
    "category": [
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Bart van Merriënboer",
      "Vincent Dumoulin",
      "Jenny Hamer",
      "Lauren Harrell",
      "Andrea Burns",
      "Tom Denton"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02213v1",
    "title": "Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction",
    "summary": "Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry.",
    "published": "2026-01-05T15:36:04Z",
    "updated": "2026-01-05T15:36:04Z",
    "link": "http://arxiv.org/pdf/2601.02213v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Haoyu Zhou",
      "Ping Xue",
      "Tianfan Fu",
      "Hao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.19815v2",
    "title": "Bayesian uncertainty-aware deep learning with noisy labels: Tackling annotation ambiguity in EEG seizure detection",
    "summary": "Deep learning is advancing EEG processing for automated epileptic seizure detection and onset zone localization, yet its performance relies heavily on high-quality annotated training data. However, scalp EEG is susceptible to high noise levels, which in turn leads to imprecise annotations of the seizure timing and characteristics. This \"label noise\" presents a significant challenge in model training and generalization. In this paper, we introduce Bayesian UncertaiNty-aware Deep Learning (BUNDL), a novel algorithm that informs a deep learning model of label ambiguities, thereby enhancing the robustness of seizure detection systems. By integrating domain knowledge into an underlying Bayesian framework, we derive a novel KL-divergence-based loss function that capitalizes on uncertainty to better learn seizure characteristics from scalp EEG. Thus, BUNDL offers a straightforward and model-agnostic method for training deep neural networks with noisy training labels that does not add any parameters to existing architectures. Additionally, we explore the impact of improved detection system on the task of automated onset zone localization. We validate BUNDL using a comprehensive simulated EEG dataset and two publicly available datasets collected by Temple University Hospital (TUH) and Boston Children's Hospital (CHB-MIT). Results show that BUNDL consistently identifies noisy labels and improves the robustness of three base models under various label noise conditions. We also evaluate cross-site generalizability and quantify computational cost of all methods. Ultimately, BUNDL presents as a reliable method that can be seamlessly integrated with existing deep models used in clinical practice, enabling the training of trustworthy models for epilepsy evaluation.",
    "published": "2024-10-17T21:19:39Z",
    "updated": "2026-01-05T15:34:26Z",
    "link": "http://arxiv.org/pdf/2410.19815v2.pdf",
    "category": [
      "eess.SP",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Deeksha M. Shama",
      "Archana Venkataraman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.19206v2",
    "title": "Matrix Manifold Neural Networks++",
    "summary": "Deep neural networks (DNNs) on Riemannian manifolds have garnered increasing interest in various applied areas. For instance, DNNs on spherical and hyperbolic manifolds have been designed to solve a wide range of computer vision and nature language processing tasks. One of the key factors that contribute to the success of these networks is that spherical and hyperbolic manifolds have the rich algebraic structures of gyrogroups and gyrovector spaces. This enables principled and effective generalizations of the most successful DNNs to these manifolds. Recently, some works have shown that many concepts in the theory of gyrogroups and gyrovector spaces can also be generalized to matrix manifolds such as Symmetric Positive Definite (SPD) and Grassmann manifolds. As a result, some building blocks for SPD and Grassmann neural networks, e.g., isometric models and multinomial logistic regression (MLR) can be derived in a way that is fully analogous to their spherical and hyperbolic counterparts. Building upon these works, we design fully-connected (FC) and convolutional layers for SPD neural networks. We also develop MLR on Symmetric Positive Semi-definite (SPSD) manifolds, and propose a method for performing backpropagation with the Grassmann logarithmic map in the projector perspective. We demonstrate the effectiveness of the proposed approach in the human action recognition and node classification tasks.",
    "published": "2024-05-29T15:47:35Z",
    "updated": "2026-01-05T15:31:00Z",
    "link": "http://arxiv.org/pdf/2405.19206v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Xuan Son Nguyen",
      "Shuo Yang",
      "Aymeric Histace"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.24601v2",
    "title": "Comparison of generalised additive models and neural networks in applications: A systematic review",
    "summary": "Neural networks have become a popular tool in predictive modelling, more commonly associated with machine learning and artificial intelligence than with statistics. Generalised Additive Models (GAMs) are flexible non-linear statistical models that retain interpretability. Both are state-of-the-art in their own right, with their respective advantages and disadvantages. This paper analyses how these two model classes have performed on real-world tabular data. Following PRISMA guidelines, we conducted a systematic review of papers that performed empirical comparisons of GAMs and neural networks. Eligible papers were identified, yielding 143 papers, with 430 datasets. Key attributes at both paper and dataset levels were extracted and reported. Beyond summarising comparisons, we analyse reported performance metrics using mixed-effects modelling to investigate potential characteristics that can explain and quantify observed differences, including application area, study year, sample size, number of predictors, and neural network complexity. Across datasets, no consistent evidence of superiority was found for either GAMs or neural networks when considering the most frequently reported metrics (RMSE, $R^2$, and AUC). Neural networks tended to outperform in larger datasets and in those with more predictors, but this advantage narrowed over time. Conversely, GAMs remained competitive, particularly in smaller data settings, while retaining interpretability. Reporting of dataset characteristics and neural network complexity was incomplete in much of the literature, limiting transparency and reproducibility. This review highlights that GAMs and neural networks should be viewed as complementary approaches rather than competitors. For many tabular applications, the performance trade-off is modest, and interpretability may favour GAMs.",
    "published": "2025-10-28T16:28:42Z",
    "updated": "2026-01-05T15:20:15Z",
    "link": "http://arxiv.org/pdf/2510.24601v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Jessica Doohan",
      "Lucas Kook",
      "Kevin Burke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02196v1",
    "title": "ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense",
    "summary": "Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.",
    "published": "2026-01-05T15:18:54Z",
    "updated": "2026-01-05T15:18:54Z",
    "link": "http://arxiv.org/pdf/2601.02196v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yu Li",
      "Sizhe Tang",
      "Rongqian Chen",
      "Fei Xu Yu",
      "Guangyu Jiang",
      "Mahdi Imani",
      "Nathaniel D. Bastian",
      "Tian Lan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02193v1",
    "title": "Learning with Monotone Adversarial Corruptions",
    "summary": "We study the extent to which standard machine learning algorithms rely on exchangeability and independence of data by introducing a monotone adversarial corruption model. In this model, an adversary, upon looking at a \"clean\" i.i.d. dataset, inserts additional \"corrupted\" points of their choice into the dataset. These added points are constrained to be monotone corruptions, in that they get labeled according to the ground-truth target function. Perhaps surprisingly, we demonstrate that in this setting, all known optimal learning algorithms for binary classification can be made to achieve suboptimal expected error on a new independent test point drawn from the same distribution as the clean dataset. On the other hand, we show that uniform convergence-based algorithms do not degrade in their guarantees. Our results showcase how optimal learning algorithms break down in the face of seemingly helpful monotone corruptions, exposing their overreliance on exchangeability.",
    "published": "2026-01-05T15:16:26Z",
    "updated": "2026-01-05T15:16:26Z",
    "link": "http://arxiv.org/pdf/2601.02193v1.pdf",
    "category": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "authors": [
      "Kasper Green Larsen",
      "Chirag Pabbaraju",
      "Abhishek Shetty"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.09166v3",
    "title": "Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates",
    "summary": "The coarse spatial resolution of gridded climate models, such as general circulation models, limits their direct use in projecting socially relevant variables like extreme precipitation. Most downscaling methods estimate the conditional distributions of extremes by generating large ensembles, complicating the assessment of robustness under distributional transformations, such as those induced by climate change. To better understand and potentially improve robustness, we propose super-resolving the parameters of the target variable's probability distribution directly using analytically tractable mappings. Within a perfect-model framework over Switzerland, we demonstrate that vector generalized linear and additive models can super-resolve the generalized extreme value distribution of summer hourly precipitation extremes from coarse precipitation fields and topography. We introduce the notion of a \"robustness gap\", defined as the difference in predictive error between present-trained and future-trained models, and use it to diagnose how model structure affects the generalization of each quantile to a pseudo-global warming scenario. By evaluating multiple model configurations, we also identify an upper limit on the super-resolution factor based on the spatial auto- and cross-correlation of precipitation and elevation, beyond which coarse precipitation loses predictive value. Our framework is broadly applicable to variables governed by parametric distributions and offers a model-agnostic diagnostic for understanding when and why empirical downscaling generalizes to climate change and extremes.",
    "published": "2025-07-12T07:04:07Z",
    "updated": "2026-01-05T14:58:49Z",
    "link": "http://arxiv.org/pdf/2507.09166v3.pdf",
    "category": [
      "physics.ao-ph",
      "cs.LG"
    ],
    "authors": [
      "Louise Largeau",
      "Tom Beucler",
      "David Leutwyler",
      "Gregoire Mariethoz",
      "Valerie Chavez-Demoulin",
      "Erwan Koch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.20692v2",
    "title": "The Human Brain as a Combinatorial Complex",
    "summary": "We propose a framework for constructing combinatorial complexes (CCs) from fMRI time series data that captures both pairwise and higher-order neural interactions through information-theoretic measures, bridging topological deep learning and network neuroscience. Current graph-based representations of brain networks systematically miss the higher-order dependencies that characterize neural complexity, where information processing often involves synergistic interactions that cannot be decomposed into pairwise relationships. Unlike topological lifting approaches that map relational structures into higher-order domains, our method directly constructs CCs from statistical dependencies in the data. Our CCs generalize graphs by incorporating higher-order cells that represent collective dependencies among brain regions, naturally accommodating the multi-scale, hierarchical nature of neural processing. The framework constructs data-driven combinatorial complexes using O-information and S-information measures computed from fMRI signals, preserving both pairwise connections and higher-order cells (e.g., triplets, quadruplets) based on synergistic dependencies. Using NetSim simulations as a controlled proof-of-concept dataset, we demonstrate our CC construction pipeline and show how both pairwise and higher-order dependencies in neural time series can be quantified and represented within a unified structure. This work provides a framework for brain network representation that preserves fundamental higher-order structure invisible to traditional graph methods, and enables the application of topological deep learning (TDL) architectures to neural data.",
    "published": "2025-11-22T19:04:13Z",
    "updated": "2026-01-05T14:58:49Z",
    "link": "http://arxiv.org/pdf/2511.20692v2.pdf",
    "category": [
      "q-bio.NC",
      "cs.LG"
    ],
    "authors": [
      "Valentina Sánchez",
      "Çiçek Güven",
      "Koen Haak",
      "Theodore Papamarkou",
      "Gonzalo Nápoles",
      "Marie Šafář Postma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02145v1",
    "title": "Feature-based Inversion of 2.5D Controlled Source Electromagnetic Data using Generative Priors",
    "summary": "In this study, we investigate feature-based 2.5D controlled source marine electromagnetic (mCSEM) data inversion using generative priors. Two-and-half dimensional modeling using finite difference method (FDM) is adopted to compute the response of horizontal electric dipole (HED) excitation. Rather than using a neural network to approximate the entire inverse mapping in a black-box manner, we adopt a plug-andplay strategy in which a variational autoencoder (VAE) is used solely to learn prior information on conductivity distributions. During the inversion process, the conductivity model is iteratively updated using the Gauss Newton method, while the model space is constrained by projections onto the learned VAE decoder. This framework preserves explicit control over data misfit and enables flexible adaptation to different survey configurations. Numerical and field experiments demonstrate that the proposed approach effectively incorporates prior information, improves reconstruction accuracy, and exhibits good generalization performance.",
    "published": "2026-01-05T14:18:14Z",
    "updated": "2026-01-05T14:18:14Z",
    "link": "http://arxiv.org/pdf/2601.02145v1.pdf",
    "category": [
      "physics.geo-ph",
      "cs.LG"
    ],
    "authors": [
      "Hongyu Zhou",
      "Haoran Sun",
      "Rui Guo",
      "Maokun Li",
      "Fan Yang",
      "Shenheng Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02138v1",
    "title": "Edge-aware GAT-based protein binding site prediction",
    "summary": "Accurate identification of protein binding sites is crucial for understanding biomolecular interaction mechanisms and for the rational design of drug targets. Traditional predictive methods often struggle to balance prediction accuracy with computational efficiency when capturing complex spatial conformations. To address this challenge, we propose an Edge-aware Graph Attention Network (Edge-aware GAT) model for the fine-grained prediction of binding sites across various biomolecules, including proteins, DNA/RNA, ions, ligands, and lipids. Our method constructs atom-level graphs and integrates multidimensional structural features, including geometric descriptors, DSSP-derived secondary structure, and relative solvent accessibility (RSA), to generate spatially aware embedding vectors. By incorporating interatomic distances and directional vectors as edge features within the attention mechanism, the model significantly enhances its representation capacity. On benchmark datasets, our model achieves an ROC-AUC of 0.93 for protein-protein binding site prediction, outperforming several state-of-the-art methods. The use of directional tensor propagation and residue-level attention pooling further improves both binding site localization and the capture of local structural details. Visualizations using PyMOL confirm the model's practical utility and interpretability. To facilitate community access and application, we have deployed a publicly accessible web server at http://119.45.201.89:5000/. In summary, our approach offers a novel and efficient solution that balances prediction accuracy, generalization, and interpretability for identifying functional sites in proteins.",
    "published": "2026-01-05T14:09:57Z",
    "updated": "2026-01-05T14:09:57Z",
    "link": "http://arxiv.org/pdf/2601.02138v1.pdf",
    "category": [
      "cs.LG",
      "q-bio.QM"
    ],
    "authors": [
      "Weisen Yang",
      "Hanqing Zhang",
      "Wangren Qiu",
      "Xuan Xiao",
      "Weizhong Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.09408v3",
    "title": "Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping",
    "summary": "Machine learning and geostatistics are two fundamentally different frameworks for predicting and spatially mapping soil properties. Geostatistics leverages the spatial structure of soil properties, while machine learning captures the relationship between available environmental features and soil properties. We propose a hybrid framework that enriches ML with spatial context through engineering of 'spatial lag' features from ordinary kriging. We call this approach 'kriging prior regression' (KpR), as it follows the inverse logic of regression kriging. To evaluate this approach, we assessed both the point and probabilistic prediction performance of KpR, using the TabPFN model across six fieldscale datasets from LimeSoDa. These datasets included soil organic carbon, clay content, and pH, along with features derived from remote sensing and in-situ proximal soil sensing. KpR with TabPFN demonstrated reliable uncertainty estimates and more accurate predictions in comparison to several other spatial techniques (e.g., regression/residual kriging with TabPFN), as well as to established non-spatial machine learning algorithms (e.g., random forest). Most notably, it significantly improved the average R2 by around 30% compared to machine learning algorithms without spatial context. This improvement was due to the strong prediction performance of the TabPFN algorithm itself and the complementary spatial information provided by KpR features. TabPFN is particularly effective for prediction tasks with small sample sizes, common in precision agriculture, whereas KpR can compensate for weak relationships between sensing features and soil properties when proximal soil sensing data are limited. Hence, we conclude that KpR with TabPFN is a very robust and versatile modelling framework for digital soil mapping in precision agriculture.",
    "published": "2025-09-11T12:43:07Z",
    "updated": "2026-01-05T14:07:44Z",
    "link": "http://arxiv.org/pdf/2509.09408v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jonas Schmidinger",
      "Viacheslav Barkov",
      "Sebastian Vogel",
      "Martin Atzmueller",
      "Gerard B M Heuvelink"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.23869v3",
    "title": "Gibbs randomness-compression proposition: An efficient deep learning",
    "summary": "A proposition that connects randomness and compression is put forward via Gibbs entropy over set of measurement vectors associated with a compression process. The proposition states that a lossy compression process is equivalent to {\\it directed randomness} that preserves information content. The proposition originated from the observed behavior in newly proposed {\\it Dual Tomographic Compression} (DTC) compress-train framework. This is akin to tomographic reconstruction of layer weight matrices via building compressed sensed projections, via so-called {\\it weight rays}. This tomographic approach is applied to previous and next layers in a dual fashion, that triggers neuronal-level pruning. This novel model compress-train scheme appears in iterative fashion and acts as a smart neural architecture search: also called {\\it compression aware training}. The experiments demonstrated the utility of this dual-tomography during training: method accelerates and supports lottery ticket hypothesis. However, random compress-train iterations having similar performance demonstrated the connection between randomness and compression from statistical physics perspective, we formulated the so-called {\\it Gibbs randomness-compression proposition}, signifying randomness-compression relationship via Gibbs entropy. The proposition is supported with the experimental evidence, resulting in very high correlation between learning performance vs. Gibbs entropy over compression ratios. Practically, the DTC framework provides a promising approach for massively energy- and resource-efficient deep learning training.",
    "published": "2025-05-29T10:48:35Z",
    "updated": "2026-01-05T14:07:39Z",
    "link": "http://arxiv.org/pdf/2505.23869v3.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "M. Süzen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.19810v3",
    "title": "Ambiguous Online Learning",
    "summary": "We propose a new variant of online learning that we call \"ambiguous online learning\". In this setting, the learner is allowed to produce multiple predicted labels. Such an \"ambiguous prediction\" is considered correct when at least one of the labels is correct, and none of the labels are \"predictably wrong\". The definition of \"predictably wrong\" comes from a hypothesis class in which hypotheses are also multi-valued. Thus, a prediction is \"predictably wrong\" if it's not allowed by the (unknown) true hypothesis. In particular, this setting is natural in the context of multivalued dynamical systems, recommendation algorithms and lossless compression. It is also strongly related to so-called \"apple tasting\". We show that in this setting, there is a trichotomy of mistake bounds: up to logarithmic factors, any hypothesis class has an optimal mistake bound of either Theta(1), Theta(sqrt(N)) or N.",
    "published": "2025-06-24T17:22:45Z",
    "updated": "2026-01-05T14:04:20Z",
    "link": "http://arxiv.org/pdf/2506.19810v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Vanessa Kosoy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02106v1",
    "title": "Prototype-Based Learning for Healthcare: A Demonstration of Interpretable AI",
    "summary": "Despite recent advances in machine learning and explainable AI, a gap remains in personalized preventive healthcare: predictions, interventions, and recommendations should be both understandable and verifiable for all stakeholders in the healthcare sector. We present a demonstration of how prototype-based learning can address these needs. Our proposed framework, ProtoPal, features both front- and back-end modes; it achieves superior quantitative performance while also providing an intuitive presentation of interventions and their simulated outcomes.",
    "published": "2026-01-05T13:34:01Z",
    "updated": "2026-01-05T13:34:01Z",
    "link": "http://arxiv.org/pdf/2601.02106v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ashish Rana",
      "Ammar Shaker",
      "Sascha Saralajew",
      "Takashi Suzuki",
      "Kosuke Yasuda",
      "Shintaro Kato",
      "Toshikazu Wada",
      "Toshiyuki Fujikawa",
      "Toru Kikutsuji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02094v1",
    "title": "Horizon Activation Mapping for Neural Networks in Time Series Forecasting",
    "summary": "Neural networks for time series forecasting have relied on error metrics and architecture-specific interpretability approaches for model selection that don't apply across models of different families. To interpret forecasting models agnostic to the types of layers across state-of-the-art model families, we introduce Horizon Activation Mapping (HAM), a visual interpretability technique inspired by grad-CAM that uses gradient norm averages to study the horizon's subseries where grad-CAM studies attention maps over image data. We introduce causal and anti-causal modes to calculate gradient update norm averages across subseries at every timestep and lines of proportionality signifying uniform distributions of the norm averages. Optimization landscape studies with respect to changes in batch sizes, early stopping, train-val-test splits, univariate forecasting and dropouts are studied with respect to performances and subseries in HAM. Interestingly, batch size based differences in activities seem to indicate potential for existence of an exponential approximation across them per epoch relative to each other. Multivariate forecasting models including MLP-based CycleNet, N-Linear, N-HITS, self attention-based FEDformer, Pyraformer, SSM-based SpaceTime and diffusion-based Multi-Resolution DDPM over different horizon sizes trained over the ETTm2 dataset are used for HAM plots in this study. NHITS' neural approximation theorem and SpaceTime's exponential autoregressive activities have been attributed to trends in HAM plots over their training, validation and test sets. In general, HAM can be used for granular model selection, validation set choices and comparisons across different neural network model families.",
    "published": "2026-01-05T13:21:30Z",
    "updated": "2026-01-05T13:21:30Z",
    "link": "http://arxiv.org/pdf/2601.02094v1.pdf",
    "category": [
      "cs.LG",
      "math.FA"
    ],
    "authors": [
      "Hans Krupakar",
      "V A Kandappan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02081v1",
    "title": "A Differentiable Adversarial Framework for Task-Aware Data Subsampling",
    "summary": "The proliferation of large-scale datasets poses a major computational challenge to model training. The traditional data subsampling method works as a static, task independent preprocessing step which usually discards information that is critical to downstream prediction. In this paper, we introduces the antagonistic soft selection subsampling (ASSS) framework as is a novel paradigm that reconstructs data reduction into a differentiable end-to-end learning problem. ASSS uses the adversarial game between selector network and task network, and selector network learning assigns continuous importance weights to samples. This direct optimization implemented by Gumbel-Softmax relaxation allows the selector to identify and retain samples with the maximum amount of information for a specific task target under the guidance of the loss function that balances the fidelity and sparsity of the prediction. Theoretical analysis links this framework with the information bottleneck principle. Comprehensive experiments on four large-scale real world datasets show that ASSS has always been better than heuristic subsampling baselines such as clustering and nearest neighbor thinning in maintaining model performance. It is worth noting that ASSS can not only match, but also sometimes exceed the training performance of the entire dataset, showcasing the effect of intelligent denoising. This work establishes task aware data subsampling as a learnable component, providing a principled solution for effective large-scale data learning.",
    "published": "2026-01-05T13:10:09Z",
    "updated": "2026-01-05T13:10:09Z",
    "link": "http://arxiv.org/pdf/2601.02081v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jiacheng Lyu",
      "Bihua Bao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.15175v3",
    "title": "A Linear Approach to Data Poisoning",
    "summary": "Backdoor and data-poisoning attacks can flip predictions with tiny training corruptions, yet a sharp theory linking poisoning strength, overparameterization, and regularization is lacking. We analyze ridge least squares with an unpenalized intercept in the high-dimensional regime \\(p,n\\to\\infty\\), \\(p/n\\to c\\). Targeted poisoning is modelled by shifting a \\(θ\\)-fraction of one class by a direction \\(\\mathbf{v}\\) and relabelling. Using resolvent techniques and deterministic equivalents from random matrix theory, we derive closed-form limits for the poisoned score explicit in the model parameters. The formulas yield scaling laws, recover the interpolation threshold as \\(c\\to1\\) in the ridgeless limit, and show that the weights align with the poisoning direction. Synthetic experiments match theory across sweeps of the parameters and MNIST backdoor tests show qualitatively consistent trends. The results provide a tractable framework for quantifying poisoning in linear models.",
    "published": "2025-05-21T06:45:06Z",
    "updated": "2026-01-05T13:05:29Z",
    "link": "http://arxiv.org/pdf/2505.15175v3.pdf",
    "category": [
      "stat.ML",
      "cs.CR",
      "cs.LG",
      "math.ST"
    ],
    "authors": [
      "Donald Flynn",
      "Diego Granziol"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.03851v2",
    "title": "Comparison of neural network training strategies for the simulation of dynamical systems",
    "summary": "Neural networks have become a widely adopted tool for modeling nonlinear dynamical systems from data. However, the choice of training strategy remains a key design decision, particularly for simulation tasks. This paper compares two predominant strategies: parallel and series-parallel training. The conducted empirical analysis spans five neural network architectures and two examples: a pneumatic valve test bench and an industrial robot benchmark. The study reveals that, even though series-parallel training dominates current practice, parallel training consistently yields better long-term prediction accuracy. Additionally, this work clarifies the often inconsistent terminology in the literature and relate both strategies to concepts from system identification. The findings suggest that parallel training should be considered the default training strategy for neural network-based simulation of dynamical systems.",
    "published": "2025-12-03T14:50:06Z",
    "updated": "2026-01-05T12:56:52Z",
    "link": "http://arxiv.org/pdf/2512.03851v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Paul Strasser",
      "Andreas Pfeffer",
      "Jakob Weber",
      "Markus Gurtner",
      "Andreas Körner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02075v1",
    "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
    "summary": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2",
    "published": "2026-01-05T12:56:51Z",
    "updated": "2026-01-05T12:56:51Z",
    "link": "http://arxiv.org/pdf/2601.02075v1.pdf",
    "category": [
      "cs.CE",
      "cs.LG"
    ],
    "authors": [
      "Zhuofan Shi",
      "Hubao A",
      "Yufei Shao",
      "Mengyan Dai",
      "Yadong Yu",
      "Pan Xiang",
      "Dongliang Huang",
      "Hongxu An",
      "Chunxiao Xin",
      "Haiyang Shen",
      "Zhenyu Wang",
      "Yunshan Na",
      "Gang Huang",
      "Xiang Jing"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.09922v3",
    "title": "Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity",
    "summary": "Euclidean diffusion models have achieved remarkable success in generative modeling across diverse domains, and they have been extended to manifold cases in recent advances. Instead of explicitly utilizing the structure of special manifolds as studied in previous works, in this paper we investigate direct sampling of the Euclidean diffusion models for general manifold-structured data. We reveal the multiscale singularity of the score function in the ambient space, which hinders the accuracy of diffusion-generated samples. We then present an elaborate theoretical analysis of the singularity structure of the score function by decomposing it along the tangential and normal directions of the manifold. To mitigate the singularity and improve the sampling accuracy, we propose two novel methods: (1) Niso-DM, which reduces the scale discrepancies in the score function by utilizing a non-isotropic noise, and (2) Tango-DM, which trains only the tangential component of the score function using a tangential-only loss function. Numerical experiments demonstrate that our methods achieve superior performance on distributions over various manifolds with complex geometries.",
    "published": "2025-05-15T03:12:27Z",
    "updated": "2026-01-05T12:56:49Z",
    "link": "http://arxiv.org/pdf/2505.09922v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zichen Liu",
      "Wei Zhang",
      "Tiejun Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.26109v2",
    "title": "Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has significantly boosted the reasoning capability of language models (LMs) recently. However, existing RLVR approaches merely train LMs based on their own generated on-policy responses and are constrained by the initial capability of LMs, thus prone to exploration stagnation, in which LMs fail to solve more training problems and cannot further learn from the training data. Some work tries to address this by leveraging off-policy solutions to training problems, but relies on external expert guidance that is limited in availability and scalability. In this work, we propose LTE (Learning to reason from Trial and Error), an approach that hints LMs with their previously self-made mistakes, not requiring any external expert guidance. Experiments validate the effectiveness of LTE, which outperforms the normal group relative policy optimization (GRPO) by 5.02 in Pass@1 and 9.96 in Pass@k on average across six mathematical reasoning benchmarks for Qwen3-8B-Base and even performs better than methods that require external gold solutions as guidance after aligning the experimental setup. Further analysis confirms that LTE successfully mitigates exploration stagnation and enhances both exploitation and exploration during training. Our code is available at https://anonymous.4open.science/r/Learning-from-Trial-and-Error.",
    "published": "2025-10-30T03:36:19Z",
    "updated": "2026-01-05T12:47:18Z",
    "link": "http://arxiv.org/pdf/2510.26109v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Chenming Tang",
      "Hsiu-Yuan Huang",
      "Weijie Liu",
      "Saiyong Yang",
      "Yunfang Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02050v1",
    "title": "Explore the Ideology of Deep Learning in ENSO Forecasts",
    "summary": "The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the \"dead\" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction.",
    "published": "2026-01-05T12:15:39Z",
    "updated": "2026-01-05T12:15:39Z",
    "link": "http://arxiv.org/pdf/2601.02050v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yanhai Gan",
      "Yipeng Chen",
      "Ning Li",
      "Xingguo Liu",
      "Junyu Dong",
      "Xianyao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02037v1",
    "title": "Multivariate Time-series Anomaly Detection via Dynamic Model Pool & Ensembling",
    "summary": "Multivariate time-series (MTS) anomaly detection is critical in domains such as service monitor, IoT, and network security. While multi-model methods based on selection or ensembling outperform single-model ones, they still face limitations: (i) selection methods rely on a single chosen model and are sensitive to the strategy; (ii) ensembling methods often combine all models or are restricted to univariate data; and (iii) most methods depend on fixed data dimensionality, limiting scalability. To address these, we propose DMPEAD, a Dynamic Model Pool and Ensembling framework for MTS Anomaly Detection. The framework first (i) constructs a diverse model pool via parameter transfer and diversity metric, then (ii) updates it with a meta-model and similarity-based strategy for adaptive pool expansion, subset selection, and pool merging, finally (iii) ensembles top-ranked models through proxy metric ranking and top-k aggregation in the selected subset, outputting the final anomaly detection result. Extensive experiments on 8 real-world datasets show that our model outperforms all baselines, demonstrating superior adaptability and scalability.",
    "published": "2026-01-05T11:48:21Z",
    "updated": "2026-01-05T11:48:21Z",
    "link": "http://arxiv.org/pdf/2601.02037v1.pdf",
    "category": [
      "cs.LG",
      "cs.DB"
    ],
    "authors": [
      "Wei Hu",
      "Zewei Yu",
      "Jianqiu Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02022v1",
    "title": "Prior Diffusiveness and Regret in the Linear-Gaussian Bandit",
    "summary": "We prove that Thompson sampling exhibits $\\tilde{O}(σd \\sqrt{T} + d r \\sqrt{\\mathrm{Tr}(Σ_0)})$ Bayesian regret in the linear-Gaussian bandit with a $\\mathcal{N}(μ_0, Σ_0)$ prior distribution on the coefficients, where $d$ is the dimension, $T$ is the time horizon, $r$ is the maximum $\\ell_2$ norm of the actions, and $σ^2$ is the noise variance. In contrast to existing regret bounds, this shows that to within logarithmic factors, the prior-dependent ``burn-in'' term $d r \\sqrt{\\mathrm{Tr}(Σ_0)}$ decouples additively from the minimax (long run) regret $σd \\sqrt{T}$. Previous regret bounds exhibit a multiplicative dependence on these terms. We establish these results via a new ``elliptical potential'' lemma, and also provide a lower bound indicating that the burn-in term is unavoidable.",
    "published": "2026-01-05T11:30:08Z",
    "updated": "2026-01-05T11:30:08Z",
    "link": "http://arxiv.org/pdf/2601.02022v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yifan Zhu",
      "John C. Duchi",
      "Benjamin Van Roy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01979v1",
    "title": "SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition",
    "summary": "Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.",
    "published": "2026-01-05T10:33:48Z",
    "updated": "2026-01-05T10:33:48Z",
    "link": "http://arxiv.org/pdf/2601.01979v1.pdf",
    "category": [
      "cs.LG",
      "cs.NE"
    ],
    "authors": [
      "Julie Keisler",
      "Anastase Alexandre Charantonis",
      "Yannig Goude",
      "Boutheina Oueslati",
      "Claire Monteleoni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01970v1",
    "title": "A Multilayered Approach to Classifying Customer Responsiveness and Credit Risk",
    "summary": "This study evaluates the performance of various classifiers in three distinct models: response, risk, and response-risk, concerning credit card mail campaigns and default prediction. In the response model, the Extra Trees classifier demonstrates the highest recall level (79.1%), emphasizing its effectiveness in identifying potential responders to targeted credit card offers. Conversely, in the risk model, the Random Forest classifier exhibits remarkable specificity of 84.1%, crucial for identifying customers least likely to default. Furthermore, in the multi-class response-risk model, the Random Forest classifier achieves the highest accuracy (83.2%), indicating its efficacy in discerning both potential responders to credit card mail campaign and low-risk credit card users. In this study, we optimized various performance metrics to solve a specific credit risk and mail responsiveness business problem.",
    "published": "2026-01-05T10:24:08Z",
    "updated": "2026-01-05T10:24:08Z",
    "link": "http://arxiv.org/pdf/2601.01970v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "authors": [
      "Ayomide Afolabi",
      "Ebere Ogburu",
      "Symon Kimitei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.09353v2",
    "title": "Low-degree lower bounds via almost orthonormal bases",
    "summary": "Low-degree polynomials have emerged as a powerful paradigm for providing evidence of statistical-computational gaps across a variety of high-dimensional statistical models [Wein25]. For detection problems -- where the goal is to test a planted distribution $\\mathbb{P}'$ against a null distribution $\\mathbb{P}$ with independent components -- the standard approach is to bound the advantage using an $\\mathbb{L}^2(\\mathbb{P})$-orthonormal family of polynomials. However, this method breaks down for estimation tasks or more complex testing problems where $\\mathbb{P}$ has some planted structures, so that no simple $\\mathbb{L}^2(\\mathbb{P})$-orthogonal polynomial family is available. To address this challenge, several technical workarounds have been proposed [SW22,SW25], though their implementation can be delicate. In this work, we propose a more direct proof strategy. Focusing on random graph models, we construct a basis of polynomials that is almost orthonormal under $\\mathbb{P}$, in precisely those regimes where statistical-computational gaps arise. This almost orthonormal basis not only yields a direct route to establishing low-degree lower bounds, but also allows us to explicitly identify the polynomials that optimize the low-degree criterion. This, in turn, provides insights into the design of optimal polynomial-time algorithms. We illustrate the effectiveness of our approach by recovering known low-degree lower bounds, and establishing new ones for problems such as hidden subcliques, stochastic block models, and seriation models.",
    "published": "2025-09-11T11:07:36Z",
    "updated": "2026-01-05T10:16:15Z",
    "link": "http://arxiv.org/pdf/2509.09353v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Alexandra Carpentier",
      "Simone Maria Giancola",
      "Christophe Giraud",
      "Nicolas Verzelen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01943v1",
    "title": "SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling",
    "summary": "We present SynRXN, a unified benchmarking framework and open-data resource for computer-aided synthesis planning (CASP). SynRXN decomposes end-to-end synthesis planning into five task families, covering reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. Curated, provenance-tracked reaction corpora are assembled from heterogeneous public sources into a harmonized representation and packaged as versioned datasets for each task family, with explicit source metadata, licence tags, and machine-readable manifests that record checksums, and row counts. For every task, SynRXN provides transparent splitting functions that generate leakage-aware train, validation, and test partitions, together with standardized evaluation workflows and metric suites tailored to classification, regression, and structured prediction settings. For sensitive benchmarking, we combine public training and validation data with held-out gold-standard test sets, and contamination-prone tasks such as reaction rebalancing and atom-to-atom mapping are distributed only as evaluation sets and are explicitly not intended for model training. Scripted build recipes enable bitwise-reproducible regeneration of all corpora across machines and over time, and the entire resource is released under permissive open licences to support reuse and extension. By removing dataset heterogeneity and packaging transparent, reusable evaluation scaffolding, SynRXN enables fair longitudinal comparison of CASP methods, supports rigorous ablations and stress tests along the full reaction-informatics pipeline, and lowers the barrier for practitioners who seek robust and comparable performance estimates for real-world synthesis planning workloads.",
    "published": "2026-01-05T09:49:58Z",
    "updated": "2026-01-05T09:49:58Z",
    "link": "http://arxiv.org/pdf/2601.01943v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Tieu-Long Phan",
      "Nhu-Ngoc Nguyen Song",
      "Peter F. Stadler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01922v1",
    "title": "Efficient temporal prediction of compressible flows in irregular domains using Fourier neural operators",
    "summary": "This paper investigates the temporal evolution of high-speed compressible fluids in irregular flow fields using the Fourier Neural Operator (FNO). We reconstruct the irregular flow field point set into sequential format compatible with FNO input requirements, and then embed temporal bundling technique within a recurrent neural network (RNN) for multi-step prediction. We further employ a composite loss function to balance errors across different physical quantities. Experiments are conducted on three different types of irregular flow fields, including orthogonal and non-orthogonal grid configurations. Then we comprehensively analyze the physical component loss curves, flow field visualizations, and physical profiles. Results demonstrate that our approach significantly surpasses traditional numerical methods in computational efficiency while achieving high accuracy, with maximum relative $L_2$ errors of (0.78, 0.57, 0.35)% for ($p$, $T$, $\\mathbf{u}$) respectively. This verifies that the method can efficiently and accurately simulate the temporal evolution of high-speed compressible flows in irregular domains.",
    "published": "2026-01-05T09:12:35Z",
    "updated": "2026-01-05T09:12:35Z",
    "link": "http://arxiv.org/pdf/2601.01922v1.pdf",
    "category": [
      "physics.flu-dyn",
      "cs.LG"
    ],
    "authors": [
      "Yifan Nie",
      "Qiaoxin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01917v1",
    "title": "Distorted Distributional Policy Evaluation for Offline Reinforcement Learning",
    "summary": "While Distributional Reinforcement Learning (DRL) methods have demonstrated strong performance in online settings, its success in offline scenarios remains limited. We hypothesize that a key limitation of existing offline DRL methods lies in their approach to uniformly underestimate return quantiles. This uniform pessimism can lead to overly conservative value estimates, ultimately hindering generalization and performance. To address this, we introduce a novel concept called quantile distortion, which enables non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data. Our approach is grounded in theoretical analysis and empirically validated, demonstrating improved performance over uniform pessimism.",
    "published": "2026-01-05T09:04:10Z",
    "updated": "2026-01-05T09:04:10Z",
    "link": "http://arxiv.org/pdf/2601.01917v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ryo Iwaki",
      "Takayuki Osogami"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01903v1",
    "title": "TT-FSI: Scalable Faithful Shapley Interactions via Tensor-Train",
    "summary": "The Faithful Shapley Interaction (FSI) index uniquely satisfies the faithfulness axiom among Shapley interaction indices, but computing FSI requires $O(d^\\ell \\cdot 2^d)$ time and existing implementations use $O(4^d)$ memory. We present TT-FSI, which exploits FSI's algebraic structure via Matrix Product Operators (MPO). Our main theoretical contribution is proving that the linear operator $v \\mapsto \\text{FSI}(v)$ admits an MPO representation with TT-rank $O(\\ell d)$, enabling an efficient sweep algorithm with $O(\\ell^2 d^3 \\cdot 2^d)$ time and $O(\\ell d^2)$ core storage an exponential improvement over existing methods. Experiments on six datasets ($d=8$ to $d=20$) demonstrate up to 280$\\times$ speedup over baseline, 85$\\times$ over SHAP-IQ, and 290$\\times$ memory reduction. TT-FSI scales to $d=20$ (1M coalitions) where all competing methods fail.",
    "published": "2026-01-05T08:49:25Z",
    "updated": "2026-01-05T08:49:25Z",
    "link": "http://arxiv.org/pdf/2601.01903v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ungsik Kim",
      "Suwon Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01901v1",
    "title": "FedBiCross: A Bi-Level Optimization Framework to Tackle Non-IID Challenges in Data-Free One-Shot Federated Learning on Medical Data",
    "summary": "Data-free knowledge distillation-based one-shot federated learning (OSFL) trains a model in a single communication round without sharing raw data, making OSFL attractive for privacy-sensitive medical applications. However, existing methods aggregate predictions from all clients to form a global teacher. Under non-IID data, conflicting predictions cancel out during averaging, yielding near-uniform soft labels that provide weak supervision for distillation. We propose FedBiCross, a personalized OSFL framework with three stages: (1) clustering clients by model output similarity to form coherent sub-ensembles, (2) bi-level cross-cluster optimization that learns adaptive weights to selectively leverage beneficial cross-cluster knowledge while suppressing negative transfer, and (3) personalized distillation for client-specific adaptation. Experiments on four medical image datasets demonstrate that FedBiCross consistently outperforms state-of-the-art baselines across different non-IID degrees.",
    "published": "2026-01-05T08:46:11Z",
    "updated": "2026-01-05T08:46:11Z",
    "link": "http://arxiv.org/pdf/2601.01901v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yuexuan Xia",
      "Yinghao Zhang",
      "Yalin Liu",
      "Hong-Ning Dai",
      "Yong Xia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01888v1",
    "title": "SafeLoad: Efficient Admission Control Framework for Identifying Memory-Overloading Queries in Cloud Data Warehouses",
    "summary": "Memory overload is a common form of resource exhaustion in cloud data warehouses. When database queries fail due to memory overload, it not only wastes critical resources such as CPU time but also disrupts the execution of core business processes, as memory-overloading (MO) queries are typically part of complex workflows. If such queries are identified in advance and scheduled to memory-rich serverless clusters, it can prevent resource wastage and query execution failure. Therefore, cloud data warehouses desire an admission control framework with high prediction precision, interpretability, efficiency, and adaptability to effectively identify MO queries. However, existing admission control frameworks primarily focus on scenarios like SLA satisfaction and resource isolation, with limited precision in identifying MO queries. Moreover, there is a lack of publicly available MO-labeled datasets with workloads for training and benchmarking. To tackle these challenges, we propose SafeLoad, the first query admission control framework specifically designed to identify MO queries. Alongside, we release SafeBench, an open-source, industrial-scale benchmark for this task, which includes 150 million real queries. SafeLoad first filters out memory-safe queries using the interpretable discriminative rule. It then applies a hybrid architecture that integrates both a global model and cluster-level models, supplemented by a misprediction correction module to identify MO queries. Additionally, a self-tuning quota management mechanism dynamically adjusts prediction quotas per cluster to improve precision. Experimental results show that SafeLoad achieves state-of-the-art prediction performance with low online and offline time overhead. Specifically, SafeLoad improves precision by up to 66% over the best baseline and reduces wasted CPU time by up to 8.09x compared to scenarios without SafeLoad.",
    "published": "2026-01-05T08:29:51Z",
    "updated": "2026-01-05T08:29:51Z",
    "link": "http://arxiv.org/pdf/2601.01888v1.pdf",
    "category": [
      "cs.DB",
      "cs.LG"
    ],
    "authors": [
      "Yifan Wu",
      "Yuhan Li",
      "Zhenhua Wang",
      "Zhongle Xie",
      "Dingyu Yang",
      "Ke Chen",
      "Lidan Shou",
      "Bo Tang",
      "Liang Lin",
      "Huan Li",
      "Gang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01877v1",
    "title": "Random-Matrix-Induced Simplicity Bias in Over-parameterized Variational Quantum Circuits",
    "summary": "Over-parameterization is commonly used to increase the expressivity of variational quantum circuits (VQCs), yet deeper and more highly parameterized circuits often exhibit poor trainability and limited generalization. In this work, we provide a theoretical explanation for this phenomenon from a function-class perspective. We show that sufficiently expressive, unstructured variational ansatze enter a Haar-like universality class in which both observable expectation values and parameter gradients concentrate exponentially with system size. As a consequence, the hypothesis class induced by such circuits collapses with high probability to a narrow family of near-constant functions, a phenomenon we term simplicity bias, with barren plateaus arising as a consequence rather than the root cause. Using tools from random matrix theory and concentration of measure, we rigorously characterize this universality class and establish uniform hypothesis-class collapse over finite datasets. We further show that this collapse is not unavoidable: tensor-structured VQCs, including tensor-network-based and tensor-hypernetwork parameterizations, lie outside the Haar-like universality class. By restricting the accessible unitary ensemble through bounded tensor rank or bond dimension, these architectures prevent concentration of measure, preserve output variability for local observables, and retain non-degenerate gradient signals even in over-parameterized regimes. Together, our results unify barren plateaus, expressivity limits, and generalization collapse under a single structural mechanism rooted in random-matrix universality, highlighting the central role of architectural inductive bias in variational quantum algorithms.",
    "published": "2026-01-05T08:04:33Z",
    "updated": "2026-01-05T08:04:33Z",
    "link": "http://arxiv.org/pdf/2601.01877v1.pdf",
    "category": [
      "quant-ph",
      "cs.LG",
      "math-ph"
    ],
    "authors": [
      "Jun Qi",
      "Chao-Han Huck Yang",
      "Pin-Yu Chen",
      "Min-Hsiu Hsieh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01860v1",
    "title": "High-Order Epistasis Detection Using Factorization Machine with Quadratic Optimization Annealing and MDR-Based Evaluation",
    "summary": "Detecting high-order epistasis is a fundamental challenge in genetic association studies due to the combinatorial explosion of candidate locus combinations. Although multifactor dimensionality reduction (MDR) is a widely used method for evaluating epistasis, exhaustive MDR-based searches become computationally infeasible as the number of loci or the interaction order increases. In this paper, we define the epistasis detection problem as a black-box optimization problem and solve it with a factorization machine with quadratic optimization annealing (FMQA). We propose an efficient epistasis detection method based on FMQA, in which the classification error rate (CER) computed by MDR is used as a black-box objective function. Experimental evaluations were conducted using simulated case-control datasets with predefined high-order epistasis. The results demonstrate that the proposed method successfully identified ground-truth epistasis across various interaction orders and the numbers of genetic loci within a limited number of iterations. These results indicate that the proposed method is effective and computationally efficient for high-order epistasis detection.",
    "published": "2026-01-05T07:41:34Z",
    "updated": "2026-01-05T07:41:34Z",
    "link": "http://arxiv.org/pdf/2601.01860v1.pdf",
    "category": [
      "cs.LG",
      "quant-ph"
    ],
    "authors": [
      "Shuta Kikuchi",
      "Shu Tanaka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.17341v2",
    "title": "Sharp Structure-Agnostic Lower Bounds for General Linear Functional Estimation",
    "summary": "We establish a general statistical optimality theory for estimation problems where the target parameter is a linear functional of an unknown nuisance component that must be estimated from data. This formulation covers many causal and predictive parameters and has applications to numerous disciplines. We adopt the structure-agnostic framework introduced by \\citet{balakrishnan2023fundamental}, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we first prove the statistical optimality of the celebrated and widely used doubly robust estimators for the Average Treatment Effect (ATE), the most central parameter in causal inference. We then characterize the minimax optimal rate under the general formulation. Notably, we differentiate between two regimes in which double robustness can and cannot be achieved and in which first-order debiasing yields different error rates. Our result implies that first-order debiasing is simultaneously optimal in both regimes. We instantiate our theory by deriving optimal error rates that recover existing results and extend to various settings of interest, including the case when the nuisance is defined by generalized regressions and when covariate shift exists for training and test distribution.",
    "published": "2025-12-19T08:34:05Z",
    "updated": "2026-01-05T07:38:29Z",
    "link": "http://arxiv.org/pdf/2512.17341v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "math.ST",
      "stat.ME"
    ],
    "authors": [
      "Jikai Jin",
      "Vasilis Syrgkanis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10968v2",
    "title": "Blade: A Derivative-free Bayesian Inversion Method using Diffusion Priors",
    "summary": "Derivative-free Bayesian inversion is an important task in many science and engineering applications, particularly when computing the forward model derivative is computationally and practically challenging. In this paper, we introduce Blade, which can produce accurate and well-calibrated posteriors for Bayesian inversion using an ensemble of interacting particles. Blade leverages powerful data-driven priors based on diffusion models, and can handle nonlinear forward models that permit only black-box access (i.e., derivative-free). Theoretically, we establish a non-asymptotic convergence analysis to characterize the effects of forward model and prior estimation errors. Empirically, Blade achieves superior performance compared to existing derivative-free Bayesian inversion methods on various inverse problems, including challenging highly nonlinear fluid dynamics.",
    "published": "2025-10-13T03:19:44Z",
    "updated": "2026-01-05T07:04:41Z",
    "link": "http://arxiv.org/pdf/2510.10968v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Hongkai Zheng",
      "Austin Wang",
      "Zihui Wu",
      "Zhengyu Huang",
      "Ricardo Baptista",
      "Yisong Yue"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01840v1",
    "title": "Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack",
    "summary": "Federated learning has drawn widespread interest from researchers, yet the data heterogeneity across edge clients remains a key challenge, often degrading model performance. Existing methods enhance model compatibility with data heterogeneity by splitting models and knowledge distillation. However, they neglect the insufficient communication bandwidth and computing power on the client, failing to strike an effective balance between addressing data heterogeneity and accommodating limited client resources. To tackle this limitation, we propose a personalized federated learning method based on cosine sparsification parameter packing and dual-weighted aggregation (FedCSPACK), which effectively leverages the limited client resources and reduces the impact of data heterogeneity on model performance. In FedCSPACK, the client packages model parameters and selects the most contributing parameter packages for sharing based on cosine similarity, effectively reducing bandwidth requirements. The client then generates a mask matrix anchored to the shared parameter package to improve the alignment and aggregation efficiency of sparse updates on the server. Furthermore, directional and distribution distance weights are embedded in the mask to implement a weighted-guided aggregation mechanism, enhancing the robustness and generalization performance of the global model. Extensive experiments across four datasets using ten state-of-the-art methods demonstrate that FedCSPACK effectively improves communication and computational efficiency while maintaining high model accuracy.",
    "published": "2026-01-05T07:03:04Z",
    "updated": "2026-01-05T07:03:04Z",
    "link": "http://arxiv.org/pdf/2601.01840v1.pdf",
    "category": [
      "cs.LG",
      "cs.DC"
    ],
    "authors": [
      "Qiantao Yang",
      "Liquan Chen",
      "Mingfu Xue",
      "Songze Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01833v1",
    "title": "FAROS: Robust Federated Learning with Adaptive Scaling against Backdoor Attacks",
    "summary": "Federated Learning (FL) enables multiple clients to collaboratively train a shared model without exposing local data. However, backdoor attacks pose a significant threat to FL. These attacks aim to implant a stealthy trigger into the global model, causing it to mislead on inputs that possess a specific trigger while functioning normally on benign data. Although pre-aggregation detection is a main defense direction, existing state-of-the-art defenses often rely on fixed defense parameters. This reliance makes them vulnerable to single-point-of-failure risks, rendering them less effective against sophisticated attackers. To address these limitations, we propose FAROS, an enhanced FL framework that incorporates Adaptive Differential Scaling (ADS) and Robust Core-set Computing (RCC). The ADS mechanism adjusts the defense's sensitivity dynamically, based on the dispersion of uploaded gradients by clients in each round. This allows it to counter attackers who strategically shift between stealthiness and effectiveness. Furthermore, the RCC effectively mitigates the risk of single-point failure by computing the centroid of a core set comprising clients with the highest confidence. We conducted extensive experiments across various datasets, models, and attack scenarios. The results demonstrate that our method outperforms current defenses in both attack success rate and main task accuracy.",
    "published": "2026-01-05T06:55:35Z",
    "updated": "2026-01-05T06:55:35Z",
    "link": "http://arxiv.org/pdf/2601.01833v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Chenyu Hu",
      "Qiming Hu",
      "Sinan Chen",
      "Nianyu Li",
      "Mingyue Zhang",
      "Jialong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01829v1",
    "title": "RealPDEBench: A Benchmark for Complex Physical Systems with Real-World Data",
    "summary": "Predicting the evolution of complex physical systems remains a central problem in science and engineering. Despite rapid progress in scientific Machine Learning (ML) models, a critical bottleneck is the lack of expensive real-world data, resulting in most current models being trained and validated on simulated data. Beyond limiting the development and evaluation of scientific ML, this gap also hinders research into essential tasks such as sim-to-real transfer. We introduce RealPDEBench, the first benchmark for scientific ML that integrates real-world measurements with paired numerical simulations. RealPDEBench consists of five datasets, three tasks, eight metrics, and ten baselines. We first present five real-world measured datasets with paired simulated datasets across different complex physical systems. We further define three tasks, which allow comparisons between real-world and simulated data, and facilitate the development of methods to bridge the two. Moreover, we design eight evaluation metrics, spanning data-oriented and physics-oriented metrics, and finally benchmark ten representative baselines, including state-of-the-art models, pretrained PDE foundation models, and a traditional method. Experiments reveal significant discrepancies between simulated and real-world data, while showing that pretraining with simulated data consistently improves both accuracy and convergence. In this work, we hope to provide insights from real-world data, advancing scientific ML toward bridging the sim-to-real gap and real-world deployment. Our benchmark, datasets, and instructions are available at https://realpdebench.github.io/.",
    "published": "2026-01-05T06:49:13Z",
    "updated": "2026-01-05T06:49:13Z",
    "link": "http://arxiv.org/pdf/2601.01829v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Peiyan Hu",
      "Haodong Feng",
      "Hongyuan Liu",
      "Tongtong Yan",
      "Wenhao Deng",
      "Tianrun Gao",
      "Rong Zheng",
      "Haoren Zheng",
      "Chenglei Yu",
      "Chuanrui Wang",
      "Kaiwen Li",
      "Zhi-Ming Ma",
      "Dezhi Zhou",
      "Xingcai Lu",
      "Dixia Fan",
      "Tailin Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01793v1",
    "title": "Distributed Federated Learning by Alternating Periods of Training",
    "summary": "Federated learning is a privacy-focused approach towards machine learning where models are trained on client devices with locally available data and aggregated at a central server. However, the dependence on a single central server is challenging in the case of a large number of clients and even poses the risk of a single point of failure. To address these critical limitations of scalability and fault-tolerance, we present a distributed approach to federated learning comprising multiple servers with inter-server communication capabilities. While providing a fully decentralized approach, the designed framework retains the core federated learning structure where each server is associated with a disjoint set of clients with server-client communication capabilities. We propose a novel DFL (Distributed Federated Learning) algorithm which uses alternating periods of local training on the client data followed by global training among servers. We show that the DFL algorithm, under a suitable choice of parameters, ensures that all the servers converge to a common model value within a small tolerance of the ideal model, thus exhibiting effective integration of local and global training models. Finally, we illustrate our theoretical claims through numerical simulations.",
    "published": "2026-01-05T05:06:58Z",
    "updated": "2026-01-05T05:06:58Z",
    "link": "http://arxiv.org/pdf/2601.01793v1.pdf",
    "category": [
      "cs.LG",
      "eess.SY"
    ],
    "authors": [
      "Shamik Bhattacharyya",
      "Rachel Kalpana Kalaimani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.05436v2",
    "title": "pyAMPACT: A Score-Audio Alignment Toolkit for Performance Data Estimation and Multi-modal Processing",
    "summary": "pyAMPACT (Python-based Automatic Music Performance Analysis and Comparison Toolkit) links symbolic and audio music representations to facilitate score-informed estimation of performance data in audio as well as general linking of symbolic and audio music representations with a variety of annotations. pyAMPACT can read a range of symbolic formats and can output note-linked audio descriptors/performance data into MEI-formatted files. The audio analysis uses score alignment to calculate time-frequency regions of importance for each note in the symbolic representation from which to estimate a range of parameters. These include tuning-, dynamics-, and timbre-related performance descriptors, with timing-related information available from the score alignment. Beyond performance data estimation, pyAMPACT also facilitates multi-modal investigations through its infrastructure for linking symbolic representations and annotations to audio.",
    "published": "2024-12-06T21:44:33Z",
    "updated": "2026-01-05T12:59:57Z",
    "link": "http://arxiv.org/pdf/2412.05436v2.pdf",
    "category": [
      "cs.SD",
      "cs.MM",
      "eess.AS"
    ],
    "authors": [
      "Johanna Devaney",
      "Daniel McKemie",
      "Alex Morgan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.05685v6",
    "title": "DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models",
    "summary": "Transfer learning of diffusion models to smaller target domains is challenging, as naively fine-tuning the model often results in poor generalization. Test-time guidance methods help mitigate this by offering controllable improvements in image fidelity through a trade-off with sample diversity. However, this benefit comes at a high computational cost, typically requiring dual forward passes during sampling. We propose the Domain-guided Fine-tuning (DogFit) method, an effective guidance mechanism for diffusion transfer learning that maintains controllability without incurring additional computational overhead. DogFit injects a domain-aware guidance offset into the training loss, effectively internalizing the guided behavior during the fine-tuning process. The domain-aware design is motivated by our observation that during fine-tuning, the unconditional source model offers a stronger marginal estimate than the target model. To support efficient controllable fidelity-diversity trade-offs at inference, we encode the guidance strength value as an additional model input through a lightweight conditioning mechanism. We further investigate the optimal placement and timing of the guidance offset during training and propose two simple scheduling strategies, i.e., late-start and cut-off, which improve generation quality and training stability. Experiments on DiT and SiT backbones across six diverse target domains show that DogFit can outperform prior guidance methods in transfer learning in terms of FID and FDDINOV2 while requiring up to 2x fewer sampling TFLOPS.",
    "published": "2025-08-05T21:33:05Z",
    "updated": "2026-01-05T16:11:10Z",
    "link": "http://arxiv.org/pdf/2508.05685v6.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Yara Bahram",
      "Mohammadhadi Shateri",
      "Eric Granger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.11618v3",
    "title": "On The Topology of Polygonal Meshes",
    "summary": "This paper is an introductory and informal exposition on the topology of polygonal meshes. We begin with a broad overview of topological notions and discuss how homeomorphisms, homotopy, and homology can be used to characterise topology. We move on to define polygonal meshes and make a distinction between intrinsic topology and extrinsic topology which depends on the space in which the mesh is immersed. A distinction is also made between quantitative topological properties and qualitative properties. Next, we outline proofs of the Euler and the Euler-Poincaré formulas. The Betti numbers are then defined in terms of the Euler-Poincaré formula and other mesh statistics rather than as cardinalities of the homology groups which allows us to avoid abstract algebra. Finally, we discuss how it is possible to cut a polygonal mesh such that it becomes a topological disc.",
    "published": "2025-11-05T13:56:24Z",
    "updated": "2026-01-05T13:07:58Z",
    "link": "http://arxiv.org/pdf/2511.11618v3.pdf",
    "category": [
      "math.HO",
      "cs.GR",
      "math.GT"
    ],
    "authors": [
      "Andreas Bærentzen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.12151v2",
    "title": "Robust and Efficient Penetration-Free Elastodynamics without Barriers",
    "summary": "We introduce a barrier-free optimization framework for non-penetration elastodynamic simulation that matches the robustness of Incremental Potential Contact (IPC) while overcoming its two primary efficiency bottlenecks: (1) reliance on logarithmic barrier functions to enforce non-penetration constraints, which leads to ill-conditioned systems and significantly slows down the convergence of iterative linear solvers; and (2) the time-of-impact (TOI) locking issue, which restricts active-set exploration in collision-intensive scenes and requires a large number of Newton iterations. We propose a novel second-order constrained optimization framework featuring a custom augmented Lagrangian solver that avoids TOI locking by immediately incorporating all requisite contact pairs detected via CCD, enabling more efficient active-set exploration and leading to significantly fewer Newton iterations. By adaptively updating Lagrange multipliers rather than increasing penalty stiffness, our method prevents stagnation at zero TOI while maintaining a well-conditioned system. We further introduce a constraint filtering and decay mechanism to keep the active set compact and stable, along with a theoretical justification of our method's finite-step termination and first-order time integration accuracy under a cumulative TOI-based termination criterion. A comprehensive set of experiments demonstrates the efficiency, robustness, and accuracy of our method. With a GPU-optimized simulator design, our method achieves an up to 103x speedup over GIPC on challenging, contact-rich benchmarks - scenarios that were previously tractable only with barrier-based methods. Our code and data will be open-sourced.",
    "published": "2025-12-13T03:17:12Z",
    "updated": "2026-01-05T05:26:44Z",
    "link": "http://arxiv.org/pdf/2512.12151v2.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Juntian Zheng",
      "Zhaofeng Luo",
      "Minchen Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.19567v2",
    "title": "LIMOncello: Iterated Error-State Kalman Filter on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry",
    "summary": "This work introduces LIMOncello, a tightly coupled LiDAR-Inertial Odometry system that models 6-DoF motion on the $\\mathrm{SGal}(3)$ manifold within an iterated error-state Kalman filter backend. Compared to state representations defined on $\\mathrm{SO}(3)\\times\\mathbb{R}^6$, the use of $\\mathrm{SGal}(3)$ provides a coherent and numerically stable discrete-time propagation model that helps limit drift in low-observability conditions.\n  LIMOncello also includes a lightweight incremental i-Octree mapping backend that enables faster updates and substantially lower memory usage than incremental kd-tree style map structures, without relying on locality-restricted search heuristics. Experiments on multiple real-world datasets show that LIMOncello achieves competitive accuracy while improving robustness in geometrically sparse environments. The system maintains real-time performance with stable memory growth and is released as an extensible open-source implementation at https://github.com/CPerezRuiz335/LIMOncello.",
    "published": "2025-12-22T16:50:10Z",
    "updated": "2026-01-05T18:58:56Z",
    "link": "http://arxiv.org/pdf/2512.19567v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Carlos Pérez-Ruiz",
      "Joan Solà"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02295v1",
    "title": "CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding",
    "summary": "Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/",
    "published": "2026-01-05T17:31:01Z",
    "updated": "2026-01-05T17:31:01Z",
    "link": "http://arxiv.org/pdf/2601.02295v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Chenyang Ma",
      "Guangyu Yang",
      "Kai Lu",
      "Shitong Xu",
      "Bill Byrne",
      "Niki Trigoni",
      "Andrew Markham"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.18960v3",
    "title": "FORTE: Tactile Force and Slip Sensing on Compliant Fingers for Delicate Manipulation",
    "summary": "Handling fragile objects remains a major challenge for robotic manipulation. Tactile sensing and soft robotics can improve delicate object handling, but typically involve high integration complexity or slow response times. We address these issues through FORTE, an easy-to-fabricate tactile sensing system. FORTE uses 3D-printed fin-ray grippers with internal air channels to provide low-latency force and slip feedback. This feedback allows us to apply just enough force to grasp objects without damaging them. We accurately estimate grasping forces from 0-8 N with an average error of 0.2 N, and detect slip events within 100 ms of occurring. FORTE can grasp a wide range of slippery, fragile, and deformable objects, including raspberries and potato chips with 92% success and achieves 93% accuracy in detecting slip events. These results highlight FORTE's potential as a robust solution for delicate robotic manipulation. Project page: https://merge-lab.github.io/FORTE/",
    "published": "2025-06-23T17:50:58Z",
    "updated": "2026-01-05T17:18:57Z",
    "link": "http://arxiv.org/pdf/2506.18960v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Siqi Shang",
      "Mingyo Seo",
      "Yuke Zhu",
      "Lillian Chin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.22042v2",
    "title": "Volume-Consistent Kneading-Based Deformation Manufacturing for Material-Efficient Shaping",
    "summary": "Conventional subtractive manufacturing inevitably involves material loss during geometric realization, while additive manufacturing still suffers from limitations in surface quality, process continuity, and productivity when fabricating complex geometries. To address these challenges, this paper proposes a volume-consistent kneading-based forming method for plastic materials, enabling continuous and controllable three-dimensional deformation under mass conservation. An integrated kneading-based manufacturing system is developed, in which geometry-aware kneading command generation, layer-wise kneading execution, and in-process point-cloud scanning are tightly coupled to form a closed-loop workflow of scanning, forming, and feedback compensation. Target geometries are analyzed through layer-wise point-cloud processing and classified into enveloping and non-enveloping types. Accordingly, an Envelope Shaping First strategy and a Similar Gradient Method are adopted to ensure stable material flow and continuous deformation. An RMSE-based compensation scheme is further introduced to correct systematic geometric deviations induced by elastic rebound and material redistribution. Experimental validation on five representative geometries demonstrates high geometric fidelity, with material utilization consistently exceeding 98%. The results indicate that kneading-based forming provides a promising alternative manufacturing paradigm for low-waste, customizable production.",
    "published": "2025-11-27T02:55:00Z",
    "updated": "2026-01-05T16:27:28Z",
    "link": "http://arxiv.org/pdf/2511.22042v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Lei Li",
      "Jiale Gong",
      "Ziyang Li",
      "Hong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.00126v2",
    "title": "Compositional Diffusion with Guided Search for Long-Horizon Planning",
    "summary": "Generative models have emerged as powerful tools for planning, with compositional approaches offering particular promise for modeling long-horizon task distributions by composing together local, modular generative models. This compositional paradigm spans diverse domains, from multi-step manipulation planning to panoramic image synthesis to long video generation. However, compositional generative models face a critical challenge: when local distributions are multimodal, existing composition methods average incompatible modes, producing plans that are neither locally feasible nor globally coherent. We propose Compositional Diffusion with Guided Search (CDGS), which addresses this mode averaging problem by embedding search directly within the diffusion denoising process. Our method explores diverse combinations of local modes through population-based sampling, prunes infeasible candidates using likelihood-based filtering, and enforces global consistency through iterative resampling between overlapping segments. CDGS matches oracle performance on seven robot manipulation tasks, outperforming baselines that lack compositionality or require long-horizon training data. The approach generalizes across domains, enabling coherent text-guided panoramic images and long videos through effective local-to-global message passing. More details: https://cdgsearch.github.io/",
    "published": "2025-12-31T22:03:19Z",
    "updated": "2026-01-05T15:30:54Z",
    "link": "http://arxiv.org/pdf/2601.00126v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Utkarsh A Mishra",
      "David He",
      "Yongxin Chen",
      "Danfei Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.06935v2",
    "title": "Interconnection and Damping Assignment Passivity-Based Control using Sparse Neural ODEs",
    "summary": "Interconnection and Damping Assignment Passivity-Based Control (IDA-PBC) is a nonlinear control technique that assigns a port-Hamiltonian (pH) structure to a controlled system using a state-feedback law. While IDA-PBC has been extensively studied and applied to many systems, its practical implementation often remains confined to academic examples and, almost exclusively, to stabilization tasks. The main limitation of IDA-PBC stems from the complexity of analytically solving a set of partial differential equations (PDEs), referred to as the matching conditions, which enforce the pH structure of the closed-loop system. However, this is extremely challenging, especially for complex physical systems and tasks.\n  In this work, we propose a novel numerical approach for designing IDA-PBC controllers without solving the matching PDEs exactly. We cast the IDA-PBC problem as the learning of a neural ordinary differential equation. In particular, we rely on sparse dictionary learning to parametrize the desired closed-loop system as a sparse linear combination of nonlinear state-dependent functions. Optimization of the controller parameters is achieved by solving a multi-objective optimization problem whose cost function is composed of a generic task-dependent cost and a matching condition-dependent cost. Our numerical results show that the proposed method enables (i) IDA-PBC to be applicable to complex tasks beyond stabilization, such as the discovery of periodic oscillatory behaviors, (ii) the derivation of closed-form expressions of the controlled system, including residual terms in case of approximate matching, and (iii) stability analysis of the learned controller.",
    "published": "2025-12-07T17:33:16Z",
    "updated": "2026-01-05T15:21:27Z",
    "link": "http://arxiv.org/pdf/2512.06935v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Nicolò Botteghi",
      "Owen Brook",
      "Urban Fasel",
      "Federico Califano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02184v1",
    "title": "Differential Barometric Altimetry for Submeter Vertical Localization and Floor Recognition Indoors",
    "summary": "Accurate altitude estimation and reliable floor recognition are critical for mobile robot localization and navigation within complex multi-storey environments. In this paper, we present a robust, low-cost vertical estimation framework leveraging differential barometric sensing integrated within a fully ROS-compliant software package. Our system simultaneously publishes real-time altitude data from both a stationary base station and a mobile sensor, enabling precise and drift-free vertical localization. Empirical evaluations conducted in challenging scenarios -- such as fully enclosed stairwells and elevators, demonstrate that our proposed barometric pipeline achieves sub-meter vertical accuracy (RMSE: 0.29 m) and perfect (100%) floor-level identification. In contrast, our results confirm that standalone height estimates, obtained solely from visual- or LiDAR-based SLAM odometry, are insufficient for reliable vertical localization. The proposed ROS-compatible barometric module thus provides a practical and cost-effective solution for robust vertical awareness in real-world robotic deployments. The implementation of our method is released as open source at https://github.com/witsir/differential-barometric.",
    "published": "2026-01-05T15:03:55Z",
    "updated": "2026-01-05T15:03:55Z",
    "link": "http://arxiv.org/pdf/2601.02184v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yuhang Zhang",
      "Sören Schwertfeger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02082v1",
    "title": "Realistic adversarial scenario generation via human-like pedestrian model for autonomous vehicle control parameter optimisation",
    "summary": "Autonomous vehicles (AVs) are rapidly advancing and are expected to play a central role in future mobility. Ensuring their safe deployment requires reliable interaction with other road users, not least pedestrians. Direct testing on public roads is costly and unsafe for rare but critical interactions, making simulation a practical alternative. Within simulation-based testing, adversarial scenarios are widely used to probe safety limits, but many prioritise difficulty over realism, producing exaggerated behaviours which may result in AV controllers that are overly conservative. We propose an alternative method, instead using a cognitively inspired pedestrian model featuring both inter-individual and intra-individual variability to generate behaviourally plausible adversarial scenarios. We provide a proof of concept demonstration of this method's potential for AV control optimisation, in closed-loop testing and tuning of an AV controller. Our results show that replacing the rule-based CARLA pedestrian with the human-like model yields more realistic gap acceptance patterns and smoother vehicle decelerations. Unsafe interactions occur only for certain pedestrian individuals and conditions, underscoring the importance of human variability in AV testing. Adversarial scenarios generated by this model can be used to optimise AV control towards safer and more efficient behaviour. Overall, this work illustrates how incorporating human-like road user models into simulation-based adversarial testing can enhance the credibility of AV evaluation and provide a practical basis to behaviourally informed controller optimisation.",
    "published": "2026-01-05T13:10:32Z",
    "updated": "2026-01-05T13:10:32Z",
    "link": "http://arxiv.org/pdf/2601.02082v1.pdf",
    "category": [
      "cs.HC",
      "cs.RO"
    ],
    "authors": [
      "Yueyang Wang",
      "Mehmet Dogar",
      "Gustav Markkula"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.02078v1",
    "title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot",
    "summary": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.",
    "published": "2026-01-05T12:59:39Z",
    "updated": "2026-01-05T12:59:39Z",
    "link": "http://arxiv.org/pdf/2601.02078v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Chenghao Yin",
      "Da Huang",
      "Di Yang",
      "Jichao Wang",
      "Nanshu Zhao",
      "Chen Xu",
      "Wenjun Sun",
      "Linjie Hou",
      "Zhijun Li",
      "Junhui Wu",
      "Zhaobo Liu",
      "Zhen Xiao",
      "Sheng Zhang",
      "Lei Bao",
      "Rui Feng",
      "Zhenquan Pang",
      "Jiayu Li",
      "Qian Wang",
      "Maoqing Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01971v1",
    "title": "Deep Robust Koopman Learning from Noisy Data",
    "summary": "Koopman operator theory has emerged as a leading data-driven approach that relies on a judicious choice of observable functions to realize global linear representations of nonlinear systems in the lifted observable space. However, real-world data is often noisy, making it difficult to obtain an accurate and unbiased approximation of the Koopman operator. The Koopman operator generated from noisy datasets is typically corrupted by noise-induced bias that severely degrades prediction and downstream tracking performance. In order to address this drawback, this paper proposes a novel autoencoder-based neural architecture to jointly learn the appropriate lifting functions and the reduced-bias Koopman operator from noisy data. The architecture initially learns the Koopman basis functions that are consistent for both the forward and backward temporal dynamics of the system. Subsequently, by utilizing the learned forward and backward temporal dynamics, the Koopman operator is synthesized with a reduced bias making the method more robust to noise compared to existing techniques. Theoretical analysis is used to demonstrate significant bias reduction in the presence of training noise. Dynamics prediction and tracking control simulations are conducted for multiple serial manipulator arms, including performance comparisons with leading alternative designs, to demonstrate its robustness under various noise levels. Experimental studies with the Franka FR3 7-DoF manipulator arm are further used to demonstrate the effectiveness of the proposed approach in a practical setting.",
    "published": "2026-01-05T10:26:26Z",
    "updated": "2026-01-05T10:26:26Z",
    "link": "http://arxiv.org/pdf/2601.01971v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Aditya Singh",
      "Rajpal Singh",
      "Jishnu Keshavan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01969v1",
    "title": "What you reward is what you learn: Comparing rewards for online speech policy optimization in public HRI",
    "summary": "Designing policies that are both efficient and acceptable for conversational service robots in open and diverse environments is non-trivial. Unlike fixed, hand-tuned parameters, online learning can adapt to non-stationary conditions. In this paper, we study how to adapt a social robot's speech policy in the wild. During a 12-day in-situ deployment with over 1,400 public encounters, we cast online policy optimization as a multi-armed bandit problem and use Thompson sampling to select among six actions defined by speech rate (slow/normal/fast) and verbosity (concise/detailed). We compare three complementary binary rewards--Ru (user rating), Rc (conversation closure), and Rt (>=2 turns)--and show that each induces distinct arm distributions and interaction behaviors. We complement the online results with offline evaluations that analyze contextual factors (e.g., crowd level, group size) using video-annotated data. Taken together, we distill ready-to-use design lessons for deploying online optimization of speech policies in real public HRI settings.",
    "published": "2026-01-05T10:22:58Z",
    "updated": "2026-01-05T10:22:58Z",
    "link": "http://arxiv.org/pdf/2601.01969v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Sichao Song",
      "Yuki Okafuji",
      "Kaito Ariu",
      "Amy Koike"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.12752v3",
    "title": "MOON: Multi-Objective Optimization-Driven Object-Goal Navigation Using a Variable-Horizon Set-Orienteering Planner",
    "summary": "This paper proposes MOON (Multi-Objective Optimization-driven Object-goal Navigation), a novel framework designed for efficient navigation in large-scale, complex indoor environments. While existing methods often rely on local heuristics, they frequently fail to address the strategic trade-offs between competing objectives in vast areas. To overcome this, we formulate the task as a multi-objective optimization problem (MOO) that balances frontier-based exploration with the exploitation of observed landmarks. Our prototype integrates three key pillars: (1) QOM [IROS05] for discriminative landmark encoding; (2) StructNav [RSS23] to enhance the navigation pipeline; and (3) a variable-horizon Set Orienteering Problem (SOP) formulation for globally coherent planning. To further support the framework's scalability, we provide a detailed theoretical foundation for the budget-constrained SOP formulation and the data-driven mode-switching strategy that enables long-horizon resource allocation. Additionally, we introduce a high-speed neural planner that distills the expert solver into a transformer-based model, reducing decision latency by a factor of nearly 10 while maintaining high planning quality.",
    "published": "2025-05-19T06:20:37Z",
    "updated": "2026-01-05T09:57:35Z",
    "link": "http://arxiv.org/pdf/2505.12752v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Daigo Nakajima",
      "Kanji Tanaka",
      "Daiki Iwata",
      "Kouki Terashima"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01948v1",
    "title": "Learning Diffusion Policy from Primitive Skills for Robot Manipulation",
    "summary": "Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies.",
    "published": "2026-01-05T09:56:24Z",
    "updated": "2026-01-05T09:56:24Z",
    "link": "http://arxiv.org/pdf/2601.01948v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Zhihao Gu",
      "Ming Yang",
      "Difan Zou",
      "Dong Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01946v1",
    "title": "From Metrics to Meaning: Insights from a Mixed-Methods Field Experiment on Retail Robot Deployment",
    "summary": "We report a mixed-methods field experiment of a conversational service robot deployed under everyday staffing discretion in a live bedding store. Over 12 days we alternated three conditions--Baseline (no robot), Robot-only, and Robot+Fixture--and video-annotated the service funnel from passersby to purchase. An explanatory sequential design then used six post-experiment staff interviews to interpret the quantitative patterns.\n  Quantitatively, the robot increased stopping per passerby (highest with the fixture), yet clerk-led downstream steps per stopper--clerk approach, store entry, assisted experience, and purchase--decreased. Interviews explained this divergence: clerks avoided interrupting ongoing robot-customer talk, struggled with ambiguous timing amid conversational latency, and noted child-centered attraction that often satisfied curiosity at the doorway. The fixture amplified visibility but also anchored encounters at the threshold, creating a well-defined micro-space where needs could ``close'' without moving inside.\n  We synthesize these strands into an integrative account from the initial show of interest on the part of a customer to their entering the store and derive actionable guidance. The results advance the understanding of interactions between customers, staff members, and the robot and offer practical recommendations for deploying service robots in high-touch retail.",
    "published": "2026-01-05T09:54:33Z",
    "updated": "2026-01-05T09:54:33Z",
    "link": "http://arxiv.org/pdf/2601.01946v1.pdf",
    "category": [
      "cs.RO",
      "cs.HC"
    ],
    "authors": [
      "Sichao Song",
      "Yuki Okafuji",
      "Takuya Iwamoto",
      "Jun Baba",
      "Hiroshi Ishiguro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.02256v2",
    "title": "Multi-Robot Data-Free Continual Communicative Learning (CCL) from Black-Box Visual Place Recognition Models",
    "summary": "In emerging multi-robot societies, heterogeneous agents must continually extract and integrate local knowledge from one another through communication, even when their internal models are completely opaque. Existing approaches to continual or collaborative learning for visual place recognition (VPR) largely assume white-box access to model parameters or shared training datasets, which is unrealistic when robots encounter unknown peers in the wild. This paper introduces \\emph{Continual Communicative Learning (CCL)}, a data-free multi-robot framework in which a traveler robot (student) continually improves its VPR capability by communicating with black-box teacher models via a constrained query--response channel. We repurpose Membership Inference Attacks (MIA), originally developed as privacy attacks on machine learning models, as a constructive communication primitive to reconstruct pseudo-training sets from black-box VPR teachers without accessing their parameters or raw data. To overcome the intrinsic communication bottleneck caused by the low sampling efficiency of black-box MIA, we propose a prior-based query strategy that leverages the student's own VPR prior to focus queries on informative regions of the embedding space, thereby reducing the knowledge transfer (KT) cost. Experimental results on a standard multi-session VPR benchmark demonstrate that the proposed CCL framework yields substantial performance gains for low-performing robots under modest communication budgets, highlighting CCL as a promising building block for scalable and fault-tolerant multi-robot systems.",
    "published": "2025-03-04T04:13:23Z",
    "updated": "2026-01-05T09:31:25Z",
    "link": "http://arxiv.org/pdf/2503.02256v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Kenta Tsukahara",
      "Kanji Tanaka",
      "Daiki Iwata",
      "Jonathan Tay Yu Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.04415v3",
    "title": "RoboBPP: Benchmarking Robotic Online Bin Packing with Physics-based Simulation",
    "summary": "Physical feasibility in 3D bin packing is a key requirement in modern industrial logistics and robotic automation. With the growing adoption of industrial automation, online bin packing has gained increasing attention. However, inconsistencies in problem settings, test datasets, and evaluation metrics have hindered progress in the field, and there is a lack of a comprehensive benchmarking system. Direct testing on real hardware is costly, and building a realistic simulation environment is also challenging. To address these limitations, we introduce RoboBPP, a benchmarking system designed for robotic online bin packing. RoboBPP integrates a physics-based simulator to assess physical feasibility. In our simulation environment, we introduce a robotic arm and boxes at real-world scales to replicate real industrial packing workflows. By simulating conditions that arise in real industrial applications, we ensure that evaluated algorithms are practically deployable. In addition, prior studies often rely on synthetic datasets whose distributions differ from real-world industrial data. To address this issue, we collect three datasets from real industrial workflows, including assembly-line production, logistics packing, and furniture manufacturing. The benchmark comprises three carefully designed test settings and extends existing evaluation metrics with new metrics for structural stability and operational safety. We design a scoring system and derive a range of insights from the evaluation results. RoboBPP is fully open-source and is equipped with visualization tools and an online leaderboard, providing a reproducible and extensible foundation for future research and industrial applications (https://robot-bin-packing-benchmark.github.io).",
    "published": "2025-12-04T03:24:03Z",
    "updated": "2026-01-05T09:02:01Z",
    "link": "http://arxiv.org/pdf/2512.04415v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Zhoufeng Wang",
      "Hang Zhao",
      "Juzhan Xu",
      "Shishun Zhang",
      "Zeyu Xiong",
      "Ruizhen Hu",
      "Chenyang Zhu",
      "Zecui Zeng",
      "Kai Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01872v1",
    "title": "CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios",
    "summary": "Autonomous language-guided navigation in large-scale outdoor environments remains a key challenge in mobile robotics, due to difficulties in semantic reasoning, dynamic conditions, and long-term stability. We propose CausalNav, the first scene graph-based semantic navigation framework tailored for dynamic outdoor environments. We construct a multi-level semantic scene graph using LLMs, referred to as the Embodied Graph, that hierarchically integrates coarse-grained map data with fine-grained object entities. The constructed graph serves as a retrievable knowledge base for Retrieval-Augmented Generation (RAG), enabling semantic navigation and long-range planning under open-vocabulary queries. By fusing real-time perception with offline map data, the Embodied Graph supports robust navigation across varying spatial granularities in dynamic outdoor environments. Dynamic objects are explicitly handled in both the scene graph construction and hierarchical planning modules. The Embodied Graph is continuously updated within a temporal window to reflect environmental changes and support real-time semantic navigation. Extensive experiments in both simulation and real-world settings demonstrate superior robustness and efficiency.",
    "published": "2026-01-05T08:00:34Z",
    "updated": "2026-01-05T08:00:34Z",
    "link": "http://arxiv.org/pdf/2601.01872v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Hongbo Duan",
      "Shangyi Luo",
      "Zhiyuan Deng",
      "Yanbo Chen",
      "Yuanhao Chiang",
      "Yi Liu",
      "Fangming Liu",
      "Xueqian Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.02294v3",
    "title": "RNBF: Real-Time RGB-D Based Neural Barrier Functions for Safe Robotic Navigation",
    "summary": "Autonomous safe navigation in unstructured and novel environments poses significant challenges, especially when environment information can only be provided through low-cost vision sensors. Although safe reactive approaches have been proposed to ensure robot safety in complex environments, many base their theory off the assumption that the robot has prior knowledge on obstacle locations and geometries. In this paper, we present a real-time, vision-based framework that constructs continuous, first-order differentiable Signed Distance Fields (SDFs) of unknown environments entirely online, without any pre-training, and is fully compatible with established SDF-based reactive controllers. To achieve robust performance under practical sensing conditions, our approach explicitly accounts for noise in affordable RGB-D cameras, refining the neural SDF representation online for smoother geometry and stable gradient estimates. We validate the proposed method in simulation and real-world experiments using a Fetch robot. Videos and supplementary material are available at https://satyajeetburla.github.io/rnbf/.",
    "published": "2025-05-04T23:43:44Z",
    "updated": "2026-01-05T05:57:23Z",
    "link": "http://arxiv.org/pdf/2505.02294v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Satyajeet Das",
      "Yifan Xue",
      "Haoming Li",
      "Nadia Figueroa"
    ]
  }
]