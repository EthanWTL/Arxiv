[
  {
    "id": "http://arxiv.org/abs/2509.17262v1",
    "title": "Optimized Learned Image Compression for Facial Expression Recognition",
    "summary": "Efficient data compression is crucial for the storage and transmission of\nvisual data. However, in facial expression recognition (FER) tasks, lossy\ncompression often leads to feature degradation and reduced accuracy. To address\nthese challenges, this study proposes an end-to-end model designed to preserve\ncritical features and enhance both compression and recognition performance. A\ncustom loss function is introduced to optimize the model, tailored to balance\ncompression and recognition performance effectively. This study also examines\nthe influence of varying loss term weights on this balance. Experimental\nresults indicate that fine-tuning the compression model alone improves\nclassification accuracy by 0.71% and compression efficiency by 49.32%, while\njoint optimization achieves significant gains of 4.04% in accuracy and 89.12%\nin efficiency. Moreover, the findings demonstrate that the jointly optimized\nclassification model maintains high accuracy on both compressed and\nuncompressed data, while the compression model reliably preserves image\ndetails, even at high compression rates.",
    "published": "2025-09-21T22:35:19Z",
    "link": "http://arxiv.org/pdf/2509.17262v1.pdf",
    "category": [
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Xiumei Li",
      "Marc Windsheimer",
      "Misha Sadeghi",
      "Björn Eskofier",
      "André Kaup"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17022v1",
    "title": "VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven\n  Module",
    "summary": "Video and audio inpainting for mixed audio-visual content has become a\ncrucial task in multimedia editing recently. However, precisely removing an\nobject and its corresponding audio from a video without affecting the rest of\nthe scene remains a significant challenge. To address this, we propose\nVAInpaint, a novel pipeline that first utilizes a segmentation model to\ngenerate masks and guide a video inpainting model in removing objects. At the\nsame time, an LLM then analyzes the scene globally, while a region-specific\nmodel provides localized descriptions. Both the overall and regional\ndescriptions will be inputted into an LLM, which will refine the content and\nturn it into text queries for our text-driven audio separation model. Our audio\nseparation model is fine-tuned on a customized dataset comprising segmented\nMUSIC instrument images and VGGSound backgrounds to enhance its generalization\nperformance. Experiments show that our method achieves performance comparable\nto current benchmarks in both audio and video inpainting.",
    "published": "2025-09-21T10:31:56Z",
    "link": "http://arxiv.org/pdf/2509.17022v1.pdf",
    "category": [
      "cs.MM",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Kam Man Wu",
      "Zeyue Tian",
      "Liya Ji",
      "Qifeng Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16994v1",
    "title": "Attentive AV-FusionNet: Audio-Visual Quality Prediction with Hybrid\n  Attention",
    "summary": "We introduce a novel deep learning-based audio-visual quality (AVQ)\nprediction model that leverages internal features from state-of-the-art\nunimodal predictors. Unlike prior approaches that rely on simple fusion\nstrategies, our model employs a hybrid representation that combines learned\nGenerative Machine Listener (GML) audio features with hand-crafted Video\nMultimethod Assessment Fusion (VMAF) video features. Attention mechanisms\ncapture cross-modal interactions and intra-modal relationships, yielding\ncontext-aware quality representations. A modality relevance estimator\nquantifies each modality's contribution per content, potentially enabling\nadaptive bitrate allocation. Experiments demonstrate improved AVQ prediction\naccuracy and robustness across diverse content types.",
    "published": "2025-09-21T09:25:09Z",
    "link": "http://arxiv.org/pdf/2509.16994v1.pdf",
    "category": [
      "eess.AS",
      "cs.MM",
      "eess.IV"
    ],
    "authors": [
      "Ina Salaj",
      "Arijit Biswas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16960v1",
    "title": "SemanticGarment: Semantic-Controlled Generation and Editing of 3D\n  Gaussian Garments",
    "summary": "3D digital garment generation and editing play a pivotal role in fashion\ndesign, virtual try-on, and gaming. Traditional methods struggle to meet the\ngrowing demand due to technical complexity and high resource costs.\nLearning-based approaches offer faster, more diverse garment synthesis based on\nspecific requirements and reduce human efforts and time costs. However, they\nstill face challenges such as inconsistent multi-view geometry or textures and\nheavy reliance on detailed garment topology and manual rigging. We propose\nSemanticGarment, a 3D Gaussian-based method that realizes high-fidelity 3D\ngarment generation from text or image prompts and supports semantic-based\ninteractive editing for flexible user customization. To ensure multi-view\nconsistency and garment fitting, we propose to leverage structural human priors\nfor the generative model by introducing a 3D semantic clothing model, which\ninitializes the geometry structure and lays the groundwork for view-consistent\ngarment generation and editing. Without the need to regenerate or rely on\nexisting mesh templates, our approach allows for rapid and diverse\nmodifications to existing Gaussians, either globally or within a local region.\nTo address the artifacts caused by self-occlusion for garment reconstruction\nbased on single image, we develop a self-occlusion optimization strategy to\nmitigate holes and artifacts that arise when directly animating self-occluded\ngarments. Extensive experiments are conducted to demonstrate our superior\nperformance in 3D garment generation and editing.",
    "published": "2025-09-21T07:46:01Z",
    "link": "http://arxiv.org/pdf/2509.16960v1.pdf",
    "category": [
      "cs.GR",
      "cs.MM"
    ],
    "authors": [
      "Ruiyan Wang",
      "Zhengxue Cheng",
      "Zonghao Lin",
      "Jun Ling",
      "Yuzhou Liu",
      "Yanru An",
      "Rong Xie",
      "Li Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16919v1",
    "title": "Bi-modal Prediction and Transformation Coding for Compressing Complex\n  Human Dynamics",
    "summary": "For dynamic human motion sequences, the original KeyNode-Driven codec often\nstruggles to retain compression efficiency when confronted with rapid movements\nor strong non-rigid deformations. This paper proposes a novel Bi-modal coding\nframework that enhances the flexibility of motion representation by integrating\nsemantic segmentation and region-specific transformation modeling. The rigid\ntransformation model (rotation & translation) is extended with a hybrid scheme\nthat selectively applies affine transformations-rotation, translation, scaling,\nand shearing-only to deformation-rich regions (e.g., the torso, where loose\nclothing induces high variability), while retaining rigid models elsewhere. The\naffine model is decomposed into minimal parameter sets for efficient coding and\ncombined through a component selection strategy guided by a Lagrangian\nRate-Distortion optimization. The results show that the Bi-modal method\nachieves more accurate mesh deformation, especially in sequences involving\ncomplex non-rigid motion, without compromising compression efficiency in\nsimpler regions, with an average bit-rate saving of 33.81% compared to the\nbaseline.",
    "published": "2025-09-21T04:57:35Z",
    "link": "http://arxiv.org/pdf/2509.16919v1.pdf",
    "category": [
      "eess.SP",
      "cs.MM"
    ],
    "authors": [
      "Huong Hoang",
      "Keito Suzuki",
      "Truong Nguyen",
      "Pamela Cosman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16869v1",
    "title": "PhysHDR: When Lighting Meets Materials and Scene Geometry in HDR\n  Reconstruction",
    "summary": "Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is a\nfundamental task in many computational vision problems. Numerous data-driven\nmethods have been proposed to address this problem; however, they lack explicit\nmodeling of illumination, lighting, and scene geometry in images. This limits\nthe quality of the reconstructed HDR images. Since lighting and shadows\ninteract differently with different materials, (e.g., specular surfaces such as\nglass and metal, and lambertian or diffuse surfaces such as wood and stone),\nmodeling material-specific properties (e.g., specular and diffuse reflectance)\nhas the potential to improve the quality of HDR image reconstruction. This\npaper presents PhysHDR, a simple yet powerful latent diffusion-based generative\nmodel for HDR image reconstruction. The denoising process is conditioned on\nlighting and depth information and guided by a novel loss to incorporate\nmaterial properties of surfaces in the scene. The experimental results\nestablish the efficacy of PhysHDR in comparison to a number of recent\nstate-of-the-art methods.",
    "published": "2025-09-21T01:41:40Z",
    "link": "http://arxiv.org/pdf/2509.16869v1.pdf",
    "category": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "eess.IV",
      "Artificial intelligence, Computer vision, Machine learning, Deep\n  learning",
      "I.3.3; I.4.5"
    ],
    "authors": [
      "Hrishav Bakul Barua",
      "Kalin Stefanov",
      "Ganesh Krishnasamy",
      "KokSheik Wong",
      "Abhinav Dhall"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17212v1",
    "title": "High Resolution UDF Meshing via Iterative Networks",
    "summary": "Unsigned Distance Fields (UDFs) are a natural implicit representation for\nopen surfaces but, unlike Signed Distance Fields (SDFs), are challenging to\ntriangulate into explicit meshes. This is especially true at high resolutions\nwhere neural UDFs exhibit higher noise levels, which makes it hard to capture\nfine details. Most current techniques perform within single voxels without\nreference to their neighborhood, resulting in missing surface and holes where\nthe UDF is ambiguous or noisy. We show that this can be remedied by performing\nseveral passes and by reasoning on previously extracted surface elements to\nincorporate neighborhood information. Our key contribution is an iterative\nneural network that does this and progressively improves surface recovery\nwithin each voxel by spatially propagating information from increasingly\ndistant neighbors. Unlike single-pass methods, our approach integrates newly\ndetected surfaces, distance values, and gradients across multiple iterations,\neffectively correcting errors and stabilizing extraction in challenging\nregions. Experiments on diverse 3D models demonstrate that our method produces\nsignificantly more accurate and complete meshes than existing approaches,\nparticularly for complex geometries, enabling UDF surface extraction at higher\nresolutions where traditional methods fail.",
    "published": "2025-09-21T19:39:54Z",
    "link": "http://arxiv.org/pdf/2509.17212v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Federico Stella",
      "Nicolas Talabot",
      "Hieu Le",
      "Pascal Fua"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17168v1",
    "title": "Beat on Gaze: Learning Stylized Generation of Gaze and Head Dynamics",
    "summary": "Head and gaze dynamics are crucial in expressive 3D facial animation for\nconveying emotion and intention. However, existing methods frequently address\nfacial components in isolation, overlooking the intricate coordination between\ngaze, head motion, and speech. The scarcity of high-quality gaze-annotated\ndatasets hinders the development of data-driven models capable of capturing\nrealistic, personalized gaze control. To address these challenges, we propose\nStyGazeTalk, an audio-driven method that generates synchronized gaze and head\nmotion styles. We extract speaker-specific motion traits from gaze-head\nsequences with a multi-layer LSTM structure incorporating a style encoder,\nenabling the generation of diverse animation styles. We also introduce a\nhigh-precision multimodal dataset comprising eye-tracked gaze, audio, head\npose, and 3D facial parameters, providing a valuable resource for training and\nevaluating head and gaze control models. Experimental results demonstrate that\nour method generates realistic, temporally coherent, and style-aware head-gaze\nmotions, significantly advancing the state-of-the-art in audio-driven facial\nanimation.",
    "published": "2025-09-21T17:27:57Z",
    "link": "http://arxiv.org/pdf/2509.17168v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Chengwei Shi",
      "Chong Cao",
      "Xin Tong",
      "Xukun Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17287v1",
    "title": "Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain\n  Cross-Correlation",
    "summary": "Visual teach-and-repeat navigation enables robots to autonomously traverse\npreviously demonstrated paths by comparing current sensory input with recorded\ntrajectories. However, conventional frame-based cameras fundamentally limit\nsystem responsiveness: their fixed frame rates (typically 30-60 Hz) create\ninherent latency between environmental changes and control responses. Here we\npresent the first event-camera-based visual teach-and-repeat system. To achieve\nthis, we develop a frequency-domain cross-correlation framework that transforms\nthe event stream matching problem into computationally efficient Fourier space\nmultiplications, capable of exceeding 300Hz processing rates, an order of\nmagnitude faster than frame-based approaches. By exploiting the binary nature\nof event frames and applying image compression techniques, we further enhance\nthe computational speed of the cross-correlation process without sacrificing\nlocalization accuracy. Extensive experiments using a Prophesee EVK4 HD event\ncamera mounted on an AgileX Scout Mini robot demonstrate successful autonomous\nnavigation across 4000+ meters of indoor and outdoor trajectories. Our system\nachieves ATEs below 24 cm while maintaining consistent high-frequency control\nupdates. Our evaluations show that our approach achieves substantially higher\nupdate rates compared to conventional frame-based systems, underscoring the\npractical viability of event-based perception for real-time robotic navigation.",
    "published": "2025-09-21T23:53:31Z",
    "link": "http://arxiv.org/pdf/2509.17287v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Gokul B. Nair",
      "Alejandro Fontan",
      "Michael Milford",
      "Tobias Fischer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17274v1",
    "title": "Learning and Optimization with 3D Orientations",
    "summary": "There exist numerous ways of representing 3D orientations. Each\nrepresentation has both limitations and unique features. Choosing the best\nrepresentation for one task is often a difficult chore, and there exist\nconflicting opinions on which representation is better suited for a set of\nfamily of tasks. Even worse, when dealing with scenarios where we need to learn\nor optimize functions with orientations as inputs and/or outputs, the set of\npossibilities (representations, loss functions, etc.) is even larger and it is\nnot easy to decide what is best for each scenario. In this paper, we attempt to\na) present clearly, concisely and with unified notation all available\nrepresentations, and \"tricks\" related to 3D orientations (including Lie Group\nalgebra), and b) benchmark them in representative scenarios. The first part\nfeels like it is missing from the robotics literature as one has to read many\ndifferent textbooks and papers in order have a concise and clear understanding\nof all possibilities, while the benchmark is necessary in order to come up with\nrecommendations based on empirical evidence. More precisely, we experiment with\nthe following settings that attempt to cover most widely used scenarios in\nrobotics: 1) direct optimization, 2) imitation/supervised learning with a\nneural network controller, 3) reinforcement learning, and 4) trajectory\noptimization using differential dynamic programming. We finally provide\nguidelines depending on the scenario, and make available a reference\nimplementation of all the orientation math described.",
    "published": "2025-09-21T23:11:03Z",
    "link": "http://arxiv.org/pdf/2509.17274v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Alexandros Ntagkas",
      "Constantinos Tsakonas",
      "Chairi Kiourt",
      "Konstantinos Chatzilygeroudis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17244v1",
    "title": "Scalable Multi Agent Diffusion Policies for Coverage Control",
    "summary": "We propose MADP, a novel diffusion-model-based approach for collaboration in\ndecentralized robot swarms. MADP leverages diffusion models to generate samples\nfrom complex and high-dimensional action distributions that capture the\ninterdependencies between agents' actions. Each robot conditions policy\nsampling on a fused representation of its own observations and perceptual\nembeddings received from peers. To evaluate this approach, we task a team of\nholonomic robots piloted by MADP to address coverage control-a canonical multi\nagent navigation problem. The policy is trained via imitation learning from a\nclairvoyant expert on the coverage control problem, with the diffusion process\nparameterized by a spatial transformer architecture to enable decentralized\ninference. We evaluate the system under varying numbers, locations, and\nvariances of importance density functions, capturing the robustness demands of\nreal-world coverage tasks. Experiments demonstrate that our model inherits\nvaluable properties from diffusion models, generalizing across agent densities\nand environments, and consistently outperforming state-of-the-art baselines.",
    "published": "2025-09-21T21:34:56Z",
    "link": "http://arxiv.org/pdf/2509.17244v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Frederic Vatnsdal",
      "Romina Garcia Camargo",
      "Saurav Agarwal",
      "Alejandro Ribeiro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17213v1",
    "title": "Neural Network and ANFIS based auto-adaptive MPC for path tracking in\n  autonomous vehicles",
    "summary": "Self-driving cars operate in constantly changing environments and are exposed\nto a variety of uncertainties and disturbances. These factors render classical\ncontrollers ineffective, especially for lateral control. Therefore, an adaptive\nMPC controller is designed in this paper for the path tracking task, tuned by\nan improved particle swarm optimization algorithm. Online parameter adaptation\nis performed using Neural Networks and ANFIS. The designed controller showed\npromising results compared to standard MPC in triple lane change and trajectory\ntracking scenarios. Code can be found here:\nhttps://github.com/yassinekebbati/NN_MPC-vs-ANFIS_MPC",
    "published": "2025-09-21T19:42:10Z",
    "link": "http://arxiv.org/pdf/2509.17213v1.pdf",
    "category": [
      "cs.RO",
      "math.OC"
    ],
    "authors": [
      "Yassine Kebbati",
      "Naima Ait-Oufroukh",
      "Vincent Vigneron",
      "Dalil Ichala"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17210v1",
    "title": "Combining Performance and Passivity in Linear Control of Series Elastic\n  Actuators",
    "summary": "When humans physically interact with robots, we need the robots to be both\nsafe and performant. Series elastic actuators (SEAs) fundamentally advance\nsafety by introducing compliant actuation. On the one hand, adding a spring\nmitigates the impact of accidental collisions between human and robot; but on\nthe other hand, this spring introduces oscillations and fundamentally decreases\nthe robot's ability to perform precise, accurate motions. So how should we\ntrade off between physical safety and performance? In this paper, we enumerate\nthe different linear control and mechanical configurations for series elastic\nactuators, and explore how each choice affects the rendered compliance,\npassivity, and tracking performance. While prior works focus on load side\ncontrol, we find that actuator side control has significant benefits. Indeed,\nsimple PD controllers on the actuator side allow for a much wider range of\ncontrol gains that maintain safety, and combining these with a damper in the\nelastic transmission yields high performance. Our simulations and real world\nexperiments suggest that, by designing a system with low physical stiffness and\nhigh controller gains, this solution enables accurate performance while also\nensuring user safety during collisions.",
    "published": "2025-09-21T19:36:21Z",
    "link": "http://arxiv.org/pdf/2509.17210v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Shaunak A. Mehta",
      "Dylan P. Losey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17204v2",
    "title": "Ratatouille: Imitation Learning Ingredients for Real-world Social Robot\n  Navigation",
    "summary": "Scaling Reinforcement Learning to in-the-wild social robot navigation is both\ndata-intensive and unsafe, since policies must learn through direct interaction\nand inevitably encounter collisions. Offline Imitation learning (IL) avoids\nthese risks by collecting expert demonstrations safely, training entirely\noffline, and deploying policies zero-shot. However, we find that naively\napplying Behaviour Cloning (BC) to social navigation is insufficient; achieving\nstrong performance requires careful architectural and training choices. We\npresent Ratatouille, a pipeline and model architecture that, without changing\nthe data, reduces collisions per meter by 6 times and improves success rate by\n3 times compared to naive BC. We validate our approach in both simulation and\nthe real world, where we collected over 11 hours of data on a dense university\ncampus. We further demonstrate qualitative results in a public food court. Our\nfindings highlight that thoughtful IL design, rather than additional data, can\nsubstantially improve safety and reliability in real-world social navigation.\nVideo: https://youtu.be/tOdLTXsaYLQ. Code will be released after acceptance.",
    "published": "2025-09-21T19:17:39Z",
    "link": "http://arxiv.org/pdf/2509.17204v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "James R. Han",
      "Mithun Vanniasinghe",
      "Hshmat Sahak",
      "Nicholas Rhinehart",
      "Timothy D. Barfoot"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17198v1",
    "title": "Certifiably Optimal Doppler Positioning using Opportunistic LEO\n  Satellites",
    "summary": "To provide backup and augmentation to global navigation satellite system\n(GNSS), Doppler shift from Low Earth Orbit (LEO) satellites can be employed as\nsignals of opportunity (SOP) for position, navigation and timing (PNT). Since\nthe Doppler positioning problem is non-convex, local searching methods may\nproduce two types of estimates: a global optimum without notice or a local\noptimum given an inexact initial estimate. As exact initialization is\nunavailable in some unknown environments, a guaranteed global optimization\nmethod in no need of initialization becomes necessary. To achieve this goal, we\npropose a certifiably optimal LEO Doppler positioning method by utilizing\nconvex optimization. In this paper, the certifiable positioning method is\nimplemented through a graduated weight approximation (GWA) algorithm and\nsemidefinite programming (SDP) relaxation. To guarantee the optimality, we\nderive the necessary conditions for optimality in ideal noiseless cases and\nsufficient noise bounds conditions in noisy cases. Simulation and real tests\nare conducted to evaluate the effectiveness and robustness of the proposed\nmethod. Specially, the real test using Iridium-NEXT satellites shows that the\nproposed method estimates an certifiably optimal solution with an 3D\npositioning error of 140 m without initial estimates while Gauss-Newton and\nDog-Leg are trapped in local optima when the initial point is equal or larger\nthan 1000 km away from the ground truth. Moreover, the certifiable estimation\ncan also be used as initialization in local searching methods to lower down the\n3D positioning error to 130 m.",
    "published": "2025-09-21T18:55:07Z",
    "link": "http://arxiv.org/pdf/2509.17198v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Baoshan Song",
      "Weisong Wen",
      "Qi Zhang",
      "Bing Xu",
      "Li-Ta Hsu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17195v1",
    "title": "MAST: Multi-Agent Spatial Transformer for Learning to Collaborate",
    "summary": "This article presents a novel multi-agent spatial transformer (MAST) for\nlearning communication policies in large-scale decentralized and collaborative\nmulti-robot systems (DC-MRS). Challenges in collaboration in DC-MRS arise from:\n(i) partial observable states as robots make only localized perception, (ii)\nlimited communication range with no central server, and (iii) independent\nexecution of actions. The robots need to optimize a common task-specific\nobjective, which, under the restricted setting, must be done using a\ncommunication policy that exhibits the desired collaborative behavior. The\nproposed MAST is a decentralized transformer architecture that learns\ncommunication policies to compute abstract information to be shared with other\nagents and processes the received information with the robot's own\nobservations. The MAST extends the standard transformer with new positional\nencoding strategies and attention operations that employ windowing to limit the\nreceptive field for MRS. These are designed for local computation,\nshift-equivariance, and permutation equivariance, making it a promising\napproach for DC-MRS. We demonstrate the efficacy of MAST on decentralized\nassignment and navigation (DAN) and decentralized coverage control. Efficiently\ntrained using imitation learning in a centralized setting, the decentralized\nMAST policy is robust to communication delays, scales to large teams, and\nperforms better than the baselines and other learning-based approaches.",
    "published": "2025-09-21T18:48:59Z",
    "link": "http://arxiv.org/pdf/2509.17195v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Damian Owerko",
      "Frederic Vatnsdal",
      "Saurav Agarwal",
      "Vijay Kumar",
      "Alejandro Ribeiro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17141v1",
    "title": "History-Aware Visuomotor Policy Learning via Point Tracking",
    "summary": "Many manipulation tasks require memory beyond the current observation, yet\nmost visuomotor policies rely on the Markov assumption and thus struggle with\nrepeated states or long-horizon dependencies. Existing methods attempt to\nextend observation horizons but remain insufficient for diverse memory\nrequirements. To this end, we propose an object-centric history representation\nbased on point tracking, which abstracts past observations into a compact and\nstructured form that retains only essential task-relevant information. Tracked\npoints are encoded and aggregated at the object level, yielding a compact\nhistory representation that can be seamlessly integrated into various\nvisuomotor policies. Our design provides full history-awareness with high\ncomputational efficiency, leading to improved overall task performance and\ndecision accuracy. Through extensive evaluations on diverse manipulation tasks,\nwe show that our method addresses multiple facets of memory requirements - such\nas task stage identification, spatial memorization, and action counting, as\nwell as longer-term demands like continuous and pre-loaded memory - and\nconsistently outperforms both Markovian baselines and prior history-based\napproaches. Project website: http://tonyfang.net/history",
    "published": "2025-09-21T16:11:04Z",
    "link": "http://arxiv.org/pdf/2509.17141v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jingjing Chen",
      "Hongjie Fang",
      "Chenxi Wang",
      "Shiquan Wang",
      "Cewu Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17131v1",
    "title": "Delay compensation of multi-input distinct delay nonlinear systems via\n  neural operators",
    "summary": "In this work, we present the first stability results for approximate\npredictors in multi-input non-linear systems with distinct actuation delays. We\nshow that if the predictor approximation satisfies a uniform (in time) error\nbound, semi-global practical stability is correspondingly achieved. For such\napproximators, the required uniform error bound depends on the desired region\nof attraction and the number of control inputs in the system. The result is\nachieved through transforming the delay into a transport PDE and conducting\nanalysis on the coupled ODE-PDE cascade. To highlight the viability of such\nerror bounds, we demonstrate our results on a class of approximators - neural\noperators - showcasing sufficiency for satisfying such a universal bound both\ntheoretically and in simulation on a mobile robot experiment.",
    "published": "2025-09-21T15:46:46Z",
    "link": "http://arxiv.org/pdf/2509.17131v1.pdf",
    "category": [
      "eess.SY",
      "cs.LG",
      "cs.RO",
      "cs.SY",
      "math.DS"
    ],
    "authors": [
      "Filip Bajraktari",
      "Luke Bhan",
      "Miroslav Krstic",
      "Yuanyuan Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17125v1",
    "title": "Imagine2Act: Leveraging Object-Action Motion Consistency from Imagined\n  Goals for Robotic Manipulation",
    "summary": "Relational object rearrangement (ROR) tasks (e.g., insert flower to vase)\nrequire a robot to manipulate objects with precise semantic and geometric\nreasoning. Existing approaches either rely on pre-collected demonstrations that\nstruggle to capture complex geometric constraints or generate goal-state\nobservations to capture semantic and geometric knowledge, but fail to\nexplicitly couple object transformation with action prediction, resulting in\nerrors due to generative noise. To address these limitations, we propose\nImagine2Act, a 3D imitation-learning framework that incorporates semantic and\ngeometric constraints of objects into policy learning to tackle high-precision\nmanipulation tasks. We first generate imagined goal images conditioned on\nlanguage instructions and reconstruct corresponding 3D point clouds to provide\nrobust semantic and geometric priors. These imagined goal point clouds serve as\nadditional inputs to the policy model, while an object-action consistency\nstrategy with soft pose supervision explicitly aligns predicted end-effector\nmotion with generated object transformation. This design enables Imagine2Act to\nreason about semantic and geometric relationships between objects and predict\naccurate actions across diverse tasks. Experiments in both simulation and the\nreal world demonstrate that Imagine2Act outperforms previous state-of-the-art\npolicies. More visualizations can be found at\nhttps://sites.google.com/view/imagine2act.",
    "published": "2025-09-21T15:35:28Z",
    "link": "http://arxiv.org/pdf/2509.17125v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Liang Heng",
      "Jiadong Xu",
      "Yiwen Wang",
      "Xiaoqi Li",
      "Muhe Cai",
      "Yan Shen",
      "Juan Zhu",
      "Guanghui Ren",
      "Hao Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17107v1",
    "title": "CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic\n  Mixture-of-Experts for Collaborative Perception",
    "summary": "Collaborative perception aims to extend sensing coverage and improve\nperception accuracy by sharing information among multiple agents. However, due\nto differences in viewpoints and spatial positions, agents often acquire\nheterogeneous observations. Existing intermediate fusion methods primarily\nfocus on aligning similar features, often overlooking the perceptual diversity\namong agents. To address this limitation, we propose CoBEVMoE, a novel\ncollaborative perception framework that operates in the Bird's Eye View (BEV)\nspace and incorporates a Dynamic Mixture-of-Experts (DMoE) architecture. In\nDMoE, each expert is dynamically generated based on the input features of a\nspecific agent, enabling it to extract distinctive and reliable cues while\nattending to shared semantics. This design allows the fusion process to\nexplicitly model both feature similarity and heterogeneity across agents.\nFurthermore, we introduce a Dynamic Expert Metric Loss (DEML) to enhance\ninter-expert diversity and improve the discriminability of the fused\nrepresentation. Extensive experiments on the OPV2V and DAIR-V2X-C datasets\ndemonstrate that CoBEVMoE achieves state-of-the-art performance. Specifically,\nit improves the IoU for Camera-based BEV segmentation by +1.5% on OPV2V and the\nAP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C, verifying the\neffectiveness of expert-based heterogeneous feature modeling in multi-agent\ncollaborative perception. The source code will be made publicly available at\nhttps://github.com/godk0509/CoBEVMoE.",
    "published": "2025-09-21T14:56:05Z",
    "link": "http://arxiv.org/pdf/2509.17107v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "authors": [
      "Lingzhao Kong",
      "Jiacheng Lin",
      "Siyu Li",
      "Kai Luo",
      "Zhiyong Li",
      "Kailun Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17080v1",
    "title": "CoPlanner: An Interactive Motion Planner with Contingency-Aware\n  Diffusion for Autonomous Driving",
    "summary": "Accurate trajectory prediction and motion planning are crucial for autonomous\ndriving systems to navigate safely in complex, interactive environments\ncharacterized by multimodal uncertainties. However, current\ngeneration-then-evaluation frameworks typically construct multiple plausible\ntrajectory hypotheses but ultimately adopt a single most likely outcome,\nleading to overconfident decisions and a lack of fallback strategies that are\nvital for safety in rare but critical scenarios. Moreover, the usual decoupling\nof prediction and planning modules could result in socially inconsistent or\nunrealistic joint trajectories, especially in highly interactive traffic. To\naddress these challenges, we propose a contingency-aware diffusion planner\n(CoPlanner), a unified framework that jointly models multi-agent interactive\ntrajectory generation and contingency-aware motion planning. Specifically, the\npivot-conditioned diffusion mechanism anchors trajectory sampling on a\nvalidated, shared short-term segment to preserve temporal consistency, while\nstochastically generating diverse long-horizon branches that capture multimodal\nmotion evolutions. In parallel, we design a contingency-aware multi-scenario\nscoring strategy that evaluates candidate ego trajectories across multiple\nplausible long-horizon evolution scenarios, balancing safety, progress, and\ncomfort. This integrated design preserves feasible fallback options and\nenhances robustness under uncertainty, leading to more realistic\ninteraction-aware planning. Extensive closed-loop experiments on the nuPlan\nbenchmark demonstrate that CoPlanner consistently surpasses state-of-the-art\nmethods on both Val14 and Test14 datasets, achieving significant improvements\nin safety and comfort under both reactive and non-reactive settings. Code and\nmodel will be made publicly available upon acceptance.",
    "published": "2025-09-21T13:54:26Z",
    "link": "http://arxiv.org/pdf/2509.17080v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Ruiguo Zhong",
      "Ruoyu Yao",
      "Pei Liu",
      "Xiaolong Chen",
      "Rui Yang",
      "Jun Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17057v1",
    "title": "RoboManipBaselines: A Unified Framework for Imitation Learning in\n  Robotic Manipulation across Real and Simulated Environments",
    "summary": "RoboManipBaselines is an open framework for robot imitation learning that\nunifies data collection, training, and evaluation across simulation and real\nrobots. We introduce it as a platform enabling systematic benchmarking of\ndiverse tasks, robots, and multimodal policies with emphasis on integration,\ngenerality, extensibility, and reproducibility.",
    "published": "2025-09-21T12:30:16Z",
    "link": "http://arxiv.org/pdf/2509.17057v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Masaki Murooka",
      "Tomohiro Motoda",
      "Ryoichi Nakajo",
      "Hanbit Oh",
      "Koshi Makihara",
      "Keisuke Shirai",
      "Yukiyasu Domae"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17053v1",
    "title": "FILIC: Dual-Loop Force-Guided Imitation Learning with Impedance Torque\n  Control for Contact-Rich Manipulation Tasks",
    "summary": "Contact-rich manipulation is crucial for robots to perform tasks requiring\nprecise force control, such as insertion, assembly, and in-hand manipulation.\nHowever, most imitation learning (IL) policies remain position-centric and lack\nexplicit force awareness, and adding force/torque sensors to collaborative\nrobot arms is often costly and requires additional hardware design. To overcome\nthese issues, we propose FILIC, a Force-guided Imitation Learning framework\nwith impedance torque control. FILIC integrates a Transformer-based IL policy\nwith an impedance controller in a dual-loop structure, enabling compliant\nforce-informed, force-executed manipulation. For robots without force/torque\nsensors, we introduce a cost-effective end-effector force estimator using joint\ntorque measurements through analytical Jacobian-based inversion while\ncompensating with model-predicted torques from a digital twin. We also design\ncomplementary force feedback frameworks via handheld haptics and VR\nvisualization to improve demonstration quality. Experiments show that FILIC\nsignificantly outperforms vision-only and joint-torque-based methods, achieving\nsafer, more compliant, and adaptable contact-rich manipulation. Our code can be\nfound in https://github.com/TATP-233/FILIC.",
    "published": "2025-09-21T12:17:20Z",
    "link": "http://arxiv.org/pdf/2509.17053v1.pdf",
    "category": [
      "cs.RO",
      "68T40, 93C85",
      "I.2.9"
    ],
    "authors": [
      "Haizhou Ge",
      "Yufei Jia",
      "Zheng Li",
      "Yue Li",
      "Zhixing Chen",
      "Ruqi Huang",
      "Guyue Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17042v1",
    "title": "Orchestrate, Generate, Reflect: A VLM-Based Multi-Agent Collaboration\n  Framework for Automated Driving Policy Learning",
    "summary": "The advancement of foundation models fosters new initiatives for policy\nlearning in achieving safe and efficient autonomous driving. However, a\ncritical bottleneck lies in the manual engineering of reward functions and\ntraining curricula for complex and dynamic driving tasks, which is a\nlabor-intensive and time-consuming process. To address this problem, we propose\nOGR (Orchestrate, Generate, Reflect), a novel automated driving policy learning\nframework that leverages vision-language model (VLM)-based multi-agent\ncollaboration. Our framework capitalizes on advanced reasoning and multimodal\nunderstanding capabilities of VLMs to construct a hierarchical agent system.\nSpecifically, a centralized orchestrator plans high-level training objectives,\nwhile a generation module employs a two-step analyze-then-generate process for\nefficient generation of reward-curriculum pairs. A reflection module then\nfacilitates iterative optimization based on the online evaluation. Furthermore,\na dedicated memory module endows the VLM agents with the capabilities of\nlong-term memory. To enhance robustness and diversity of the generation\nprocess, we introduce a parallel generation scheme and a human-in-the-loop\ntechnique for augmentation of the reward observation space. Through efficient\nmulti-agent cooperation and leveraging rich multimodal information, OGR enables\nthe online evolution of reinforcement learning policies to acquire\ninteraction-aware driving skills. Extensive experiments in the CARLA simulator\ndemonstrate the superior performance, robust generalizability across distinct\nurban scenarios, and strong compatibility with various RL algorithms. Further\nreal-world experiments highlight the practical viability and effectiveness of\nour framework. The source code will be available upon acceptance of the paper.",
    "published": "2025-09-21T11:43:25Z",
    "link": "http://arxiv.org/pdf/2509.17042v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Zengqi Peng",
      "Yusen Xie",
      "Yubin Wang",
      "Rui Yang",
      "Qifeng Chen",
      "Jun Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17010v1",
    "title": "Generalized Momenta-Based Koopman Formalism for Robust Control of\n  Euler-Lagrangian Systems",
    "summary": "This paper presents a novel Koopman operator formulation for Euler Lagrangian\ndynamics that employs an implicit generalized momentum-based state space\nrepresentation, which decouples a known linear actuation channel from state\ndependent dynamics and makes the system more amenable to linear Koopman\nmodeling. By leveraging this structural separation, the proposed formulation\nonly requires to learn the unactuated dynamics rather than the complete\nactuation dependent system, thereby significantly reducing the number of\nlearnable parameters, improving data efficiency, and lowering overall model\ncomplexity. In contrast, conventional explicit formulations inherently couple\ninputs with the state dependent terms in a nonlinear manner, making them more\nsuitable for bilinear Koopman models, which are more computationally expensive\nto train and deploy. Notably, the proposed scheme enables the formulation of\nlinear models that achieve superior prediction performance compared to\nconventional bilinear models while remaining substantially more efficient. To\nrealize this framework, we present two neural network architectures that\nconstruct Koopman embeddings from actuated or unactuated data, enabling\nflexible and efficient modeling across different tasks. Robustness is ensured\nthrough the integration of a linear Generalized Extended State Observer (GESO),\nwhich explicitly estimates disturbances and compensates for them in real time.\nThe combined momentum-based Koopman and GESO framework is validated through\ncomprehensive trajectory tracking simulations and experiments on robotic\nmanipulators, demonstrating superior accuracy, robustness, and learning\nefficiency relative to state of the art alternatives.",
    "published": "2025-09-21T09:56:13Z",
    "link": "http://arxiv.org/pdf/2509.17010v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Rajpal Singh",
      "Aditya Singh",
      "Chidre Shravista Kashyap",
      "Jishnu Keshavan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16998v1",
    "title": "IDfRA: Self-Verification for Iterative Design in Robotic Assembly",
    "summary": "As robots proliferate in manufacturing, Design for Robotic Assembly (DfRA),\nwhich is designing products for efficient automated assembly, is increasingly\nimportant. Traditional approaches to DfRA rely on manual planning, which is\ntime-consuming, expensive and potentially impractical for complex objects.\nLarge language models (LLM) have exhibited proficiency in semantic\ninterpretation and robotic task planning, stimulating interest in their\napplication to the automation of DfRA. But existing methodologies typically\nrely on heuristic strategies and rigid, hard-coded physics simulators that may\nnot translate into real-world assembly contexts. In this work, we present\nIterative Design for Robotic Assembly (IDfRA), a framework using iterative\ncycles of planning, execution, verification, and re-planning, each informed by\nself-assessment, to progressively enhance design quality within a fixed yet\ninitially under-specified environment, thereby eliminating the physics\nsimulation with the real world itself. The framework accepts as input a target\nstructure together with a partial environmental representation. Through\nsuccessive refinement, it converges toward solutions that reconcile semantic\nfidelity with physical feasibility. Empirical evaluation demonstrates that\nIDfRA attains 73.3\\% top-1 accuracy in semantic recognisability, surpassing the\nbaseline on this metric. Moreover, the resulting assembly plans exhibit robust\nphysical feasibility, achieving an overall 86.9\\% construction success rate,\nwith design quality improving across iterations, albeit not always\nmonotonically. Pairwise human evaluation further corroborates the advantages of\nIDfRA relative to alternative approaches. By integrating self-verification with\ncontext-aware adaptation, the framework evidences strong potential for\ndeployment in unstructured manufacturing scenarios.",
    "published": "2025-09-21T09:31:49Z",
    "link": "http://arxiv.org/pdf/2509.16998v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Nishka Khendry",
      "Christos Margadji",
      "Sebastian W. Pattinson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16966v1",
    "title": "Geometric Interpolation of Rigid Body Motions",
    "summary": "The problem of interpolating a rigid body motion is to find a spatial\ntrajectory between a prescribed initial and terminal pose. Two variants of this\ninterpolation problem are addressed. The first is to find a solution that\nsatisfies initial conditions on the k-1 derivatives of the rigid body twist.\nThis is called the kth-order initial value trajectory interpolation problem\n(k-IV-TIP). The second is to find a solution that satisfies conditions on the\nrigid body twist and its k-1 derivatives at the initial and terminal pose. This\nis called the kth-order boundary value trajectory interpolation problem\n(k-BV-TIP). Solutions to the k-IV-TIP for k=1,...,4, i.e. the initial twist and\nup to the 4th time derivative are prescribed. Further, a solution to the\n1-IV-TBP is presented, i.e. the initial and terminal twist are prescribed. The\nlatter is a novel cubic interpolation between two spatial configurations with\ngiven initial and terminal twist. This interpolation is automatically identical\nto the minimum acceleration curve when the twists are set to zero. The general\napproach to derive higher-order solutions is presented. Numerical results are\nshown for two examples.",
    "published": "2025-09-21T07:55:35Z",
    "link": "http://arxiv.org/pdf/2509.16966v1.pdf",
    "category": [
      "cs.RO",
      "cs.NA",
      "math.DG",
      "math.GR",
      "math.NA",
      "math.OC"
    ],
    "authors": [
      "Andreas Mueller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16963v1",
    "title": "A Reliable Robot Motion Planner in Complex Real-world Environments via\n  Action Imagination",
    "summary": "Humans and animals can make real-time adjustments to movements by imagining\ntheir action outcomes to prevent unanticipated or even catastrophic motion\nfailures in unknown unstructured environments. Action imagination, as a refined\nsensorimotor strategy, leverages perception-action loops to handle physical\ninteraction-induced uncertainties in perception and system modeling within\ncomplex systems. Inspired by the action-awareness capability of animal\nintelligence, this study proposes an imagination-inspired motion planner (I-MP)\nframework that specifically enhances robots' action reliability by imagining\nplausible spatial states for approaching. After topologizing the workspace,\nI-MP build perception-action loop enabling robots autonomously build contact\nmodels. Leveraging fixed-point theory and Hausdorff distance, the planner\ncomputes convergent spatial states under interaction characteristics and\nmission constraints. By homogenously representing multi-dimensional\nenvironmental characteristics through work, the robot can approach the imagined\nspatial states via real-time computation of energy gradients. Consequently,\nexperimental results demonstrate the practicality and robustness of I-MP in\ncomplex cluttered environments.",
    "published": "2025-09-21T07:50:40Z",
    "link": "http://arxiv.org/pdf/2509.16963v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Chengjin Wang",
      "Yanmin Zhou",
      "Zhipeng Wang",
      "Zheng Yan",
      "Feng Luan",
      "Shuo Jiang",
      "Runjie Shen",
      "Hongrui Sang",
      "Bin He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16920v1",
    "title": "SwarmChat: An LLM-Based, Context-Aware Multimodal Interaction System for\n  Robotic Swarms",
    "summary": "Traditional Human-Swarm Interaction (HSI) methods often lack intuitive\nreal-time adaptive interfaces, making decision making slower and increasing\ncognitive load while limiting command flexibility. To solve this, we present\nSwarmChat, a context-aware, multimodal interaction system powered by Large\nLanguage Models (LLMs). SwarmChat enables users to issue natural language\ncommands to robotic swarms using multiple modalities, such as text, voice, or\nteleoperation. The system integrates four LLM-based modules: Context Generator,\nIntent Recognition, Task Planner, and Modality Selector. These modules\ncollaboratively generate context from keywords, detect user intent, adapt\ncommands based on real-time robot state, and suggest optimal communication\nmodalities. Its three-layer architecture offers a dynamic interface with both\nfixed and customizable command options, supporting flexible control while\noptimizing cognitive effort. The preliminary evaluation also shows that the\nSwarmChat's LLM modules provide accurate context interpretation, relevant\nintent recognition, and effective command delivery, achieving high user\nsatisfaction.",
    "published": "2025-09-21T04:59:21Z",
    "link": "http://arxiv.org/pdf/2509.16920v1.pdf",
    "category": [
      "cs.RO",
      "cs.HC"
    ],
    "authors": [
      "Ettilla Mohiuddin Eumi",
      "Hussein Abbass",
      "Nadine Marcus"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16909v1",
    "title": "SLAM-Former: Putting SLAM into One Transformer",
    "summary": "We present SLAM-Former, a novel neural approach that integrates full SLAM\ncapabilities into a single transformer. Similar to traditional SLAM systems,\nSLAM-Former comprises both a frontend and a backend that operate in tandem. The\nfrontend processes sequential monocular images in real-time for incremental\nmapping and tracking, while the backend performs global refinement to ensure a\ngeometrically consistent result. This alternating execution allows the frontend\nand backend to mutually promote one another, enhancing overall system\nperformance. Comprehensive experimental results demonstrate that SLAM-Former\nachieves superior or highly competitive performance compared to\nstate-of-the-art dense SLAM methods.",
    "published": "2025-09-21T04:04:47Z",
    "link": "http://arxiv.org/pdf/2509.16909v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Yijun Yuan",
      "Zhuoguang Chen",
      "Kenan Li",
      "Weibang Wang",
      "Hang Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16894v1",
    "title": "End2Race: Efficient End-to-End Imitation Learning for Real-Time F1Tenth\n  Racing",
    "summary": "F1Tenth is a widely adopted reduced-scale platform for developing and testing\nautonomous racing algorithms, hosting annual competitions worldwide. With high\noperating speeds, dynamic environments, and head-to-head interactions,\nautonomous racing requires algorithms that diverge from those in classical\nautonomous driving. Training such algorithms is particularly challenging: the\nneed for rapid decision-making at high speeds severely limits model capacity.\nTo address this, we propose End2Race, a novel end-to-end imitation learning\nalgorithm designed for head-to-head autonomous racing. End2Race leverages a\nGated Recurrent Unit (GRU) architecture to capture continuous temporal\ndependencies, enabling both short-term responsiveness and long-term strategic\nplanning. We also adopt a sigmoid-based normalization function that transforms\nraw LiDAR scans into spatial pressure tokens, facilitating effective model\ntraining and convergence. The algorithm is extremely efficient, achieving an\ninference time of less than 0.5 milliseconds on a consumer-class GPU.\nExperiments in the F1Tenth simulator demonstrate that End2Race achieves a 94.2%\nsafety rate across 2,400 overtaking scenarios, each with an 8-second time\nlimit, and successfully completes overtakes in 59.2% of cases. This surpasses\nprevious methods and establishes ours as a leading solution for the F1Tenth\nracing testbed. Code is available at\nhttps://github.com/michigan-traffic-lab/End2Race.",
    "published": "2025-09-21T03:08:51Z",
    "link": "http://arxiv.org/pdf/2509.16894v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Zhijie Qiao",
      "Haowei Li",
      "Zhong Cao",
      "Henry X. Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16871v1",
    "title": "HOGraspFlow: Exploring Vision-based Generative Grasp Synthesis with\n  Hand-Object Priors and Taxonomy Awareness",
    "summary": "We propose Hand-Object\\emph{(HO)GraspFlow}, an affordance-centric approach\nthat retargets a single RGB with hand-object interaction (HOI) into multi-modal\nexecutable parallel jaw grasps without explicit geometric priors on target\nobjects. Building on foundation models for hand reconstruction and vision, we\nsynthesize $SE(3)$ grasp poses with denoising flow matching (FM), conditioned\non the following three complementary cues: RGB foundation features as visual\nsemantics, HOI contact reconstruction, and taxonomy-aware prior on grasp types.\nOur approach demonstrates high fidelity in grasp synthesis without explicit HOI\ncontact input or object geometry, while maintaining strong contact and taxonomy\nrecognition. Another controlled comparison shows that \\emph{HOGraspFlow}\nconsistently outperforms diffusion-based variants (\\emph{HOGraspDiff}),\nachieving high distributional fidelity and more stable optimization in $SE(3)$.\nWe demonstrate a reliable, object-agnostic grasp synthesis from human\ndemonstrations in real-world experiments, where an average success rate of over\n$83\\%$ is achieved.",
    "published": "2025-09-21T01:46:59Z",
    "link": "http://arxiv.org/pdf/2509.16871v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yitian Shi",
      "Zicheng Guo",
      "Rosa Wolf",
      "Edgar Welte",
      "Rania Rayyes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16858v1",
    "title": "Benchmarking Offline Reinforcement Learning for Emotion-Adaptive Social\n  Robotics",
    "summary": "The ability of social robots to respond to human emotions is crucial for\nbuilding trust and acceptance in human-robot collaborative environments.\nHowever, developing such capabilities through online reinforcement learning is\nsometimes impractical due to the prohibitive cost of data collection and the\nrisk of generating unsafe behaviors. In this paper, we study the use of offline\nreinforcement learning as a practical and efficient alternative. This technique\nuses pre-collected data to enable emotion-adaptive social robots. We present a\nsystem architecture that integrates multimodal sensing and recognition,\ndecision-making, and adaptive responses. Using a limited dataset from a\nhuman-robot game-playing scenario, we establish a benchmark for comparing\noffline reinforcement learning algorithms that do not require an online\nenvironment. Our results show that BCQ and CQL are more robust to data\nsparsity, achieving higher state-action values compared to NFQ, DQN, and DDQN.\nThis work establishes a foundation for benchmarking offline RL in\nemotion-adaptive robotics and informs future deployment in real-world HRI. Our\nfindings provide empirical insight into the performance of offline\nreinforcement learning algorithms in data-constrained HRI. This work\nestablishes a foundation for benchmarking offline RL in emotion-adaptive\nrobotics and informs its future deployment in real-world HRI, such as in\nconversational agents, educational partners, and personal assistants, require\nreliable emotional responsiveness.",
    "published": "2025-09-21T01:02:35Z",
    "link": "http://arxiv.org/pdf/2509.16858v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Soon Jynn Chu",
      "Raju Gottumukkala",
      "Alan Barhorst"
    ]
  }
]